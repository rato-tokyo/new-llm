GPU: NVIDIA L4 (23.8GB)

======================================================================
EXPERIMENT RUNNER
======================================================================
Start time: 2025-11-28 11:57:29

Experiments:
  A: Deep & Narrow (6å±¤, 768dim) (context_dim=768)
  B: Shallow & Wide (3å±¤, 1536dim) (context_dim=1536)

Sample sizes: [500, 1000]
num_input_tokens: 1
Embedding freeze: True
Total experiments: 4

======================================================================
Loading data...
======================================================================

  Loading 1050 samples from UltraChat...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 168kB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.39MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 350MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 5.73MB/s]
config.json: 100% 665/665 [00:00<00:00, 5.76MB/s]
README.md: 3.90kB [00:00, 18.0MB/s]
data/train_sft-00000-of-00003-a3ecf92756(â€¦): 100% 244M/244M [00:02<00:00, 106MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(â€¦): 100% 244M/244M [00:01<00:00, 198MB/s] 
data/train_sft-00002-of-00003-ee46ed25cf(â€¦): 100% 244M/244M [00:01<00:00, 186MB/s]
data/test_sft-00000-of-00001-f7dfac4afe5(â€¦): 100% 81.2M/81.2M [00:01<00:00, 79.3MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(â€¦): 100% 244M/244M [00:01<00:00, 167MB/s]
data/train_gen-00001-of-00003-d6a0402e41(â€¦): 100% 243M/243M [00:01<00:00, 206MB/s]
data/train_gen-00002-of-00003-c0db75b92a(â€¦): 100% 243M/243M [00:01<00:00, 162MB/s]
data/test_gen-00000-of-00001-3d4cd830914(â€¦): 100% 80.4M/80.4M [00:00<00:00, 98.7MB/s]
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 49898.20 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 53781.90 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 71534.80 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 70918.96 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1204 > 1024). Running this sequence through the model will result in indexing errors
  Total tokens: 192,670
  Samples loaded: 1050

======================================================================
Running experiments...
======================================================================

[1/4] Running expA_s500...

  Experiment A: Deep & Narrow (6å±¤, 768dim)
    num_layers=6, context_dim=768
    num_input_tokens=1
    num_samples=500
    ContextBlock params: 15.39M
    TokenBlock params: 8.57M
    Phase 1 å­¦ç¿’å¯¾è±¡: 15.39M
    Phase 2 å­¦ç¿’å¯¾è±¡: 47.16M
2025-11-28 11:58:16.411393: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-28 11:58:16.428937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764331096.450418    1209 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764331096.456901    1209 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764331096.473470    1209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764331096.473505    1209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764331096.473508    1209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764331096.473510    1209 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-28 11:58:16.478662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
model.safetensors: 100% 548M/548M [00:00<00:00, 575MB/s]
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using Eæ¡ˆ architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
âœ“ Weight Tying enabled: token_output shares weights with token_embedding
  â†’ Saved ~38.60M parameters
    Train tokens: 86,925
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 86,925
  Max iterations: 40
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 15,394,304
Iteration 1/40: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ« [87.05s]
Iteration 2/40: åŽæŸ=100.0% | Loss=-0.140731 | CVFP=0.014798 | Div=-0.296259 [2.44s]
Iteration 3/40: åŽæŸ=0.0% | Loss=0.268931 | CVFP=0.806735 | Div=-0.268872 [1.60s]
Iteration 4/40: åŽæŸ=0.0% | Loss=0.302203 | CVFP=0.889173 | Div=-0.284766 [1.56s]
Iteration 5/40: åŽæŸ=0.0% | Loss=0.332703 | CVFP=0.956273 | Div=-0.290868 [1.57s]
Iteration 6/40: åŽæŸ=0.0% | Loss=0.322895 | CVFP=0.938629 | Div=-0.292839 [1.59s]
Iteration 7/40: åŽæŸ=0.0% | Loss=0.328811 | CVFP=0.953465 | Div=-0.295843 [1.57s]
Iteration 8/40: åŽæŸ=0.0% | Loss=0.282203 | CVFP=0.857925 | Div=-0.293520 [1.58s]
Iteration 9/40: åŽæŸ=0.0% | Loss=0.225518 | CVFP=0.747100 | Div=-0.296064 [1.58s]
Iteration 10/40: åŽæŸ=0.0% | Loss=0.146648 | CVFP=0.592009 | Div=-0.298712 [1.59s]
Iteration 11/40: åŽæŸ=0.0% | Loss=0.076102 | CVFP=0.453184 | Div=-0.300980 [1.59s]
Iteration 12/40: åŽæŸ=1.0% | Loss=0.019064 | CVFP=0.340549 | Div=-0.302420 [1.62s]
Iteration 13/40: åŽæŸ=7.6% | Loss=-0.031319 | CVFP=0.239189 | Div=-0.301827 [1.64s]
Iteration 14/40: åŽæŸ=8.3% | Loss=-0.060517 | CVFP=0.182637 | Div=-0.303671 [1.57s]
Iteration 15/40: åŽæŸ=21.3% | Loss=-0.067234 | CVFP=0.169312 | Div=-0.303779 [1.57s]
Iteration 16/40: åŽæŸ=26.6% | Loss=-0.054865 | CVFP=0.194607 | Div=-0.304338 [1.62s]
Iteration 17/40: åŽæŸ=21.1% | Loss=-0.014777 | CVFP=0.274886 | Div=-0.304439 [1.62s]
Iteration 18/40: åŽæŸ=11.7% | Loss=0.002610 | CVFP=0.310154 | Div=-0.304934 [1.63s]
Iteration 19/40: åŽæŸ=16.9% | Loss=0.001145 | CVFP=0.307028 | Div=-0.304738 [1.63s]
Iteration 20/40: åŽæŸ=31.8% | Loss=-0.030070 | CVFP=0.244537 | Div=-0.304678 [1.63s]
Iteration 21/40: åŽæŸ=37.7% | Loss=-0.069969 | CVFP=0.165082 | Div=-0.305020 [1.62s]
Iteration 22/40: åŽæŸ=53.2% | Loss=-0.099492 | CVFP=0.105704 | Div=-0.304688 [1.58s]
Iteration 23/40: åŽæŸ=69.7% | Loss=-0.123609 | CVFP=0.057479 | Div=-0.304698 [1.58s]
Iteration 24/40: åŽæŸ=82.3% | Loss=-0.137632 | CVFP=0.029706 | Div=-0.304971 [1.58s]
Iteration 25/40: åŽæŸ=90.9% | Loss=-0.145198 | CVFP=0.014736 | Div=-0.305132 [1.58s]
Iteration 26/40: åŽæŸ=96.0% | Loss=-0.149238 | CVFP=0.006985 | Div=-0.305461 [1.63s]
  â†’ Early stopping (min_iterations=5 satisfied)

Phase 1 å®Œäº†: 83455/86925 ãƒˆãƒ¼ã‚¯ãƒ³åŽæŸ


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 1

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000046
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000046
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000003

Verdict: âœ… CONVERGED: Loss is stable - model has converged

    Phase 1 done: 2.1min
      Train ER: 76.1/768 (9.9%)
      Val ER: 71.9/768 (9.4%)
    Phase 2 starting (freeze_embedding=True)...
âœ“ ContextBlock frozen
âœ“ Embedding frozen (Weight Tying: Output Head also frozen)
  â†’ Only TokenBlock will be trained
âœ“ Training TokenBlock only: 8,567,808/62,559,488 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (Eæ¡ˆ - ãƒ¬ã‚¤ãƒ¤ãƒ¼å¯¾å¿œç‰ˆ)
======================================================================

Training tokens: 86,925
Validation tokens: 9,050
Epochs: 10
Batch size: 1024
Learning rate: 0.001
Gradient clip: 1.0
Early stopping patience: 1

Architecture: Eæ¡ˆ (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1ã§å­¦ç¿’æ¸ˆã¿)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - Eæ¡ˆ: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§

Epoch 1/10 [377.1s]:
  Train Loss: 8.0866 | Train PPL: 3250.59
  Val Loss: 7.3033 | Val PPL: 1485.24 | Val Acc: 10.07%
  âœ“ New best validation loss: 7.3033

Epoch 2/10 [378.3s]:
  Train Loss: 6.8916 | Train PPL: 983.98
  Val Loss: 6.8356 | Val PPL: 930.39 | Val Acc: 12.13%
  âœ“ New best validation loss: 6.8356

Epoch 3/10 [378.3s]:
  Train Loss: 6.5117 | Train PPL: 672.95
  Val Loss: 6.6614 | Val PPL: 781.61 | Val Acc: 13.17%
  âœ“ New best validation loss: 6.6614

Epoch 4/10 [378.2s]:
  Train Loss: 6.2384 | Train PPL: 512.02
  Val Loss: 6.5680 | Val PPL: 711.97 | Val Acc: 14.31%
  âœ“ New best validation loss: 6.5680

Epoch 5/10 [376.1s]:
  Train Loss: 5.9874 | Train PPL: 398.38
  Val Loss: 6.5519 | Val PPL: 700.60 | Val Acc: 14.47%
  âœ“ New best validation loss: 6.5519

Epoch 6/10 [375.9s]:
  Train Loss: 5.7656 | Train PPL: 319.13
  Val Loss: 6.5883 | Val PPL: 726.56 | Val Acc: 14.78%
  âš ï¸ No improvement (1/1)

â›” Early stopping triggered at epoch 6
   Val loss hasn't improved for 1 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 5
Best validation loss: 6.5519
Best validation PPL: 700.60
Best validation accuracy: 14.47%
Early stopped at epoch: 6

    Phase 2 done: 37.7min
    RESULT: Val PPL=700.60, Val Acc=14.47%

[2/4] Running expA_s1000...

  Experiment A: Deep & Narrow (6å±¤, 768dim)
    num_layers=6, context_dim=768
    num_input_tokens=1
    num_samples=1000
    ContextBlock params: 15.39M
    TokenBlock params: 8.57M
    Phase 1 å­¦ç¿’å¯¾è±¡: 15.39M
    Phase 2 å­¦ç¿’å¯¾è±¡: 47.16M
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using Eæ¡ˆ architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
âœ“ Weight Tying enabled: token_output shares weights with token_embedding
  â†’ Saved ~38.60M parameters
    Train tokens: 183,620
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 183,620
  Max iterations: 40
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 15,394,304
Iteration 1/40: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ« [185.66s]
Iteration 2/40: åŽæŸ=100.0% | Loss=-0.140011 | CVFP=0.014787 | Div=-0.294808 [3.56s]
Iteration 3/40: åŽæŸ=0.0% | Loss=0.268586 | CVFP=0.804896 | Div=-0.267723 [3.42s]
Iteration 4/40: åŽæŸ=0.0% | Loss=0.302685 | CVFP=0.888677 | Div=-0.283306 [3.42s]
Iteration 5/40: åŽæŸ=0.0% | Loss=0.334282 | CVFP=0.957955 | Div=-0.289391 [3.42s]
Iteration 6/40: åŽæŸ=0.0% | Loss=0.325658 | CVFP=0.942621 | Div=-0.291306 [3.41s]
Iteration 7/40: åŽæŸ=0.0% | Loss=0.331867 | CVFP=0.957999 | Div=-0.294266 [3.44s]
Iteration 8/40: åŽæŸ=0.0% | Loss=0.283961 | CVFP=0.860007 | Div=-0.292086 [3.41s]
Iteration 9/40: åŽæŸ=0.0% | Loss=0.228049 | CVFP=0.750482 | Div=-0.294383 [3.42s]
Iteration 10/40: åŽæŸ=0.0% | Loss=0.147079 | CVFP=0.591119 | Div=-0.296962 [3.41s]
Iteration 11/40: åŽæŸ=0.0% | Loss=0.079308 | CVFP=0.458037 | Div=-0.299422 [3.41s]
Iteration 12/40: åŽæŸ=1.0% | Loss=0.025377 | CVFP=0.351630 | Div=-0.300875 [3.42s]
Iteration 13/40: åŽæŸ=7.3% | Loss=-0.024131 | CVFP=0.252311 | Div=-0.300573 [3.42s]
Iteration 14/40: åŽæŸ=8.2% | Loss=-0.060772 | CVFP=0.180616 | Div=-0.302160 [3.42s]
Iteration 15/40: åŽæŸ=24.6% | Loss=-0.076976 | CVFP=0.148450 | Div=-0.302402 [3.43s]
Iteration 16/40: åŽæŸ=30.0% | Loss=-0.064896 | CVFP=0.173101 | Div=-0.302893 [3.42s]
Iteration 17/40: åŽæŸ=19.9% | Loss=-0.020172 | CVFP=0.262606 | Div=-0.302950 [3.42s]
Iteration 18/40: åŽæŸ=10.5% | Loss=0.006557 | CVFP=0.316514 | Div=-0.303401 [3.45s]
Iteration 19/40: åŽæŸ=15.7% | Loss=0.015315 | CVFP=0.333880 | Div=-0.303250 [3.41s]
Iteration 20/40: åŽæŸ=28.4% | Loss=-0.017959 | CVFP=0.267224 | Div=-0.303141 [3.41s]
Iteration 21/40: åŽæŸ=39.4% | Loss=-0.058953 | CVFP=0.185595 | Div=-0.303501 [3.41s]
Iteration 22/40: åŽæŸ=50.4% | Loss=-0.092227 | CVFP=0.118736 | Div=-0.303191 [3.40s]
Iteration 23/40: åŽæŸ=68.5% | Loss=-0.118700 | CVFP=0.065781 | Div=-0.303181 [3.42s]
Iteration 24/40: åŽæŸ=80.4% | Loss=-0.134122 | CVFP=0.035204 | Div=-0.303448 [3.43s]
Iteration 25/40: åŽæŸ=89.6% | Loss=-0.142805 | CVFP=0.017986 | Div=-0.303595 [3.43s]
Iteration 26/40: åŽæŸ=95.1% | Loss=-0.147551 | CVFP=0.008777 | Div=-0.303880 [3.41s]
  â†’ Early stopping (min_iterations=5 satisfied)

Phase 1 å®Œäº†: 174606/183620 ãƒˆãƒ¼ã‚¯ãƒ³åŽæŸ


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 1

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000047
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000047
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000003

Verdict: âœ… CONVERGED: Loss is stable - model has converged

    Phase 1 done: 4.5min
      Train ER: 73.9/768 (9.6%)
      Val ER: 69.6/768 (9.1%)
    Phase 2 starting (freeze_embedding=True)...
âœ“ ContextBlock frozen
âœ“ Embedding frozen (Weight Tying: Output Head also frozen)
  â†’ Only TokenBlock will be trained
âœ“ Training TokenBlock only: 8,567,808/62,559,488 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (Eæ¡ˆ - ãƒ¬ã‚¤ãƒ¤ãƒ¼å¯¾å¿œç‰ˆ)
======================================================================

Training tokens: 183,620
Validation tokens: 9,050
Epochs: 10
Batch size: 1024
Learning rate: 0.001
Gradient clip: 1.0
Early stopping patience: 1

Architecture: Eæ¡ˆ (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1ã§å­¦ç¿’æ¸ˆã¿)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - Eæ¡ˆ: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§

Epoch 1/10 [784.5s]:
  Train Loss: 7.5038 | Train PPL: 1814.97
  Val Loss: 6.8226 | Val PPL: 918.36 | Val Acc: 12.50%
  âœ“ New best validation loss: 6.8226

Epoch 2/10 [784.1s]:
  Train Loss: 6.5421 | Train PPL: 693.73
  Val Loss: 6.5039 | Val PPL: 667.72 | Val Acc: 13.89%
  âœ“ New best validation loss: 6.5039

Epoch 3/10 [784.8s]:
  Train Loss: 6.2614 | Train PPL: 523.94
  Val Loss: 6.3788 | Val PPL: 589.25 | Val Acc: 14.94%
  âœ“ New best validation loss: 6.3788

Epoch 4/10 [785.0s]:
  Train Loss: 6.0567 | Train PPL: 426.95
  Val Loss: 6.3332 | Val PPL: 562.93 | Val Acc: 15.21%
  âœ“ New best validation loss: 6.3332

Epoch 5/10 [785.3s]:
  Train Loss: 5.8765 | Train PPL: 356.56
  Val Loss: 6.3462 | Val PPL: 570.31 | Val Acc: 15.29%
  âš ï¸ No improvement (1/1)

â›” Early stopping triggered at epoch 5
   Val loss hasn't improved for 1 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 4
Best validation loss: 6.3332
Best validation PPL: 562.93
Best validation accuracy: 15.21%
Early stopped at epoch: 5

    Phase 2 done: 1.1h
    RESULT: Val PPL=562.93, Val Acc=15.21%

[3/4] Running expB_s500...

  Experiment B: Shallow & Wide (3å±¤, 1536dim)
    num_layers=3, context_dim=1536
    num_input_tokens=1
    num_samples=500
    ContextBlock params: 20.73M
    TokenBlock params: 5.91M
    Phase 1 å­¦ç¿’å¯¾è±¡: 20.73M
    Phase 2 å­¦ç¿’å¯¾è±¡: 44.50M
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using Eæ¡ˆ architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 1
âœ“ Weight Tying enabled: token_output shares weights with token_embedding
  â†’ Saved ~38.60M parameters
    Train tokens: 86,925
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 86,925
  Max iterations: 40
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 20,730,880
Iteration 1/40: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ« [44.72s]
Iteration 2/40: åŽæŸ=100.0% | Loss=-0.204383 | CVFP=0.012092 | Div=-0.420858 [2.33s]
Iteration 3/40: åŽæŸ=0.0% | Loss=0.032880 | CVFP=0.474732 | Div=-0.408971 [2.33s]
Iteration 4/40: åŽæŸ=0.0% | Loss=0.036504 | CVFP=0.485560 | Div=-0.412553 [2.32s]
Iteration 5/40: åŽæŸ=0.0% | Loss=0.069707 | CVFP=0.556546 | Div=-0.417132 [2.31s]
Iteration 6/40: åŽæŸ=0.0% | Loss=0.105525 | CVFP=0.630887 | Div=-0.419838 [2.31s]
Iteration 7/40: åŽæŸ=0.0% | Loss=0.112454 | CVFP=0.647420 | Div=-0.422512 [2.32s]
Iteration 8/40: åŽæŸ=0.0% | Loss=0.118527 | CVFP=0.660918 | Div=-0.423863 [2.31s]
Iteration 9/40: åŽæŸ=0.0% | Loss=0.113027 | CVFP=0.651497 | Div=-0.425443 [2.31s]
Iteration 10/40: åŽæŸ=0.0% | Loss=0.106821 | CVFP=0.638370 | Div=-0.424729 [2.32s]
Iteration 11/40: åŽæŸ=0.0% | Loss=0.097502 | CVFP=0.620291 | Div=-0.425287 [2.31s]
Iteration 12/40: åŽæŸ=0.0% | Loss=0.077351 | CVFP=0.581261 | Div=-0.426559 [2.33s]
Iteration 13/40: åŽæŸ=0.0% | Loss=0.047325 | CVFP=0.522471 | Div=-0.427820 [2.31s]
Iteration 14/40: åŽæŸ=0.0% | Loss=0.002904 | CVFP=0.435817 | Div=-0.430008 [2.32s]
Iteration 15/40: åŽæŸ=0.1% | Loss=-0.045617 | CVFP=0.341958 | Div=-0.433192 [2.31s]
Iteration 16/40: åŽæŸ=1.2% | Loss=-0.093029 | CVFP=0.248880 | Div=-0.434938 [2.31s]
Iteration 17/40: åŽæŸ=6.8% | Loss=-0.128019 | CVFP=0.180484 | Div=-0.436522 [2.32s]
Iteration 18/40: åŽæŸ=17.9% | Loss=-0.149766 | CVFP=0.137850 | Div=-0.437381 [2.31s]
Iteration 19/40: åŽæŸ=30.6% | Loss=-0.162696 | CVFP=0.112792 | Div=-0.438184 [2.31s]
Iteration 20/40: åŽæŸ=40.1% | Loss=-0.172268 | CVFP=0.094228 | Div=-0.438764 [2.31s]
Iteration 21/40: åŽæŸ=47.9% | Loss=-0.181613 | CVFP=0.076384 | Div=-0.439611 [2.30s]
Iteration 22/40: åŽæŸ=57.9% | Loss=-0.190198 | CVFP=0.060034 | Div=-0.440430 [2.33s]
Iteration 23/40: åŽæŸ=67.9% | Loss=-0.197541 | CVFP=0.046045 | Div=-0.441128 [2.32s]
Iteration 24/40: åŽæŸ=75.6% | Loss=-0.203012 | CVFP=0.035830 | Div=-0.441854 [2.33s]
Iteration 25/40: åŽæŸ=81.2% | Loss=-0.207322 | CVFP=0.028043 | Div=-0.442687 [2.32s]
Iteration 26/40: åŽæŸ=85.4% | Loss=-0.210378 | CVFP=0.022727 | Div=-0.443482 [2.31s]
Iteration 27/40: åŽæŸ=88.2% | Loss=-0.212608 | CVFP=0.019032 | Div=-0.444248 [2.33s]
Iteration 28/40: åŽæŸ=90.0% | Loss=-0.214394 | CVFP=0.016327 | Div=-0.445116 [2.31s]
Iteration 29/40: åŽæŸ=91.6% | Loss=-0.215846 | CVFP=0.014318 | Div=-0.446009 [2.31s]
Iteration 30/40: åŽæŸ=92.6% | Loss=-0.217018 | CVFP=0.012825 | Div=-0.446862 [2.32s]
Iteration 31/40: åŽæŸ=93.6% | Loss=-0.218152 | CVFP=0.011458 | Div=-0.447762 [2.31s]
Iteration 32/40: åŽæŸ=94.3% | Loss=-0.219159 | CVFP=0.010379 | Div=-0.448698 [2.34s]
Iteration 33/40: åŽæŸ=94.9% | Loss=-0.220031 | CVFP=0.009549 | Div=-0.449612 [2.31s]
Iteration 34/40: åŽæŸ=95.5% | Loss=-0.220875 | CVFP=0.008786 | Div=-0.450535 [2.32s]
  â†’ Early stopping (min_iterations=5 satisfied)

Phase 1 å®Œäº†: 82973/86925 ãƒˆãƒ¼ã‚¯ãƒ³åŽæŸ


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 1

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000513
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000513
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000034

Verdict: âœ… CONVERGED: Loss is stable - model has converged

    Phase 1 done: 2.0min
      Train ER: 232.3/1536 (15.1%)
      Val ER: 213.5/1536 (13.9%)
    Phase 2 starting (freeze_embedding=True)...
âœ“ ContextBlock frozen
âœ“ Embedding frozen (Weight Tying: Output Head also frozen)
  â†’ Only TokenBlock will be trained
âœ“ Training TokenBlock only: 5,906,688/65,234,944 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (Eæ¡ˆ - ãƒ¬ã‚¤ãƒ¤ãƒ¼å¯¾å¿œç‰ˆ)
======================================================================

Training tokens: 86,925
Validation tokens: 9,050
Epochs: 10
Batch size: 1024
Learning rate: 0.001
Gradient clip: 1.0
Early stopping patience: 1

Architecture: Eæ¡ˆ (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1ã§å­¦ç¿’æ¸ˆã¿)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - Eæ¡ˆ: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§

Epoch 1/10 [220.4s]:
  Train Loss: 8.1515 | Train PPL: 3468.64
  Val Loss: 7.2825 | Val PPL: 1454.58 | Val Acc: 10.22%
  âœ“ New best validation loss: 7.2825

Epoch 2/10 [219.9s]:
  Train Loss: 6.8402 | Train PPL: 934.64
  Val Loss: 6.8103 | Val PPL: 907.18 | Val Acc: 12.48%
  âœ“ New best validation loss: 6.8103

Epoch 3/10 [221.3s]:
  Train Loss: 6.4024 | Train PPL: 603.32
  Val Loss: 6.6219 | Val PPL: 751.35 | Val Acc: 13.60%
  âœ“ New best validation loss: 6.6219

Epoch 4/10 [220.7s]:
  Train Loss: 6.0679 | Train PPL: 431.78
  Val Loss: 6.5629 | Val PPL: 708.31 | Val Acc: 13.74%
  âœ“ New best validation loss: 6.5629

Epoch 5/10 [219.9s]:
  Train Loss: 5.7748 | Train PPL: 322.08
  Val Loss: 6.5725 | Val PPL: 715.13 | Val Acc: 14.29%
  âš ï¸ No improvement (1/1)

â›” Early stopping triggered at epoch 5
   Val loss hasn't improved for 1 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 4
Best validation loss: 6.5629
Best validation PPL: 708.31
Best validation accuracy: 13.74%
Early stopped at epoch: 5

    Phase 2 done: 18.4min
    RESULT: Val PPL=708.31, Val Acc=13.74%

[4/4] Running expB_s1000...

  Experiment B: Shallow & Wide (3å±¤, 1536dim)
    num_layers=3, context_dim=1536
    num_input_tokens=1
    num_samples=1000
    ContextBlock params: 20.73M
    TokenBlock params: 5.91M
    Phase 1 å­¦ç¿’å¯¾è±¡: 20.73M
    Phase 2 å­¦ç¿’å¯¾è±¡: 44.50M
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using Eæ¡ˆ architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 1
âœ“ Weight Tying enabled: token_output shares weights with token_embedding
  â†’ Saved ~38.60M parameters
    Train tokens: 183,620
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 183,620
  Max iterations: 40
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 20,730,880
Iteration 1/40: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ« [95.37s]
Iteration 2/40: åŽæŸ=100.0% | Loss=-0.203315 | CVFP=0.012080 | Div=-0.418711 [4.87s]
Iteration 3/40: åŽæŸ=0.0% | Loss=0.034366 | CVFP=0.475600 | Div=-0.406869 [4.91s]
Iteration 4/40: åŽæŸ=0.0% | Loss=0.038034 | CVFP=0.486464 | Div=-0.410396 [4.92s]
Iteration 5/40: åŽæŸ=0.0% | Loss=0.071344 | CVFP=0.557673 | Div=-0.414985 [4.91s]
Iteration 6/40: åŽæŸ=0.0% | Loss=0.107136 | CVFP=0.631963 | Div=-0.417691 [4.91s]
Iteration 7/40: åŽæŸ=0.0% | Loss=0.114229 | CVFP=0.648804 | Div=-0.420346 [4.92s]
Iteration 8/40: åŽæŸ=0.0% | Loss=0.120478 | CVFP=0.662710 | Div=-0.421754 [4.93s]
Iteration 9/40: åŽæŸ=0.0% | Loss=0.115193 | CVFP=0.653586 | Div=-0.423201 [4.92s]
Iteration 10/40: åŽæŸ=0.0% | Loss=0.109285 | CVFP=0.640963 | Div=-0.422392 [4.93s]
Iteration 11/40: åŽæŸ=0.0% | Loss=0.100706 | CVFP=0.624255 | Div=-0.422842 [4.94s]
Iteration 12/40: åŽæŸ=0.0% | Loss=0.080161 | CVFP=0.584556 | Div=-0.424235 [4.93s]
Iteration 13/40: åŽæŸ=0.0% | Loss=0.048134 | CVFP=0.521970 | Div=-0.425701 [4.92s]
Iteration 14/40: åŽæŸ=0.0% | Loss=0.001738 | CVFP=0.431481 | Div=-0.428005 [4.94s]
Iteration 15/40: åŽæŸ=0.1% | Loss=-0.048590 | CVFP=0.333941 | Div=-0.431121 [4.96s]
Iteration 16/40: åŽæŸ=1.3% | Loss=-0.096944 | CVFP=0.239047 | Div=-0.432936 [4.93s]
Iteration 17/40: åŽæŸ=7.2% | Loss=-0.132320 | CVFP=0.169884 | Div=-0.434523 [4.92s]
Iteration 18/40: åŽæŸ=19.4% | Loss=-0.154353 | CVFP=0.126647 | Div=-0.435353 [4.93s]
Iteration 19/40: åŽæŸ=32.9% | Loss=-0.166666 | CVFP=0.102785 | Div=-0.436116 [4.92s]
Iteration 20/40: åŽæŸ=42.7% | Loss=-0.175450 | CVFP=0.085787 | Div=-0.436687 [4.94s]
Iteration 21/40: åŽæŸ=50.1% | Loss=-0.183525 | CVFP=0.070429 | Div=-0.437480 [4.92s]
Iteration 22/40: åŽæŸ=58.7% | Loss=-0.190661 | CVFP=0.056913 | Div=-0.438236 [4.94s]
Iteration 23/40: åŽæŸ=68.0% | Loss=-0.197050 | CVFP=0.044812 | Div=-0.438911 [4.91s]
Iteration 24/40: åŽæŸ=75.0% | Loss=-0.201964 | CVFP=0.035710 | Div=-0.439638 [4.92s]
Iteration 25/40: åŽæŸ=80.2% | Loss=-0.205940 | CVFP=0.028569 | Div=-0.440450 [4.92s]
Iteration 26/40: åŽæŸ=84.3% | Loss=-0.208856 | CVFP=0.023520 | Div=-0.441232 [4.92s]
Iteration 27/40: åŽæŸ=87.1% | Loss=-0.211077 | CVFP=0.019852 | Div=-0.442005 [4.95s]
Iteration 28/40: åŽæŸ=89.1% | Loss=-0.212825 | CVFP=0.017214 | Div=-0.442864 [4.93s]
Iteration 29/40: åŽæŸ=90.7% | Loss=-0.214263 | CVFP=0.015212 | Div=-0.443739 [4.96s]
Iteration 30/40: åŽæŸ=91.7% | Loss=-0.215407 | CVFP=0.013775 | Div=-0.444588 [4.93s]
Iteration 31/40: åŽæŸ=92.7% | Loss=-0.216495 | CVFP=0.012490 | Div=-0.445480 [4.92s]
Iteration 32/40: åŽæŸ=93.6% | Loss=-0.217482 | CVFP=0.011434 | Div=-0.446399 [4.92s]
Iteration 33/40: åŽæŸ=94.2% | Loss=-0.218369 | CVFP=0.010568 | Div=-0.447306 [4.92s]
Iteration 34/40: åŽæŸ=94.8% | Loss=-0.219277 | CVFP=0.009666 | Div=-0.448221 [4.93s]
Iteration 35/40: åŽæŸ=95.2% | Loss=-0.220078 | CVFP=0.009006 | Div=-0.449162 [4.93s]
  â†’ Early stopping (min_iterations=5 satisfied)

Phase 1 å®Œäº†: 174829/183620 ãƒˆãƒ¼ã‚¯ãƒ³åŽæŸ


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 1

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000524
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000524
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000035

Verdict: âœ… CONVERGED: Loss is stable - model has converged

    Phase 1 done: 4.4min
      Train ER: 231.3/1536 (15.1%)
      Val ER: 212.4/1536 (13.8%)
    Phase 2 starting (freeze_embedding=True)...
âœ“ ContextBlock frozen
âœ“ Embedding frozen (Weight Tying: Output Head also frozen)
  â†’ Only TokenBlock will be trained
âœ“ Training TokenBlock only: 5,906,688/65,234,944 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (Eæ¡ˆ - ãƒ¬ã‚¤ãƒ¤ãƒ¼å¯¾å¿œç‰ˆ)
======================================================================

Training tokens: 183,620
Validation tokens: 9,050
Epochs: 10
Batch size: 1024
Learning rate: 0.001
Gradient clip: 1.0
Early stopping patience: 1

Architecture: Eæ¡ˆ (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1ã§å­¦ç¿’æ¸ˆã¿)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - Eæ¡ˆ: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§

Epoch 1/10 [454.6s]:
  Train Loss: 7.5280 | Train PPL: 1859.42
  Val Loss: 6.7878 | Val PPL: 886.98 | Val Acc: 12.09%
  âœ“ New best validation loss: 6.7878

Epoch 2/10 [455.6s]:
  Train Loss: 6.4791 | Train PPL: 651.39
  Val Loss: 6.4730 | Val PPL: 647.44 | Val Acc: 14.37%
  âœ“ New best validation loss: 6.4730

Epoch 3/10 [455.5s]:
  Train Loss: 6.1535 | Train PPL: 470.36
  Val Loss: 6.3789 | Val PPL: 589.28 | Val Acc: 14.83%
  âœ“ New best validation loss: 6.3789

Epoch 4/10 [455.2s]:
  Train Loss: 5.9152 | Train PPL: 370.63
  Val Loss: 6.3470 | Val PPL: 570.76 | Val Acc: 15.31%
  âœ“ New best validation loss: 6.3470

Epoch 5/10 [455.1s]:
  Train Loss: 5.7046 | Train PPL: 300.26
  Val Loss: 6.3718 | Val PPL: 585.10 | Val Acc: 15.37%
  âš ï¸ No improvement (1/1)

â›” Early stopping triggered at epoch 5
   Val loss hasn't improved for 1 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 4
Best validation loss: 6.3470
Best validation PPL: 570.76
Best validation accuracy: 15.31%
Early stopped at epoch: 5

    Phase 2 done: 37.9min
    RESULT: Val PPL=570.76, Val Acc=15.31%

======================================================================
ALL EXPERIMENTS COMPLETE
======================================================================
Total time: 3.0h

Results saved to: ./results/layer_context_comparison/all_results.json
Summary saved to: ./results/layer_context_comparison/summary.md

====================================================================================================
COMPARISON RESULTS
====================================================================================================

 Exp | Layers |  CtxDim | Samples |    Val PPL |   Val Acc% |  Train ER% |    Val ER%
-------------------------------------------------------------------------------------
   A |      6 |     768 |     500 |     700.60 |     14.47% |       9.9% |       9.4%
   B |      3 |    1536 |     500 |     708.31 |     13.74% |      15.1% |      13.9%
   A |      6 |     768 |    1000 |     562.93 |     15.21% |       9.6% |       9.1%
   B |      3 |    1536 |    1000 |     570.76 |     15.31% |      15.1% |      13.8%

======================================================================
SCALING LAW ANALYSIS
======================================================================

Model: PPL = A Ã— tokens^Î± (log-log linear regression)

Experiment A (6å±¤, 768dim):
  PPL = 19518.93 Ã— tokens^(-0.2926)
  Î± = -0.2926 (negative = improvement with more data)
    tokens=86,925: actual=700.60, pred=700.60, err=0.0%
    tokens=183,620: actual=562.93, pred=562.93, err=0.0%

Experiment B (3å±¤, 1536dim):
  PPL = 18890.21 Ã— tokens^(-0.2887)
  Î± = -0.2887 (negative = improvement with more data)
    tokens=86,925: actual=708.31, pred=708.31, err=0.0%
    tokens=183,620: actual=570.76, pred=570.76, err=0.0%

Scaling Efficiency Comparison:
--------------------------------------------------
  ðŸ¥‡ Exp A: Î±=-0.2926 (6å±¤, 768dim)
  ðŸ¥ˆ Exp B: Î±=-0.2887 (3å±¤, 1536dim)

  Lower Î± = better scaling (more improvement with data)

Scaling analysis saved to: ./results/layer_context_comparison/scaling_analysis.json
