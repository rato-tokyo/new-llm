# Why Sequential Fails Now But Succeeded Before?

**Date**: 2025-11-21
**Question**: Sequential architecture succeeded in orthogonal discovery, but fails in current Phase 1 tests. Why?

---

## üîç Key Discovery

**Previous Success**: Repetition Training (trained model)
**Current Failure**: Phase 1 Fixed-Point Search (untrained model)

The crucial difference is **trained vs untrained**.

---

## üìä Comparison Table

| Aspect | Previous (Success) | Current (Failure) |
|--------|-------------------|------------------|
| **Training Method** | Repetition Training | Phase 1 Fixed-Point Search |
| **Model State** | **Trained** | **Untrained (random init)** |
| **Loss Function** | Convergence Loss: `MSE(context[t], context[t-cycle])` | No training (inference only) |
| **Objective** | Train model to reach fixed points | Find fixed points directly |
| **Architecture** | Sequential (2 layers) | Sequential (2 layers) |
| **Iterations** | Multiple epochs with gradient descent | Max 200 iterations, no training |

---

## üí° The Critical Difference

### Previous Approach: Repetition Training

**What it did**:
```python
# Train model to minimize context change across repetitions
loss = MSE(context[t], context[t - cycle_length])
optimizer.step()  # UPDATE WEIGHTS

# After training, model LEARNED to reach fixed points
```

**Key insight**: The model's **weights were optimized** to create a function that converges to fixed points.

### Current Approach: Phase 1 Fixed-Point Search

**What it does**:
```python
# Try to find fixed point with random initial weights
for iteration in range(max_iterations):
    context_new = f(context, token)  # f has RANDOM weights
    if ||context_new - context|| < threshold:
        break  # Converged

# No weight updates - just iterating
```

**Problem**: With **random weights**, the function `f` is not guaranteed to be a contraction mapping.

---

## üßÆ Mathematical Explanation

### Fixed-Point Theory

For a fixed point to exist and be reachable, we need:

**Contraction Mapping Theorem**:
```
||f(x) - f(y)|| ‚â§ k ||x - y||  where k < 1
```

If `k < 1`, the function is a **contraction** and fixed-point iteration converges.

### Why Random Weights Fail

**Sequential with random weights**:
```python
hidden1 = ReLU(W1 @ [token, context])  # Random W1
hidden2 = ReLU(W2 @ hidden1)           # Random W2
context_new = tanh(W3 @ hidden2)       # Random W3
```

With random initialization:
- `W1, W2, W3` are drawn from `N(0, 0.02)`
- The composed function `f = tanh ‚àò W3 ‚àò ReLU ‚àò W2 ‚àò ReLU ‚àò W1` is **not guaranteed** to be a contraction
- Deep composition of random linear layers + non-linearities typically has `k > 1` (expansion)

### Why Trained Weights Succeed

**After repetition training**:
```python
# Weights are optimized such that:
context[t] ‚âà context[t - cycle_length]

# This implicitly creates a contraction mapping
# because the model LEARNED to minimize context change
```

Training adjusts `W1, W2, W3` to satisfy the contraction condition.

---

## üî¨ Evidence from Experiments

### Sequential Architecture

| Test | Model State | Result |
|------|-------------|--------|
| **1 token (untrained)** | Random init | 0% convergence |
| **10 tokens (untrained)** | Random init | 0% convergence |
| **512 tokens (untrained)** | Random init | 0% convergence |
| **Repetition training (trained)** | Trained weights | ‚úÖ Success (orthogonal discovery) |

**Pattern**: Fails with random weights, succeeds with trained weights.

### Layer-wise Architecture

| Test | Model State | Result |
|------|-------------|--------|
| **1 token (untrained)** | Random init | 100% convergence |
| **10 tokens (untrained)** | Random init | 90% convergence |
| **512 tokens (untrained)** | Random init | 75.8% convergence |

**Pattern**: Works even with random weights!

---

## üéØ Why Layer-wise Works Without Training?

**Hypothesis**: Layer-wise architecture has **implicit contraction properties** even with random initialization.

### Mechanism Analysis

**Layer-wise update**:
```python
# Layer 1
hidden1 = ReLU(W1 @ [token, context])
context_delta1 = tanh(Wd1 @ hidden1)
forget1 = sigmoid(Wf1 @ hidden1)
input1 = sigmoid(Wi1 @ hidden1)
context = forget1 * context + input1 * context_delta1  # Gated update
context = LayerNorm(context)

# Layer 2
hidden2 = ReLU(W2 @ [token, context'])
context_delta2 = tanh(Wd2 @ hidden2)
forget2 = sigmoid(Wf2 @ hidden2)
input2 = sigmoid(Wi2 @ hidden2)
context = forget2 * context + input2 * context_delta2  # Gated update
context = LayerNorm(context)
```

**Key properties**:

1. **Gated update is a weighted average**:
   ```
   context_new = forget * context_old + input * delta
   where forget, input ‚àà [0, 1] (sigmoid)
   ```
   This is inherently stable (convex combination).

2. **LayerNorm provides magnitude control**:
   ```
   context = LayerNorm(context)  # Normalizes to mean=0, std=1
   ```
   Prevents explosive growth.

3. **Multiple small steps instead of one large step**:
   - Sequential: One large transformation `f = g3 ‚àò g2 ‚àò g1`
   - Layer-wise: Multiple small adjustments `context ‚Üí context' ‚Üí context''`

**Statistical argument**:
- With random initialization, `forget` and `input` gates have **random values** around 0.5
- This means each update is approximately:
  ```
  context_new ‚âà 0.5 * context_old + 0.5 * delta
  ```
- This is a **damped update** (coefficient 0.5 < 1), which tends to be contractive

---

## üß™ Experimental Validation

### Test: Random vs Trained Weights

We should test:

1. **Sequential with trained weights** (from repetition training):
   - Hypothesis: Should converge even in Phase 1 fixed-point search

2. **Layer-wise with trained weights**:
   - Hypothesis: Should improve convergence rate (75.8% ‚Üí higher)

If this hypothesis is correct, we should see:
- Sequential (trained) ‚Üí convergence improves
- Layer-wise (trained) ‚Üí convergence improves

---

## üìù Conclusions

### Why Sequential Failed in Current Tests

**Not a bug** - the architecture itself is correct.

**Real reason**:
- Sequential requires **trained weights** to create a contraction mapping
- With random initialization, deep sequential transformations are **not contractive**
- Therefore, fixed-point iteration fails to converge

### Why Layer-wise Succeeds

**Architectural advantage**:
- Gated updates + LayerNorm provide **implicit contraction properties**
- Works even with random initialization
- Training would further improve, but not required for basic convergence

### Why Previous Sequential Succeeded

**Training made the difference**:
- Repetition training optimized weights to minimize `context[t] - context[t-cycle]`
- This implicitly created a contraction mapping
- Therefore, fixed points emerged

---

## üöÄ Next Steps

### Immediate Experiments

1. **Load trained Sequential model** (from repetition training):
   ```bash
   # Use checkpoint from orthogonal discovery experiment
   python test_single_token.py --checkpoint path/to/trained_sequential.pt
   ```

2. **Train Sequential with Phase 1 convergence loss**:
   ```python
   # New training objective: minimize fixed-point iteration error
   for iteration in range(max_iterations):
       context_new = model(token, context)
       loss = MSE(context_new, context)  # Minimize self-consistency
       loss.backward()
       optimizer.step()
   ```

3. **Compare trained vs untrained for both architectures**:
   - Sequential (untrained) vs Sequential (trained)
   - Layer-wise (untrained) vs Layer-wise (trained)

### Long-term Strategy

**For Phase 1**:
- Use **Layer-wise architecture** (works without training)
- Continue with current Phase 1 fixed-point learning

**For Phase 2**:
- Both architectures should work (token prediction training)
- Layer-wise likely still better due to implicit stability

---

## üìä Summary Table

| Factor | Sequential | Layer-wise |
|--------|-----------|-----------|
| **Random init convergence** | ‚ùå Fails | ‚úÖ Works (75-100%) |
| **Trained convergence** | ‚úÖ Works (orthogonal discovery) | ‚úÖ Works (likely better) |
| **Requires training for Phase 1** | Yes | No |
| **Contraction mapping (random)** | No (k > 1) | Yes (k < 1) |
| **Contraction mapping (trained)** | Yes (k < 1) | Yes (k < 1) |

**Recommendation**: Use **Layer-wise** for Phase 1, as it converges without training.

---

**End of Analysis**
