# Baseline WikiText-2 Experiment - 実験結果

**実験日**: 2025-01-18
**モデル**: New-LLM (Baseline)
**データセット**: WikiText-2

---

## 📋 実験設定

### モデル構成

| パラメータ | 値 |
|-----------|-----|
| 総パラメータ数 | **2.74M** |
| Embed Dim | 256 |
| Hidden Dim | 512 |
| Context Vector Dim | 256 |
| Num Layers | 6 |
| Dropout | 0.1 |

### 訓練設定

| パラメータ | 値 |
|-----------|-----|
| Batch Size | 32 |
| Learning Rate | 0.0001 |
| Optimizer | Adam |
| Weight Decay | 0.0 |
| Gradient Clip | Adaptive (0.5 → 1.0 → 2.0) |
| Max Epochs | 50 |
| Early Stopping Patience | 15 |
| Device | CPU |

### データセット

| 項目 | 値 |
|------|-----|
| 訓練サンプル数 | 51,026 sequences |
| 検証サンプル数 | 5,272 sequences |
| Vocabulary Size | 1,000 |
| Max Sequence Length | 64 |

---

## 📊 訓練結果

### 最終性能 (Epoch 27)

| 指標 | Train | Validation |
|------|-------|-----------|
| **Loss** | 3.107 | **3.150** ✓ |
| **Perplexity** | 22.36 | **23.34** ✓ |
| **Accuracy** | - | ~31% |

### 改善推移

| 指標 | 初期 (Epoch 1) | 最良 (Epoch 27) | 改善率 |
|------|---------------|----------------|-------|
| Val Loss | 3.884 | **3.150** | **-18.9%** |
| Val PPL | 48.60 | **23.34** | **-52.0%** |

### エポック別推移

| Epoch | Train Loss | Val Loss | Train PPL | Val PPL | 備考 |
|-------|-----------|----------|-----------|---------|------|
| 1 | 4.195 | 3.884 | 66.3 | 48.6 | - |
| 5 | 3.666 | 3.603 | 39.1 | 36.7 | checkpoint保存 |
| 10 | 3.411 | 3.385 | 30.3 | 29.5 | checkpoint保存 |
| 15 | 3.262 | 3.259 | 26.1 | 26.0 | checkpoint保存 |
| 20 | 3.171 | 3.201 | 23.8 | 24.6 | checkpoint保存 |
| 25 | 3.127 | 3.165 | 22.8 | 23.7 | checkpoint保存 |
| **27** | **3.107** | **3.150** | **22.36** | **23.34** | **Best** ✓ |

---

## 📈 学習曲線の特徴

### 良好な学習の証拠

1. **継続的な改善**: Val Lossが27エポック連続で改善
2. **過学習なし**: Train/Val Lossが並行して低下
3. **安定した学習**: 大きなスパイクなし
4. **収束傾向**: 改善幅が徐々に小さくなる（収束に近づいている）

### 観察されたパターン

- Epoch 1-10: 急速な改善期（PPL 48.6 → 29.5）
- Epoch 10-20: 中程度の改善期（PPL 29.5 → 24.6）
- Epoch 20-27: 緩やかな改善期（PPL 24.6 → 23.34）

---

## 💾 保存されたチェックポイント

| ファイル名 | Epoch | Val PPL | サイズ |
|-----------|-------|---------|--------|
| `best_new_llm_wikitext.pt` | 27 | **23.34** | 31 MB |
| `new_llm_wikitext_epoch_5.pt` | 5 | 36.70 | 31 MB |
| `new_llm_wikitext_epoch_10.pt` | 10 | 29.50 | 31 MB |
| `new_llm_wikitext_epoch_15.pt` | 15 | 26.04 | 31 MB |
| `new_llm_wikitext_epoch_20.pt` | 20 | 24.57 | 31 MB |
| `new_llm_wikitext_epoch_25.pt` | 25 | 23.48 | 31 MB |

---

## 🎯 評価

### 成功点

✅ **順調な訓練**: 過学習なく27エポックで52%改善
✅ **安定性**: Adaptive gradient clippingが効果的
✅ **効率性**: 2.74Mパラメータで合理的な性能
✅ **再現性**: チェックポイントで訓練再開可能

### 課題

⚠️ **訓練時間**: CPU訓練で約1時間/epoch（合計27時間）
⚠️ **収束の遅さ**: 50エポックまで訓練すればさらに改善の余地あり

---

## 🔄 次のステップ

### 1. GPU訓練（進行中）

**Google Colab Advanced実験**:
- Context Vector Dim: 512（2倍）
- Num Layers: 12（2倍）
- パラメータ数: 4.84M（1.77倍）
- 訓練速度: 0.6分/epoch（100倍速）
- **期待性能**: PPL < 20（より大きいモデルで改善）

### 2. INT8量子化実験（計画中）

**目的**: メモリ削減と推論高速化
- 現在のモデル: 31 MB (fp32)
- 量子化後: 約8 MB (int8)
- 精度低下: 1-3% PPL上昇（許容範囲）

### 3. TinyGPT2との比較

**比較項目**:
- 同等パラメータ数（2.74M）での性能
- メモリ使用量（O(1) vs O(n²)）
- 長文処理能力

### 4. 対話データでのファインチューニング

**次の段階**:
- WikiTextで事前学習完了
- DailyDialogでファインチューニング
- 簡単な会話タスクで評価

---

## 📌 結論

**New-LLM Baseline (2.74M params)** は WikiText-2 で **Val PPL 23.34** を達成。

- ✅ 安定した訓練（過学習なし）
- ✅ 継続的な改善（27エポック連続）
- ✅ 再現性（チェックポイント保存）
- ⏭️ GPU版（4.84M params）でさらなる改善期待

**次の実験**: INT8量子化またはGoogle Colab Advanced実験の完了待ち
