# 実験結果サマリー: Transformerとの性能差を縮めるための試行

## 目標

Transformer (Attention) の性能 (Val Loss: 4.8379) に近づけること

## 実験結果一覧

| 実験 | 設定変更 | パラメータ数 | Best Val Loss | Transformer比 |
|-----|---------|-------------|---------------|--------------|
| **ベースライン** | context_dim=256, hidden=512, layers=8 | 3,009,768 | **5.6358** | +16.5% |
| **実験1** | context_dim=256→**512** | 3,272,168 | 5.6572 | +16.9% |
| **実験2** | hidden=512→**1024**, layers=8→**10** | 12,047,848 | 5.7088 | +18.0% |
| **実験3** | 実験2 + epochs=50→**150** | 12,047,848 | 5.7121 | +18.1% |
| **Transformer** | Multi-head attention | 5,260,264 | **4.8379** | baseline |

## 詳細分析

### 実験1: 文脈ベクトル次元数の増加 (256 → 512)

**仮説**: 文脈ベクトルの容量不足がボトルネック

**結果**: ❌ **効果なし**
- Val Loss: 5.6358 → 5.6572 (わずかに悪化)
- パラメータ数: +8.7%

**結論**:
- 文脈ベクトルの次元数はボトルネックではない
- 固定サイズの情報圧縮能力の問題ではない
- モデル全体の表現力不足の可能性

### 実験2: FNN容量の増加

**仮説**: FNN層の表現力不足が問題

**結果**: ❌ **逆効果**
- Val Loss: 5.6358 → 5.7088 (悪化)
- パラメータ数: 3.0M → 12.0M (4倍増加)

**結論**:
- モデルサイズを4倍にしても性能が改善しない
- むしろ過学習の兆候
- パラメータ数の問題ではない

### 実験3: 長期訓練 (50 → 150 epochs)

**仮説**: 訓練不足で収束していない

**結果**: ❌ **効果なし**
- Val Loss: 5.7088 → 5.7121 (ほぼ同じ)
- Best epoch: 20 (その後改善なし)

**結論**:
- 早期にプラトーに到達している
- 長期訓練では改善しない
- 訓練量の問題ではない

## 総合結論

### 主要な発見

1. **アテンション層は必須ではない** ✓
   - New-LLMは機能することが証明された
   - ただし性能ギャップは大きい (16-18%)

2. **文脈ベクトル次元数は問題ではない**
   - 256次元でも512次元でも同じ
   - ボトルネックは別のところにある

3. **モデルサイズは問題ではない**
   - パラメータ4倍でも改善なし
   - むしろ悪化した

4. **訓練量は問題ではない**
   - 150エポックでも改善なし
   - Epoch 20で収束

### 根本的な問題: アーキテクチャ的限界

**Transformerとの根本的な違い**:

```
Transformer (Attention):
  - 各位置が全ての過去位置にアクセス可能
  - 必要な情報を選択的に取得できる
  - 位置ごとに異なる情報を参照

New-LLM (Context Vector):
  - 全情報を固定サイズベクトルに圧縮
  - 一度圧縮した情報は選択的取得不可
  - 全位置で同じ文脈を使用
```

### 性能ギャップの原因

1. **情報の選択的アクセスの欠如**
   - Attention: "この単語の意味を判断するには、3つ前の単語を見よう"
   - Context Vector: "全ての過去情報を平均的に使うしかない"

2. **情報の非可逆圧縮**
   - 一度ベクトルに圧縮すると元の詳細情報にアクセスできない
   - Attentionは元のトークン列に直接アクセス可能

3. **位置固有の文脈の欠如**
   - 全位置で同じ文脈ベクトルを使用
   - Attentionは位置ごとに異なる重み付けで文脈を構築

## 改善の可能性

### これらは効果が期待できない:
- ✗ 文脈ベクトル次元数の増加
- ✗ FNN層の容量増加
- ✗ 訓練エポック数の増加

### 潜在的な改善策:
1. **複数の文脈ベクトル**
   - 異なる種類の情報を別々に保持
   - 例: 構文情報ベクトル + 意味情報ベクトル

2. **階層的な文脈管理**
   - 短期文脈 + 長期文脈
   - 異なる時間スケールの情報保持

3. **ハイブリッドアプローチ**
   - スパースアテンション + 文脈ベクトル
   - 重要な位置のみアテンション

4. **ゲート機構の追加**
   - 文脈の更新を選択的に制御
   - LSTMのようなゲート機構

## 最終評価

### 成功点:
- ✓ アテンションなしでLLMが機能することを実証
- ✓ パラメータ効率が良い (3M vs 5.3M)
- ✓ 実装がシンプル

### 限界点:
- ✗ 性能ギャップ 16-18% は縮まらない
- ✗ 根本的なアーキテクチャ限界
- ✗ 単純な改良では改善困難

### 実用性評価:
- **実用レベル**: 困難
- **研究価値**: 高い（アテンション機構の重要性を実証）
- **今後の方向性**: ハイブリッドアプローチの探索

## グラフ保存場所

全ての訓練曲線グラフは以下に保存されています：

- `checkpoints/new_llm_training_curves.png` (各実験の訓練曲線)
- Transformer baseline: `checkpoints/transformer_baseline_training_curves.png`
