# WikiText-2 Advanced Training on L4 GPU - 2025-11-18

## å®Ÿé¨“æ¦‚è¦

**ç›®çš„**: ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ï¼ˆcontext=512, layers=12ï¼‰ã§L4 GPUã®æ€§èƒ½ã‚’æ´»ç”¨ã—ã€Baselineã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç›®æŒ‡ã™

**å®Ÿé¨“æ—¥**: 2025å¹´11æœˆ18æ—¥
**Git Commit**: `b3e3be1` (Centralize learning_rate in config.py)
**å®Ÿé¨“ç’°å¢ƒ**: Google Colab Pro - L4 GPU

## ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿è¨­å®š

### ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆAdvancedï¼‰
- **Model**: New-LLM Advanced (Context Vector Propagation)
- **Parameters**: æŽ¨å®š 4.84Mï¼ˆBaselineã®1.77å€ï¼‰
- **Embed Dim**: 256
- **Hidden Dim**: 512
- **Num Layers**: 12ï¼ˆ**Baselineã®2å€**ï¼‰
- **Context Vector Dim**: 512ï¼ˆ**Baselineã®2å€**ï¼‰
- **Dropout**: 0.1

**ãƒ¢ãƒ‡ãƒ«å®¹é‡æ¯”è¼ƒ**:
| ãƒ¢ãƒ‡ãƒ« | Layers | Context Dim | æŽ¨å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•° |
|--------|--------|------------|----------------|
| Baseline | 6 | 256 | 2.74M |
| **Advanced** | **12** | **512** | **4.84Mï¼ˆ1.77å€ï¼‰** |

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
- **Dataset**: WikiText-2
- **Vocabulary Size**: 1000
- **Max Sequence Length**: 64
- **Training Sequences**: 51,026
- **Validation Sequences**: 5,272

## è¨“ç·´ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

### æœ€é©åŒ–è¨­å®š
- **Batch Size**: 2048ï¼ˆL4ç”¨ã«æœ€é©åŒ–ï¼‰
- **Learning Rate**: 0.0004ï¼ˆLinear Scaling Ruleé©ç”¨ï¼‰
- **Optimizer**: Adam
- **Weight Decay**: 0.0
- **Gradient Clipping**: 1.0
- **Total Epochs**: 50
- **Early Stopping Patience**: 15
- **Precision**: FP16 Mixed Precision

## è¨“ç·´çµæžœ

### æœ€çµ‚æ€§èƒ½
| Metric | Value | Epoch |
|--------|-------|-------|
| **Best Val Loss** | 3.7119 | 50 |
| **Best Val PPL** | **40.93** | 50 |
| **Best Val Accuracy** | 31.3% | 50 |

### è¨“ç·´æ™‚é–“
- **Time per Epoch**: ~0.4 minutesï¼ˆ24ç§’ï¼‰
- **Total 50 Epochs**: æŽ¨å®š20åˆ†

## å®Ÿé¨“çµæžœã®æ¯”è¼ƒåˆ†æž

### åŒæ—¥å®Ÿé¨“ã¨ã®æ¯”è¼ƒ

| å®Ÿé¨“ | Model | Params | Layers | Context Dim | batch_size | learning_rate | Best PPL | Best Acc | Time/Epoch |
|------|-------|--------|--------|------------|-----------|--------------|----------|----------|-----------|
| **FP16 Baseline** | New-LLM | 2.74M | 6 | 256 | 2048 | 0.0004 | **30.78** âœ“ | **34.4%** âœ“ | 21.6s |
| **Advanced** | New-LLM | 4.84M | 12 | 512 | 2048 | 0.0004 | **40.93** âœ— | **31.3%** âœ— | 24s |

### éŽåŽ»å®Ÿé¨“ã¨ã®æ¯”è¼ƒ

| å®Ÿé¨“ | ç’°å¢ƒ | Model Size | Best PPL | Best Acc | å‚™è€ƒ |
|------|------|-----------|----------|----------|------|
| **CPU Baseline** | CPU | 2.74M | **23.34** âœ“ | 31.0% | éŽåŽ»æœ€è‰¯ |
| **L4 FP16 Baseline** | L4 GPU | 2.74M | **30.78** | **34.4%** âœ“ | ä»Šå›žï¼ˆå°ãƒ¢ãƒ‡ãƒ«ï¼‰|
| **L4 Advanced** | L4 GPU | 4.84M | **40.93** âœ— | 31.3% | ä»Šå›žï¼ˆå¤§ãƒ¢ãƒ‡ãƒ«ï¼‰|

## é‡å¤§ãªç™ºè¦‹: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨æ€§èƒ½ã®é€†ç›¸é–¢

### ðŸš¨ å•é¡Œ: å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã»ã©æ€§èƒ½ãŒæ‚ªã„

**äºˆæƒ³**: Advancedï¼ˆ4.84Mï¼‰> Baselineï¼ˆ2.74Mï¼‰
**å®Ÿéš›**: Advancedï¼ˆPPL 40.93ï¼‰< Baselineï¼ˆPPL 30.78ï¼‰

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹å·®**:
- PPLå·®: 40.93 vs 30.78 = **33%æ‚ªåŒ–**
- Accuracyå·®: 31.3% vs 34.4% = **3.1ãƒã‚¤ãƒ³ãƒˆä½Žä¸‹**

### åŽŸå› ã®ä»®èª¬

#### ä»®èª¬1: å­¦ç¿’çŽ‡ãŒå¤§ãã™ãŽã‚‹ï¼ˆæœ€ã‚‚å¯èƒ½æ€§é«˜ï¼‰

**å•é¡Œ**:
- Advancedãƒ¢ãƒ‡ãƒ«ã¯Baselineã®1.77å€ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
- **åŒã˜learning_rate (0.0004)ã‚’ä½¿ç”¨** â† å•é¡Œã®å¯èƒ½æ€§

**ç†è«–**:
```
å¤§ããªãƒ¢ãƒ‡ãƒ« â†’ ã‚ˆã‚Šç¹Šç´°ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…è¦
â†’ å°ã•ã‚ã®learning_rateãŒé©åˆ‡
```

**æŽ¨å¥¨ä¿®æ­£**:
```python
# Baseline (2.74M)
learning_rate = 0.0004

# Advanced (4.84M, 1.77x larger)
learning_rate = 0.0002  # åŠåˆ†ã«æ¸›ã‚‰ã™
# ã¾ãŸã¯
learning_rate = 0.0001  # Baseline CPUç‰ˆã¨åŒã˜
```

#### ä»®èª¬2: è¨“ç·´ä¸è¶³ï¼ˆ50ã‚¨ãƒãƒƒã‚¯ã§ã¯ä¸ååˆ†ï¼‰

**å•é¡Œ**:
- å¤§ããªãƒ¢ãƒ‡ãƒ« â†’ åŽæŸã«æ™‚é–“ãŒã‹ã‹ã‚‹
- 50ã‚¨ãƒãƒƒã‚¯ã§ã¯Advancedãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ãã‚Œã¦ã„ãªã„

**è¨¼æ‹ **:
- Epoch 50ã§ã‚‚ã¾ã æ”¹å–„å‚¾å‘ï¼ˆPPL 40.9ï¼‰
- Early stoppingã«å¼•ã£ã‹ã‹ã£ã¦ã„ãªã„

**æŽ¨å¥¨**: 150ã‚¨ãƒãƒƒã‚¯è¨“ç·´

#### ä»®èª¬3: Overfittingï¼ˆéŽå­¦ç¿’ï¼‰

**å•é¡Œ**:
- 4.84Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯51,026è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã—ã¦å¤§ãã™ãŽã‚‹ï¼Ÿ
- ãƒ‡ãƒ¼ã‚¿ã‚ãŸã‚Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 94ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿/ã‚µãƒ³ãƒ—ãƒ«

**ã—ã‹ã—**:
- Accuracyã‚‚æ‚ªåŒ–ã—ã¦ã„ã‚‹ãŸã‚ã€overfittingã§ã¯ãªãunderfittingã®å¯èƒ½æ€§é«˜

#### ä»®èª¬4: Gradient Explosionã¾ãŸã¯ä¸å®‰å®šæ€§

**å•é¡Œ**:
- 12å±¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ â†’ gradient flowãŒä¸å®‰å®š
- FP16ç²¾åº¦ã®å½±éŸ¿

**ç¢ºèªæ–¹æ³•**:
- è¨“ç·´ãƒ­ã‚°ã§æ€¥æ¿€ãªlosså¢—åŠ ãŒãªã„ã‹ç¢ºèª
- Gradient normã‚’ç›£è¦–

## æ”¹å–„ç­–ã®ææ¡ˆ

### å„ªå…ˆåº¦1: Learning Rateèª¿æ•´

```python
# æ–°ã—ã„è¨­å®šã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
class NewLLMAdvancedL4ConfigLowLR(NewLLMAdvancedL4Config):
    """Advanced model with lower learning rate"""
    learning_rate = 0.0002  # åŠåˆ†ã«æ¸›ã‚‰ã™
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æžœ**: PPL 40.93 â†’ 25-30ç¨‹åº¦

### å„ªå…ˆåº¦2: 100-150ã‚¨ãƒãƒƒã‚¯å»¶é•·è¨“ç·´

```python
# Extended training
num_epochs = 150
patience = 30
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æžœ**: ã‚ˆã‚ŠåŽæŸãŒé€²ã‚€

### å„ªå…ˆåº¦3: Learning Rate Warmupã®è¿½åŠ 

```python
# Warmup for large models
warmup_epochs = 5
initial_lr = 0.0001
target_lr = 0.0004
```

### å„ªå…ˆåº¦4: Gradient Clippingã®èª¿æ•´

```python
# ã‚ˆã‚Šä¿å®ˆçš„ãªclipping
gradient_clip = 0.5  # ç¾åœ¨ã¯1.0
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### å®Ÿé¨“A: Learning RateåŠæ¸›ï¼ˆæŽ¨å¥¨ï¼‰

**è¨­å®š**:
```python
class AdvancedConfigLowLR(NewLLMAdvancedL4Config):
    learning_rate = 0.0002  # 0.0004 â†’ 0.0002
    num_epochs = 50
```

**æœŸå¾…**: PPL 40.93 â†’ 30ä»¥ä¸‹

### å®Ÿé¨“B: 100ã‚¨ãƒãƒƒã‚¯å»¶é•·ï¼ˆç¾åœ¨ã®LRç¶­æŒï¼‰

**è¨­å®š**:
```python
num_epochs = 150
checkpoint_to_resume = "best_new_llm_wikitext_ctx512_layers12.pt"
```

**æœŸå¾…**: PPL 40.93 â†’ 35ç¨‹åº¦ï¼ˆé™å®šçš„æ”¹å–„ï¼‰

### å®Ÿé¨“C: Learning RateåŠæ¸› + 150ã‚¨ãƒãƒƒã‚¯ï¼ˆæœ€è‰¯ï¼‰

**è¨­å®š**:
```python
learning_rate = 0.0002
num_epochs = 150
```

**æœŸå¾…**: PPL 40.93 â†’ 25ä»¥ä¸‹ï¼ˆBaselineè¶…ãˆï¼‰

## ãƒ‡ãƒ¼ã‚¿ä¿å­˜

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- **Best Checkpoint**: `checkpoints/best_new_llm_wikitext_ctx512_layers12.pt`
- **Final Checkpoint**: `checkpoints/new_llm_wikitext_ctx512_layers12_final.pt`

### ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
- `/content/ctx1024_log.txt` (Colab)

## æ•™è¨“ã¨ã‚¤ãƒ³ã‚µã‚¤ãƒˆ

### âœ… å­¦ã‚“ã ã“ã¨

1. **ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨learning_rateã®é–¢ä¿‚**
   - å¤§ããªãƒ¢ãƒ‡ãƒ« â‰  åŒã˜learning_rate
   - Linear Scaling Ruleã¯batch_sizeç”¨ã§ã‚ã‚Šã€model_sizeç”¨ã§ã¯ãªã„

2. **capacity â‰  performance**
   - ã‚ˆã‚Šå¤šãã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¸¸ã«è‰¯ã„ã¨ã¯é™ã‚‰ãªã„
   - é©åˆ‡ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒå¿…é ˆ

3. **å®Ÿé¨“ã®é‡è¦æ€§**
   - ä»®èª¬: Advanced > Baseline
   - å®Ÿæ¸¬: Advanced < Baseline
   - â†’ å®Ÿé¨“ãªã—ã§ã¯åˆ†ã‹ã‚‰ãªã„

### âš ï¸ å¤±æ•—ã®åŽŸå› 

1. **Learning Rateæœªèª¿æ•´**
   - batch_sizeã¯èª¿æ•´ã—ãŸãŒã€model_sizeã«å¯¾ã™ã‚‹èª¿æ•´ã‚’å¿˜ã‚ŒãŸ
   - 0.0004ã¯Advancedãƒ¢ãƒ‡ãƒ«ã«ã¯å¤§ãã™ãŽãŸå¯èƒ½æ€§

2. **è¨“ç·´æœŸé–“ä¸è¶³**
   - 50ã‚¨ãƒãƒƒã‚¯ã¯å¤§ããªãƒ¢ãƒ‡ãƒ«ã«ã¯çŸ­ã„
   - 100-150ã‚¨ãƒãƒƒã‚¯å¿…è¦

## ã¾ã¨ã‚

### å®Ÿé¨“çµæžœ
- âŒ **PPL 40.93**ï¼ˆBaseline 30.78ã‚ˆã‚Š33%æ‚ªã„ï¼‰
- âŒ **Accuracy 31.3%**ï¼ˆBaseline 34.4%ã‚ˆã‚Š3.1ãƒã‚¤ãƒ³ãƒˆä½Žã„ï¼‰
- â±ï¸ **è¨“ç·´æ™‚é–“**: 24ç§’/ã‚¨ãƒãƒƒã‚¯ï¼ˆBaselineã‚ˆã‚Š12%é…ã„ï¼‰

### é‡è¦ãªç™ºè¦‹
**Large Model Paradox**: ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ãŒå¿…ãšã—ã‚‚è‰¯ã„æ€§èƒ½ã‚’å‡ºã™ã¨ã¯é™ã‚‰ãªã„

**åŽŸå› **: Learning rateã®ä¸é©åˆ‡ãªè¨­å®šï¼ˆ0.0004ã¯å¤§ãã™ãŽã‚‹å¯èƒ½æ€§ï¼‰

### æ¬¡å›žå®Ÿé¨“ã¸ã®æŽ¨å¥¨
1. âœ… **learning_rate = 0.0002ã«åŠæ¸›**
2. âœ… **150ã‚¨ãƒãƒƒã‚¯è¨“ç·´**
3. âœ… **Warmupè¿½åŠ **ã‚’æ¤œè¨Ž

### ç·åˆè©•ä¾¡
ã“ã®å®Ÿé¨“ã¯ã€Œå¤±æ•—ã€ã§ã¯ãªãã€Œè²´é‡ãªå­¦ã³ã€ã€‚ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨learning_rateã®é–¢ä¿‚ã«ã¤ã„ã¦é‡è¦ãªã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’å¾—ãŸã€‚é©åˆ‡ãªèª¿æ•´ã«ã‚ˆã‚Šã€Advancedãƒ¢ãƒ‡ãƒ«ã¯ç¢ºå®Ÿã«Baselineã‚’è¶…ãˆã‚‰ã‚Œã‚‹ã¯ãšã€‚

---

**å®Ÿé¨“è€…**: Claude Code
**è¨˜éŒ²æ—¥**: 2025-11-18
**Git Commit**: b3e3be1
**Status**: Learning rateèª¿æ•´ãŒå¿…è¦ï¼ˆæ¬¡å›žå®Ÿé¨“ã§æ”¹å–„è¦‹è¾¼ã¿ï¼‰
