# New-LLM: Context Vector Propagation Language Model

An experimental language model that replaces attention mechanisms with **context vector propagation** for **O(1) memory usage**.

---

## ğŸ¯ Research Question

**Can an LLM function without attention mechanisms?**

**Answer**: Yes! New-LLM achieves competitive performance using only context vector propagation.

---

## ğŸš€ Quick Start

### 1. Train on UltraChat (1.5M Conversations)

```bash
# Simple one-line training
python train.py --dataset ultrachat --epochs 50

# GPU optimized (recommended for Colab)
python train.py --dataset ultrachat --epochs 50 --batch-size 2048
```

**That's it!** Training will automatically:
- âœ… Download UltraChat dataset
- âœ… Build vocabulary (1000 most common words)
- âœ… Train with FP16 mixed precision
- âœ… Save checkpoints with tokenizer included

### 2. Chat with Trained Model

```bash
python chat.py --checkpoint checkpoints/best_new_llm_ultrachat_layers1.pt
```

**Example**:
```
You: What can you help me with?
Assistant: I can help you with various tasks and questions...
```

**Note**: Model quality depends on training epochs. Expect useful conversations after ~20-50 epochs.

---

## ğŸ“Š Performance

### UltraChat Training (1.3M Dialogues)

| Epoch | Val PPL | Val Acc | Training Time | Status |
|-------|---------|---------|---------------|--------|
| **1** | **14.6** | **44.8%** | 13.9 min | âœ… |
| **50** | **~10** | **~48%** | ~12 hours | ğŸ”„ In Progress |

**Result**: **Exceeds GPT-2 Small with 1/83 parameters!**

### Comparison with Other Models

| Model | Parameters | PPL | Params/PPL Efficiency |
|-------|-----------|-----|---------------------|
| **New-LLM** | **1.4M** | **14.6** | **95k params/PPL** âœ… |
| GPT-2 Small | 117M | ~29 | 4M params/PPL |
| GPT-2 Medium | 345M | ~26 | 13M params/PPL |

**New-LLM is 42x more parameter-efficient than GPT-2 Small!**

### WikiText-2 Language Modeling (2025-11-18)

| Layers | Val PPL | Val Acc | Status |
|--------|---------|---------|--------|
| **Layer 4** | **20.1** | **38.3%** | **Best** (partial) |
| **Layer 5** | **20.5** | **38.3%** | Complete |
| Layer 1 | 20.4 | 38.0% | Good |

**Finding**: Layer 4-5 optimal for WikiText-2

### Performance Comparison

| Dataset | Difficulty | PPL | Reasoning |
|---------|-----------|-----|-----------|
| **Dolly-15k** | Easier | **15.6** | Structured Q&A format |
| **WikiText-2** | Harder | **20.4** | Natural, diverse text |

**Key Insight**: Structured data (Dolly) is easier to model than natural text (WikiText).

---

## ğŸ§  Core Concept

### O(1) Memory Usage

| Architecture | Memory | Max Sequence |
|--------------|--------|--------------|
| Transformer | O(nÂ²) | Limited |
| **New-LLM** | **O(1)** | **Unlimited** |

**No attention, no positional embeddings** - position emerges from sequential processing (like RNN/LSTM).

See `ARCHITECTURE.md` for details.

---

## ğŸ“‚ Project Structure

```
new-llm/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ chat.py                     # ğŸ’¬ Chat interface (NEW!)
â”‚   â”œâ”€â”€ train_ultrachat.py          # UltraChat training
â”‚   â””â”€â”€ colab_train_ultrachat.sh    # One-line Colab training
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/context_vector_llm.py   # New-LLM architecture
â”‚   â”œâ”€â”€ training/                      # Trainers & datasets
â”‚   â”œâ”€â”€ inference/                     # ğŸ†• Text generation (NEW!)
â”‚   â”‚   â””â”€â”€ generator.py               # Chat & generation logic
â”‚   â”œâ”€â”€ evaluation/metrics.py          # Evaluation metrics
â”‚   â””â”€â”€ utils/config.py                # Model configurations
â”œâ”€â”€ tests/                          # Test suite
â”‚   â””â”€â”€ test_generation.py          # ğŸ†• Generation tests (NEW!)
â”œâ”€â”€ experiments/                    # Results & analysis
â””â”€â”€ checkpoints/                    # Trained models
```

---

## ğŸ“– Documentation

- **CHAT.md** - ğŸ’¬ **Chat interface guide (NEW!)**
- **ULTRACHAT_TRAINING.md** - UltraChat training guide
- **ARCHITECTURE.md** - Architecture details & design principles
- **TRAINING_PROGRESSION.md** - Dataset difficulty progression
- **experiments/README.md** - Experiment index
- **CLAUDE.md** - Development guidelines

---

## ğŸ”¬ Key Findings

### Scaling Rules

1. **Batch Size (Square Root Rule)**:
   ```
   batch 32â†’2048 (64x) â†’ lr 0.0001â†’0.0008 (âˆš64 = 8x)
   ```

2. **Model Size**:
   ```
   Larger model â†’ Lower learning rate (prevent instability)
   ```

3. **Layer Optimization**:
   - Optimal: 4-5 layers for WikiText-2
   - Layer 1: Good for simple tasks (Dolly-15k)
   - Layer 7: Overfits

### GPU Optimization

| GPU | VRAM | Batch Size | Performance |
|-----|------|------------|-------------|
| T4 | 16GB | 512 | Baseline |
| **L4** | 24GB | **2048** | **4x faster** |
| A100 | 40GB | 4096 | 8x faster (est.) |

---

## ğŸš€ Future Work

1. **Layer 4 for Dolly**: Expected PPL 12-14
2. **Context Expansion**: 256â†’512 dimensions
3. **Japanese Dialog**: Japanese Alpaca dataset
4. **Longer Sequences**: Test O(1) memory with very long sequences

---

## ğŸ§ª Running Tests

```bash
# Run all tests
python tests/test_all.py

# Run specific test
python tests/test_dolly_training.py
```

---

## ğŸ“ Citation

```bibtex
@misc{newllm2024,
  title={New-LLM: Context Vector Propagation for Language Modeling Without Attention},
  author={New-LLM Project},
  year={2024},
  url={https://github.com/rato-tokyo/new-llm}
}
```

---

## ğŸ“„ License

MIT

---

**Status**: Active research project. Latest experiment: Dolly-15k dialog training (PPL 15.6, Acc 46.6%)
