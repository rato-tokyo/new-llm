# New-LLM: Context-KV Attention for Efficient LLM Inference

A research project to improve pretrained LLMs (Pythia-70M) with Context-KV Attention, achieving **50% KV cache memory reduction** while maintaining or improving performance.

## ğŸ¯ Project Goal

**Replace early attention layers in Pythia-70M with Context-KV Attention to reduce KV cache memory usage by 50%.**

### Target Model: Pythia-70M

| Parameter | Value |
|-----------|-------|
| Layers | 6 |
| Hidden Size | 512 |
| Attention Heads | 8 |
| Total Parameters | 70M |

### Approach

1. **Replace Layer 0** of Pythia with Context-KV Attention
2. **Keep Layers 1-5** as original Pythia attention
3. **Two-phase training**:
   - Phase 1: Train ContextBlock (OACD algorithm for diversity)
   - Phase 2: Fine-tune entire model

### Expected Benefits

- **50% KV cache reduction** (conservative target)
- **Maintained or improved PPL** on evaluation benchmarks
- **Minimal architectural changes** to pretrained model

## Core Concept: Context-KV Attention

Instead of storing all token KV pairs, we compress context into fixed-size vectors:

```
Standard Attention (Layer 0):
  KV Cache = [kvâ‚€, kvâ‚, kvâ‚‚, ..., kvâ‚â‚€â‚‚â‚ƒ]  // 1024 KV pairs

Context-KV Attention (Layer 0):
  KV Cache = [ctxâ‚€, ctxâ‚ƒâ‚‚, ctxâ‚†â‚„, ...]      // ~32 context vectors
  â†’ 32x compression at interval=32
```

### How It Works

```
Position 350, interval=32, max_contexts=32:
  Context KV = [ctx[350], ctx[318], ctx[286], ..., ctx[30]]
               â†‘current   â†‘-32      â†‘-64

  Query: from current token embedding
  Key/Value: projected from context vectors
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Pythia-70M + Context-KV                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Token Embedding (Pythia pretrained, 512-dim)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Layer 0: Context-KV Attention (REPLACED)            â”‚ â”‚
â”‚  â”‚  - ContextBlock: learns compressed representations    â”‚ â”‚
â”‚  â”‚  - Context-KV Attention: uses context as KV cache    â”‚ â”‚
â”‚  â”‚  - context_dim=256 (compression ratio ~2x)           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Layers 1-5: Original Pythia Attention (PRESERVED)   â”‚ â”‚
â”‚  â”‚  - Standard self-attention                           â”‚ â”‚
â”‚  â”‚  - Pretrained weights maintained                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Output Head (Pythia pretrained)                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Training Pipeline

### Phase 1: Context Diversity Learning (OACD)

Train only the ContextBlock to produce diverse context representations:

```python
def oacd_loss(contexts, centroid_weight=0.1):
    """Origin-Anchored Centroid Dispersion"""
    centroid = contexts.mean(dim=0)
    dispersion_loss = -torch.norm(contexts - centroid) / len(contexts)
    centroid_loss = torch.norm(centroid) ** 2
    return dispersion_loss + centroid_weight * centroid_loss
```

### Phase 2: Full Model Fine-tuning

- Freeze ContextBlock
- Fine-tune Context-KV Attention layer + remaining Pythia layers
- Cross-entropy loss for next-token prediction

## Evaluation

### Primary Metrics

| Metric | Purpose |
|--------|---------|
| **PPL (Perplexity)** | Language modeling quality |
| **LAMBADA Accuracy** | Long-range dependency (final word prediction) |
| **KV Cache Memory** | Actual memory usage measurement |

### Comparison

```
Baseline: Pythia-70M (original)
Ours:     Pythia-70M + Context-KV (Layer 0 replaced)

Evaluate on:
- WikiText-2 PPL
- Pile test set PPL
- LAMBADA accuracy
- torch.cuda.max_memory_allocated()
```

## Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Development Experiments (Limited Data)

```bash
# Quick test with minimal samples
python3 scripts/experiment_context_kv.py -s 100

# Medium-scale test
python3 scripts/experiment_context_kv.py -s 1600
```

### Full Experiments (Pile Dataset)

```bash
# Full training (requires significant compute)
python3 scripts/experiment_pythia_context_kv.py  # TBD
```

## Project Structure

```
new-llm/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ experiment_context_kv.py      # Current Context-KV experiments
â”‚   â””â”€â”€ experiment_pythia_context_kv.py  # Pythia integration (TBD)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ context_kv.py             # ContextKVAttentionLLM
â”‚   â”‚   â”œâ”€â”€ blocks.py                 # ContextBlock (1-layer)
â”‚   â”‚   â””â”€â”€ layers.py                 # ContextLayer
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â””â”€â”€ phase1/
â”‚   â”‚       â””â”€â”€ memory.py             # Phase 1 trainer (OACD)
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ diversity.py              # OACD algorithm
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ base.py                       # Model architecture config
â”‚   â”œâ”€â”€ phase1.py                     # Phase 1 training config
â”‚   â””â”€â”€ phase2.py                     # Phase 2 training config
â”œâ”€â”€ CLAUDE.md                         # Development guidelines
â””â”€â”€ README.md                         # This file
```

## Configuration

Key parameters in `config/base.py`:

```python
# Context-KV Attention
context_dim = 256           # Context vector dimension
context_interval = 32       # Interval between contexts
max_contexts = 32           # Maximum contexts (context window)
num_heads = 8               # Attention heads

# Data (development)
num_samples = 1600          # Limited samples for development
```

## Current Status (2025-12-03)

### Completed
- âœ… Context-KV Attention implementation
- âœ… OACD algorithm for Phase 1
- âœ… Two-phase training pipeline
- âœ… Config-driven architecture (no hardcoding)

### In Progress
- ğŸ”„ Pythia-70M integration
- ğŸ”„ Pile dataset support
- ğŸ”„ LAMBADA evaluation

### Planned
- â¬š Layer 0 replacement experiments
- â¬š Memory usage benchmarking
- â¬š Comparison with original Pythia-70M

## References

- [Pythia: A Suite for Analyzing Large Language Models](https://arxiv.org/abs/2304.01373)
- [DeepSeek MLA (Multi-Head Latent Attention)](https://arxiv.org/abs/2401.02954)
- [EleutherAI/pythia-70m](https://huggingface.co/EleutherAI/pythia-70m)

## License

MIT
