# New-LLM: Context Vector Fixed-Point Property

A novel language model architecture based on the hypothesis that context vectors converge to fixed points with high dimensional diversity.

## Core Concept: CVFP (Context Vector Fixed-Point Property)

New-LLM explores the idea that meaningful context representations emerge through iterative refinement to fixed points, rather than traditional recurrent or transformer-based approaches.

## Features

- **Two-Phase Training**: Separate fixed-point learning and token prediction
- **Shallow & Wide Architecture**: 3 layers, 1536 context_dim, 2 input tokens (best performance)
- **Best Scaling Law**: Î± = **-0.5402** (RÂ² = 0.977), PPL 197.0, Acc 22.9%
- **Token Input All Layers**: `token_input_all_layers=True` is essential for performance
- **Parallel Cache Collection**: 51s â†’ few seconds with batch processing (context similarity 99.7%)
- **Phase 2 Cache Reuse**: Pass cache from Phase 1 to Phase 2, saving 627s (40% faster)
- **Parallel Processing**: **23x speedup** (265s â†’ 11s) with parallel batch processing
- **Auto Batch Size**: GPU memory-based batch size calculation with OOM prevention
- **Diversity Regularization**: Global mean-based tracking for parallel processing
- **Function-Based Architecture**: Clean, efficient implementation in [src/trainers/phase1/memory.py](src/trainers/phase1/memory.py)
- **Flexible Data Loading**: Supports UltraChat, text files, and custom datasets
- **Full Reproducibility**: Fixed random seed (42) for deterministic training
- **GPU-Ready**: Optimized for Colab (22GB VRAM)

## Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Basic Training

```bash
# Quick test (100 tokens)
python3 train.py --test

# Standard test with fixed train/val data (6400 train + 1280 val tokens)
python3 test.py

# Full training with configuration (skips Phase 1 if checkpoint exists)
python3 train.py
```

### Configuration

Edit `config.py` to adjust:
- Model architecture (layers, dimensions)
- Training parameters (learning rates, iterations)
- Data sources and preprocessing
- Distribution regularization settings

### Scaling Experiments

```bash
# Standard scaling law experiment
python3 scripts/scaling_experiment.py --input-tokens 1 --layers 1 --context-dim 768

# 9-config matrix (input_tokens Ã— layers)
python3 scripts/scaling_experiment.py --matrix

# Alpha progression analysis: measure how Î± changes with more data
# Generates sample sizes: [50, 100, 200, 400, 800]
# Window 1: [50-400] â†’ Î±â‚, Window 2: [100-800] â†’ Î±â‚‚
python3 scripts/scaling_experiment.py --alpha-scaling \
  --init-samples 50 --multiplier 2 --window-size 4 --num-windows 2
```

**Alpha Scaling Mode**: Measures how scaling efficiency (Î±) changes as data amount increases. Uses sliding window analysis to track Î± progression.

### Diversity Algorithm Experiments

```bash
# Phase 1 only: Compare diversity algorithms on Effective Rank
python3 scripts/diversity_algorithm_experiment.py -a MCDL ODCM SDL NUC -s 50 100

# Phase 1 + Phase 2: Full experiment with Î± analysis (CVFP disabled)
# Uses 4 algorithms, samples=[50,100,200], context_dim=1000
python3 scripts/diversity_full_experiment.py
```

**Available Algorithms**:
- **MCDL**: Mean-Centered Dispersion Loss (fastest baseline)
- **ODCM**: Off-Diagonal Covariance Minimization (VICReg-style, recommended)
- **SDL**: Spectral Diversity Loss (direct ER maximization, highest ER)
- **NUC**: Nuclear Norm Maximization (high ER, high cost)

## Project Structure

```
new-llm/
â”œâ”€â”€ train.py                       # Main training script
â”œâ”€â”€ test.py                        # Standard test script
â”œâ”€â”€ config.py                      # Configuration
â”œâ”€â”€ CLAUDE.md                      # Design guidelines and architecture decisions
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ llm.py                 # Main model architecture (LLM)
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â”œâ”€â”€ phase1/
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py            # Phase 1 abstract base class
â”‚   â”‚   â”‚   â””â”€â”€ memory.py          # Memory-based Phase 1 trainer
â”‚   â”‚   â””â”€â”€ phase2.py              # Phase 2: Token prediction
â”‚   â”œâ”€â”€ experiments/
â”‚   â”‚   â”œâ”€â”€ config.py              # Shared config classes (DataConfig, Phase1Config, Phase2Config)
â”‚   â”‚   â””â”€â”€ runner.py              # ExperimentRunner
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ diversity.py           # Diversity loss algorithms (MCDL, ODCM, SDL, NUC)
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â””â”€â”€ data/
â”‚   â”‚       â””â”€â”€ memory.py          # Memory-based data provider
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ memory.py              # GPU memory management
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ metrics.py             # Analysis and metrics
â”‚       â””â”€â”€ diagnostics.py         # Identity mapping check
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ scaling_experiment.py      # Scaling law experiments (with alpha progression)
â”‚   â”œâ”€â”€ diversity_algorithm_experiment.py  # Phase 1 diversity algorithm comparison
â”‚   â”œâ”€â”€ diversity_full_experiment.py       # Phase 1+2 with Î± analysis
â”‚   â””â”€â”€ create_val_from_train.py   # Generate validation data
â”œâ”€â”€ data/
â”‚   â””â”€â”€ example_val.txt            # Validation data file (auto-generated)
â””â”€â”€ importants/
    â””â”€â”€ *.md                       # Experimental reports
```

## Architecture Highlights

### Parallel Processing with Diversity Optimization

Our implementation achieves **23x speedup** through parallel batch processing while maintaining high diversity:

**Implementation in phase1_train() (src/trainers/phase1.py):**
```python
def compute_diversity_loss(contexts):
    """
    å¤šæ§˜æ€§æå¤±: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡ã‹ã‚‰ã®åå·®ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰

    Args:
        contexts: ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ [num_tokens, context_dim]

    Returns:
        diversity_loss: å¤šæ§˜æ€§æå¤±ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰
    """
    context_mean = contexts.mean(dim=0)  # [context_dim]
    deviation = contexts - context_mean  # [num_tokens, context_dim]
    diversity_loss = -torch.norm(deviation, p=2) / len(contexts)
    return diversity_loss

# Combined loss with parallel optimization
total_loss = (1 - dist_reg_weight) * cvfp_loss + dist_reg_weight * diversity_loss
```

**Parallel Processing Design:**
- **Iteration 0**: Sequential processing (establishes fixed-point target)
- **Iteration 1+**: Parallel batch processing (uses previous iteration's contexts)
- **1-token shift**: Token i uses previous_contexts[i-1] from prior iteration
- **Information delay**: Compensated by `dist_reg_weight = 0.9` (90% diversity)

**Key Benefits:**
- **23x speedup**: 265s â†’ 11s (vs sequential version)
- **High Effective Rank**: 55.9% (val) / ~60% (train) with parallel optimization
- **Diversity-first optimization**: `dist_reg_weight = 0.9` compensates information delay
- **Stable training**: Gradient clipping, deterministic results

### Two-Phase Training

**Phase 1: Parallel Fixed-Point Learning**
- **Parallel batch processing**: 23x speedup (265s â†’ 11s)
- **Iteration 0**: Sequential processing to establish fixed-point target
- **Iteration 1+**: Parallel processing with context propagation
- **Global mean-based diversity**: Enforces high dimensional spread (55.9% Effective Rank)
- **Diversity-first optimization**: `dist_reg_weight = 0.9` (90% diversity, 10% CVFP)
- Gradient clipping ensures training stability
- Early stopping based on convergence rate (95% of tokens)

**Phase 2: Token Prediction**
- Context propagation across tokens (matches Phase 1 behavior)
- Prediction from concatenated context + token embeddings (both utilized)
- Context providesæ–‡è„ˆinformation, token_embed provides local representation
- Next-token prediction with CrossEntropyLoss
- Full model fine-tuning with small learning rate (0.002)
- CVFP layers remain trainable for end-to-end optimization

## Development Guidelines

See `CLAUDE.md` for:
- Design principles and architecture decisions
- Critical bug fixes and lessons learned
- Mandatory numerical reporting rules
- Code quality standards

## Current Status

**Architecture Comparison Results (2025-11-29):**

| Config | Layers | context_dim | input_tokens | Î± | Best PPL | Best Acc |
|--------|--------|-------------|--------------|------|----------|----------|
| baseline | 6 | 768 | 1 | -0.4860 | 249.3 | 21.3% |
| input_tokens_2 | 6 | 768 | 2 | -0.4702 | 198.1 | 22.5% |
| context_dim_1152 | 6 | 1152 | 1 | -0.4988 | 246.9 | 21.4% |
| layers_9 | 9 | 768 | 1 | -0.4818 | 256.8 | 21.1% |
| **shallow_wide** | **3** | **1536** | **2** | **-0.5402** | **197.0** | **22.9%** |

**Key Discovery: CVFP benefits from input richness over depth**
- shallow_wide achieves best Î± (-0.5402) with only 3 layers
- Doubling context_dim (768â†’1536) + 2 input tokens is optimal
- 9 layers provides no benefit over 6 layers (unlike Transformers)

See [importants/experiment-results-20251129-architecture-comparison.md](importants/experiment-results-20251129-architecture-comparison.md) for full analysis.

**Recommended Configuration:**
```python
num_layers = 3
context_dim = 1536
num_input_tokens = 2
embed_dim = 768
```

**Working:**
- âœ… Shallow & wide architecture (3L/1536d/2tok)
- âœ… Best scaling law Î± = -0.5402
- âœ… Parallel cache collection (51s â†’ few seconds)
- âœ… Phase 2 cache reuse (skip 627s rebuild)
- âœ… Auto batch size with OOM prevention
- âœ… Parallel batch processing (23x speedup)
- âœ… Two-phase training pipeline
- âœ… GPT-2 pre-trained embeddings (768-dim, frozen in Phase 2)
- âœ… Weight tying (embedding = output head)
- âœ… Deterministic training (seed=42)

**Current Research Focus (2025-12-01):**
- ğŸ”¬ Diversity algorithm comparison (MCDL, ODCM, SDL, NUC)
- ğŸ”¬ Phase 1 diversity-only training (CVFP disabled)
- ğŸ”¬ Î± value comparison across algorithms

**Next Steps:**
- ğŸ¯ Scale to 1000+ samples with shallow_wide config
- ğŸ¯ Test even wider architectures (context_dim=2048+)
- ğŸ¯ Explore num_input_tokens=3

## License

MIT
