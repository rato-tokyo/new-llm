# New-LLM: Context Vector Fixed-Point Property

A novel language model architecture based on the hypothesis that context vectors converge to fixed points with high dimensional diversity.

## Core Concept: CVFP (Context Vector Fixed-Point Property)

New-LLM explores the idea that meaningful context representations emerge through iterative refinement to fixed points, rather than traditional recurrent or transformer-based approaches.

## Features

- **Two-Phase Training**: Separate fixed-point learning and token prediction
- **High Dimensional Diversity**: Achieves 80%+ Effective Rank using LayerNorm + fixed dimension assignment
- **Diversity Regularization**: LayerNorm prevents value explosion, fixed dimension assignment forces diversity
- **Clean Architecture**: Object-oriented design with CVFPLayer encapsulation
- **Flexible Data Loading**: Supports UltraChat, text files, and custom datasets

## Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Basic Training

```bash
# Quick test with 10 tokens (for development)
python3 tests/test_refactored.py

# Full training with configuration
python3 train.py
```

### Configuration

Edit `config.py` to adjust:
- Model architecture (layers, dimensions)
- Training parameters (learning rates, iterations)
- Data sources and preprocessing
- Distribution regularization settings

## Project Structure

```
new-llm/
â”œâ”€â”€ train.py              # Main training script
â”œâ”€â”€ config.py             # Configuration
â”œâ”€â”€ CLAUDE.md             # Design guidelines and architecture decisions
â”œâ”€â”€ CONTEXT.md            # Development history and insights
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ cvfp/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py        # CVFP module exports
â”‚   â”‚   â”‚   â”œâ”€â”€ layer.py           # CVFPLayer (basic unit)
â”‚   â”‚   â”‚   â””â”€â”€ block.py           # CVFPBlock (multi-layer)
â”‚   â”‚   â””â”€â”€ new_llm_residual.py    # Main model architecture
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ phase1.py              # Fixed-point learning
â”‚   â”‚   â””â”€â”€ phase2.py              # Token prediction
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ loader.py              # Data loading utilities
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ metrics.py             # Analysis and metrics
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_refactored.py         # Quick development test (10 tokens)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ (experimental scripts)     # Future experiments and utilities
â””â”€â”€ data/
    â”œâ”€â”€ example_train.txt          # Example training data
    â””â”€â”€ example_val.txt            # Example validation data
```

## Architecture Highlights

### Diversity Regularization: LayerNorm + Fixed Dimension Assignment

Our breakthrough approach combines two complementary techniques:

**1. LayerNorm (Value Explosion Prevention)**
```python
# Prevents residual connection value explosion
if layernorm_mix > 0:
    new_context = (1 - mix) * new_context + mix * layer_norm(new_context)
```

**2. Fixed Dimension Assignment (Diversity Enforcement)**
```python
# Each token assigned to specific dimensions via hash
token_hash = hash(token_idx) % context_dim
assigned_dims = [(token_hash + i) % context_dim for i in range(dims_per_token)]
```

Benefits:
- **High Effective Rank**: Achieves 12.84/16 (80.3%) on training data
- **Stable Training**: No value explosion (norms stay controlled)
- **Forced Diversity**: Each token uses different dimension subsets
- **Simple & Effective**: No complex covariance or orthogonality constraints

### Two-Phase Training

**Phase 1: Fixed-Point Learning with Diversity Regularization**
- Contexts converge through iterative refinement
- LayerNorm prevents value explosion in residual connections
- Fixed dimension assignment forces high dimensional diversity
- Gradient clipping ensures training stability
- Early stopping based on convergence rate (95% of tokens)

**Phase 2: Token Prediction** (Optional)
- Standard next-token prediction
- Uses fixed contexts from Phase 1
- Optional context freezing

## Development Guidelines

See `CLAUDE.md` for:
- Design principles and philosophy
- Token-wise vs batch normalization rationale
- Object-oriented architecture patterns
- Code quality standards

## Current Status

**Recent Breakthrough (2025-11-23):**
- âœ… **80.3% Effective Rank achieved** using LayerNorm + fixed dimension assignment
- âœ… Stable training with no value explosion
- âœ… Validation data contamination issue identified and fixed
- âœ… Unified architecture (removed obsolete EMA/covariance/contrastive methods)

**Working:**
- âœ… High dimensional diversity (Effective Rank: 12.84/16 = 80.3%)
- âœ… Clean CVFPLayer architecture with LayerNorm
- âœ… Fixed dimension assignment for diversity enforcement
- âœ… Two-phase training pipeline
- âœ… Flexible data loading
- âœ… Gradient clipping for stability

**Next Steps:**
- ðŸŽ¯ Scale to larger datasets (UltraChat)
- ðŸŽ¯ Phase 2 token prediction evaluation
- ðŸŽ¯ Perplexity and generation quality assessment

## License

MIT
