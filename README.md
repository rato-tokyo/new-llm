# New-LLM: Context Vector Fixed-Point Property

A novel language model architecture based on the hypothesis that context vectors converge to fixed points.

## Core Concept: CVFP (Context Vector Fixed-Point Property)

New-LLM explores the idea that meaningful context representations emerge through iterative refinement to fixed points, rather than traditional recurrent or transformer-based approaches.

## Features

- **Two-Phase Training**: Separate fixed-point learning and token prediction
- **Distribution Regularization**: Token-wise normalization using Exponential Moving Average (EMA)
- **Clean Architecture**: Object-oriented design with CVFPLayer encapsulation
- **Flexible Data Loading**: Supports UltraChat, text files, and custom datasets

## Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Basic Training

```bash
# Quick test with 10 tokens (for development)
python3 tests/test_refactored.py

# Full training with configuration
python3 train.py
```

### Configuration

Edit `config.py` to adjust:
- Model architecture (layers, dimensions)
- Training parameters (learning rates, iterations)
- Data sources and preprocessing
- Distribution regularization settings

## Project Structure

```
new-llm/
â”œâ”€â”€ train.py              # Main training script
â”œâ”€â”€ config.py             # Configuration
â”œâ”€â”€ CLAUDE.md             # Design guidelines and architecture decisions
â”œâ”€â”€ CONTEXT.md            # Development history and insights
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ layers.py              # CVFPLayer and CVFPBlock
â”‚   â”‚   â””â”€â”€ new_llm_residual.py    # Main model architecture
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ phase1.py              # Fixed-point learning
â”‚   â”‚   â””â”€â”€ phase2.py              # Token prediction
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ loader.py              # Data loading utilities
â”‚   â””â”€â”€ evaluation/
â”‚       â””â”€â”€ metrics.py             # Analysis and metrics
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_refactored.py         # Quick development test (10 tokens)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ (experimental scripts)     # Future experiments and utilities
â””â”€â”€ data/
    â”œâ”€â”€ example_train.txt          # Example training data
    â””â”€â”€ example_val.txt            # Example validation data
```

## Architecture Highlights

### CVFPLayer (Token-wise Normalization)

Unlike traditional batch normalization, our approach uses Exponential Moving Average (EMA) to track statistics per token:

```python
# Running statistics updated automatically during forward pass
running_mean = 0.99 * running_mean + 0.01 * current_mean
running_var = 0.99 * running_var + 0.01 * current_var
```

Benefits:
- Prevents trivial identity mapping solutions
- Works with any sequence length
- Theoretically correct for sequential processing
- Better gradient flow

### Two-Phase Training

**Phase 1: Fixed-Point Learning**
- Contexts converge through iterative refinement
- Distribution regularization ensures N(0,1) per dimension
- Early stopping based on convergence rate

**Phase 2: Token Prediction**
- Standard next-token prediction
- Uses fixed contexts from Phase 1
- Optional context freezing

## Development Guidelines

See `CLAUDE.md` for:
- Design principles and philosophy
- Token-wise vs batch normalization rationale
- Object-oriented architecture patterns
- Code quality standards

## Current Status

**Working:**
- âœ… Refactored CVFPLayer architecture
- âœ… Token-wise distribution regularization
- âœ… Two-phase training pipeline
- âœ… Flexible data loading

**Under Investigation:**
- âš ï¸ Identity mapping tendency (model preserves input too much)
- âš ï¸ Rapid convergence (2 iterations) - may indicate trivial solutions
- ğŸ”¬ CVFP loss function design

## License

MIT
