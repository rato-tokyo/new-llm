remote: Enumerating objects: 9, done.
remote: Counting objects: 100% (9/9), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)
Unpacking objects: 100% (5/5), 607 bytes | 607.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   f358c04..27ef22a  main       -> origin/main
Updating f358c04..27ef22a
Fast-forward
 src/trainers/phase2.py | 26 +++++++++++++-------------
 1 file changed, 13 insertions(+), 13 deletions(-)
============================================================
SCALING EXPERIMENT
============================================================
Samples: [50, 100, 200, 500] | Model: 6L/768D | Seed: 42
GPU: NVIDIA L4 (22.2GB)

--- 50 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (6289 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   6289 tokens
  Data: 56,602 train / 6,289 val tokens
2025-11-29 10:10:09.096357: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-29 10:10:09.113680: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764411009.134708    6813 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764411009.141084    6813 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764411009.157572    6813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764411009.157602    6813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764411009.157605    6813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764411009.157607    6813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-29 10:10:09.162422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力

[Phase 1] Train (50 samples): 56,602 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0281 [1.2s]
  Iter 3: conv=0% loss=-0.1266 [0.8s]
  Iter 4: conv=0% loss=-0.1890 [0.8s]
  Iter 5: conv=0% loss=-0.2310 [0.7s]
  Iter 6: conv=42% loss=-0.2455 [0.7s]
  Iter 7: conv=100% loss=-0.2509 [0.8s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.2s]
  Train Effective Rank: 578.61/768 (75.3%)
  Val Effective Rank: 550.58/768 (71.7%)
✓ ContextBlock frozen
✓ token_output unfrozen, Embedding frozen
✓ Training TokenBlock only: 45,740,881/91,429,969 parameters

[Phase 2] 56,602 train / 6,289 val tokens, 10 epochs
  Epoch 1: train_ppl=16643.4 val_ppl=2558.5 acc=8.7% [2.1s] ★
  Epoch 2: train_ppl=768.9 val_ppl=1440.7 acc=9.5% [2.1s] ★
  Epoch 3: train_ppl=359.4 val_ppl=1196.4 acc=12.8% [2.2s] ★
  Epoch 4: train_ppl=220.0 val_ppl=1005.1 acc=14.1% [2.2s] ★
  Epoch 5: train_ppl=135.8 val_ppl=892.2 acc=16.2% [2.2s] ★
  Epoch 6: train_ppl=87.9 val_ppl=841.2 acc=16.8% [2.2s] ★
  Epoch 7: train_ppl=59.5 val_ppl=814.8 acc=17.5% [2.2s] ★
  Epoch 8: train_ppl=42.3 val_ppl=787.3 acc=17.8% [2.2s] ★
  Epoch 9: train_ppl=31.5 val_ppl=790.9 acc=18.2% [2.2s]
  → Early stop at epoch 9
  Best: epoch 8, ppl=787.3, acc=17.8%

  ✓ 50 samples: PPL=787.3, Acc=17.8%, ER=71.7%, Time=0.5min

--- 100 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (12279 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   12279 tokens
  Data: 110,516 train / 12,279 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力

[Phase 1] Train (100 samples): 110,516 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0232 [1.6s]
  Iter 3: conv=0% loss=-0.1207 [1.5s]
  Iter 4: conv=0% loss=-0.1808 [1.5s]
  Iter 5: conv=0% loss=-0.2239 [1.5s]
  Iter 6: conv=40% loss=-0.2385 [1.5s]
  Iter 7: conv=100% loss=-0.2441 [1.5s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.3s]
  Train Effective Rank: 581.92/768 (75.8%)
  Val Effective Rank: 562.92/768 (73.3%)
✓ ContextBlock frozen
✓ token_output unfrozen, Embedding frozen
✓ Training TokenBlock only: 45,740,881/91,429,969 parameters

[Phase 2] 110,516 train / 12,279 val tokens, 10 epochs
  Epoch 1: train_ppl=5746.4 val_ppl=1594.7 acc=8.8% [4.3s] ★
  Epoch 2: train_ppl=551.5 val_ppl=1058.7 acc=11.7% [4.4s] ★
  Epoch 3: train_ppl=276.5 val_ppl=822.5 acc=14.3% [4.4s] ★
  Epoch 4: train_ppl=154.1 val_ppl=738.6 acc=15.4% [4.5s] ★
  Epoch 5: train_ppl=95.1 val_ppl=698.9 acc=16.6% [4.4s] ★
  Epoch 6: train_ppl=62.5 val_ppl=664.3 acc=17.8% [4.4s] ★
  Epoch 7: train_ppl=44.1 val_ppl=651.0 acc=18.7% [4.4s] ★
  Epoch 8: train_ppl=32.3 val_ppl=648.8 acc=19.2% [4.4s] ★
  Epoch 9: train_ppl=24.5 val_ppl=684.5 acc=19.3% [4.4s]
  → Early stop at epoch 9
  Best: epoch 8, ppl=648.8, acc=19.2%

  ✓ 100 samples: PPL=648.8, Acc=19.2%, ER=73.3%, Time=1.1min

--- 200 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (24013 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   24013 tokens
  Data: 216,119 train / 24,013 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力

[Phase 1] Train (200 samples): 216,119 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0226 [2.9s]
  Iter 3: conv=0% loss=-0.1206 [2.8s]
  Iter 4: conv=0% loss=-0.1823 [2.7s]
  Iter 5: conv=0% loss=-0.2243 [2.7s]
  Iter 6: conv=42% loss=-0.2392 [2.7s]
  Iter 7: conv=100% loss=-0.2446 [2.7s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [4.5s]
  Train Effective Rank: 586.79/768 (76.4%)
  Val Effective Rank: 567.94/768 (74.0%)
✓ ContextBlock frozen
✓ token_output unfrozen, Embedding frozen
✓ Training TokenBlock only: 45,740,881/91,429,969 parameters

[Phase 2] 216,119 train / 24,013 val tokens, 10 epochs
  Epoch 1: train_ppl=2567.7 val_ppl=779.1 acc=15.6% [8.5s] ★
  Epoch 2: train_ppl=397.3 val_ppl=565.4 acc=18.5% [8.7s] ★
  Epoch 3: train_ppl=194.0 val_ppl=497.9 acc=20.4% [8.7s] ★
  Epoch 4: train_ppl=110.4 val_ppl=482.4 acc=21.1% [8.7s] ★
  Epoch 5: train_ppl=72.7 val_ppl=517.1 acc=21.3% [8.7s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=482.4, acc=21.1%

  ✓ 200 samples: PPL=482.4, Acc=21.1%, ER=74.0%, Time=1.5min

--- 500 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (58797 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   58797 tokens
  Data: 529,173 train / 58,797 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力

[Phase 1] Train (500 samples): 529,173 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0224 [7.2s]
  Iter 3: conv=0% loss=-0.1200 [7.0s]
  Iter 4: conv=0% loss=-0.1804 [7.0s]
  Iter 5: conv=0% loss=-0.2231 [7.0s]
  Iter 6: conv=39% loss=-0.2378 [7.0s]
  Iter 7: conv=100% loss=-0.2433 [7.0s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [11.0s]
  Train Effective Rank: 588.46/768 (76.6%)
  Val Effective Rank: 582.73/768 (75.9%)
✓ ContextBlock frozen
✓ token_output unfrozen, Embedding frozen
✓ Training TokenBlock only: 45,740,881/91,429,969 parameters

[Phase 2] 529,173 train / 58,797 val tokens, 10 epochs
  Epoch 1: train_ppl=1037.7 val_ppl=433.1 acc=15.8% [21.0s] ★
  Epoch 2: train_ppl=219.9 val_ppl=331.7 acc=18.3% [21.0s] ★
  Epoch 3: train_ppl=111.0 val_ppl=324.5 acc=19.0% [20.9s] ★
  Epoch 4: train_ppl=69.8 val_ppl=351.9 acc=19.2% [21.0s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=324.5, acc=19.0%

  ✓ 500 samples: PPL=324.5, Acc=19.0%, ER=75.9%, Time=3.4min

============================================================
RESULTS
============================================================
 Samples     Tokens    Val PPL  Val Acc   Val ER
--------------------------------------------------
      50     56,602      787.3    17.8%    71.7%
     100    110,516      648.8    19.2%    73.3%
     200    216,119      482.4    21.1%    74.0%
     500    529,173      324.5    19.0%    75.9%

Scaling: α=-0.403 (R²=0.992)
Total: 6.8 min
Saved: ./results/unified_scaling/results.json
