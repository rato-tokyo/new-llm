Already up to date.
======================================================================
CONTEXT-KV ATTENTION EXPERIMENT
======================================================================
Samples: 400
Context dim: 256
Chunk size (interval): 20
Num heads: 8
Max contexts: 32
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)
Loading data...
Loading training data...
  Loading from cache: ./cache/ultrachat_400samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 473429 tokens (400 samples)
  Val:   22723 tokens
Data: 473,429 train, 22,723 val tokens
Memory after data load: CPU: 0.8GB, GPU: 0.0GB

Creating ContextKVAttentionLLM (cd=256, heads=8, chunk=20)...
2025-12-03 11:35:05.148602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-03 11:35:05.164923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764761705.183501    5071 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764761705.188882    5071 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764761705.202964    5071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764761705.202991    5071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764761705.202994    5071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764761705.202996    5071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-03 11:35:05.207529: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding
Parameters: 45,163,264 total
Memory after model init: CPU: 1.5GB, GPU: 0.2GB

[Phase 1] Training ContextBlock (cd=256)...

[Phase 1] Context: 473,429 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=5.3976 [1.9s]
  Iter 3: conv=0% loss=6.4529 [1.3s]
  Iter 4: conv=0% loss=5.3798 [1.3s]
  Iter 5: conv=0% loss=3.6661 [1.2s]
  Iter 6: conv=0% loss=2.2661 [1.3s]
  Iter 7: conv=1% loss=1.6417 [1.3s]
  Iter 8: conv=1% loss=1.6784 [1.3s]
  Iter 9: conv=3% loss=1.9345 [1.3s]
  Iter 10: conv=7% loss=2.0713 [1.3s]
  Iter 11: conv=14% loss=1.9719 [1.3s]
  Iter 12: conv=22% loss=1.7204 [1.2s]
  Iter 13: conv=28% loss=1.4871 [1.3s]
  Iter 14: conv=37% loss=1.3792 [1.3s]
  Iter 15: conv=54% loss=1.3748 [1.3s]
  Iter 16: conv=73% loss=1.3912 [1.3s]
  Iter 17: conv=85% loss=1.3718 [1.3s]
  Iter 18: conv=91% loss=1.3056 [1.2s]
  → Early stop: conv 91% >= 90%
  Done: 91% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.0s]
✓ ContextBlock frozen
Memory after Phase 1: CPU: 4.4GB, GPU: 0.2GB

[Phase 2 Prep] Preparing context cache...
Memory after train_result release: CPU: 4.4GB, GPU: 0.2GB
Memory before val cache: CPU: 4.4GB, GPU: 0.2GB
  Collecting val cache (parallel)...
Memory after val cache: CPU: 4.4GB, GPU: 0.2GB
Cache preparation: 1.7s
  Train: torch.Size([473428, 256])
  Val: torch.Size([22722, 256])
  Context interval: 20 (theoretical max: 23672, using max: 32)
  Cache sizes: train_ctx=462MB, train_emb=1387MB, val_ctx=22MB, val_emb=67MB (total=1938MB)
Memory after Phase 2 Prep: CPU: 4.4GB, GPU: 0.2GB
Effective Rank: Val=82.3%

[Phase 2] Training Context-KV Attention...
✓ All 1 ContextBlocks frozen
✓ Embedding frozen
✓ Training 6,301,440/45,163,264 parameters

[Phase 2] 473,428 train / 22,722 val tokens, 40 epochs
    Epoch 1: batch 1/925 (0.0s)...
    Epoch 1: batch 51/925 (ppl=634374.9, acc=6.5%, 2.1s)
    Epoch 1: batch 101/925 (ppl=24727.6, acc=8.4%, 4.2s)
    Epoch 1: batch 151/925 (ppl=8014.8, acc=9.4%, 6.2s)
    Epoch 1: batch 201/925 (ppl=4404.2, acc=10.0%, 8.3s)
    Epoch 1: batch 251/925 (ppl=2973.3, acc=10.5%, 10.4s)
    Epoch 1: batch 301/925 (ppl=2228.2, acc=11.0%, 12.4s)
    Epoch 1: batch 351/925 (ppl=1792.5, acc=11.4%, 14.5s)
    Epoch 1: batch 401/925 (ppl=1500.1, acc=11.7%, 16.6s)
    Epoch 1: batch 451/925 (ppl=1302.4, acc=12.1%, 18.6s)
    Epoch 1: batch 501/925 (ppl=1155.6, acc=12.4%, 20.7s)
    Epoch 1: batch 551/925 (ppl=1041.2, acc=12.7%, 22.8s)
    Epoch 1: batch 601/925 (ppl=957.1, acc=13.0%, 24.8s)
    Epoch 1: batch 651/925 (ppl=886.8, acc=13.2%, 26.9s)
    Epoch 1: batch 701/925 (ppl=825.6, acc=13.4%, 29.0s)
    Epoch 1: batch 751/925 (ppl=776.3, acc=13.6%, 31.0s)
    Epoch 1: batch 801/925 (ppl=736.2, acc=13.8%, 33.1s)
    Epoch 1: batch 851/925 (ppl=699.8, acc=14.0%, 35.2s)
    Epoch 1: batch 901/925 (ppl=669.8, acc=14.1%, 37.2s)
    Epoch 1: batch 925/925 (ppl=654.7, acc=14.2%, 38.2s)
    Epoch 1: train_ppl=654.4 val_ppl=316.4 acc=15.8% [39.3s] *
    Epoch 2: batch 1/925 (0.0s)...
    Epoch 2: batch 51/925 (ppl=280.0, acc=17.1%, 2.1s)
    Epoch 2: batch 101/925 (ppl=275.2, acc=17.3%, 4.1s)
    Epoch 2: batch 151/925 (ppl=271.1, acc=17.4%, 6.2s)
    Epoch 2: batch 201/925 (ppl=272.9, acc=17.5%, 8.3s)
    Epoch 2: batch 251/925 (ppl=270.4, acc=17.5%, 10.4s)
    Epoch 2: batch 301/925 (ppl=266.7, acc=17.7%, 12.4s)
    Epoch 2: batch 351/925 (ppl=264.6, acc=17.7%, 14.5s)
    Epoch 2: batch 401/925 (ppl=262.8, acc=17.7%, 16.6s)
    Epoch 2: batch 451/925 (ppl=261.1, acc=17.7%, 18.6s)
    Epoch 2: batch 501/925 (ppl=258.0, acc=17.8%, 20.7s)
    Epoch 2: batch 551/925 (ppl=255.4, acc=17.9%, 22.8s)
    Epoch 2: batch 601/925 (ppl=253.0, acc=18.0%, 24.8s)
    Epoch 2: batch 651/925 (ppl=251.3, acc=18.1%, 26.9s)
    Epoch 2: batch 701/925 (ppl=249.1, acc=18.1%, 29.0s)
    Epoch 2: batch 751/925 (ppl=247.3, acc=18.1%, 31.1s)
    Epoch 2: batch 801/925 (ppl=244.6, acc=18.3%, 33.2s)
    Epoch 2: batch 851/925 (ppl=242.6, acc=18.3%, 35.2s)
    Epoch 2: batch 901/925 (ppl=240.7, acc=18.4%, 37.3s)
    Epoch 2: batch 925/925 (ppl=239.9, acc=18.4%, 38.3s)
    Epoch 2: train_ppl=239.9 val_ppl=248.2 acc=17.2% [39.4s] *
    Epoch 3: batch 1/925 (0.0s)...
    Epoch 3: batch 51/925 (ppl=173.8, acc=20.2%, 2.1s)
    Epoch 3: batch 101/925 (ppl=176.7, acc=20.2%, 4.2s)
    Epoch 3: batch 151/925 (ppl=177.0, acc=20.4%, 6.3s)
    Epoch 3: batch 201/925 (ppl=174.9, acc=20.5%, 8.3s)
    Epoch 3: batch 251/925 (ppl=175.0, acc=20.5%, 10.4s)
    Epoch 3: batch 301/925 (ppl=173.8, acc=20.6%, 12.5s)
    Epoch 3: batch 351/925 (ppl=173.9, acc=20.6%, 14.6s)
    Epoch 3: batch 401/925 (ppl=174.1, acc=20.6%, 16.7s)
    Epoch 3: batch 451/925 (ppl=172.7, acc=20.6%, 18.7s)
    Epoch 3: batch 501/925 (ppl=171.0, acc=20.8%, 20.8s)
    Epoch 3: batch 551/925 (ppl=170.2, acc=20.8%, 22.9s)
    Epoch 3: batch 601/925 (ppl=169.8, acc=20.9%, 25.0s)
    Epoch 3: batch 651/925 (ppl=169.1, acc=20.9%, 27.1s)
    Epoch 3: batch 701/925 (ppl=168.4, acc=20.9%, 29.1s)
    Epoch 3: batch 751/925 (ppl=167.3, acc=21.0%, 31.2s)
    Epoch 3: batch 801/925 (ppl=166.6, acc=21.0%, 33.3s)
    Epoch 3: batch 851/925 (ppl=165.7, acc=21.1%, 35.4s)
    Epoch 3: batch 901/925 (ppl=164.9, acc=21.1%, 37.5s)
    Epoch 3: batch 925/925 (ppl=164.4, acc=21.1%, 38.5s)
    Epoch 3: train_ppl=164.4 val_ppl=212.5 acc=18.6% [39.6s] *
    Epoch 4: batch 1/925 (0.0s)...
    Epoch 4: batch 51/925 (ppl=114.5, acc=23.9%, 2.1s)
    Epoch 4: batch 101/925 (ppl=115.3, acc=23.8%, 4.2s)
    Epoch 4: batch 151/925 (ppl=116.8, acc=23.8%, 6.3s)
    Epoch 4: batch 201/925 (ppl=116.8, acc=23.9%, 8.4s)
    Epoch 4: batch 251/925 (ppl=117.6, acc=23.8%, 10.4s)
    Epoch 4: batch 301/925 (ppl=118.3, acc=23.7%, 12.5s)
    Epoch 4: batch 351/925 (ppl=118.7, acc=23.7%, 14.6s)
    Epoch 4: batch 401/925 (ppl=118.5, acc=23.7%, 16.7s)
    Epoch 4: batch 451/925 (ppl=117.9, acc=23.8%, 18.8s)
    Epoch 4: batch 501/925 (ppl=117.8, acc=23.7%, 20.9s)
    Epoch 4: batch 551/925 (ppl=118.0, acc=23.8%, 23.0s)
    Epoch 4: batch 601/925 (ppl=118.0, acc=23.8%, 25.1s)
    Epoch 4: batch 651/925 (ppl=117.7, acc=23.8%, 27.2s)
    Epoch 4: batch 701/925 (ppl=117.4, acc=23.8%, 29.3s)
    Epoch 4: batch 751/925 (ppl=116.9, acc=23.8%, 31.4s)
    Epoch 4: batch 801/925 (ppl=116.8, acc=23.8%, 33.5s)
    Epoch 4: batch 851/925 (ppl=116.3, acc=23.9%, 35.6s)
    Epoch 4: batch 901/925 (ppl=115.9, acc=23.9%, 37.6s)
    Epoch 4: batch 925/925 (ppl=115.8, acc=23.9%, 38.7s)
    Epoch 4: train_ppl=115.9 val_ppl=201.1 acc=19.4% [39.7s] *
    Epoch 5: batch 1/925 (0.0s)...
    Epoch 5: batch 51/925 (ppl=78.1, acc=26.8%, 2.1s)
    Epoch 5: batch 101/925 (ppl=79.4, acc=26.6%, 4.2s)
    Epoch 5: batch 151/925 (ppl=79.9, acc=26.6%, 6.3s)
    Epoch 5: batch 201/925 (ppl=80.2, acc=26.7%, 8.4s)
    Epoch 5: batch 251/925 (ppl=80.6, acc=26.7%, 10.5s)
    Epoch 5: batch 301/925 (ppl=81.0, acc=26.7%, 12.6s)
    Epoch 5: batch 351/925 (ppl=81.8, acc=26.7%, 14.8s)
    Epoch 5: batch 401/925 (ppl=82.3, acc=26.6%, 16.8s)
    Epoch 5: batch 451/925 (ppl=82.5, acc=26.6%, 18.9s)
    Epoch 5: batch 501/925 (ppl=82.8, acc=26.5%, 21.0s)
    Epoch 5: batch 551/925 (ppl=83.2, acc=26.5%, 23.1s)
    Epoch 5: batch 601/925 (ppl=83.4, acc=26.5%, 25.2s)
    Epoch 5: batch 651/925 (ppl=83.4, acc=26.5%, 27.3s)
    Epoch 5: batch 701/925 (ppl=83.7, acc=26.5%, 29.4s)
    Epoch 5: batch 751/925 (ppl=83.7, acc=26.5%, 31.5s)
    Epoch 5: batch 801/925 (ppl=84.0, acc=26.5%, 33.6s)
    Epoch 5: batch 851/925 (ppl=84.0, acc=26.5%, 35.7s)
    Epoch 5: batch 901/925 (ppl=84.1, acc=26.5%, 37.8s)
    Epoch 5: batch 925/925 (ppl=84.0, acc=26.6%, 38.8s)
    Epoch 5: train_ppl=84.0 val_ppl=194.9 acc=19.4% [39.9s] *
    Epoch 6: batch 1/925 (0.0s)...
    Epoch 6: batch 51/925 (ppl=53.6, acc=30.3%, 2.1s)
    Epoch 6: batch 101/925 (ppl=54.8, acc=30.1%, 4.2s)
    Epoch 6: batch 151/925 (ppl=55.8, acc=30.0%, 6.3s)
    Epoch 6: batch 201/925 (ppl=56.6, acc=29.9%, 8.4s)
    Epoch 6: batch 251/925 (ppl=57.4, acc=29.7%, 10.5s)
    Epoch 6: batch 301/925 (ppl=57.7, acc=29.6%, 12.5s)
    Epoch 6: batch 351/925 (ppl=58.5, acc=29.5%, 14.6s)
    Epoch 6: batch 401/925 (ppl=59.3, acc=29.4%, 16.7s)
    Epoch 6: batch 451/925 (ppl=59.9, acc=29.3%, 18.8s)
    Epoch 6: batch 501/925 (ppl=60.0, acc=29.3%, 20.9s)
    Epoch 6: batch 551/925 (ppl=60.2, acc=29.3%, 23.0s)
    Epoch 6: batch 601/925 (ppl=60.5, acc=29.4%, 25.1s)
    Epoch 6: batch 651/925 (ppl=60.9, acc=29.3%, 27.2s)
    Epoch 6: batch 701/925 (ppl=61.3, acc=29.3%, 29.3s)
    Epoch 6: batch 751/925 (ppl=61.6, acc=29.3%, 31.4s)
    Epoch 6: batch 801/925 (ppl=61.7, acc=29.3%, 33.5s)
    Epoch 6: batch 851/925 (ppl=61.8, acc=29.2%, 35.6s)
    Epoch 6: batch 901/925 (ppl=62.1, acc=29.2%, 37.7s)
    Epoch 6: batch 925/925 (ppl=62.1, acc=29.2%, 38.7s)
    Epoch 6: train_ppl=62.1 val_ppl=204.6 acc=19.9% [39.8s]
    Epoch 7: batch 1/925 (0.0s)...
    Epoch 7: batch 51/925 (ppl=38.7, acc=34.0%, 2.1s)
    Epoch 7: batch 101/925 (ppl=39.0, acc=33.7%, 4.2s)
    Epoch 7: batch 151/925 (ppl=39.7, acc=33.5%, 6.3s)
    Epoch 7: batch 201/925 (ppl=40.6, acc=33.2%, 8.4s)
