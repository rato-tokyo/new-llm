remote: Enumerating objects: 11, done.
remote: Counting objects: 100% (11/11), done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 6 (delta 4), reused 6 (delta 4), pack-reused 0 (from 0)
Unpacking objects: 100% (6/6), 765 bytes | 382.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   77c2591..e25d83d  main       -> origin/main
Updating 77c2591..e25d83d
Fast-forward
 src/trainers/phase1/memory.py | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)
================================================================================
LAYER CONFIGURATION COMPARISON EXPERIMENT
================================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Samples: 500
Context dim: 500
Output: importants/logs/20251201_144712_layer_comparison

Configurations to compare:
  - L1_F1: layer=1, fnn=1 (baseline)
  - L1_F2: layer=1, fnn=2 (FFN deepened)
  - L2_F1: layer=2, fnn=1 (layer added)
================================================================================

[L1_F1] layer=1, fnn=1 (baseline)
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
    Data: 587,970 train, 31,024 val tokens
2025-12-01 14:47:14.588689: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 14:47:14.603818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764600434.624382    3424 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764600434.630680    3424 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764600434.646613    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764600434.646641    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764600434.646645    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764600434.646649    3424 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 14:47:14.651441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 40,210,540 params (layers=1, fnn=1)

[Phase 1] OACD: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2640 [3.3s]
  Iter 3: conv=0% loss=13.1792 [2.8s]
  Iter 4: conv=0% loss=10.6700 [2.3s]
  Iter 5: conv=0% loss=7.2443 [2.3s]
    Val ER: 82.0%
  Iter 6: conv=0% loss=4.6400 [2.4s]
  Iter 7: conv=0% loss=3.4187 [2.4s]
  Iter 8: conv=1% loss=3.0906 [2.4s]
  Iter 9: conv=1% loss=3.0127 [2.4s]
  Iter 10: conv=1% loss=2.9479 [2.4s]
    Val ER: 80.2%
  → Val early stop: ER not improving for 1 checks
  Done: 1% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.9s]
    Phase 1: 35.9s, 10 iter, ER=83.8%/82.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=7666.3 val_ppl=636.2 acc=17.0% [13.2s] ★
  Epoch 2: train_ppl=479.9 val_ppl=376.1 acc=18.5% [13.4s] ★
  Epoch 3: train_ppl=307.1 val_ppl=299.7 acc=19.5% [13.6s] ★
  Epoch 4: train_ppl=238.2 val_ppl=263.7 acc=20.3% [13.9s] ★
  Epoch 5: train_ppl=199.4 val_ppl=242.9 acc=20.8% [14.0s] ★
  Epoch 6: train_ppl=173.7 val_ppl=229.7 acc=21.1% [14.0s] ★
  Epoch 7: train_ppl=155.2 val_ppl=221.1 acc=21.3% [13.8s] ★
  Epoch 8: train_ppl=141.2 val_ppl=215.1 acc=21.5% [13.9s] ★
  Epoch 9: train_ppl=130.1 val_ppl=211.1 acc=21.7% [13.8s] ★
  Epoch 10: train_ppl=121.0 val_ppl=208.5 acc=21.9% [13.9s] ★
  Epoch 11: train_ppl=113.5 val_ppl=206.9 acc=21.9% [13.9s] ★
  Epoch 12: train_ppl=107.2 val_ppl=206.0 acc=22.0% [14.0s] ★
  Epoch 13: train_ppl=101.7 val_ppl=205.8 acc=22.1% [14.0s] ★
  Epoch 14: train_ppl=97.0 val_ppl=206.0 acc=22.2% [14.0s]
  → Early stop at epoch 14
  Best: epoch 13, ppl=205.8, acc=22.1%
    Phase 2: 193.5s, PPL=205.8, Acc=22.1%

[L1_F2] layer=1, fnn=2 (FFN deepened)
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
    Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 43,428,724 params (layers=1, fnn=2)

[Phase 1] OACD: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=17.6509 [2.5s]
  Iter 3: conv=0% loss=13.1665 [2.5s]
  Iter 4: conv=0% loss=9.0031 [2.5s]
  Iter 5: conv=0% loss=8.7487 [2.5s]
    Val ER: 76.3%
  Iter 6: conv=0% loss=9.2060 [2.5s]
  Iter 7: conv=0% loss=8.7084 [2.5s]
  Iter 8: conv=0% loss=7.5860 [2.5s]
  Iter 9: conv=0% loss=6.7310 [2.4s]
  Iter 10: conv=0% loss=5.9002 [2.5s]
    Val ER: 75.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.5s]
    Phase 1: 36.7s, 10 iter, ER=78.8%/76.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,586,756/43,428,724 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=985.1 val_ppl=386.1 acc=18.1% [14.8s] ★
  Epoch 2: train_ppl=314.9 val_ppl=299.4 acc=19.4% [14.8s] ★
  Epoch 3: train_ppl=222.1 val_ppl=269.5 acc=20.1% [14.7s] ★
  Epoch 4: train_ppl=173.6 val_ppl=256.6 acc=20.7% [14.6s] ★
  Epoch 5: train_ppl=142.9 val_ppl=252.6 acc=20.9% [14.5s] ★
  Epoch 6: train_ppl=121.0 val_ppl=253.7 acc=21.1% [14.5s]
  → Early stop at epoch 6
  Best: epoch 5, ppl=252.6, acc=20.9%
    Phase 2: 87.8s, PPL=252.6, Acc=20.9%

[L2_F1] layer=2, fnn=1 (layer added)
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
    Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 41,822,168 params (layers=2, fnn=1)

[Phase 1] OACD: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=10.9541 [2.3s]
  Iter 3: conv=0% loss=7.7263 [2.2s]
  Iter 4: conv=0% loss=3.4605 [2.1s]
  Iter 5: conv=0% loss=1.9626 [2.1s]
    Val ER: 81.1%
  Iter 6: conv=0% loss=1.8639 [2.1s]
  Iter 7: conv=2% loss=1.7660 [2.1s]
  Iter 8: conv=4% loss=1.4989 [2.1s]
  Iter 9: conv=10% loss=1.3076 [2.1s]
  Iter 10: conv=27% loss=1.2462 [2.1s]
    Val ER: 79.7%
  → Val early stop: ER not improving for 1 checks
  Done: 27% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [4.4s]
    Phase 1: 36.7s, 10 iter, ER=82.2%/81.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,953,792/41,822,168 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1154.9 val_ppl=404.4 acc=18.2% [14.7s] ★
  Epoch 2: train_ppl=324.5 val_ppl=298.6 acc=19.4% [14.8s] ★
  Epoch 3: train_ppl=225.0 val_ppl=260.6 acc=20.5% [14.6s] ★
  Epoch 4: train_ppl=174.9 val_ppl=242.3 acc=21.1% [14.5s] ★
  Epoch 5: train_ppl=143.8 val_ppl=232.7 acc=21.5% [14.4s] ★
  Epoch 6: train_ppl=122.2 val_ppl=228.6 acc=21.7% [14.4s] ★
  Epoch 7: train_ppl=106.0 val_ppl=227.6 acc=21.8% [14.7s] ★
  Epoch 8: train_ppl=93.4 val_ppl=228.9 acc=21.9% [14.8s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=227.6, acc=21.8%
    Phase 2: 116.9s, PPL=227.6, Acc=21.8%

================================================================================
SUMMARY
================================================================================

Config     Layers   FFN    Params       Val PPL    Acc      ER%      Time      
--------------------------------------------------------------------------------
L1_F1      1        1      40,210,540  205.8      22.1    % 81.2     229.3     s
L1_F2      1        2      43,428,724  252.6      20.9    % 76.7     124.5     s
L2_F1      2        1      41,822,168  227.6      21.8    % 80.8     153.5     s

--------------------------------------------------------------------------------

Best PPL:  L1_F1 (PPL=205.8)
Best Acc:  L1_F1 (Acc=22.1%)

Total time: 561.6s (9.4 min)

Results saved to: importants/logs/20251201_144712_layer_comparison/results.txt

================================================================================
DONE
================================================================================
