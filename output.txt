remote: Enumerating objects: 28, done.
remote: Counting objects: 100% (28/28), done.
remote: Compressing objects: 100% (9/9), done.
remote: Total 19 (delta 15), reused 14 (delta 10), pack-reused 0 (from 0)
Unpacking objects: 100% (19/19), 4.41 KiB | 282.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   3c1d295..52874c0  main       -> origin/main
Updating 3c1d295..52874c0
Fast-forward
 CLAUDE.md                             | 100 +++++-----------
 output.txt                            | 216 +++++++++++++++++++---------------
 scripts/experiment_cascade_context.py |  27 +----
 src/trainers/phase1/memory.py         |  75 ++----------
 4 files changed, 163 insertions(+), 255 deletions(-)
======================================================================
DUAL CONTEXT EXPERIMENT
======================================================================
Samples: 2000
Context dim per block: 500
Combined context dim: 1000
Output: importants/logs/20251202_135540_dual_context
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)
Loading data...
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
Data: 2,403,563 train, 22,723 val tokens
Split: A=1,201,781, B=1,201,781 tokens

Creating CascadeContextLLM (cd=500x2=1000)...
2025-12-02 13:55:43.392771: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-02 13:55:43.407992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764683743.428770   93972 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764683743.435177   93972 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764683743.451243   93972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764683743.451273   93972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764683743.451276   93972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764683743.451279   93972 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-02 13:55:43.456315: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding
Parameters: 41,230,040 total
  ContextBlocks (2): 1,271,000
    Block 0: 635,500
    Block 1: 635,500
  TokenBlock: 1,360,128

[Phase 1A] Training ContextBlock A on first half...

[Phase 1] ContextA: 1,201,782 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2763 [6.6s]
  Iter 3: conv=0% loss=13.1935 [6.1s]
  Iter 4: conv=0% loss=10.6984 [5.0s]
  Iter 5: conv=0% loss=7.2667 [5.1s]
  Iter 6: conv=0% loss=4.6423 [5.1s]
  Iter 7: conv=0% loss=3.4091 [5.1s]
  Iter 8: conv=0% loss=3.0891 [5.1s]
  Iter 9: conv=0% loss=3.0212 [5.1s]
  Iter 10: conv=1% loss=2.9584 [5.1s]
  Iter 11: conv=1% loss=2.8995 [5.2s]
  Iter 12: conv=1% loss=2.8369 [5.1s]
  Iter 13: conv=2% loss=2.7363 [5.1s]
  Iter 14: conv=3% loss=2.5687 [5.0s]
  Iter 15: conv=5% loss=2.3403 [5.1s]
  Iter 16: conv=8% loss=2.1033 [5.1s]
  Iter 17: conv=11% loss=1.9120 [5.0s]
  Iter 18: conv=17% loss=1.7692 [5.1s]
  Iter 19: conv=25% loss=1.6510 [5.0s]
  Iter 20: conv=33% loss=1.5469 [5.1s]
  Iter 21: conv=43% loss=1.4549 [5.0s]
  Iter 22: conv=53% loss=1.3674 [5.2s]
  Iter 23: conv=62% loss=1.2780 [5.1s]
  Iter 24: conv=69% loss=1.1922 [5.1s]
  Iter 25: conv=74% loss=1.1194 [5.1s]
  Iter 26: conv=79% loss=1.0628 [5.0s]
  Iter 27: conv=84% loss=1.0208 [5.1s]
  Iter 28: conv=87% loss=0.9951 [5.1s]
  Iter 29: conv=90% loss=0.9862 [5.1s]
  Iter 30: conv=92% loss=0.9849 [5.1s]
  → Early stop: conv 92% >= 90%
  Done: 92% converged
  Collecting cache (parallel)...
    Preparing combined tokens (1,201,781 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [2.9s]
Phase 1A: 163.4s, 30 iter, conv=92%
✓ ContextBlock A frozen

[Phase 1B] Training ContextBlock B on second half...

[Phase 1] ContextB: 1,201,782 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2812 [5.2s]
  Iter 3: conv=0% loss=12.4629 [5.1s]
  Iter 4: conv=0% loss=9.7826 [5.1s]
  Iter 5: conv=0% loss=6.4455 [5.2s]
  Iter 6: conv=0% loss=4.2491 [5.0s]
  Iter 7: conv=0% loss=3.1795 [5.1s]
  Iter 8: conv=0% loss=2.7425 [5.0s]
  Iter 9: conv=0% loss=2.6633 [5.1s]
  Iter 10: conv=0% loss=2.7222 [5.1s]
  Iter 11: conv=0% loss=2.6922 [5.1s]
  Iter 12: conv=0% loss=2.4701 [5.1s]
  Iter 13: conv=1% loss=2.1442 [5.1s]
  Iter 14: conv=2% loss=1.8758 [5.1s]
  Iter 15: conv=3% loss=1.7583 [5.1s]
  Iter 16: conv=6% loss=1.7727 [5.1s]
  Iter 17: conv=10% loss=1.8318 [5.1s]
  Iter 18: conv=16% loss=1.8458 [5.1s]
  Iter 19: conv=25% loss=1.7847 [5.1s]
  Iter 20: conv=34% loss=1.6816 [5.1s]
  Iter 21: conv=43% loss=1.5835 [5.1s]
  Iter 22: conv=53% loss=1.5084 [5.2s]
  Iter 23: conv=63% loss=1.4408 [5.1s]
  Iter 24: conv=72% loss=1.3518 [5.1s]
  Iter 25: conv=79% loss=1.2282 [5.1s]
  Iter 26: conv=83% loss=1.0915 [5.1s]
  Iter 27: conv=86% loss=0.9869 [5.2s]
  Iter 28: conv=89% loss=0.9370 [5.0s]
  Iter 29: conv=92% loss=0.9240 [5.1s]
  → Early stop: conv 92% >= 90%
  Done: 92% converged
  Collecting cache (parallel)...
    Preparing combined tokens (1,201,781 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [2.9s]
Phase 1B: 156.2s, 29 iter, conv=92%
✓ ContextBlock B frozen

[Phase 2 Prep] Collecting dual context cache on full data...
    Collecting dual context cache (2,403,562 tokens)...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
    Dual context cache collected [1057.7s]
    Collecting dual context cache (22,722 tokens)...
    Dual context cache collected [10.0s]
Cache collection: 1068.1s
  Train cache: torch.Size([2403562, 1000])
  Val cache: torch.Size([22722, 1000])
Effective Rank: Train=75.3%, Val=73.6%

[Phase 2] Training TokenBlock with concatenated context (cd=1000)...
✓ All 2 ContextBlocks frozen
✓ Embedding frozen
✓ Training TokenBlock only: 1,360,128/41,230,040 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 20 epochs
  Epoch 1: train_ppl=332.6 val_ppl=170.9 acc=22.1% [56.3s] ★
  Epoch 2: train_ppl=163.5 val_ppl=143.1 acc=23.4% [56.1s] ★
  Epoch 3: train_ppl=135.3 val_ppl=133.0 acc=24.1% [56.3s] ★
  Epoch 4: train_ppl=121.2 val_ppl=128.0 acc=24.4% [56.2s] ★
  Epoch 5: train_ppl=112.6 val_ppl=125.1 acc=24.6% [56.4s] ★
  Epoch 6: train_ppl=106.6 val_ppl=123.2 acc=24.7% [56.3s] ★
  Epoch 7: train_ppl=102.2 val_ppl=121.9 acc=24.8% [56.4s] ★
  Epoch 8: train_ppl=98.7 val_ppl=121.0 acc=25.0% [56.4s] ★
  Epoch 9: train_ppl=95.9 val_ppl=120.3 acc=25.2% [56.4s] ★
  Epoch 10: train_ppl=93.6 val_ppl=119.8 acc=25.3% [56.3s] ★
  Epoch 11: train_ppl=91.6 val_ppl=119.5 acc=25.4% [56.1s] ★
  → Early stop at epoch 11 (PPL improvement 0.33 < 0.4)
  Best: epoch 11, ppl=119.5, acc=25.4%

Phase 2: 619.4s, Best epoch 11
Result: PPL=119.5, Acc=25.4%
Total time: 2007.1s

======================================================================
SUMMARY - Dual Context Experiment
======================================================================
Architecture: DualContextLLM (A+B, 1L each)
  ContextBlock A: cd=500, trained on first 1,201,781 tokens
  ContextBlock B: cd=500, trained on last 1,201,782 tokens
  TokenBlock: cd=1000 (concatenated)
Parameters: 41,230,040
Phase 1A: 163.4s, conv=92%
Phase 1B: 156.2s, conv=92%
Cache collection: 1068.1s
Phase 2: 619.4s, epoch 11
Effective Rank: 73.6% (of 1000)
Val PPL: 119.5
Val Acc: 25.4%
Total time: 2007.1s
======================================================================

Results saved to: importants/logs/20251202_135540_dual_context/results.txt

======================================================================
DONE
======================================================================
