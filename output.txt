remote: Enumerating objects: 7, done.
remote: Counting objects: 100% (7/7), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)
Unpacking objects: 100% (4/4), 603 bytes | 603.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   bf03e6d..f2dce15  main       -> origin/main
Updating bf03e6d..f2dce15
Fast-forward
 scripts/experiment_ka_comparison.py | 4 ++++
 1 file changed, 4 insertions(+)
Device: cuda (NVIDIA L4, 23.8GB)
======================================================================
KA-ATTENTION vs PYTHIA COMPARISON
======================================================================
Samples: 10,000
Sequence length: 128
Epochs: 10
Learning rate: 0.0001
======================================================================

Architecture comparison:
  Pythia:      hidden_size=512, intermediate=2048
  KA-Pythia:   hidden_size=512, intermediate=2048
  (Same architecture, different attention mechanism)
======================================================================

[Data] Loading Pile data...
Preparing data: 10,000 samples, seq_len=128
Downloading Pile dataset: 1,280,128 tokens
  Loading tokenizer: EleutherAI/pythia-70m
tokenizer_config.json: 100% 396/396 [00:00<00:00, 2.64MB/s]
tokenizer.json: 2.11MB [00:00, 130MB/s]
special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 1.15MB/s]
  Loading dataset (streaming)...
README.md: 100% 776/776 [00:00<00:00, 8.14MB/s]
Resolving data files: 100% 30/30 [00:00<00:00, 107.87it/s]
  Tokenizing...
  Saved 1,280,128 tokens to cache: cache/pile_tokens/pile_1280128.pt
  Train: 9,000 samples
  Val: 1,000 samples

======================================================================
1. PYTHIA-70M (Baseline)
======================================================================

[Pythia-70M] Training...
  Trainable: 70,426,624 / 70,426,624 parameters
  Epoch  1: train_ppl=1255.6 val_ppl=875.6 [50.4s] *
  Epoch  2: train_ppl=275.4 val_ppl=597.4 [51.8s] *
  Epoch  3: train_ppl=150.1 val_ppl=499.0 [53.4s] *
  Epoch  4: train_ppl=97.3 val_ppl=452.4 [52.7s] *
  Epoch  5: train_ppl=68.2 val_ppl=442.2 [53.0s] *
  Epoch  6: train_ppl=49.6 val_ppl=452.8 [53.1s]
  Epoch  7: train_ppl=36.9 val_ppl=472.6 [52.9s]
  Epoch  8: train_ppl=27.6 val_ppl=531.2 [53.0s]
  → Early stop: val_ppl worsened for 3 epochs
  Best: epoch 5, ppl=442.2

======================================================================
2. KA-PYTHIA (KA-Attention)
======================================================================

[KA-Pythia] Training...
  Trainable: 70,426,624 / 70,426,624 parameters
  Epoch  1: train_ppl=1380.0 val_ppl=949.3 [247.9s] *
  Epoch  2: train_ppl=312.5 val_ppl=642.9 [246.8s] *
  Epoch  3: train_ppl=170.2 val_ppl=533.1 [247.5s] *
  Epoch  4: train_ppl=111.7 val_ppl=475.7 [248.3s] *
  Epoch  5: train_ppl=80.0 val_ppl=453.6 [247.5s] *
  Epoch  6: train_ppl=59.5 val_ppl=449.3 [246.7s] *
  Epoch  7: train_ppl=45.5 val_ppl=454.5 [247.0s]
  Epoch  8: train_ppl=35.4 val_ppl=481.4 [246.6s]
  Epoch  9: train_ppl=27.8 val_ppl=507.6 [246.8s]
  → Early stop: val_ppl worsened for 3 epochs
  Best: epoch 6, ppl=449.3

======================================================================
SUMMARY
======================================================================
  Pythia: val_ppl=442.2
  KA-Pythia: val_ppl=449.3

  Difference: +7.1 ppl (+1.6%)
