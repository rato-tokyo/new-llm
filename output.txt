remote: Enumerating objects: 9, done.
remote: Counting objects: 100% (9/9), done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 5 (delta 3), reused 5 (delta 3), pack-reused 0 (from 0)
Unpacking objects: 100% (5/5), 974 bytes | 974.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   b426d85..d93e4fa  main       -> origin/main
Updating b426d85..d93e4fa
Fast-forward
 src/models/pretrained_mka.py | 22 ++++++++++++----------
 1 file changed, 12 insertions(+), 10 deletions(-)
2025-12-04 08:38:07.379129: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-04 08:38:07.396509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764837487.417722   23929 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764837487.424133   23929 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764837487.440511   23929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764837487.440544   23929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764837487.440547   23929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764837487.440550   23929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-04 08:38:07.445283: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Device: cuda (NVIDIA L4, 23.8GB)
======================================================================
PRETRAINED MKA-ATTENTION EXPERIMENT
======================================================================
Samples: 10,000
Sequence length: 128
Epochs: 10
Learning rate: 0.0001
Eval only: False
======================================================================

Architecture:
  Pythia-70M (pretrained): Frozen baseline
  PretrainedMKA V2: Pythia frozen + KA attention trainable
======================================================================

[Data] Loading Pile data...
Preparing data: 10,000 samples, seq_len=128
Loading cached tokens: cache/pile_tokens/pile_1280128.pt
  Loaded 1,280,128 tokens from cache
  Train: 9,000 samples
  Val: 1,000 samples

======================================================================
1. PYTHIA-70M (Pretrained, Frozen)
======================================================================

[Pythia-70M] Evaluating pretrained model...
  Pretrained val_ppl: 26.6

======================================================================
2. PRETRAINED-MKA V2 (Pythia frozen + KA trainable)
======================================================================
  Total parameters: 71,215,616
  Trainable: 788,992
  Frozen: 70,426,624

[PretrainedMKA V2] Training...
  Epoch  1: train_ppl=434.5 val_ppl=303.1 [27.5s] *
  Epoch  2: train_ppl=193.8 val_ppl=167.4 [28.4s] *
  Epoch  3: train_ppl=119.7 val_ppl=114.2 [28.7s] *
  Epoch  4: train_ppl=87.7 val_ppl=89.0 [28.5s] *
  Epoch  5: train_ppl=69.7 val_ppl=74.1 [28.4s] *
  Epoch  6: train_ppl=58.3 val_ppl=63.2 [28.4s] *
  Epoch  7: train_ppl=50.6 val_ppl=57.3 [28.4s] *
  Epoch  8: train_ppl=45.0 val_ppl=51.1 [28.4s] *
  Epoch  9: train_ppl=40.9 val_ppl=47.3 [28.5s] *
  Epoch 10: train_ppl=37.8 val_ppl=44.1 [28.4s] *
  Best: epoch 10, ppl=44.1

======================================================================
SUMMARY
======================================================================

| Model | PPL | Note |
|-------|-----|------|
| Pythia-70M (pretrained) | 26.6 | frozen baseline |
| PretrainedMKA V2 | 44.1 | epoch 10 |

Difference: +17.4 ppl

DONE
