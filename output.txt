Already up to date.
======================================================================
CONTEXT-KV ATTENTION EXPERIMENT
======================================================================
Samples: 500
Context dim: 256
Chunk size (interval): 20
Num heads: 8
Max contexts: 32
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)
Loading data...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 188kB/s]
config.json: 100% 665/665 [00:00<00:00, 5.05MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 3.93MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 1.04MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 2.10MB/s]
Loading training data...
  Loading 500 samples from UltraChat...
README.md: 3.90kB [00:00, 6.29MB/s]
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:04<00:00, 60.7MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:01<00:00, 204MB/s]
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:01<00:00, 211MB/s]
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:02<00:00, 29.7MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:01<00:00, 204MB/s]
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:03<00:00, 74.4MB/s]
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:01<00:00, 189MB/s]
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:02<00:00, 30.9MB/s]
Generating train_sft split: 100% 207865/207865 [00:03<00:00, 52471.27 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 52731.93 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 69703.84 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 73357.70 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_500samples_full.pt
Loading validation data...
  Validation file not found, generating: ./cache/example_val.txt
  Generated 20 validation samples (indices 50000-50019)
  Train: 587970 tokens (500 samples)
  Val:   22723 tokens
Data: 587,970 train, 22,723 val tokens
Memory after data load: CPU: 1.2GB, GPU: 0.0GB

Creating ContextKVAttentionLLM (cd=256, heads=8, chunk=20)...
2025-12-03 11:24:01.752078: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-03 11:24:01.768404: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764761041.786992    1851 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764761041.792370    1851 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764761041.806225    1851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764761041.806256    1851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764761041.806259    1851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764761041.806278    1851 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-03 11:24:01.811047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 5.69MB/s]
model.safetensors: 100% 548M/548M [00:02<00:00, 198MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding
Parameters: 45,163,264 total
Memory after model init: CPU: 2.1GB, GPU: 0.2GB

[Phase 1] Training ContextBlock (cd=256)...

[Phase 1] Context: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=5.4113 [2.9s]
  Iter 3: conv=0% loss=6.4735 [1.7s]
  Iter 4: conv=0% loss=5.3983 [1.7s]
  Iter 5: conv=0% loss=3.6782 [1.6s]
  Iter 6: conv=0% loss=2.2666 [1.6s]
  Iter 7: conv=0% loss=1.6356 [1.5s]
  Iter 8: conv=1% loss=1.6766 [1.6s]
  Iter 9: conv=2% loss=1.9403 [1.6s]
  Iter 10: conv=6% loss=2.0809 [1.6s]
  Iter 11: conv=14% loss=1.9790 [1.6s]
  Iter 12: conv=21% loss=1.7216 [1.6s]
  Iter 13: conv=27% loss=1.4829 [1.5s]
  Iter 14: conv=37% loss=1.3730 [1.6s]
  Iter 15: conv=54% loss=1.3697 [1.6s]
  Iter 16: conv=73% loss=1.3865 [1.5s]
  Iter 17: conv=85% loss=1.3648 [1.6s]
  Iter 18: conv=91% loss=1.2950 [1.6s]
  → Early stop: conv 91% >= 90%
  Done: 91% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.1s]
✓ ContextBlock frozen
Memory after Phase 1: CPU: 5.6GB, GPU: 0.2GB

[Phase 2 Prep] Preparing context cache...
Memory after train_result release: CPU: 5.6GB, GPU: 0.2GB
Memory before val cache: CPU: 5.6GB, GPU: 0.2GB
  Collecting val cache (parallel)...
Memory after val cache: CPU: 5.6GB, GPU: 0.2GB
Cache preparation: 1.7s
  Train: torch.Size([587969, 256])
  Val: torch.Size([22722, 256])
  Context interval: 20 (theoretical max: 29399, using max: 32)
  Cache sizes: train_ctx=574MB, train_emb=1723MB, val_ctx=22MB, val_emb=67MB (total=2386MB)
Memory after Phase 2 Prep: CPU: 5.6GB, GPU: 0.2GB
Effective Rank: Val=82.1%

[Phase 2] Training Context-KV Attention...
✓ All 1 ContextBlocks frozen
✓ Embedding frozen
✓ Training 6,301,440/45,163,264 parameters

[Phase 2] 587,969 train / 22,722 val tokens, 40 epochs
    Epoch 1: batch 1/1149 (0.0s)...
    Epoch 1: batch 51/1149 (ppl=453516.5, acc=6.4%, 2.3s)
    Epoch 1: batch 101/1149 (ppl=21467.6, acc=7.9%, 4.3s)
    Epoch 1: batch 151/1149 (ppl=7346.4, acc=8.8%, 6.4s)
    Epoch 1: batch 201/1149 (ppl=4064.5, acc=9.5%, 8.5s)
    Epoch 1: batch 251/1149 (ppl=2766.2, acc=10.1%, 10.5s)
    Epoch 1: batch 301/1149 (ppl=2090.6, acc=10.7%, 12.6s)
    Epoch 1: batch 351/1149 (ppl=1703.0, acc=11.1%, 14.7s)
    Epoch 1: batch 401/1149 (ppl=1438.3, acc=11.5%, 16.8s)
    Epoch 1: batch 451/1149 (ppl=1253.8, acc=11.9%, 18.8s)
    Epoch 1: batch 501/1149 (ppl=1120.6, acc=12.2%, 20.9s)
    Epoch 1: batch 551/1149 (ppl=1016.7, acc=12.5%, 22.9s)
    Epoch 1: batch 601/1149 (ppl=935.0, acc=12.7%, 25.0s)
    Epoch 1: batch 651/1149 (ppl=870.9, acc=12.9%, 27.1s)
    Epoch 1: batch 701/1149 (ppl=815.6, acc=13.1%, 29.1s)
    Epoch 1: batch 751/1149 (ppl=770.4, acc=13.3%, 31.2s)
    Epoch 1: batch 801/1149 (ppl=731.0, acc=13.5%, 33.3s)
    Epoch 1: batch 851/1149 (ppl=697.0, acc=13.7%, 35.3s)
    Epoch 1: batch 901/1149 (ppl=668.1, acc=13.8%, 37.4s)
    Epoch 1: batch 951/1149 (ppl=642.1, acc=14.0%, 39.5s)
    Epoch 1: batch 1001/1149 (ppl=617.3, acc=14.1%, 41.6s)
    Epoch 1: batch 1051/1149 (ppl=596.6, acc=14.3%, 43.6s)
    Epoch 1: batch 1101/1149 (ppl=577.7, acc=14.4%, 45.7s)
    Epoch 1: batch 1149/1149 (ppl=559.8, acc=14.5%, 47.7s)
    Epoch 1: train_ppl=559.6 val_ppl=291.2 acc=16.2% [48.8s] *
    Epoch 2: batch 1/1149 (0.0s)...
    Epoch 2: batch 51/1149 (ppl=264.4, acc=17.5%, 2.1s)
    Epoch 2: batch 101/1149 (ppl=261.3, acc=17.5%, 4.2s)
    Epoch 2: batch 151/1149 (ppl=259.0, acc=17.5%, 6.2s)
    Epoch 2: batch 201/1149 (ppl=256.7, acc=17.7%, 8.3s)
    Epoch 2: batch 251/1149 (ppl=254.8, acc=17.7%, 10.3s)
    Epoch 2: batch 301/1149 (ppl=252.6, acc=17.8%, 12.4s)
    Epoch 2: batch 351/1149 (ppl=251.9, acc=17.8%, 14.5s)
    Epoch 2: batch 401/1149 (ppl=250.2, acc=17.9%, 16.6s)
    Epoch 2: batch 451/1149 (ppl=247.9, acc=18.0%, 18.6s)
    Epoch 2: batch 501/1149 (ppl=245.0, acc=18.1%, 20.7s)
    Epoch 2: batch 551/1149 (ppl=242.4, acc=18.1%, 22.7s)
    Epoch 2: batch 601/1149 (ppl=240.9, acc=18.2%, 24.8s)
    Epoch 2: batch 651/1149 (ppl=239.1, acc=18.2%, 27.0s)
    Epoch 2: batch 701/1149 (ppl=236.9, acc=18.3%, 29.0s)
    Epoch 2: batch 751/1149 (ppl=235.5, acc=18.3%, 31.1s)
    Epoch 2: batch 801/1149 (ppl=234.1, acc=18.3%, 33.2s)
    Epoch 2: batch 851/1149 (ppl=232.0, acc=18.4%, 35.2s)
    Epoch 2: batch 901/1149 (ppl=229.8, acc=18.5%, 37.3s)
    Epoch 2: batch 951/1149 (ppl=228.3, acc=18.6%, 39.4s)
    Epoch 2: batch 1001/1149 (ppl=226.7, acc=18.6%, 41.5s)
    Epoch 2: batch 1051/1149 (ppl=225.5, acc=18.7%, 43.6s)
    Epoch 2: batch 1101/1149 (ppl=223.9, acc=18.7%, 45.6s)
    Epoch 2: batch 1149/1149 (ppl=222.4, acc=18.7%, 47.6s)
    Epoch 2: train_ppl=222.4 val_ppl=225.1 acc=17.7% [48.7s] *
    Epoch 3: batch 1/1149 (0.0s)...
    Epoch 3: batch 51/1149 (ppl=157.0, acc=21.0%, 2.1s)
    Epoch 3: batch 101/1149 (ppl=159.1, acc=21.0%, 4.2s)
    Epoch 3: batch 151/1149 (ppl=158.2, acc=21.1%, 6.3s)
    Epoch 3: batch 201/1149 (ppl=158.8, acc=21.0%, 8.3s)
    Epoch 3: batch 251/1149 (ppl=160.1, acc=21.0%, 10.4s)
    Epoch 3: batch 301/1149 (ppl=159.2, acc=21.0%, 12.5s)
    Epoch 3: batch 351/1149 (ppl=158.0, acc=21.2%, 14.6s)
    Epoch 3: batch 401/1149 (ppl=157.7, acc=21.2%, 16.7s)
    Epoch 3: batch 451/1149 (ppl=157.2, acc=21.2%, 18.8s)
    Epoch 3: batch 501/1149 (ppl=157.2, acc=21.2%, 20.9s)
    Epoch 3: batch 551/1149 (ppl=156.7, acc=21.2%, 23.0s)
    Epoch 3: batch 601/1149 (ppl=155.6, acc=21.3%, 25.1s)
    Epoch 3: batch 651/1149 (ppl=154.9, acc=21.3%, 27.1s)
    Epoch 3: batch 701/1149 (ppl=154.3, acc=21.4%, 29.2s)
    Epoch 3: batch 751/1149 (ppl=154.2, acc=21.4%, 31.3s)
    Epoch 3: batch 801/1149 (ppl=153.6, acc=21.4%, 33.4s)
    Epoch 3: batch 851/1149 (ppl=152.8, acc=21.5%, 35.5s)
    Epoch 3: batch 901/1149 (ppl=152.0, acc=21.5%, 37.6s)
    Epoch 3: batch 951/1149 (ppl=151.8, acc=21.5%, 39.6s)
    Epoch 3: batch 1001/1149 (ppl=151.2, acc=21.6%, 41.7s)
    Epoch 3: batch 1051/1149 (ppl=150.3, acc=21.6%, 43.8s)
    Epoch 3: batch 1101/1149 (ppl=149.7, acc=21.7%, 45.8s)
    Epoch 3: batch 1149/1149 (ppl=149.1, acc=21.7%, 47.9s)
    Epoch 3: train_ppl=149.1 val_ppl=196.7 acc=19.4% [49.0s] *
    Epoch 4: batch 1/1149 (0.0s)...
    Epoch 4: batch 51/1149 (ppl=103.0, acc=24.5%, 2.1s)
    Epoch 4: batch 101/1149 (ppl=103.3, acc=24.5%, 4.2s)
    Epoch 4: batch 151/1149 (ppl=104.9, acc=24.3%, 6.2s)
    Epoch 4: batch 201/1149 (ppl=105.1, acc=24.4%, 8.3s)
    Epoch 4: batch 251/1149 (ppl=105.9, acc=24.4%, 10.4s)
    Epoch 4: batch 301/1149 (ppl=107.1, acc=24.3%, 12.5s)
    Epoch 4: batch 351/1149 (ppl=107.3, acc=24.3%, 14.6s)
    Epoch 4: batch 401/1149 (ppl=107.2, acc=24.3%, 16.7s)
    Epoch 4: batch 451/1149 (ppl=107.1, acc=24.4%, 18.7s)
    Epoch 4: batch 501/1149 (ppl=107.2, acc=24.3%, 20.8s)
    Epoch 4: batch 551/1149 (ppl=107.2, acc=24.3%, 22.9s)
    Epoch 4: batch 601/1149 (ppl=107.3, acc=24.3%, 25.0s)
    Epoch 4: batch 651/1149 (ppl=106.9, acc=24.4%, 27.1s)
    Epoch 4: batch 701/1149 (ppl=106.8, acc=24.4%, 29.1s)
    Epoch 4: batch 751/1149 (ppl=106.7, acc=24.4%, 31.2s)
    Epoch 4: batch 801/1149 (ppl=106.7, acc=24.4%, 33.4s)
    Epoch 4: batch 851/1149 (ppl=106.6, acc=24.4%, 35.5s)
    Epoch 4: batch 901/1149 (ppl=106.7, acc=24.4%, 37.6s)
    Epoch 4: batch 951/1149 (ppl=106.6, acc=24.5%, 39.6s)
    Epoch 4: batch 1001/1149 (ppl=106.1, acc=24.5%, 41.7s)
    Epoch 4: batch 1051/1149 (ppl=106.0, acc=24.6%, 43.8s)
    Epoch 4: batch 1101/1149 (ppl=106.0, acc=24.6%, 45.9s)
    Epoch 4: batch 1149/1149 (ppl=106.0, acc=24.6%, 47.9s)
    Epoch 4: train_ppl=106.0 val_ppl=181.9 acc=19.9% [49.0s] *
    Epoch 5: batch 1/1149 (0.0s)...
    Epoch 5: batch 51/1149 (ppl=72.7, acc=27.7%, 2.1s)
    Epoch 5: batch 101/1149 (ppl=73.7, acc=27.5%, 4.2s)
    Epoch 5: batch 151/1149 (ppl=74.7, acc=27.3%, 6.3s)
    Epoch 5: batch 201/1149 (ppl=75.1, acc=27.2%, 8.4s)
    Epoch 5: batch 251/1149 (ppl=76.0, acc=27.1%, 10.5s)
    Epoch 5: batch 301/1149 (ppl=76.4, acc=27.1%, 12.5s)
    Epoch 5: batch 351/1149 (ppl=76.5, acc=27.1%, 14.6s)
    Epoch 5: batch 401/1149 (ppl=76.8, acc=27.1%, 16.7s)
    Epoch 5: batch 451/1149 (ppl=76.8, acc=27.1%, 18.8s)
    Epoch 5: batch 501/1149 (ppl=76.9, acc=27.1%, 20.9s)
    Epoch 5: batch 551/1149 (ppl=77.4, acc=27.1%, 23.0s)
    Epoch 5: batch 601/1149 (ppl=77.6, acc=27.1%, 25.1s)
    Epoch 5: batch 651/1149 (ppl=77.8, acc=27.0%, 27.2s)
    Epoch 5: batch 701/1149 (ppl=77.9, acc=27.0%, 29.3s)
    Epoch 5: batch 751/1149 (ppl=78.0, acc=27.1%, 31.4s)
    Epoch 5: batch 801/1149 (ppl=78.0, acc=27.1%, 33.5s)
    Epoch 5: batch 851/1149 (ppl=78.1, acc=27.1%, 35.6s)
    Epoch 5: batch 901/1149 (ppl=78.3, acc=27.1%, 37.6s)
    Epoch 5: batch 951/1149 (ppl=78.2, acc=27.1%, 39.7s)
    Epoch 5: batch 1001/1149 (ppl=78.2, acc=27.1%, 41.8s)
    Epoch 5: batch 1051/1149 (ppl=78.3, acc=27.1%, 43.9s)
    Epoch 5: batch 1101/1149 (ppl=78.3, acc=27.1%, 46.0s)
    Epoch 5: batch 1149/1149 (ppl=78.4, acc=27.1%, 48.0s)
    Epoch 5: train_ppl=78.4 val_ppl=175.7 acc=20.9% [49.1s] *
    Epoch 6: batch 1/1149 (0.0s)...
    Epoch 6: batch 51/1149 (ppl=52.8, acc=31.1%, 2.1s)
    Epoch 6: batch 101/1149 (ppl=53.0, acc=30.7%, 4.2s)
    Epoch 6: batch 151/1149 (ppl=53.4, acc=30.6%, 6.3s)
    Epoch 6: batch 201/1149 (ppl=53.8, acc=30.4%, 8.4s)
    Epoch 6: batch 251/1149 (ppl=54.4, acc=30.2%, 10.5s)
    Epoch 6: batch 301/1149 (ppl=55.1, acc=30.1%, 12.6s)
    Epoch 6: batch 351/1149 (ppl=55.5, acc=30.1%, 14.7s)
    Epoch 6: batch 401/1149 (ppl=55.9, acc=30.1%, 16.8s)
    Epoch 6: batch 451/1149 (ppl=56.3, acc=30.1%, 18.9s)
    Epoch 6: batch 501/1149 (ppl=56.7, acc=30.0%, 21.0s)
    Epoch 6: batch 551/1149 (ppl=57.1, acc=29.9%, 23.0s)
    Epoch 6: batch 601/1149 (ppl=57.4, acc=29.8%, 25.1s)
    Epoch 6: batch 651/1149 (ppl=57.7, acc=29.8%, 27.2s)
    Epoch 6: batch 701/1149 (ppl=58.0, acc=29.8%, 29.3s)
    Epoch 6: batch 751/1149 (ppl=58.3, acc=29.7%, 31.4s)
    Epoch 6: batch 801/1149 (ppl=58.5, acc=29.7%, 33.5s)
    Epoch 6: batch 851/1149 (ppl=58.8, acc=29.7%, 35.6s)
    Epoch 6: batch 901/1149 (ppl=59.0, acc=29.6%, 37.7s)
    Epoch 6: batch 951/1149 (ppl=59.2, acc=29.6%, 39.8s)
    Epoch 6: batch 1001/1149 (ppl=59.2, acc=29.6%, 41.9s)
    Epoch 6: batch 1051/1149 (ppl=59.3, acc=29.6%, 43.9s)
    Epoch 6: batch 1101/1149 (ppl=59.5, acc=29.5%, 46.0s)
    Epoch 6: batch 1149/1149 (ppl=59.6, acc=29.5%, 48.0s)
    Epoch 6: train_ppl=59.6 val_ppl=183.0 acc=20.6% [49.2s]
    Epoch 7: batch 1/1149 (0.0s)...
    Epoch 7: batch 51/1149 (ppl=38.3, acc=34.1%, 2.1s)
    Epoch 7: batch 101/1149 (ppl=38.3, acc=33.9%, 4.3s)
    Epoch 7: batch 151/1149 (ppl=39.0, acc=33.7%, 6.4s)
    Epoch 7: batch 201/1149 (ppl=40.1, acc=33.4%, 8.5s)
    Epoch 7: batch 251/1149 (ppl=40.5, acc=33.3%, 10.6s)
    Epoch 7: batch 301/1149 (ppl=41.1, acc=33.1%, 12.7s)
    Epoch 7: batch 351/1149 (ppl=41.7, acc=33.0%, 14.8s)
    Epoch 7: batch 401/1149 (ppl=42.3, acc=32.8%, 16.9s)
    Epoch 7: batch 451/1149 (ppl=42.6, acc=32.7%, 18.9s)
    Epoch 7: batch 501/1149 (ppl=42.9, acc=32.6%, 21.0s)
    Epoch 7: batch 551/1149 (ppl=43.2, acc=32.5%, 23.1s)
    Epoch 7: batch 601/1149 (ppl=43.5, acc=32.5%, 25.3s)
    Epoch 7: batch 651/1149 (ppl=43.9, acc=32.4%, 27.3s)
    Epoch 7: batch 701/1149 (ppl=44.2, acc=32.3%, 29.4s)
    Epoch 7: batch 751/1149 (ppl=44.5, acc=32.2%, 31.5s)
    Epoch 7: batch 801/1149 (ppl=44.8, acc=32.2%, 33.6s)
    Epoch 7: batch 851/1149 (ppl=44.9, acc=32.1%, 35.8s)
    Epoch 7: batch 901/1149 (ppl=45.2, acc=32.1%, 37.8s)
    Epoch 7: batch 951/1149 (ppl=45.4, acc=32.1%, 39.9s)
    Epoch 7: batch 1001/1149 (ppl=45.6, acc=32.0%, 42.0s)
    Epoch 7: batch 1051/1149 (ppl=45.7, acc=32.0%, 44.1s)
    Epoch 7: batch 1101/1149 (ppl=45.9, acc=32.0%, 46.2s)
    Epoch 7: batch 1149/1149 (ppl=46.1, acc=32.0%, 48.3s)
    Epoch 7: train_ppl=46.1 val_ppl=193.1 acc=20.9% [49.4s]
    Epoch 8: batch 1/1149 (0.0s)...
    Epoch 8: batch 51/1149 (ppl=28.5, acc=37.5%, 2.1s)
    Epoch 8: batch 101/1149 (ppl=29.2, acc=37.3%, 4.2s)
    Epoch 8: batch 151/1149 (ppl=30.2, acc=36.9%, 6.3s)
    Epoch 8: batch 201/1149 (ppl=30.9, acc=36.4%, 8.3s)
