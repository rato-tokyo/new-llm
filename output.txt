remote: Enumerating objects: 7, done.
remote: Counting objects: 100% (7/7), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)
Unpacking objects: 100% (4/4), 613 bytes | 122.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   e610016..0da0d98  main       -> origin/main
Updating e610016..0da0d98
Fast-forward
 scripts/experiment_dual_context.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)
======================================================================
DUAL CONTEXT EXPERIMENT
======================================================================
Samples: 2000
Context dim per block: 500
Combined context dim: 1000
Num layers: 1
Output: importants/logs/20251202_093000_dual_context
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)
Loading data...
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
Data: 2,403,563 train, 22,723 val tokens
Split: A=1,201,782, B=1,201,782 tokens

Creating DualContextLLM (cd=500x2=1000)...
2025-12-02 09:30:02.161166: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-02 09:30:02.176518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764667802.197566   27835 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764667802.204232   27835 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764667802.220437   27835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764667802.220478   27835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764667802.220481   27835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764667802.220484   27835 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-02 09:30:02.225395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding
Parameters: 41,230,040 total
  ContextBlock A: 635,500
  ContextBlock B: 635,500
  TokenBlock: 1,360,128

[Phase 1A] Training ContextBlock A on first half...

[Phase 1] ContextA: 1,201,782 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2763 [6.2s]
  Iter 3: conv=0% loss=13.1935 [5.7s]
  Iter 4: conv=0% loss=10.6984 [4.7s]
  Iter 5: conv=0% loss=7.2667 [4.7s]
  Iter 6: conv=0% loss=4.6423 [4.8s]
  Iter 7: conv=0% loss=3.4091 [4.8s]
  Iter 8: conv=0% loss=3.0891 [4.7s]
  Iter 9: conv=0% loss=3.0212 [4.7s]
  Iter 10: conv=1% loss=2.9584 [4.8s]
  Iter 11: conv=1% loss=2.8995 [4.8s]
  Iter 12: conv=1% loss=2.8369 [4.9s]
  Iter 13: conv=2% loss=2.7363 [4.9s]
  Iter 14: conv=3% loss=2.5687 [4.7s]
  Iter 15: conv=5% loss=2.3403 [4.7s]
  Iter 16: conv=8% loss=2.1033 [4.7s]
  Iter 17: conv=11% loss=1.9120 [4.9s]
  Iter 18: conv=17% loss=1.7692 [4.8s]
  Iter 19: conv=25% loss=1.6510 [4.7s]
  Iter 20: conv=33% loss=1.5469 [4.7s]
  Iter 21: conv=43% loss=1.4549 [4.9s]
  Iter 22: conv=53% loss=1.3674 [4.8s]
  Iter 23: conv=62% loss=1.2781 [4.8s]
  Iter 24: conv=69% loss=1.1922 [4.7s]
  Iter 25: conv=74% loss=1.1194 [4.8s]
  Iter 26: conv=79% loss=1.0628 [4.9s]
  Iter 27: conv=84% loss=1.0208 [4.7s]
  Iter 28: conv=87% loss=0.9951 [4.9s]
  Iter 29: conv=90% loss=0.9862 [4.8s]
  Iter 30: conv=92% loss=0.9849 [4.8s]
  → Early stop: conv 92% >= 90%
  Done: 92% converged
  Collecting cache (parallel)...
    Preparing combined tokens (1,201,781 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [2.9s]
Phase 1A: 154.8s, 30 iter, conv=92%

[Phase 1B] Training ContextBlock B on second half...

[Phase 1] ContextB: 1,201,782 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2812 [4.9s]
  Iter 3: conv=0% loss=12.4629 [4.9s]
  Iter 4: conv=0% loss=9.7826 [4.9s]
  Iter 5: conv=0% loss=6.4455 [4.9s]
  Iter 6: conv=0% loss=4.2491 [5.0s]
  Iter 7: conv=0% loss=3.1795 [5.0s]
  Iter 8: conv=0% loss=2.7425 [4.9s]
  Iter 9: conv=0% loss=2.6633 [4.9s]
  Iter 10: conv=0% loss=2.7222 [4.8s]
  Iter 11: conv=0% loss=2.6922 [5.0s]
  Iter 12: conv=0% loss=2.4701 [4.9s]
  Iter 13: conv=1% loss=2.1442 [4.9s]
  Iter 14: conv=2% loss=1.8758 [4.9s]
  Iter 15: conv=3% loss=1.7583 [5.0s]
  Iter 16: conv=6% loss=1.7727 [4.8s]
  Iter 17: conv=10% loss=1.8318 [4.8s]
  Iter 18: conv=16% loss=1.8458 [4.9s]
  Iter 19: conv=25% loss=1.7847 [4.8s]
  Iter 20: conv=34% loss=1.6816 [4.9s]
  Iter 21: conv=43% loss=1.5835 [4.8s]
  Iter 22: conv=53% loss=1.5084 [4.9s]
  Iter 23: conv=63% loss=1.4408 [4.9s]
  Iter 24: conv=72% loss=1.3518 [4.8s]
  Iter 25: conv=79% loss=1.2282 [4.7s]
  Iter 26: conv=83% loss=1.0915 [4.8s]
  Iter 27: conv=86% loss=0.9869 [4.7s]
  Iter 28: conv=89% loss=0.9370 [4.7s]
  Iter 29: conv=92% loss=0.9240 [4.9s]
  → Early stop: conv 92% >= 90%
  Done: 92% converged
  Collecting cache (parallel)...
    Preparing combined tokens (1,201,781 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [3.0s]
Phase 1B: 150.2s, 29 iter, conv=92%

[Phase 2 Prep] Collecting dual context cache on full data...
    Collecting dual context cache (2,403,562 tokens)...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
    Dual context cache collected [983.3s]
    Collecting dual context cache (22,722 tokens)...
    Dual context cache collected [9.2s]
Cache collection: 995.0s
  Train cache: torch.Size([2403562, 1000])
  Val cache: torch.Size([22722, 1000])
Effective Rank: Train=75.3%, Val=73.6%

[Phase 2] Training TokenBlock with concatenated context (cd=1000)...
✓ Both ContextBlocks frozen
✓ Embedding frozen
✓ Training TokenBlock only: 1,360,128/41,230,040 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 20 epochs
  Epoch 1: train_ppl=330.5 val_ppl=154.2 acc=23.0% [54.3s] ★
  Epoch 2: train_ppl=145.2 val_ppl=128.1 acc=24.3% [55.0s] ★
  Epoch 3: train_ppl=118.3 val_ppl=119.7 acc=24.7% [55.5s] ★
  Epoch 4: train_ppl=105.6 val_ppl=116.0 acc=25.0% [55.7s] ★
  Epoch 5: train_ppl=97.9 val_ppl=114.0 acc=25.2% [55.9s] ★
  Epoch 6: train_ppl=92.7 val_ppl=112.9 acc=25.3% [56.0s] ★
  Epoch 7: train_ppl=88.9 val_ppl=112.3 acc=25.5% [56.0s] ★
  Epoch 8: train_ppl=86.0 val_ppl=111.9 acc=25.6% [56.1s] ★
  → Early stop at epoch 8 (PPL improvement 0.37 < 0.4)
  Best: epoch 8, ppl=111.9, acc=25.6%

Phase 2: 444.6s, Best epoch 8
Result: PPL=111.9, Acc=25.6%
Total time: 1744.7s

======================================================================
SUMMARY - Dual Context Experiment
======================================================================
Architecture: DualContextLLM (C1T1 x2)
  ContextBlock A: cd=500, trained on first 1,201,781 tokens
  ContextBlock B: cd=500, trained on last 1,201,782 tokens
  TokenBlock: cd=1000 (concatenated)
Parameters: 41,230,040
Phase 1A: 154.8s, conv=92%
Phase 1B: 150.2s, conv=92%
Phase 2: 444.6s, epoch 8
Effective Rank: 73.6% (of 1000)
Val PPL: 111.9
Val Acc: 25.6%
Total time: 1744.7s
======================================================================

Results saved to: importants/logs/20251202_093000_dual_context/results.txt

======================================================================
DONE
======================================================================
