remote: Enumerating objects: 53, done.
remote: Counting objects: 100% (53/53), done.
remote: Compressing objects: 100% (22/22), done.
remote: Total 40 (delta 30), reused 28 (delta 18), pack-reused 0 (from 0)
Unpacking objects: 100% (40/40), 7.14 KiB | 487.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   c6be880..6685997  main       -> origin/main
Updating c6be880..6685997
Fast-forward
 CLAUDE.md                             |  15 ++++
 new.txt                               |   0
 old.txt                               |   0
 scripts/unified_scaling_experiment.py | 120 +++++++-------------------------
 src/evaluation/metrics.py             | 124 +++-------------------------------
 src/trainers/phase1/memory.py         |  49 ++++----------
 src/trainers/phase2.py                |  95 ++++++--------------------
 src/utils/memory.py                   |   8 ---
 8 files changed, 84 insertions(+), 327 deletions(-)
 create mode 100644 new.txt
 create mode 100644 old.txt
============================================================
SCALING EXPERIMENT
============================================================
Samples: [50, 100, 200, 500] | Model: 6L/768D | Seed: 42
GPU: NVIDIA L4 (22.2GB)

--- 50 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (6289 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   6289 tokens
  Data: 56,602 train / 6,289 val tokens
2025-11-29 09:08:19.225923: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-29 09:08:19.243801: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764407299.265333   45225 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764407299.271836   45225 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764407299.288426   45225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764407299.288460   45225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764407299.288463   45225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764407299.288466   45225 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-29 09:08:19.293361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token_input_all_layers: False (等差減少設計)

[Phase 1] Train (50 samples): 56,602 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0331 [1.4s]
  Iter 3: conv=0% loss=0.1086 [1.1s]
  Iter 4: conv=0% loss=0.0339 [1.0s]
  Iter 5: conv=0% loss=-0.0130 [1.0s]
  Iter 6: conv=0% loss=-0.0917 [1.0s]
  Iter 7: conv=0% loss=-0.1207 [1.0s]
  Iter 8: conv=1% loss=-0.1143 [1.0s]
  Iter 9: conv=11% loss=-0.1516 [1.0s]
  Iter 10: conv=19% loss=-0.1760 [1.0s]
  Iter 11: conv=32% loss=-0.2027 [1.0s]
  Iter 12: conv=58% loss=-0.2307 [1.0s]
  Iter 13: conv=82% loss=-0.2474 [1.0s]
  Iter 14: conv=92% loss=-0.2528 [1.0s]
  Iter 15: conv=95% loss=-0.2551 [1.0s]
  Iter 16: conv=97% loss=-0.2567 [1.0s]
  Iter 17: conv=98% loss=-0.2576 [1.0s]
  Iter 18: conv=99% loss=-0.2584 [1.0s]
  Iter 19: conv=99% loss=-0.2591 [1.0s]
  Iter 20: conv=99% loss=-0.2598 [1.0s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache...
  Cache collected [63.8s]
  Train Effective Rank: 73.70/768 (9.6%)
  Val Effective Rank: 64.90/768 (8.5%)
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 47,215,441/101,207,121 parameters

[Phase 2] 56,602 train / 6,289 val tokens, 10 epochs
  Epoch 1: train_ppl=12169.7 val_ppl=1947.4 acc=7.4% [2.1s] ★
  Epoch 2: train_ppl=763.8 val_ppl=1503.3 acc=9.9% [2.1s] ★
  Epoch 3: train_ppl=480.0 val_ppl=1592.9 acc=9.7% [2.1s]
  → Early stop at epoch 3
  Best: epoch 2, ppl=1503.3, acc=9.9%

  ✓ 50 samples: PPL=1503.3, Acc=9.9%, ER=8.5%, Time=1.6min

--- 100 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (12279 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   12279 tokens
  Data: 110,516 train / 12,279 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token_input_all_layers: False (等差減少設計)

[Phase 1] Train (100 samples): 110,516 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0281 [2.1s]
  Iter 3: conv=0% loss=0.1154 [2.1s]
  Iter 4: conv=0% loss=0.0411 [2.0s]
  Iter 5: conv=0% loss=-0.0073 [2.0s]
  Iter 6: conv=0% loss=-0.0830 [2.0s]
  Iter 7: conv=0% loss=-0.0950 [2.0s]
  Iter 8: conv=1% loss=-0.0924 [2.0s]
  Iter 9: conv=14% loss=-0.1398 [2.0s]
  Iter 10: conv=17% loss=-0.1612 [2.0s]
  Iter 11: conv=30% loss=-0.1927 [2.0s]
  Iter 12: conv=56% loss=-0.2236 [2.0s]
  Iter 13: conv=78% loss=-0.2368 [2.0s]
  Iter 14: conv=83% loss=-0.2414 [2.0s]
  Iter 15: conv=85% loss=-0.2440 [2.0s]
  Iter 16: conv=89% loss=-0.2462 [2.0s]
  Iter 17: conv=92% loss=-0.2477 [2.1s]
  Iter 18: conv=95% loss=-0.2492 [2.0s]
  Iter 19: conv=97% loss=-0.2504 [2.0s]
  Iter 20: conv=98% loss=-0.2514 [2.0s]
  Iter 21: conv=99% loss=-0.2523 [2.1s]
  Iter 22: conv=99% loss=-0.2530 [2.0s]
  → Converged at iter 22
  Done: 99% converged
  Collecting cache...
  Cache collected [122.4s]
  Train Effective Rank: 65.14/768 (8.5%)
  Val Effective Rank: 59.79/768 (7.8%)
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 47,215,441/101,207,121 parameters

[Phase 2] 110,516 train / 12,279 val tokens, 10 epochs
  Epoch 1: train_ppl=4271.4 val_ppl=1475.1 acc=8.5% [4.1s] ★
  Epoch 2: train_ppl=738.2 val_ppl=1388.1 acc=8.9% [4.2s] ★
  Epoch 3: train_ppl=517.5 val_ppl=1364.2 acc=10.2% [4.2s] ★
  Epoch 4: train_ppl=390.2 val_ppl=1247.2 acc=11.3% [4.2s] ★
  Epoch 5: train_ppl=300.2 val_ppl=1127.5 acc=12.7% [4.2s] ★
  Epoch 6: train_ppl=240.9 val_ppl=1036.3 acc=13.7% [4.2s] ★
  Epoch 7: train_ppl=199.2 val_ppl=1079.1 acc=14.1% [4.2s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=1036.3, acc=13.7%

  ✓ 100 samples: PPL=1036.3, Acc=13.7%, ER=7.8%, Time=3.5min

--- 200 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (24013 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   24013 tokens
  Data: 216,119 train / 24,013 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token_input_all_layers: False (等差減少設計)

[Phase 1] Train (200 samples): 216,119 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0275 [4.1s]
  Iter 3: conv=0% loss=0.1168 [4.1s]
  Iter 4: conv=0% loss=0.0436 [4.0s]
  Iter 5: conv=0% loss=-0.0019 [4.0s]
  Iter 6: conv=0% loss=-0.0745 [4.0s]
  Iter 7: conv=0% loss=-0.0932 [4.0s]
  Iter 8: conv=0% loss=-0.0972 [4.0s]
  Iter 9: conv=14% loss=-0.1365 [4.0s]
  Iter 10: conv=15% loss=-0.1525 [4.0s]
  Iter 11: conv=24% loss=-0.1819 [4.0s]
  Iter 12: conv=48% loss=-0.2146 [4.0s]
  Iter 13: conv=71% loss=-0.2335 [4.0s]
  Iter 14: conv=80% loss=-0.2410 [4.0s]
  Iter 15: conv=83% loss=-0.2441 [4.0s]
  Iter 16: conv=87% loss=-0.2471 [4.0s]
  Iter 17: conv=91% loss=-0.2489 [4.0s]
  Iter 18: conv=94% loss=-0.2504 [4.1s]
  Iter 19: conv=96% loss=-0.2517 [4.0s]
  Iter 20: conv=98% loss=-0.2527 [4.0s]
  Iter 21: conv=99% loss=-0.2536 [4.0s]
  Iter 22: conv=99% loss=-0.2543 [4.0s]
  Iter 23: conv=99% loss=-0.2550 [4.0s]
  → Converged at iter 23
  Done: 99% converged
  Collecting cache...
  Cache collected [235.6s]
  Train Effective Rank: 67.09/768 (8.7%)
  Val Effective Rank: 64.56/768 (8.4%)
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 47,215,441/101,207,121 parameters

[Phase 2] 216,119 train / 24,013 val tokens, 10 epochs
  Epoch 1: train_ppl=2183.7 val_ppl=979.5 acc=14.3% [8.5s] ★
  Epoch 2: train_ppl=666.7 val_ppl=857.5 acc=14.9% [8.6s] ★
  Epoch 3: train_ppl=445.4 val_ppl=776.4 acc=16.6% [8.6s] ★
  Epoch 4: train_ppl=322.8 val_ppl=757.7 acc=17.6% [8.5s] ★
  Epoch 5: train_ppl=243.9 val_ppl=770.6 acc=17.9% [8.5s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=757.7, acc=17.6%

  ✓ 200 samples: PPL=757.7, Acc=17.6%, ER=8.4%, Time=6.6min

--- 500 samples ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (58797 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   58797 tokens
  Data: 529,173 train / 58,797 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token_input_all_layers: False (等差減少設計)

[Phase 1] Train (500 samples): 529,173 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0274 [9.4s]
  Iter 3: conv=0% loss=0.1163 [9.6s]
  Iter 4: conv=0% loss=0.0428 [9.3s]
  Iter 5: conv=0% loss=-0.0045 [9.4s]
  Iter 6: conv=0% loss=-0.0798 [9.4s]
  Iter 7: conv=0% loss=-0.0945 [9.5s]
  Iter 8: conv=1% loss=-0.0964 [9.4s]
  Iter 9: conv=13% loss=-0.1397 [9.4s]
  Iter 10: conv=17% loss=-0.1576 [9.3s]
  Iter 11: conv=29% loss=-0.1875 [9.4s]
  Iter 12: conv=54% loss=-0.2191 [9.3s]
  Iter 13: conv=75% loss=-0.2346 [9.3s]
  Iter 14: conv=82% loss=-0.2401 [9.3s]
  Iter 15: conv=84% loss=-0.2431 [9.3s]
  Iter 16: conv=89% loss=-0.2456 [9.3s]
  Iter 17: conv=93% loss=-0.2472 [9.4s]
  Iter 18: conv=95% loss=-0.2487 [9.4s]
  Iter 19: conv=97% loss=-0.2499 [9.4s]
  Iter 20: conv=98% loss=-0.2509 [9.4s]
  Iter 21: conv=99% loss=-0.2516 [9.4s]
  → Converged at iter 21
  Done: 99% converged
  Collecting cache...
  Cache collected [580.5s]
  Train Effective Rank: 69.25/768 (9.0%)
  Val Effective Rank: 66.26/768 (8.6%)
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 47,215,441/101,207,121 parameters

[Phase 2] 529,173 train / 58,797 val tokens, 10 epochs
  Epoch 1: train_ppl=918.2 val_ppl=544.6 acc=14.5% [25.3s] ★
  Epoch 2: train_ppl=306.1 val_ppl=536.0 acc=15.4% [25.2s] ★
  Epoch 3: train_ppl=205.4 val_ppl=579.9 acc=15.6% [25.0s]
  → Early stop at epoch 3
  Best: epoch 2, ppl=536.0, acc=15.4%

  ✓ 500 samples: PPL=536.0, Acc=15.4%, ER=8.6%, Time=15.2min

============================================================
RESULTS
============================================================
 Samples     Tokens    Val PPL  Val Acc   Val ER
--------------------------------------------------
      50     56,602     1503.3     9.9%     8.5%
     100    110,516     1036.3    13.7%     7.8%
     200    216,119      757.7    17.6%     8.4%
     500    529,173      536.0    15.4%     8.6%

Scaling: α=-0.459 (R²=0.993)
Total: 27.3 min
Saved: ./results/unified_scaling/results.json
