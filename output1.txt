remote: Enumerating objects: 6, done.
remote: Counting objects: 100% (6/6), done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 4 (delta 2), reused 4 (delta 2), pack-reused 0 (from 0)
Unpacking objects: 100% (4/4), 4.61 KiB | 4.61 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   b856ed9..d165524  main       -> origin/main
Updating b856ed9..d165524
Fast-forward
 scripts/experiment_vdproj_2phase.py | 543 ++++++++++++++++++++++++++++++++++++
 1 file changed, 543 insertions(+)
 create mode 100644 scripts/experiment_vdproj_2phase.py
Device: cuda (NVIDIA L4, 23.8GB)
======================================================================
V-DPROJ 2-PHASE EXPERIMENT
======================================================================
Samples: 10,000
Sequence length: 128
Phase 1 epochs: 10 (V reconstruction)
Phase 2 epochs: 30 (LM training)
Phase 1 LR: 0.001
Phase 2 LR: 0.0001
V proj dim: 320
======================================================================

[Data] Loading Pile data...
Preparing data: 10,000 samples, seq_len=128
Loading cached tokens: cache/pile_tokens/pile_1280128.pt
  Loaded 1,280,128 tokens from cache
  Train: 9,000 samples
  Val: 1,000 samples

[Model] Creating V-DProj Pythia...
  Total parameters: 72,391,552
  V projection: 1,971,072
  V proj trainable: 1,971,072

======================================================================
PHASE 1: V RECONSTRUCTION PRETRAINING
======================================================================
Training only v_compress and v_restore
Loss: ||V - V_restored||^2
  Epoch  1: train_recon=0.013636 val_recon=0.003100 [37.5s] *
  Epoch  2: train_recon=0.002571 val_recon=0.002442 [37.5s] *
  Epoch  3: train_recon=0.002462 val_recon=0.002390 [37.2s] *
  Epoch  4: train_recon=0.002424 val_recon=0.002467 [37.3s] 
  Epoch  5: train_recon=0.002450 val_recon=0.002378 [37.4s] *
  Epoch  6: train_recon=0.002412 val_recon=0.002403 [37.3s] 
  Epoch  7: train_recon=0.002437 val_recon=0.002386 [37.4s] 
  Epoch  8: train_recon=0.002403 val_recon=0.002416 [37.4s] 
  -> Early stop (reconstruction converged)
  Best reconstruction loss: 0.002378

======================================================================
PHASE 2: LM TRAINING (V projection frozen)
======================================================================
  Trainable: 70,420,480
  Frozen (V proj): 1,971,072

[Phase 2] Training...
  Epoch  1: train_ppl=609.0 val_ppl=628.0 recon=0.011390 [77.4s] *
  Epoch  2: train_ppl=154.0 val_ppl=464.5 recon=0.012269 [77.6s] *
  Epoch  3: train_ppl=79.3 val_ppl=415.1 recon=0.013078 [77.7s] *
  Epoch  4: train_ppl=47.3 val_ppl=419.7 recon=0.013705 [77.7s] 
  Epoch  5: train_ppl=29.8 val_ppl=460.9 recon=0.014271 [77.7s] 
  Epoch  6: train_ppl=19.1 val_ppl=543.8 recon=0.014833 [77.7s] 
  -> Early stop
  Best: epoch 3, ppl=415.1

  Position-wise PPL:
    Position 0-16: 758.8
    Position 16-32: 589.5
    Position 32-64: 514.9
    Position 64-96: 509.4
    Position 96-128: 498.3

======================================================================
SUMMARY
======================================================================

Phase 1 (V Reconstruction):
  Best reconstruction loss: 0.002378
  Epochs: 8

Phase 2 (LM Training):
  Best PPL: 415.1
  Best epoch: 3

KV Cache reduction: 18.8%

DONE
