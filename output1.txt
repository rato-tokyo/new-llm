remote: Enumerating objects: 29, done.
remote: Counting objects: 100% (29/29), done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 18 (delta 12), reused 18 (delta 12), pack-reused 0 (from 0)
Unpacking objects: 100% (18/18), 6.47 KiB | 1.29 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   f536a41..e03db15  main       -> origin/main
Updating f536a41..e03db15
Fast-forward
 output.txt                 |  87 +++++++++++++++++++++-----------
 scripts/experiment_mla.py  |  60 +++++++++++++++++++++-
 src/data/__init__.py       |  15 ++++++
 src/data/reversal_pairs.py | 122 +++++++++++++++++++++++++++++++++++++++++++++
 src/models/alibi.py        |   6 ++-
 src/utils/__init__.py      |   3 +-
 src/utils/evaluation.py    | 118 ++++++++++++++++++++++++++++++++++++++++++-
 src/utils/training.py      |  14 ++++++
 8 files changed, 390 insertions(+), 35 deletions(-)
 create mode 100644 src/data/__init__.py
 create mode 100644 src/data/reversal_pairs.py
Device: cuda (NVIDIA L4, 23.8GB)
======================================================================
MLA EXPERIMENT: KV Cache Compression with ALiBi
======================================================================
Samples: 10,000
Sequence length: 128
Epochs: 30
Learning rate: 0.0001
KV dim: 128 (from 512)
ALiBi slope: 0.0625
Skip baseline: True
======================================================================

KV Cache reduction: 87.5%
======================================================================

[Data] Loading Pile data...
Preparing data: 10,000 samples, seq_len=128
Downloading Pile dataset: 1,280,128 tokens
  Loading tokenizer: EleutherAI/pythia-70m
  Loading dataset (streaming)...
Resolving data files: 100% 30/30 [00:00<00:00, 103.86it/s]
  Tokenizing...
    Rate limited (429). Retry 1/5 after 30.0s...
Resolving data files: 100% 30/30 [00:00<00:00, 239674.51it/s]
    Rate limited (429). Retry 2/5 after 30.0s...
Resolving data files: 100% 30/30 [00:00<00:00, 211122.68it/s]
    Rate limited (429). Retry 3/5 after 30.0s...
Resolving data files: 100% 30/30 [00:00<00:00, 253177.30it/s]
  Saved 1,280,128 tokens to cache: cache/pile_tokens/pile_1280128.pt
  Train: 9,000 samples
  Val: 1,000 samples

======================================================================
1. PYTHIA-70M (Baseline, RoPE) - SKIPPED
======================================================================

======================================================================
2. MLA-PYTHIA (kv_dim=128, ALiBi)
======================================================================
  Total parameters: 68,442,112
  MLA attention: 4,325,376
  KV Cache reduction: 87.5%

[MLA] Training...
  Epoch  1: train_ppl=638.6 val_ppl=693.4 [70.3s] *
  Epoch  2: train_ppl=174.1 val_ppl=522.1 [71.3s] *
  Epoch  3: train_ppl=94.5 val_ppl=469.2 [71.2s] *
  Epoch  4: train_ppl=59.7 val_ppl=454.6 [71.3s] *
  Epoch  5: train_ppl=40.6 val_ppl=479.1 [71.6s] 
  -> Early stop
  Best: epoch 4, ppl=454.6

  Position-wise PPL:
    Position 0-16: 620.8
    Position 16-32: 506.8
    Position 32-64: 458.9
    Position 64-96: 458.9
    Position 96-128: 446.2

  Reversal Curse evaluation:
    Forward PPL: 7135.6
    Backward PPL: 4807.7
    Reversal Ratio: 1.484
    Reversal Gap: -2327.9

======================================================================
SUMMARY
======================================================================

| Model | PPL | Epoch | KV Reduction |
|-------|-----|-------|--------------|
| MLA (ALiBi) | 454.6 | 4 | 87.5% |

DONE
