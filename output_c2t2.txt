======================================================================
ASYMMETRIC LAYER EXPERIMENT
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Mode: c2t2
Samples: 2000
Context dim: 500
Output: importants/logs/20251202_075329_asymmetric_c2t2
======================================================================

============================================================
Running C2T2: Context 2L, Token 2L
============================================================
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 166kB/s]
config.json: 100% 665/665 [00:00<00:00, 5.37MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.42MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 1.08MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 1.60MB/s]
Loading training data...
  Loading 2000 samples from UltraChat...
README.md: 3.90kB [00:00, 17.4MB/s]
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:01<00:00, 127MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:01<00:00, 214MB/s] 
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:00<00:00, 251MB/s]    
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:00<00:00, 97.6MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:01<00:00, 217MB/s]
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:01<00:00, 222MB/s]
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:01<00:00, 221MB/s]  
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:00<00:00, 115MB/s] 
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 44455.88 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 58333.10 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 72379.14 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 74289.68 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
  Validation file not found, generating: ./cache/example_val.txt
  Generated 20 validation samples (indices 50000-50019)
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
Data: 2,403,563 train, 22,723 val tokens
2025-12-02 07:54:13.016543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-02 07:54:13.032619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764662053.051085    2772 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764662053.056447    2772 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764662053.071554    2772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764662053.071586    2772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764662053.071590    2772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764662053.071595    2772 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-02 07:54:13.076549: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 5.60MB/s]
model.safetensors: 100% 548M/548M [00:00<00:00, 569MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using G案 architecture: ContextBlock(2L) + TokenBlock(2L)
  num_input_tokens: 1
  Context injection: Layer1=prev, LayerN=current
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
Parameters: 41,822,168 total
  ContextBlock: 1,271,000
  TokenBlock: 1,952,256

[Phase 1] OACD: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=10.9875 [14.3s]
  Iter 3: conv=0% loss=7.7593 [11.8s]
  Iter 4: conv=0% loss=3.4557 [11.3s]
  Iter 5: conv=0% loss=1.9233 [11.4s]
  Iter 6: conv=0% loss=1.8303 [11.3s]
  Iter 7: conv=1% loss=1.7446 [11.2s]
  Iter 8: conv=3% loss=1.4748 [11.4s]
  Iter 9: conv=9% loss=1.2781 [11.7s]
  Iter 10: conv=26% loss=1.2127 [11.5s]
  Iter 11: conv=58% loss=1.1600 [11.4s]
  Iter 12: conv=80% loss=1.0139 [11.8s]
  Iter 13: conv=89% loss=0.8139 [11.3s]
  Iter 14: conv=93% loss=0.6642 [11.7s]
  Iter 15: conv=96% loss=0.5857 [11.2s]
  Iter 16: conv=99% loss=0.5263 [11.2s]
  Iter 17: conv=99% loss=0.4611 [11.5s]
  → Early stop: conv 99% >= 99%
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [11.9s]
Phase 1: 217.2s, 17 iter, conv=99%, ER=80.7%/79.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,953,792/41,822,168 parameters

[Phase 2] 2,403,563 train / 22,723 val tokens, 20 epochs
  Epoch 1: train_ppl=424.7 val_ppl=211.1 acc=20.7% [55.3s] ★
  Epoch 2: train_ppl=184.2 val_ppl=167.4 acc=22.3% [54.8s] ★
  Epoch 3: train_ppl=142.5 val_ppl=150.9 acc=23.2% [55.2s] ★
  Epoch 4: train_ppl=121.0 val_ppl=142.5 acc=23.6% [55.0s] ★
  Epoch 5: train_ppl=107.3 val_ppl=137.8 acc=24.0% [55.0s] ★
  Epoch 6: train_ppl=97.7 val_ppl=135.2 acc=24.1% [55.0s] ★
  Epoch 7: train_ppl=90.5 val_ppl=133.8 acc=24.2% [55.1s] ★
  Epoch 8: train_ppl=84.8 val_ppl=133.1 acc=24.3% [55.1s] ★
  Epoch 9: train_ppl=80.1 val_ppl=133.0 acc=24.5% [55.0s] ★
  Epoch 10: train_ppl=76.3 val_ppl=133.2 acc=24.5% [55.1s]
  → Early stop at epoch 10
  Best: epoch 9, ppl=133.0, acc=24.5%
Phase 2: 550.7s, Best epoch 9
Result: PPL=133.0, Acc=24.5%
Total time: 767.9s

======================================================================
SUMMARY
======================================================================

Config     Context  Token    Params       Val PPL    Acc      ER%      Time    
--------------------------------------------------------------------------------
C2T2       2        2        41,822,168  133.0      24.5     79.1     767.9   

Results saved to: importants/logs/20251202_075329_asymmetric_c2t2/results_c2t2.txt

======================================================================
DONE
======================================================================
