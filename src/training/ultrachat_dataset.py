"""UltraChat dataset handling for large-scale dialog training

UltraChat is a large-scale, multi-turn dialogue dataset generated by GPT-3.5.

Dataset: stingning/ultrachat
- 1.5M+ diverse conversations
- English only
- Multi-turn dialogues
- Very diverse topics (science, tech, history, culture, etc.)
- Generated by GPT-3.5 for high diversity
"""

import torch
from torch.utils.data import Dataset
from typing import List, Tuple
from datasets import load_dataset
from tqdm import tqdm


class UltraChatDataset(Dataset):
    """Dataset for UltraChat large-scale dialog training

    Format:
        Multi-turn conversations with diverse topics
        Each conversation has multiple exchanges

    Note: UltraChat is much larger than HH-RLHF (1.5M vs 85k)
    """

    def __init__(self, data, tokenizer, max_length: int = 128):
        """
        Args:
            data: HuggingFace dataset split (train or validation)
            tokenizer: SimpleTokenizer instance
            max_length: Maximum sequence length
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.sequences = []

        # Add progress bar for large datasets
        print(f"Processing {len(data):,} conversations...")
        for example in tqdm(data, desc="Creating sequences", ncols=80):
            # UltraChat format: 'data' field contains conversation
            # Each conversation is a list of turns
            if 'data' in example:
                conversation = example['data']

                # Convert conversation to text
                # Format: "Human: ... Assistant: ... Human: ..."
                text_parts = []
                for turn in conversation:
                    # Turn format varies, handle both string and dict
                    if isinstance(turn, str):
                        text_parts.append(turn)
                    elif isinstance(turn, dict):
                        # Extract text from dict (format may vary)
                        if 'content' in turn:
                            text_parts.append(turn['content'])
                        elif 'text' in turn:
                            text_parts.append(turn['text'])

                if text_parts:
                    text = ' '.join(text_parts)
                    # Clean up text (remove extra whitespace)
                    text = ' '.join(text.split())

                    tokens = tokenizer.encode(text)

                    # Truncate or pad to max_length
                    if len(tokens) > max_length:
                        tokens = tokens[:max_length]
                    else:
                        # Pad with 0 (PAD token)
                        tokens = tokens + [0] * (max_length - len(tokens))

                    self.sequences.append(tokens)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        seq = self.sequences[idx]
        # Input is all tokens except last, target is all tokens except first
        # This is for language modeling (predict next token)
        return torch.tensor(seq[:-1]), torch.tensor(seq[1:])


def load_ultrachat_data(config, max_samples: int = None) -> Tuple[UltraChatDataset, UltraChatDataset, object]:
    """Load UltraChat dataset using HuggingFace datasets

    Args:
        config: Configuration object with vocab_size, max_seq_length, etc.
        max_samples: Optional maximum number of samples to use (for faster experiments)
                     If None, use full dataset (1.5M+)

    Returns:
        train_dataset, val_dataset, tokenizer
    """
    print(f"Loading UltraChat dataset...")
    if max_samples:
        print(f"   Limiting to {max_samples:,} samples for faster training")

    # Load UltraChat dataset
    # Note: This is a very large dataset, loading may take time
    dataset_name = "stingning/ultrachat"

    try:
        dataset = load_dataset(dataset_name)
    except Exception as e:
        print(f"⚠️  Error loading dataset: {e}")
        print(f"   Trying alternative: HuggingFaceH4/ultrachat_200k (smaller version)")
        dataset = load_dataset("HuggingFaceH4/ultrachat_200k")

    # UltraChat structure:
    # - train split
    # - test split (use as validation)
    train_data = dataset['train']

    # Use test split as validation, or create from train split
    if 'test' in dataset:
        val_data = dataset['test']
    elif 'validation' in dataset:
        val_data = dataset['validation']
    else:
        # Split train data if no test/validation split
        print("   No test/validation split found, using 10% of train as validation")
        train_val_split = train_data.train_test_split(test_size=0.1, seed=42)
        train_data = train_val_split['train']
        val_data = train_val_split['test']

    # Limit samples if specified (for faster experiments)
    if max_samples:
        train_samples = min(max_samples, len(train_data))
        val_samples = min(max_samples // 10, len(val_data))  # 10% for validation

        train_data = train_data.select(range(train_samples))
        val_data = val_data.select(range(val_samples))

    print(f"Loaded {len(train_data):,} training examples")
    print(f"Loaded {len(val_data):,} validation examples")

    # Build vocabulary from training data (sample for speed)
    # For large datasets, sample a subset for vocab building
    vocab_sample_size = min(10000, len(train_data))
    print(f"Building vocabulary from {vocab_sample_size:,} samples...")

    train_texts = []
    for i, example in enumerate(tqdm(train_data, total=vocab_sample_size, desc="Extracting vocab text", ncols=80)):
        if i >= vocab_sample_size:
            break

        # Extract text from conversation
        if 'data' in example:
            conversation = example['data']
            text_parts = []
            for turn in conversation:
                if isinstance(turn, str):
                    text_parts.append(turn)
                elif isinstance(turn, dict):
                    if 'content' in turn:
                        text_parts.append(turn['content'])
                    elif 'text' in turn:
                        text_parts.append(turn['text'])

            if text_parts:
                text = ' '.join(text_parts)
                text = ' '.join(text.split())  # Clean whitespace
                train_texts.append(text)

    # Use SimpleTokenizer from existing code
    from .dataset import SimpleTokenizer
    tokenizer = SimpleTokenizer(vocab_size=config.vocab_size)
    tokenizer.build_vocab(train_texts)

    print(f"Built vocabulary: {len(tokenizer.word2idx)} words")

    # Create datasets
    print(f"\n{'='*80}")
    print(f"Creating datasets (this may take 10-20 minutes for full dataset)...")
    print(f"{'='*80}")
    train_dataset = UltraChatDataset(train_data, tokenizer, config.max_seq_length)
    val_dataset = UltraChatDataset(val_data, tokenizer, config.max_seq_length)

    print(f"Created {len(train_dataset):,} training sequences")
    print(f"Created {len(val_dataset):,} validation sequences")

    return train_dataset, val_dataset, tokenizer
