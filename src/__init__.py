"""
New-LLM: Context-Pythia for KV Cache Compression

Pythia-70M + Context-based KV cache compression (50% reduction).
"""
