# スケーリング実験分析: Effective Rank vs トークン数

## 実験日: 2025-11-29

## 実験条件

- **アーキテクチャ**: E案（等差減少設計）、`token_input_all_layers: False`
- **モデル**: 6層 / 768次元
- **GPU**: NVIDIA L4 (22.2GB)
- **サンプル数**: 50, 100, 200, 500

## 実験結果サマリー

| Samples | Tokens | Val PPL | Val Acc | Val ER |
|---------|--------|---------|---------|--------|
| 50 | 56,602 | 1503.3 | 9.9% | 8.5% |
| 100 | 110,516 | 1036.3 | 13.7% | 7.8% |
| 200 | 216,119 | 757.7 | 17.6% | 8.4% |
| 500 | 529,173 | 536.0 | 15.4% | 8.6% |

**スケーリング係数**: α = -0.459 (R² = 0.993)

---

## 分析: Effective Rank vs トークン数、どちらが重要か？

### 1. Effective Rank (ER) の安定性

ERは全実験で **7.8% ~ 9.0%** の狭い範囲に収まっている：

| Samples | Train ER | Val ER |
|---------|----------|--------|
| 50 | 9.6% | 8.5% |
| 100 | 8.5% | 7.8% |
| 200 | 8.7% | 8.4% |
| 500 | 9.0% | 8.6% |

**観察**: ERはサンプル数・トークン数に**ほぼ依存しない**。768次元中の約65次元（8-9%）のみが実質的に使用されている。

### 2. トークン数とパフォーマンスの関係

| Tokens | Val PPL | Val Acc |
|--------|---------|---------|
| 56K | 1503.3 | 9.9% |
| 110K | 1036.3 | 13.7% |
| 216K | 757.7 | 17.6% |
| 529K | 536.0 | 15.4% |

**観察**:
- トークン数が増えるとPPLは**明確に改善**（1503 → 536、約65%削減）
- Accuracyは200サンプルまで改善（9.9% → 17.6%）、500サンプルでやや低下（15.4%）

### 3. 結論: **トークン数が支配的要因**

#### ERが重要でない理由

1. **ERの変動が小さい**: 7.8%〜9.0%（約1.2ポイントの差）
2. **ERとパフォーマンスの相関なし**:
   - 100サンプル: ER=7.8%（最低）→ Acc=13.7%（良好）
   - 50サンプル: ER=8.5% → Acc=9.9%（最低）
   - ERが低い方がパフォーマンスが良いケースすらある

#### トークン数が重要な理由

1. **PPLとの強い相関**: R² = 0.993（非常に高い説明力）
2. **スケーリング則が成立**: α = -0.459（トークン数が2倍でPPLが約27%減少）
3. **Phase 2の学習データ量が決定的**: TokenBlockの学習に十分なデータが必要

### 4. 500サンプルでのAccuracy低下の考察

200 → 500サンプルでAccuracyが 17.6% → 15.4% に低下した理由：

1. **早期停止が早すぎた可能性**: Epoch 3で停止（200サンプルはEpoch 5）
2. **PPLは改善継続**: 757.7 → 536.0（改善している）
3. **過学習の兆候**: train_pplが急激に下がりすぎている（Epoch 1: 918 → Epoch 2: 306）

**仮説**: 500サンプルでは訓練データが多いため、モデルが訓練データに過度にフィットし、一般化性能が低下した可能性。

---

## 等差減少設計の影響

今回の実験は `token_input_all_layers: False`（等差減少設計）で実行された。

### 過去の比較（参考）

| 設計 | Val ER | 特徴 |
|------|--------|------|
| 旧構造（token_input_all_layers: True） | 55-66% | 高ER、全レイヤーでtoken入力 |
| 等差減少（token_input_all_layers: False） | 7-9% | 低ER、最初のレイヤーのみtoken入力 |

**等差減少設計ではERが大幅に低下**するが、Phase 2のパフォーマンスには必ずしも悪影響を与えていない。

---

## 推奨事項

### 1. トークン数の増加を優先

ERの改善よりも、訓練データ量（トークン数）を増やすことがパフォーマンス向上に効果的。

### 2. 早期停止の調整

500サンプル以上では、早期停止のパラメータを緩めることを検討：
- `phase2_patience` を増やす
- または固定エポック数で訓練

### 3. 旧構造（高ER）との比較実験

同一条件で `token_input_all_layers: True`（高ER）と比較し、ERがPhase 2パフォーマンスに与える影響を確認する価値あり。

---

## まとめ

| 要因 | 重要度 | 根拠 |
|------|--------|------|
| **トークン数** | ★★★★★ | PPLと強い相関 (R²=0.993)、スケーリング則成立 |
| **Effective Rank** | ★☆☆☆☆ | 変動小さく、パフォーマンスとの相関なし |

**結論**: 現在のアーキテクチャでは、**トークン数（データ量）がパフォーマンスの支配的要因**であり、Effective Rankの高低は二次的な影響しか持たない。
