# v4実験: context_dim=1085 (768×√2) の検証 (2025-11-30)

## 実験目的

context_dimを768から1085（768×√2）に増加させた場合の効果を検証。
2トークン入力（v3）と比較し、次元増加 vs 入力トークン数増加の効果を比較。

## 実験設定

| 項目 | 値 |
|------|-----|
| Config | 1L_1085d_1tok |
| context_dim | 1085 (= 768 × √2) |
| num_layers | 1 |
| num_input_tokens | 1 |
| サンプル数 | 50, 100, 200, 400, 800 |
| 実行時間 | 29分12秒 |
| Phase 1 Iter | 60（全サンプル共通） |

## 結果サマリー

| 指標 | 値 |
|------|-----|
| α | **-0.440** |
| A | 73,419 |
| R² | 0.9891 |
| Best PPL | **180.0** |
| Best Acc | **24.1%** |
| Best Val ER | 70.8% |

## 各サンプル数での詳細

| Samples | Tokens | Val ER | Val PPL | Val Acc | Train ER | Train PPL |
|---------|--------|--------|---------|---------|----------|-----------|
| 50 | 62,891 | 71.0% | 598.6 | 17.8% | 71.7% | 91.1 |
| 100 | 122,795 | 70.8% | 406.9 | 19.6% | 71.9% | 95.9 |
| 200 | 240,132 | 70.8% | 298.8 | 20.8% | 71.9% | 105.5 |
| 400 | 473,429 | 70.9% | 227.0 | 22.2% | 72.2% | 96.0 |
| 800 | 948,524 | 70.8% | **180.0** | **24.1%** | 72.2% | 90.7 |

## α値スケーリング分析（スライディングウィンドウ）

| Window | Samples | Tokens | α | A | R² |
|--------|---------|--------|------|-----|-----|
| 1 | 50-400 | 63K-473K | **-0.478** | 114,303 | 0.9934 |
| 2 | 100-800 | 123K-949K | **-0.399** | 42,899 | 0.9950 |

**α変化**: -0.478 → -0.399（+16.5%悪化）

## v3（768d/2tok）との比較

| 指標 | v3 (768d/2tok) | v4 (1085d/1tok) | 差分 |
|------|----------------|-----------------|------|
| context_dim | 768 | 1085 | +41% |
| num_input_tokens | 2 | 1 | -50% |
| α (全体) | **-0.466** | -0.440 | +5.6%悪化 |
| Best PPL | **168.0** | 180.0 | +7.1%悪化 |
| Best Val ER | **81.3%** | 70.8% | -10.5% |
| Best Acc | 24.2% | **24.1%** | ほぼ同等 |

### αスケーリング劣化の比較

| 実験 | Window 1 α | Window 2 α | 劣化率 |
|------|------------|------------|--------|
| v3 (768d/2tok) | -0.504 | -0.432 | +14.3% |
| v4 (1085d/1tok) | -0.478 | -0.399 | **+16.5%** |

v4の方がスケーリング劣化が激しい（+2.2%差）

## 分析

### 1. 次元増加の効果は限定的

- context_dim 768→1085（+41%増加）にしても、Best PPL は 180（v3の168より7%悪い）
- **次元を増やすより入力トークン数を増やす方が効果的**

### 2. Val ERの低下

- v3: 81.3% → v4: 70.8%（-10.5%）
- 次元を増やすと、活用できない次元が増える
- GPT-2のembedding次元（768）を超えると効率が落ちる

### 3. αスケーリング効率の低下

- v4は v3 より α が悪い（-0.440 vs -0.466）
- データ量増加時の劣化も v4 の方が激しい（+16.5% vs +14.3%）

### 4. Phase 1収束の遅さ

- v4: 60イテレーション（上限まで）
- v3: 23-28イテレーション
- 次元が大きいと収束に時間がかかる

## 結論

### context_dim増加 vs num_input_tokens増加

| 戦略 | Best PPL | Val ER | α | 結論 |
|------|----------|--------|------|------|
| 768d → 1085d (次元増加) | 180.0 | 70.8% | -0.440 | 効果薄い |
| 1tok → 2tok (入力増加) | **168.0** | **81.3%** | **-0.466** | **効果的** |

**結論**:
- **次元増加よりトークン数増加が効果的**
- context_dim=768（GPT-2と同じ）が最も効率的
- 2トークン入力は全指標で優位

### 推奨設定

```python
# 推奨（v3設定）
context_dim = 768      # GPT-2と同じ
num_input_tokens = 2   # 直近2トークン

# 非推奨（v4設定）
context_dim = 1085     # 次元増加は効果薄い
num_input_tokens = 1   # 情報量不足
```

## ログファイル参照

- v4実験: `importants/logs/1130_v4/`
- 比較対象v3: `importants/logs/1130_v3/`
