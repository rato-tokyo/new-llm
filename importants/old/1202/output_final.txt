From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
Already up to date.
================================================================================
LAYER CONFIGURATION COMPARISON EXPERIMENT
================================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Samples: 2000
Context dim: 500
Output: importants/logs/20251201_153820_layer_comparison

Configurations to compare:
  - L1_F1: layer=1, fnn=1 (baseline)
  - L1_F2: layer=1, fnn=2 (FFN deepened)
  - L2_F1: layer=2, fnn=1 (layer added)
================================================================================

[L1_F1] layer=1, fnn=1 (baseline)
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 180kB/s]
config.json: 100% 665/665 [00:00<00:00, 5.48MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.50MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 2.14MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.12MB/s]
Loading training data...
  Loading 2000 samples from UltraChat...
README.md: 3.90kB [00:00, 18.9MB/s]
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:02<00:00, 117MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:01<00:00, 196MB/s]  
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:00<00:00, 246MB/s]    
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:00<00:00, 94.4MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:03<00:00, 70.6MB/s]
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:01<00:00, 241MB/s]  
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:01<00:00, 192MB/s]
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:00<00:00, 106MB/s] 
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 51710.75 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 56205.20 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 71854.90 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 74017.05 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
  Validation file not found, generating: ./cache/example_val.txt
  Generated 20 validation samples (indices 50000-50019)
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
    Data: 2,403,563 train, 22,723 val tokens
2025-12-01 15:39:06.185748: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 15:39:06.201433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764603546.219500    1527 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764603546.224780    1527 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764603546.239783    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764603546.239810    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764603546.239813    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764603546.239816    1527 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 15:39:06.244812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 4.88MB/s]
model.safetensors: 100% 548M/548M [00:00<00:00, 558MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 40,210,540 params (layers=1, fnn=1)

[Phase 1] OACD: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2666 [12.5s]
  Iter 3: conv=0% loss=13.2686 [9.9s]
  Iter 4: conv=0% loss=10.7604 [9.4s]
  Iter 5: conv=0% loss=7.2892 [9.6s]
  Iter 6: conv=0% loss=4.6364 [9.4s]
  Iter 7: conv=0% loss=3.3867 [9.5s]
  Iter 8: conv=0% loss=3.0639 [9.5s]
  Iter 9: conv=0% loss=3.0013 [9.5s]
  Iter 10: conv=0% loss=2.9394 [9.8s]
  Iter 11: conv=1% loss=2.8770 [9.4s]
  Iter 12: conv=1% loss=2.8110 [9.5s]
  Iter 13: conv=2% loss=2.7113 [9.3s]
  Iter 14: conv=3% loss=2.5479 [9.4s]
  Iter 15: conv=4% loss=2.3238 [9.4s]
  Iter 16: conv=7% loss=2.0882 [9.6s]
  Iter 17: conv=11% loss=1.8952 [9.3s]
  Iter 18: conv=17% loss=1.7504 [9.6s]
  Iter 19: conv=24% loss=1.6307 [9.6s]
  Iter 20: conv=33% loss=1.5246 [9.4s]
  Iter 21: conv=43% loss=1.4308 [9.5s]
  Iter 22: conv=53% loss=1.3427 [9.4s]
  Iter 23: conv=62% loss=1.2548 [9.3s]
  Iter 24: conv=69% loss=1.1714 [9.6s]
  Iter 25: conv=75% loss=1.1009 [9.4s]
  Iter 26: conv=80% loss=1.0451 [9.4s]
  Iter 27: conv=84% loss=1.0027 [9.7s]
  Iter 28: conv=87% loss=0.9750 [9.5s]
  Iter 29: conv=90% loss=0.9633 [9.6s]
  Iter 30: conv=92% loss=0.9593 [9.4s]
  → Early stop: conv 92% >= 90%
  Done: 92% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [6.1s]
    Phase 1: 306.4s, 30 iter, conv=92%, ER=79.9%/77.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 2,403,563 train / 22,723 val tokens, 20 epochs
  Epoch 1: train_ppl=840.9 val_ppl=248.1 acc=19.7% [55.4s] ★
  Epoch 2: train_ppl=233.8 val_ppl=186.3 acc=21.3% [54.9s] ★
  Epoch 3: train_ppl=183.9 val_ppl=163.9 acc=22.4% [55.3s] ★
  Epoch 4: train_ppl=159.9 val_ppl=152.5 acc=22.8% [55.1s] ★
  Epoch 5: train_ppl=145.5 val_ppl=145.7 acc=23.3% [55.2s] ★
  Epoch 6: train_ppl=135.7 val_ppl=141.2 acc=23.6% [55.1s] ★
  Epoch 7: train_ppl=128.7 val_ppl=138.0 acc=23.7% [55.2s] ★
  Epoch 8: train_ppl=123.3 val_ppl=135.7 acc=23.9% [55.2s] ★
  Epoch 9: train_ppl=119.0 val_ppl=133.9 acc=24.0% [55.2s] ★
  Epoch 10: train_ppl=115.5 val_ppl=132.5 acc=24.1% [55.1s] ★
  Epoch 11: train_ppl=112.5 val_ppl=131.5 acc=24.2% [55.2s] ★
  Epoch 12: train_ppl=110.0 val_ppl=130.6 acc=24.3% [55.2s] ★
  Epoch 13: train_ppl=107.9 val_ppl=129.9 acc=24.4% [55.2s] ★
  Epoch 14: train_ppl=106.0 val_ppl=129.3 acc=24.5% [55.1s] ★
  Epoch 15: train_ppl=104.3 val_ppl=128.8 acc=24.6% [55.2s] ★
  Epoch 16: train_ppl=102.9 val_ppl=128.4 acc=24.6% [55.2s] ★
  Epoch 17: train_ppl=101.5 val_ppl=128.0 acc=24.6% [55.2s] ★
  Epoch 18: train_ppl=100.3 val_ppl=127.7 acc=24.6% [55.1s] ★
  Epoch 19: train_ppl=99.2 val_ppl=127.5 acc=24.7% [55.1s] ★
  Epoch 20: train_ppl=98.2 val_ppl=127.2 acc=24.7% [55.2s] ★
  Best: epoch 20, ppl=127.2, acc=24.7%
    Phase 2: 1103.3s, PPL=127.2, Acc=24.7%

[L1_F2] layer=1, fnn=2 (FFN deepened)
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
    Data: 2,403,563 train, 22,723 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 43,428,724 params (layers=1, fnn=2)

[Phase 1] OACD: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=17.6605 [12.7s]
  Iter 3: conv=0% loss=13.2272 [12.6s]
  Iter 4: conv=0% loss=9.0354 [12.6s]
  Iter 5: conv=0% loss=8.8438 [12.5s]
  Iter 6: conv=0% loss=9.2883 [12.5s]
  Iter 7: conv=0% loss=8.7603 [12.6s]
  Iter 8: conv=0% loss=7.6322 [12.6s]
  Iter 9: conv=0% loss=6.7968 [12.6s]
  Iter 10: conv=0% loss=5.9760 [12.6s]
  Iter 11: conv=0% loss=4.9586 [12.6s]
  Iter 12: conv=1% loss=4.3262 [12.6s]
  Iter 13: conv=3% loss=3.9083 [12.5s]
  Iter 14: conv=5% loss=3.4660 [12.6s]
  Iter 15: conv=11% loss=3.0892 [12.6s]
  Iter 16: conv=17% loss=2.7454 [12.6s]
  Iter 17: conv=21% loss=2.4347 [12.6s]
  Iter 18: conv=27% loss=2.1785 [12.7s]
  Iter 19: conv=32% loss=1.8865 [12.6s]
  Iter 20: conv=36% loss=1.6230 [12.5s]
  Iter 21: conv=41% loss=1.4990 [12.7s]
  Iter 22: conv=53% loss=1.4033 [12.6s]
  Iter 23: conv=61% loss=1.2085 [12.6s]
  Iter 24: conv=63% loss=0.9999 [12.6s]
  Iter 25: conv=66% loss=0.9009 [12.6s]
  Iter 26: conv=74% loss=0.8783 [12.6s]
  Iter 27: conv=81% loss=0.8189 [12.6s]
  Iter 28: conv=82% loss=0.7114 [12.6s]
  Iter 29: conv=82% loss=0.6261 [12.6s]
  Iter 30: conv=86% loss=0.5888 [12.6s]
  Iter 31: conv=91% loss=0.5486 [12.6s]
  → Early stop: conv 91% >= 90%
  Done: 91% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [7.0s]
    Phase 1: 406.7s, 31 iter, conv=91%, ER=71.0%/69.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,586,756/43,428,724 parameters

[Phase 2] 2,403,563 train / 22,723 val tokens, 20 epochs
  Epoch 1: train_ppl=414.1 val_ppl=224.8 acc=19.9% [58.4s] ★
  Epoch 2: train_ppl=212.7 val_ppl=186.8 acc=21.4% [57.9s] ★
  Epoch 3: train_ppl=171.5 val_ppl=172.1 acc=21.9% [59.0s] ★
  Epoch 4: train_ppl=148.8 val_ppl=165.0 acc=22.3% [58.5s] ★
  Epoch 5: train_ppl=133.6 val_ppl=161.2 acc=22.7% [58.9s] ★
  Epoch 6: train_ppl=122.4 val_ppl=158.6 acc=22.7% [58.6s] ★
  Epoch 7: train_ppl=113.7 val_ppl=157.2 acc=22.9% [58.9s] ★
  Epoch 8: train_ppl=106.5 val_ppl=156.9 acc=23.0% [58.9s] ★
  Epoch 9: train_ppl=100.6 val_ppl=157.3 acc=23.0% [59.0s]
  → Early stop at epoch 9
  Best: epoch 8, ppl=156.9, acc=23.0%
    Phase 2: 528.1s, PPL=156.9, Acc=23.0%

[L2_F1] layer=2, fnn=1 (layer added)
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
    Data: 2,403,563 train, 22,723 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 41,822,168 params (layers=2, fnn=1)

[Phase 1] OACD: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=10.9875 [11.1s]
  Iter 3: conv=0% loss=7.7593 [10.9s]
  Iter 4: conv=0% loss=3.4557 [10.9s]
  Iter 5: conv=0% loss=1.9233 [10.9s]
  Iter 6: conv=0% loss=1.8303 [10.8s]
  Iter 7: conv=1% loss=1.7446 [10.9s]
  Iter 8: conv=3% loss=1.4748 [10.8s]
  Iter 9: conv=9% loss=1.2781 [10.9s]
  Iter 10: conv=26% loss=1.2127 [10.8s]
  Iter 11: conv=58% loss=1.1600 [10.8s]
  Iter 12: conv=80% loss=1.0139 [10.8s]
  Iter 13: conv=89% loss=0.8139 [10.8s]
  Iter 14: conv=93% loss=0.6642 [10.9s]
  → Early stop: conv 93% >= 90%
  Done: 93% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [13.0s]
    Phase 1: 172.9s, 14 iter, conv=93%, ER=81.5%/79.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,953,792/41,822,168 parameters

[Phase 2] 2,403,563 train / 22,723 val tokens, 20 epochs
  Epoch 1: train_ppl=427.9 val_ppl=210.9 acc=20.9% [58.0s] ★
  Epoch 2: train_ppl=196.9 val_ppl=167.0 acc=22.4% [57.4s] ★
  Epoch 3: train_ppl=154.8 val_ppl=150.0 acc=23.2% [57.8s] ★
  Epoch 4: train_ppl=132.7 val_ppl=141.0 acc=23.9% [57.6s] ★
  Epoch 5: train_ppl=118.4 val_ppl=135.8 acc=24.2% [57.8s] ★
  Epoch 6: train_ppl=108.3 val_ppl=132.6 acc=24.5% [57.6s] ★
  Epoch 7: train_ppl=100.5 val_ppl=130.5 acc=24.5% [57.7s] ★
  Epoch 8: train_ppl=94.4 val_ppl=129.2 acc=24.7% [57.7s] ★
  Epoch 9: train_ppl=89.4 val_ppl=128.4 acc=24.7% [57.7s] ★
  Epoch 10: train_ppl=85.2 val_ppl=128.1 acc=24.8% [57.7s] ★
  Epoch 11: train_ppl=81.7 val_ppl=128.1 acc=24.9% [57.7s] ★
  Epoch 12: train_ppl=78.7 val_ppl=128.3 acc=24.8% [57.8s]
  → Early stop at epoch 12
  Best: epoch 11, ppl=128.1, acc=24.9%
    Phase 2: 692.5s, PPL=128.1, Acc=24.9%

================================================================================
SUMMARY
================================================================================

Config     Layers   FFN    Params       Val PPL    Acc      ER%      Time      
--------------------------------------------------------------------------------
L1_F1      1        1      40,210,540  127.2      24.7    % 77.8     1409.7    s
L1_F2      1        2      43,428,724  156.9      23.0    % 69.2     934.9     s
L2_F1      2        1      41,822,168  128.1      24.9    % 79.7     865.4     s

--------------------------------------------------------------------------------

Best PPL:  L1_F1 (PPL=127.2)
Best Acc:  L2_F1 (Acc=24.9%)

Total time: 3311.0s (55.2 min)

Results saved to: importants/logs/20251201_153820_layer_comparison/results.txt

================================================================================
DONE
================================================================================
