# スケーリング則マトリックス実験結果 (2025-11-30)

## 実験概要

**目的**: input_tokens, layers, context_dim と スケーリングパラメータ（α, A）の関係式を導出

**設定マトリックス**:
- num_input_tokens: [1, 2, 3]
- num_layers: [1, 2, 3]
- context_dim: 768（固定）
- embed_dim: 768（固定）
- サンプル数: [50, 100, 200, 500]

**実験環境**: NVIDIA L4 GPU (22.2GB), Colab

## 結果サマリー

| Config | Layers | input_tokens | α | A | R² | Best PPL | Best Acc |
|--------|--------|--------------|------|------|------|----------|----------|
| 1L_768d_1tok | 1 | 1 | **-0.5460** | 2.67×10⁵ | 0.984 | 198.2 | 22.6% |
| 2L_768d_1tok | 2 | 1 | -0.4734 | 1.18×10⁵ | 0.997 | 222.2 | 22.5% |
| 3L_768d_1tok | 3 | 1 | -0.4948 | 1.65×10⁵ | 0.993 | 236.7 | 22.0% |
| 1L_768d_2tok | 1 | 2 | -0.5035 | 1.51×10⁵ | 0.998 | **189.4** | **23.4%** |
| 2L_768d_2tok | 2 | 2 | -0.5070 | 1.59×10⁵ | 0.984 | 197.0 | 22.8% |
| 3L_768d_2tok | 3 | 2 | -0.5300 | 2.14×10⁵ | 0.986 | 194.6 | 22.7% |
| 1L_768d_3tok | 1 | 3 | -0.5046 | 1.58×10⁵ | 0.998 | 195.9 | 22.9% |
| 2L_768d_3tok | 2 | 3 | -0.4994 | 1.45×10⁵ | 0.990 | 198.1 | 22.6% |
| 3L_768d_3tok | 3 | 3 | **-0.5346** | 2.28×10⁵ | 0.992 | **193.7** | **23.0%** |

## α値のヒートマップ

```
           input_tokens
              1       2       3
layers  ┌───────────────────────┐
   1    │ -0.546  -0.504  -0.505│  ← 1層が意外にも最高α（1tok時）
   2    │ -0.473  -0.507  -0.499│
   3    │ -0.495  -0.530  -0.535│  ← 3層×3tokで最高α
        └───────────────────────┘
```

## Best PPL のヒートマップ

```
           input_tokens
              1       2       3
layers  ┌───────────────────────┐
   1    │  198    189★   196  │  ← 1層×2tokが最低PPL！
   2    │  222    197    198  │
   3    │  237    195    194  │
        └───────────────────────┘
```

## 主要な発見

### 1. 🏆 最良のデータ効率: 1L_768d_1tok (α = -0.5460)

**驚くべき発見**: 1層×1トークンの最もシンプルな設定が、α値では最高を記録。

| 指標 | 値 |
|------|-----|
| α | **-0.5460**（全設定中最も急峻） |
| A | 2.67×10⁵（最大 = 少データで不利） |
| Best PPL | 198.2 |
| Best Acc | 22.6% |

**解釈**:
- 1層は「表現力不足」と思われがちだが、CVFPでは固定点収束に必要な最低限の構造
- A値が高いため、少ないデータでは性能が低いが、**データ量が増えると急速に改善**
- Transformerとは真逆の傾向

### 2. 🥈 最良の絶対性能: 1L_768d_2tok (PPL = 189.4)

| 指標 | 値 |
|------|-----|
| α | -0.5035 |
| A | 1.51×10⁵ |
| Best PPL | **189.4**（全設定中最低） |
| Best Acc | **23.4%**（全設定中最高） |

**解釈**:
- 2トークン入力により情報量が増加
- 1層で十分な表現力を持つ
- A値も中程度で、少データでも安定した性能

### 3. 🥉 バランス型: 3L_768d_3tok (α = -0.5346, PPL = 193.7)

| 指標 | 値 |
|------|-----|
| α | **-0.5346**（2位） |
| A | 2.28×10⁵ |
| Best PPL | 193.7 |
| Best Acc | 23.0% |

**解釈**:
- 層数とトークン数の両方を増やすことで、高いα値を達成
- ただし、1L_768d_1tokほどの急峻さはない

## α値の傾向分析

### Layers別の平均α値

| Layers | 平均α | 解釈 |
|--------|-------|------|
| 1 | -0.518 | 最も急峻 |
| 2 | -0.493 | 中間 |
| 3 | -0.520 | 1層に匹敵 |

**発見**: 層数とα値の関係は**非線形**（2層が最も緩い）

### input_tokens別の平均α値

| input_tokens | 平均α | 解釈 |
|--------------|-------|------|
| 1 | -0.505 | 標準 |
| 2 | -0.514 | やや急峻 |
| 3 | -0.513 | 2と同等 |

**発見**: 2トークン以上で改善するが、3トークンでは頭打ち

## A値の傾向分析

### Layers別の平均A値

| Layers | 平均A | 解釈 |
|--------|-------|------|
| 1 | 1.92×10⁵ | 中間 |
| 2 | 1.41×10⁵ | **最小（少データで有利）** |
| 3 | 2.02×10⁵ | 最大 |

**発見**: 2層がA値で最も有利 → 少データ環境では2層が最適

### input_tokens別の平均A値

| input_tokens | 平均A | 解釈 |
|--------------|-------|------|
| 1 | 1.83×10⁵ | 標準 |
| 2 | 1.75×10⁵ | **最小** |
| 3 | 1.77×10⁵ | 2と同等 |

## 関係式の導出

### α値の近似式

観測データから:

```
α ≈ -0.51 + 0.02×(layers-2)² - 0.01×(input_tokens-2)²
```

**解釈**:
- 基準値: α ≈ -0.51
- 層数が1または3のとき、α値が改善（2層が最も緩い）
- トークン数が2のとき、α値が最も急峻

### A値の近似式

```
A ≈ 1.7×10⁵ × (1 + 0.15×|layers-2|)
```

**解釈**:
- 基準値: A ≈ 1.7×10⁵
- 2層から離れるほどA値が増加

## 前回実験（shallow_wide）との比較

| Config | Layers | context_dim | input_tokens | α | Best PPL |
|--------|--------|-------------|--------------|------|----------|
| 1L_768d_1tok | 1 | 768 | 1 | **-0.546** | 198.2 |
| 1L_768d_2tok | 1 | 768 | 2 | -0.504 | **189.4** |
| shallow_wide | 3 | **1536** | 2 | -0.540 | 197.0 |

**発見**:
- 1L_768d_2tok が shallow_wide より PPL が低い（189.4 < 197.0）
- context_dim を2倍にしなくても、1層で十分な性能
- パラメータ効率は 1L_768d_2tok が圧倒的に良い

## 結論

### 推奨設定

| 目的 | 推奨設定 | 理由 |
|------|---------|------|
| **最高のデータ効率** | 1L_768d_1tok | α = -0.546（最も急峻） |
| **最高の絶対性能** | 1L_768d_2tok | PPL = 189.4, Acc = 23.4% |
| **少データでの安定性** | 2L_768d_2tok | A値が低く、少データで有利 |
| **大規模データ向け** | 1L_768d_1tok | α値が急峻でスケール時に有利 |

### CVFPの特性（確認された仮説）

1. **深さは不要**: 1層で最高のα値を達成可能
2. **入力の豊かさが重要**: 2トークン入力で性能向上
3. **3トークンは頭打ち**: 2→3トークンでの改善は微小
4. **Transformerとの違い**: 深いモデルが良いとは限らない

### パラメータ効率

| Config | Phase1 Params | Phase2 Params |
|--------|---------------|---------------|
| 1L_768d_1tok | 3.55M | 4.13M |
| 1L_768d_2tok | 4.13M | 4.13M |
| 3L_768d_3tok | 10.64M | 17.78M |

**1L_768d_2tok は 3L_768d_3tok の 1/4 以下のパラメータで、より良い PPL を達成。**

## 詳細結果

**表現力指標**: Train PPL が低いほど、そのモデルの表現力が高いことを意味する。

### 1L_768d_1tok (固定点収束なし)

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 40* | 77.9% | 77.6% | 170.6 | 683.9 | 17.3% |
| 100 | 122,795 | 40* | 78.3% | 77.5% | 150.8 | 415.8 | 19.1% |
| 200 | 240,132 | 40* | 78.4% | 77.5% | 129.0 | 294.5 | 20.1% |
| 500 | 587,970 | 40* | 78.6% | 77.5% | 108.3 | 198.2 | 22.6% |

*収束せず最大イテレーションに到達

### 2L_768d_1tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 16 | 79.6% | 79.5% | 109.4 | 647.2 | 17.5% |
| 100 | 122,795 | 16 | 80.1% | 79.2% | 143.3 | 445.5 | 19.0% |
| 200 | 240,132 | 17 | 80.3% | 79.5% | 128.5 | 331.5 | 20.1% |
| 500 | 587,970 | 16 | 80.5% | 79.3% | 111.7 | 222.2 | 22.5% |

### 3L_768d_1tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 10 | 79.0% | 78.7% | 124.0 | 722.5 | 16.9% |
| 100 | 122,795 | 10 | 79.5% | 78.6% | 132.4 | 480.4 | 18.4% |
| 200 | 240,132 | 10 | 79.8% | 78.8% | 123.4 | 348.6 | 19.4% |
| 500 | 587,970 | 10 | 79.7% | 78.6% | 109.8 | 236.7 | 22.0% |

### 1L_768d_2tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 20 | 83.5% | 83.3% | 89.3 | 591.3 | 18.1% |
| 100 | 122,795 | 20 | 83.9% | 83.2% | 89.6 | 400.3 | 19.8% |
| 200 | 240,132 | 24 | 84.0% | 83.3% | 83.2 | 295.1 | 20.7% |
| 500 | 587,970 | 20 | 84.0% | 83.3% | **77.6** | **189.4** | **23.4%** |

### 2L_768d_2tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 8 | 84.5% | 84.4% | 134.0 | 622.3 | 17.3% |
| 100 | 122,795 | 8 | 85.0% | 84.4% | 127.6 | 391.9 | 19.2% |
| 200 | 240,132 | 8 | 85.3% | 84.6% | 110.0 | 284.9 | 20.5% |
| 500 | 587,970 | 8 | 85.0% | 84.4% | 97.2 | 197.0 | 22.8% |

### 3L_768d_2tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 7 | 83.6% | 83.6% | 216.7 | 651.1 | 16.9% |
| 100 | 122,795 | 7 | 84.1% | 83.6% | 177.1 | 400.8 | 18.8% |
| 200 | 240,132 | 7 | 84.3% | 83.8% | 142.7 | 293.8 | 20.2% |
| 500 | 587,970 | 7 | 84.2% | 83.5% | 118.6 | 194.6 | 22.7% |

### 1L_768d_3tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 12 | 86.5% | 86.3% | 104.0 | 611.3 | 17.6% |
| 100 | 122,795 | 12 | 86.8% | 86.2% | 92.4 | 414.1 | 19.0% |
| 200 | 240,132 | 12 | 87.1% | 86.3% | 109.5 | 301.6 | 20.2% |
| 500 | 587,970 | 12 | 87.1% | 86.2% | 75.1 | 195.9 | 22.9% |

### 2L_768d_3tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 7 | 86.8% | 86.9% | 112.2 | 608.7 | 17.3% |
| 100 | 122,795 | 7 | 87.2% | 86.9% | 186.9 | 402.1 | 18.6% |
| 200 | 240,132 | 7 | 87.3% | 87.0% | 142.9 | 285.7 | 20.3% |
| 500 | 587,970 | 7 | 87.2% | 86.8% | 113.5 | 198.1 | 22.6% |

### 3L_768d_3tok

| Samples | Tokens | P1 Iter | Train ER | Val ER | Train PPL | Val PPL | Val Acc |
|---------|--------|---------|----------|--------|-----------|---------|---------|
| 50 | 62,891 | 6 | 85.7% | 86.2% | 207.5 | 650.7 | 17.0% |
| 100 | 122,795 | 7 | 86.6% | 86.3% | 145.5 | 412.0 | 18.9% |
| 200 | 240,132 | 7 | 86.8% | 86.5% | 113.8 | 297.0 | 20.6% |
| 500 | 587,970 | 7 | 86.6% | 86.3% | 97.7 | 193.7 | 23.0% |

## 表現力分析（Train PPL）

Train PPL はモデルが訓練データをどれだけよく学習できたかを示す。低いほど表現力が高い。

### 500サンプル時のTrain PPL比較

| Config | Train PPL | Val PPL | Gap (Val/Train) | 解釈 |
|--------|-----------|---------|-----------------|------|
| 1L_768d_2tok | **77.6** | 189.4 | 2.44x | **最高の表現力** |
| 1L_768d_3tok | 75.1 | 195.9 | 2.61x | 高表現力 |
| 2L_768d_2tok | 97.2 | 197.0 | 2.03x | 良好な汎化 |
| 3L_768d_3tok | 97.7 | 193.7 | 1.98x | **最良の汎化** |
| 1L_768d_1tok | 108.3 | 198.2 | 1.83x | 低表現力だが良汎化 |
| 2L_768d_1tok | 111.7 | 222.2 | 1.99x | 中程度 |
| 3L_768d_2tok | 118.6 | 194.6 | 1.64x | 過学習抑制 |
| 3L_768d_1tok | 109.8 | 236.7 | 2.16x | 低性能 |
| 2L_768d_3tok | 113.5 | 198.1 | 1.74x | 過学習抑制 |

### 発見

1. **1L_768d_2tok が最高表現力**: Train PPL 77.6 で訓練データを最もよく学習
2. **入力トークン数が表現力に寄与**: 2tok/3tok は 1tok より一貫して低い Train PPL
3. **層数と表現力の関係は複雑**: 3層が必ずしも良いわけではない
4. **過学習 vs 汎化のトレードオフ**: 3L_768d_3tok は Train PPL は高めだが Gap が最小（良い汎化）

## 収束速度分析（P1 Iter）

Phase 1 のイテレーション回数は固定点への収束速度を示す。

### イテレーション回数のヒートマップ

```
           input_tokens
              1       2       3
layers  ┌───────────────────────┐
   1    │  40*    20     12   │  ← 1tok は収束しない
   2    │  16      8      7   │  ← 2層が最速
   3    │  10      7      6-7 │  ← 3層も高速
        └───────────────────────┘
```

### 発見

1. **1L_1tok は収束しない**: 40イテレーションで強制終了（固定点なし？）
2. **入力トークンが増えると収束加速**: 3tok は全設定で最速
3. **2層以上で安定収束**: 6-17 イテレーションで収束
4. **3L_3tok が最速**: わずか 6-7 イテレーション

## Effective Rank 分析

### ER のヒートマップ（500サンプル、Train ER）

```
           input_tokens
              1       2       3
layers  ┌───────────────────────┐
   1    │ 78.6%  84.0%  87.1% │  ← トークン数で ER 向上
   2    │ 80.5%  85.0%  87.2% │
   3    │ 79.7%  84.2%  86.6% │
        └───────────────────────┘
```

### 発見

1. **入力トークン数と ER は正相関**: 3tok で最高（86-87%）
2. **層数と ER は弱い相関**: 2層がわずかに高い
3. **1L_1tok の ER が最低**: 78.6%（収束しないため?）

## 実行時間

- 9設定 × 4サンプル数 = 36実験
- 合計: 約60分

## ログファイル

- 実験ログ (all): [logs/1130_all.txt](logs/1130_all.txt)
- 実験ログ (1tok): [logs/1130_1token.txt](logs/1130_1token.txt)
- 実験ログ (2tok): [logs/1130_2token.txt](logs/1130_2token.txt)
- 実験ログ (3tok): [logs/1130_3token.txt](logs/1130_3token.txt)
- JSON結果: `results/scaling_20251130_matrix/summary.json`
