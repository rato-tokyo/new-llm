# 並列化チューニング実験結果 (2025-11-25)

## 実験目的

並列版のEffective Rankを改善するため、以下の2つのパラメータを調整：
1. **max_iterations**: イテレーション数を増やして収束を改善
2. **dist_reg_weight**: 多様性促進の重みを増やしてEffective Rankを向上

## 実験設定

- **データ**: 1000トークン (data/small_train.txt)
- **共通設定**:
  - `layernorm_mix = 1.0` (ベースライン)
  - `learning_rate = 0.002`
  - 並列版のみ（シーケンシャル版は比較用）

## 実験グループ

### グループA: イテレーション数増加（dwr=0.5固定）
- **Exp-A1**: iter=20, dwr=0.5
- **Exp-A2**: iter=30, dwr=0.5
- **Exp-A3**: iter=50, dwr=0.5

### グループB: 多様性強化（iter=10固定）
- **Exp-B1**: iter=10, dwr=0.7
- **Exp-B2**: iter=10, dwr=0.8
- **Exp-B3**: iter=10, dwr=0.9 ⭐

### グループC: 組み合わせ
- **Exp-C1**: iter=30, dwr=0.7
- **Exp-C2**: iter=30, dwr=0.8

## 全実験結果

| 実験名 | Iter | DWR | 時間(秒) | ER(%) | ベースライン比 | 判定 |
|--------|------|-----|----------|-------|---------------|------|
| Baseline | 10 | 0.5 | 11.35 | 34.5 | - | ❌ |
| Exp-A1 | 20 | 0.5 | 12.68 | 36.9 | +2.4% | ❌ |
| Exp-A2 | 30 | 0.5 | 12.61 | 36.9 | +2.4% | ❌ |
| Exp-A3 | 50 | 0.5 | 13.24 | 36.9 | +2.4% | ❌ |
| Exp-B1 | 10 | 0.7 | 11.81 | 44.4 | +9.9% | ❌ |
| Exp-B2 | 10 | 0.8 | 11.57 | 48.6 | +14.1% | ❌ |
| **Exp-B3** | **10** | **0.9** | **11.59** | **55.9** | **+21.4%** | **✅** |
| Exp-C1 | 30 | 0.7 | 14.92 | 43.3 | +8.8% | ❌ |
| Exp-C2 | 30 | 0.8 | 15.40 | 45.6 | +11.1% | ❌ |

## 最良結果: Exp-B3

### 性能

```
設定:
- max_iterations: 10
- dist_reg_weight: 0.9 (多様性90%, CVFP 10%)

結果:
- Effective Rank: 55.9% (429.26/768)
- Actual Rank: 100.0% (768/768)
- 処理時間: 11.59秒
- 収束率: 27.2% (272/1000トークン)
```

### ベースライン比較

| 指標 | シーケンシャル | 並列(Baseline) | 並列(Exp-B3) | 改善 |
|------|--------------|---------------|-------------|------|
| Effective Rank | 64.3% | 34.5% | **55.9%** | **+21.4%** |
| 処理時間 | 265.98秒 | 11.35秒 | **11.59秒** | 23x高速化 |
| ベースライン66.6%比 | -2.3% | -32.1% | **-10.7%** | **+21.4%改善** |

## 重要な発見

### 1. イテレーション数増加は効果薄い ❌

**結果**:
- Exp-A1/A2/A3 (iter=20/30/50, dwr=0.5): **全て36.9% ER**
- イテレーション数を2倍、3倍、5倍にしても改善なし

**原因**:
- 並列版の「1トークン分のずれ」による情報遅延は、イテレーション数では解決できない
- 固定点への収束精度の問題であり、収束回数の問題ではない

**結論**: **イテレーション数増加は無効**

### 2. dist_reg_weight増加が極めて効果的 ✅

**結果**:
| DWR | ER(%) | 改善 |
|-----|-------|------|
| 0.5 | 34.5 | - |
| 0.7 | 44.4 | +9.9% |
| 0.8 | 48.6 | +14.1% |
| **0.9** | **55.9** | **+21.4%** |

**原因**:
- 多様性損失の重みを増やすことで、次元の偏りを直接改善
- 並列版の弱点（情報遅延による偏り）を多様性正則化で補償

**結論**: **dist_reg_weight=0.9が最適**

### 3. 組み合わせは微増のみ ⚠️

**結果**:
- Exp-C1 (iter=30, dwr=0.7): 43.3% ER
- Exp-C2 (iter=30, dwr=0.8): 45.6% ER
- Exp-B3 (iter=10, dwr=0.9): **55.9% ER** ← 最良

**原因**:
- イテレーション増加の効果が薄いため、組み合わせてもdwr単独には及ばない
- むしろイテレーション増加により処理時間が増える（11.6秒 → 15.4秒）

**結論**: **単純にdwr=0.9のみで十分**

## トレードオフ分析

### メリット

1. **圧倒的な高速化**: 23x (265.98秒 → 11.59秒)
2. **Effective Rank大幅改善**: ベースライン34.5% → 55.9% (+21.4%)
3. **実装が単純**: パラメータ1つの変更のみ
4. **処理時間変化なし**: 11.35秒 → 11.59秒（+0.24秒のみ）

### デメリット

1. **ベースライン未達**: 66.6% → 55.9% (-10.7%)
2. **シーケンシャル未達**: 64.3% → 55.9% (-8.4%)
3. **CVFP学習の弱体化**: dwr=0.9により、CVFP損失の重みが10%のみ

## 判定

### ⚠️ 条件付き採用

**判定基準**:
- ✅ ER ≥ 50%: **55.9%** クリア
- ⚠️ ベースライン比: -10.7%（許容範囲ギリギリ）
- ✅ 高速化: 23x（圧倒的）

**総合判定**:
- **速度優先の場合**: ✅ 推奨（23x高速化、ER 55.9%は実用的）
- **品質優先の場合**: ❌ 不採用（ベースライン66.6%に及ばず）
- **バランス重視の場合**: ⚠️ 検討の余地あり（トレードオフ次第）

## 推奨設定

### 採用する場合の設定

```python
# 並列版の最適設定
max_iterations = 10  # イテレーション増加は不要
dist_reg_weight = 0.9  # 多様性を最大限強化
learning_rate = 0.002  # 変更なし
layernorm_mix = 1.0  # 変更なし
```

**期待される性能**:
- Effective Rank: **55.9%**
- 処理時間: **11.6秒**（シーケンシャル265秒の4.4%）
- 高速化率: **23x**

## 今後の改善案

### さらなるチューニングの可能性

1. **dist_reg_weight = 0.95 を試す**:
   - dwr=0.9で55.9%達成
   - dwr=0.95でさらに改善の可能性
   - ただし、CVFP学習がさらに弱くなるリスク

2. **learning_rateの調整**:
   - 現状0.002
   - 0.001に下げて精密な学習を試す

3. **ハイブリッド手法**:
   - 最初の数イテレーションはdwr=0.9
   - 後半はdwr=0.5に戻してCVFP学習強化

## 結論

**dist_reg_weight=0.9により、並列版のEffective Rankを34.5%から55.9%に改善することに成功した。**

**採用推奨条件**:
- ✅ リアルタイム推論など、速度が最優先の場合
- ✅ Effective Rank 55.9%を許容できる場合
- ❌ ベースライン66.6%の品質が必須の場合

**最終推奨**:
- **速度優先ユースケース**: ✅ **Exp-B3設定を採用**
- **品質優先ユースケース**: ❌ シーケンシャル版を維持

---

**Last Updated**: 2025-11-25
**Commit**: 9ee3281
**Status**: Tuning Complete - Conditional Recommendation (dwr=0.9)
