# Scaling Experiment Report (2025-11-27)

## 実験概要

Colab L4 GPUを使用したサンプル数スケーリング実験。
訓練トークン数とValidation PPLの関係を調査し、スケーリング則を導出。

## 実験環境

- **GPU**: NVIDIA L4 (23.8GB)
- **モデル**: E案アーキテクチャ (ContextBlock + TokenBlock)
  - ContextBlock: 3 layers
  - TokenBlock: 3 layers
  - context_dim: 768
- **パラメータ数**: 84,338,257 (全体), 42,193,489 (Phase 2学習対象)
- **Phase 1設定**:
  - max_iterations: 10
  - learning_rate: 0.002
  - dist_reg_weight: 0.5
- **Phase 2設定**:
  - epochs: 10
  - batch_size: 512
  - patience: 2 (early stopping)

## 実験結果

| Samples | Train Tokens | Val Tokens | Val PPL | Val Acc | Train ER% | Val ER% | Time |
|---------|-------------|------------|---------|---------|-----------|---------|------|
| 50 | 3,495 | 690 | 7,152.28 | 10.01% | 77.9% | 58.0% | 1.4min |
| 100 | 7,466 | 690 | 3,360.15 | 7.40% | 79.7% | 57.4% | 1.9min |
| 200 | 15,832 | 690 | 2,439.31 | 9.14% | 79.6% | 56.9% | 3.9min |
| 500 | 39,814 | 690 | 1,171.15 | 13.79% | 78.9% | 56.3% | 9.8min |

**Total experiment time**: 17.2 minutes

## スケーリング則

### PPL vs Train Tokens

PPLはべき乗則（Power Law）に従う：

```
PPL = A × N^(-α)
```

対数線形回帰の結果：

| log(N) | N (tokens) | log(PPL) | PPL |
|--------|------------|----------|-----|
| 8.159 | 3,495 | 8.876 | 7,152 |
| 8.918 | 7,466 | 8.120 | 3,360 |
| 9.670 | 15,832 | 7.800 | 2,439 |
| 10.592 | 39,814 | 7.066 | 1,171 |

### 導出された数式

```
log(PPL) = 14.9 - 0.74 × log(N)
```

または：

```
PPL = 2,970,000 × N^(-0.74)
```

**スケーリング指数 α = 0.74**

## PPL予測表

| Train Tokens | 予測 PPL | 備考 |
|--------------|----------|------|
| 3,495 | 7,152 | 実測値 |
| 7,466 | 3,360 | 実測値 |
| 15,832 | 2,439 | 実測値 |
| 39,814 | 1,171 | 実測値 |
| 100,000 | ~350 | 予測 |
| 500,000 | ~110 | 予測 |
| 1,000,000 | ~70 | 予測 |
| 10,000,000 | ~12 | 予測 |

## 観察事項

### Phase 1 (CVFP Learning)
- 全サンプル数で**9イテレーション**で早期停止（収束率 > 95%）
- Train Effective Rank: 77.9% - 79.7%（安定）
- Val Effective Rank: 56.3% - 58.0%（サンプル増加で微減）
- 検証データはすべて**CONVERGED**状態

### Phase 2 (Next-Token Prediction)
- 全実験で**Epoch 1が最良**（early stoppingでEpoch 3-4で停止）
- 過学習傾向が強い（TrainとValの乖離が大きい）
- サンプル数増加でVal Accが改善（10.01% → 13.79%）

### 問題点・課題
1. **Val Accの不規則性**: 100サンプルで7.40%と低下
   - early stoppingが早すぎた可能性
2. **過学習**: Train Lossは下がるがVal Lossが上昇
   - 正則化の強化が必要かもしれない
3. **データポイント不足**: 4点のみでスケーリング則を推定
   - より多くのデータポイントで精度向上が期待される

## GPU消費量

| Samples | Train Tokens | GPU Memory |
|---------|-------------|------------|
| 50 | 3,495 | ~4.8GB |
| 200 | 15,832 | ~6GB (batch_size=512) |
| 500 | 39,814 | ~8GB (推定) |

**注意**: `phase2_batch_size=16000`だと16GB超消費。512推奨。

## 実験データ

- **出力ファイル**: `results/colab_scaling_experiment.json`
- **ログ**: `colab_output.txt`

## 結論

1. **スケーリング則の確認**: PPLはトークン数のべき乗に反比例（α=0.74）
2. **実用的なPPL達成には大量データが必要**: PPL<100には約50万トークン以上が必要
3. **モデルアーキテクチャの検証**: E案（レイヤー対応版）は機能している
4. **過学習対策が次の課題**: 正則化、dropout、データ増強などの検討
