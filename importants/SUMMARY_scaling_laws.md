# スケーリング則まとめ (Scaling Laws Summary)

**作成日**: 2025-12-01
**統合元ファイル**: scaling_experiment_*.md, analysis-phase1-ppl-correlation.md, key-findings-2025-11-29.md

---

## 1. スケーリング則の基本式

```
PPL = A × tokens^α
```

- **α (スケーリング指数)**: 負の値。絶対値が大きいほどデータ増加の恩恵が大きい
- **A**: 定数項（モデル・設定依存）
- **R²**: 決定係数（1に近いほどフィットが良い）

### α値の解釈

| α値 | 2倍データでのPPL削減率 |
|-----|----------------------|
| -0.3 | 約19% |
| -0.4 | 約24% |
| -0.5 | 約29% |
| -0.6 | 約34% |
| -0.7 | 約38% |

---

## 2. 重要な発見

### 2.1 トークン数が支配的要因

**結論**: PPLの改善はEffective Rank (ER)ではなく、**トークン数（データ量）**に支配される

| 要因 | 重要度 | 根拠 |
|------|--------|------|
| **トークン数** | ★★★★★ | PPLと強い相関 (R²=0.99+) |
| **Effective Rank** | ★★☆☆☆ | PPLとの相関は弱い |

### 2.2 ERとαのトレードオフ

- **Val ER ↑ → α が緩やか（悪化）**
- **Val ER ↓ → α が急峻（改善）**

ただし、**最終PPLではER高い設定が有利**（絶対性能が重要）

### 2.3 モデルキャパシティの限界

α値がデータ量増加に伴い悪化する傾向：
- Window 1 (50-400 samples): α = -0.504
- Window 2 (100-800 samples): α = -0.432

**解釈**: 1層・768次元ではデータ増加の恩恵を十分に受けられない → レイヤー数増加が必要

---

## 3. 実験結果サマリー

### 3.1 アルゴリズム別最良結果（2025-12-01時点）

| Algorithm | Best PPL | Best α | 条件 |
|-----------|----------|--------|------|
| **MCDL** | **289.1** | -0.423 | 通常Layer, cd=500 |
| ODCM | 308.5 | **-0.568** | 通常Layer, cd=500 |
| OACD | 290.1 | -0.509 | dwr=1.0 |

### 3.2 設定別α値比較

| 設定 | α | Val ER | Best PPL |
|------|------|--------|----------|
| 1L_768d_2tok (v3) | -0.466 | 81.3% | 168.0 |
| 1L_768d_1tok (v1) | -0.546 | 77.5% | 198.2 |
| 1L_1200d_1tok (v1) | -0.513 | 72.4% | 199.5 |
| 1L_1537d_1tok (v1) | -0.510 | 68.6% | 198.3 |

### 3.3 context_dimの影響

| context_dim | Val ER | 効果 |
|-------------|--------|------|
| 500 | 82-83% | 最良PPL達成 |
| 768 | 77-81% | バランス良好 |
| 1000+ | 70-75% | ER低下、効果薄い |

**結論**: context_dim=500-768が最適

---

## 4. Phase 1 品質判定基準

### Val ER による判定

| Val ER | 判定 | 期待される結果 |
|--------|------|---------------|
| > 80% | 🟢 良好 | 良好な結果が期待 |
| 75-80% | 🟡 中程度 | 標準的な結果 |
| < 75% | 🔴 要検討 | 設定見直し推奨 |

### PPL予測式（簡易）

```
Val PPL ≈ 500 - 4 × Val_ER (%)
誤差: 約10-15%
```

---

## 5. token継ぎ足し方式の重要性

**token_input_all_layers=True（全レイヤーでtoken入力）が本質的**

| 設定 | ER | Val PPL | Val Acc |
|------|-----|---------|---------|
| token継ぎ足しあり | 76.3% | **334** | **18.9%** |
| 等差減少（なし） | 8.6% | 536 | 15.4% |

- PPL 38%改善、Acc 23%向上
- **ERはtoken継ぎ足しの副産物**

---

## 6. PPL予測表（参考値）

### MCDL + 通常Layer + cd=500 の場合 (α ≈ -0.42)

| Tokens | 予測 PPL |
|--------|----------|
| 100,000 | ~450 |
| 250,000 | ~300 |
| 500,000 | ~220 |
| 1,000,000 | ~160 |

---

## 7. 実用的推奨事項

1. **データ量を優先**: ERの改善よりトークン数増加が効果的
2. **context_dim=500-768**: これ以上は効果薄い
3. **token継ぎ足し有効**: token_input_all_layers=True
4. **早期停止調整**: 大規模データではpatienceを増やす

---

## 関連ログファイル

- `importants/logs/` 以下に各実験の詳細ログ

---

*Last Updated: 2025-12-01*
