remote: Enumerating objects: 32, done.
remote: Counting objects: 100% (32/32), done.
remote: Compressing objects: 100% (8/8), done.
remote: Total 20 (delta 12), reused 20 (delta 12), pack-reused 0 (from 0)
Unpacking objects: 100% (20/20), 19.02 KiB | 2.11 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   4f277dd..52bed97  main       -> origin/main
Updating 4f277dd..52bed97
Fast-forward
 CLAUDE.md                                    | 104 +++-
 README.md                                    |  38 +-
 importants/logs/1201_dwr_090.txt             |   0
 importants/logs/1201_dwr_100.txt             | 900 +++++++++++++++++++++++++++
 importants/logs/{1202_v2.txt => 1201_v2.txt} |   0
 scripts/README.md                            |  52 +-
 scripts/diversity_algorithm_experiment.py    |  33 +-
 scripts/diversity_full_experiment.py         |  69 +-
 src/experiments/__init__.py                  |   9 +-
 src/experiments/config.py                    | 174 ++++++
 src/experiments/runner.py                    |  86 +--
 11 files changed, 1291 insertions(+), 174 deletions(-)
 create mode 100644 importants/logs/1201_dwr_090.txt
 create mode 100644 importants/logs/1201_dwr_100.txt
 rename importants/logs/{1202_v2.txt => 1201_v2.txt} (100%)
 create mode 100644 src/experiments/config.py
======================================================================
DIVERSITY ALGORITHM FULL EXPERIMENT
(Phase 1 + Phase 2 + α Analysis)
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Algorithms: ['MCDL', 'ODCM', 'SDL', 'NUC']
Sample sizes: [50, 100, 200]
Context dim: 500
Output: importants/logs/20251201_111633_diversity_full

Config:
  num_layers: 1
  dist_reg_weight: 0.9
  phase1_max_iterations: 60
  phase2_epochs: 20

======================================================================
Algorithm: MCDL - Mean-Centered Dispersion Loss (現行ベースライン)
======================================================================

[1/12] MCDL | ctx_dim=500 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
2025-12-01 11:16:35.848684: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 11:16:35.864270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764587795.884231  103610 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764587795.890669  103610 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764587795.906924  103610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764587795.906961  103610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764587795.906964  103610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764587795.906966  103610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 11:16:35.911746: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1322 [0.9s]
  Iter 3: conv=0% loss=-0.0948 [0.5s]
  Iter 4: conv=0% loss=-0.1341 [0.5s]
  Iter 5: conv=0% loss=-0.1889 [0.5s]
    Val ER: 79.0%
  Iter 6: conv=0% loss=-0.2220 [0.5s]
  Iter 7: conv=0% loss=-0.2431 [0.5s]
  Iter 8: conv=2% loss=-0.2596 [0.5s]
  Iter 9: conv=4% loss=-0.2710 [0.5s]
  Iter 10: conv=9% loss=-0.2780 [0.5s]
    Val ER: 78.1%
  → Val early stop: ER not improving for 1 checks
  Done: 9% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
    Phase 1: 12.1s, 10 iter, ER=79.7%/79.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=8620.1 val_ppl=1388.4 acc=14.3% [2.1s] ★
  Epoch 2: train_ppl=694.8 val_ppl=801.9 acc=15.7% [2.1s] ★
  Epoch 3: train_ppl=314.9 val_ppl=616.9 acc=16.5% [2.1s] ★
  Epoch 4: train_ppl=181.5 val_ppl=568.0 acc=17.4% [2.1s] ★
  Epoch 5: train_ppl=115.4 val_ppl=571.0 acc=17.4% [2.1s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=568.0, acc=17.4%
    Phase 2: 10.6s, PPL=568.0, Acc=17.4%

[2/12] MCDL | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1307 [1.0s]
  Iter 3: conv=0% loss=-0.0928 [0.9s]
  Iter 4: conv=0% loss=-0.1312 [0.9s]
  Iter 5: conv=0% loss=-0.1858 [0.9s]
    Val ER: 78.9%
  Iter 6: conv=0% loss=-0.2188 [0.9s]
  Iter 7: conv=0% loss=-0.2406 [0.9s]
  Iter 8: conv=0% loss=-0.2572 [0.9s]
  Iter 9: conv=4% loss=-0.2687 [0.9s]
  Iter 10: conv=8% loss=-0.2760 [0.9s]
    Val ER: 78.0%
  → Val early stop: ER not improving for 1 checks
  Done: 8% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.0s]
    Phase 1: 17.3s, 10 iter, ER=80.0%/78.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3479.5 val_ppl=824.0 acc=15.3% [3.8s] ★
  Epoch 2: train_ppl=505.1 val_ppl=524.1 acc=17.4% [3.8s] ★
  Epoch 3: train_ppl=271.0 val_ppl=443.0 acc=18.0% [3.8s] ★
  Epoch 4: train_ppl=170.7 val_ppl=422.2 acc=18.4% [3.8s] ★
  Epoch 5: train_ppl=111.4 val_ppl=440.0 acc=18.5% [3.9s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=422.2, acc=18.4%
    Phase 2: 19.2s, PPL=422.2, Acc=18.4%

[3/12] MCDL | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1295 [2.0s]
  Iter 3: conv=0% loss=-0.0931 [1.9s]
  Iter 4: conv=0% loss=-0.1301 [1.8s]
  Iter 5: conv=0% loss=-0.1853 [1.8s]
    Val ER: 79.2%
  Iter 6: conv=0% loss=-0.2182 [1.8s]
  Iter 7: conv=0% loss=-0.2386 [1.9s]
  Iter 8: conv=2% loss=-0.2550 [1.8s]
  Iter 9: conv=4% loss=-0.2666 [1.8s]
  Iter 10: conv=7% loss=-0.2737 [1.8s]
    Val ER: 78.4%
  → Val early stop: ER not improving for 1 checks
  Done: 7% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.9s]
    Phase 1: 28.9s, 10 iter, ER=80.4%/79.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1608.0 val_ppl=556.5 acc=16.6% [7.3s] ★
  Epoch 2: train_ppl=356.7 val_ppl=395.2 acc=18.0% [7.3s] ★
  Epoch 3: train_ppl=212.1 val_ppl=353.5 acc=18.7% [7.2s] ★
  Epoch 4: train_ppl=141.2 val_ppl=353.4 acc=19.1% [7.2s] ★
  Epoch 5: train_ppl=101.3 val_ppl=367.8 acc=19.3% [7.2s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=353.4, acc=19.1%
    Phase 2: 36.1s, PPL=353.4, Acc=19.1%

======================================================================
Algorithm: ODCM - Off-Diagonal Covariance Minimization (VICReg風, 推奨)
======================================================================

[4/12] ODCM | ctx_dim=500 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.5353 [0.5s]
  Iter 3: conv=0% loss=0.3857 [0.5s]
  Iter 4: conv=0% loss=0.3527 [0.5s]
  Iter 5: conv=0% loss=0.3228 [0.5s]
    Val ER: 79.5%
  Iter 6: conv=0% loss=0.2778 [0.5s]
  Iter 7: conv=0% loss=0.2349 [0.5s]
  Iter 8: conv=0% loss=0.2112 [0.5s]
  Iter 9: conv=0% loss=0.1894 [0.5s]
  Iter 10: conv=0% loss=0.1682 [0.5s]
    Val ER: 82.8%
  Iter 11: conv=0% loss=0.1564 [0.5s]
  Iter 12: conv=0% loss=0.1483 [0.5s]
  Iter 13: conv=0% loss=0.1347 [0.5s]
  Iter 14: conv=0% loss=0.1249 [0.5s]
  Iter 15: conv=0% loss=0.1222 [0.5s]
    Val ER: 84.9%
  Iter 16: conv=0% loss=0.1189 [0.5s]
  Iter 17: conv=0% loss=0.1147 [0.5s]
  Iter 18: conv=0% loss=0.1108 [0.5s]
  Iter 19: conv=0% loss=0.1081 [0.5s]
  Iter 20: conv=0% loss=0.1046 [0.5s]
    Val ER: 87.0%
  Iter 21: conv=0% loss=0.0974 [0.5s]
  Iter 22: conv=0% loss=0.0945 [0.5s]
  Iter 23: conv=0% loss=0.0921 [0.5s]
  Iter 24: conv=0% loss=0.0902 [0.5s]
  Iter 25: conv=0% loss=0.0882 [0.5s]
    Val ER: 88.4%
  Iter 26: conv=0% loss=0.0836 [0.5s]
  Iter 27: conv=0% loss=0.0830 [0.5s]
  Iter 28: conv=0% loss=0.0801 [0.5s]
  Iter 29: conv=0% loss=0.0797 [0.5s]
  Iter 30: conv=0% loss=0.0804 [0.5s]
    Val ER: 88.6%
  Iter 31: conv=0% loss=0.0775 [0.5s]
  Iter 32: conv=0% loss=0.0785 [0.5s]
  Iter 33: conv=0% loss=0.0760 [0.5s]
  Iter 34: conv=0% loss=0.0751 [0.5s]
  Iter 35: conv=0% loss=0.0736 [0.5s]
    Val ER: 89.6%
  Iter 36: conv=0% loss=0.0702 [0.5s]
  Iter 37: conv=0% loss=0.0717 [0.5s]
  Iter 38: conv=0% loss=0.0687 [0.5s]
  Iter 39: conv=0% loss=0.0677 [0.5s]
  Iter 40: conv=0% loss=0.0674 [0.5s]
    Val ER: 89.9%
  Iter 41: conv=0% loss=0.0642 [0.5s]
  Iter 42: conv=0% loss=0.0647 [0.5s]
  Iter 43: conv=0% loss=0.0616 [0.5s]
  Iter 44: conv=0% loss=0.0614 [0.5s]
  Iter 45: conv=0% loss=0.0607 [0.5s]
    Val ER: 90.4%
  Iter 46: conv=0% loss=0.0577 [0.5s]
  Iter 47: conv=0% loss=0.0568 [0.5s]
  Iter 48: conv=0% loss=0.0548 [0.5s]
  Iter 49: conv=0% loss=0.0542 [0.5s]
  Iter 50: conv=0% loss=0.0528 [0.5s]
    Val ER: 91.1%
  Iter 51: conv=0% loss=0.0510 [0.5s]
  Iter 52: conv=0% loss=0.0501 [0.5s]
  Iter 53: conv=0% loss=0.0492 [0.5s]
  Iter 54: conv=0% loss=0.0480 [0.5s]
  Iter 55: conv=0% loss=0.0473 [0.5s]
    Val ER: 91.6%
  Iter 56: conv=0% loss=0.0462 [0.5s]
  Iter 57: conv=0% loss=0.0454 [0.5s]
  Iter 58: conv=0% loss=0.0448 [0.5s]
  Iter 59: conv=0% loss=0.0443 [0.5s]
  Iter 60: conv=0% loss=0.0435 [0.5s]
    Val ER: 91.9%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
    Phase 1: 70.2s, 60 iter, ER=92.7%/91.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=9284.0 val_ppl=1590.4 acc=13.5% [2.2s] ★
  Epoch 2: train_ppl=678.5 val_ppl=878.6 acc=15.1% [2.2s] ★
  Epoch 3: train_ppl=303.1 val_ppl=669.8 acc=15.9% [2.2s] ★
  Epoch 4: train_ppl=164.3 val_ppl=636.4 acc=16.3% [2.2s] ★
  Epoch 5: train_ppl=98.3 val_ppl=675.7 acc=16.6% [2.2s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=636.4, acc=16.3%
    Phase 2: 11.0s, PPL=636.4, Acc=16.3%

[5/12] ODCM | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.5293 [1.0s]
  Iter 3: conv=0% loss=0.3825 [1.0s]
  Iter 4: conv=0% loss=0.3486 [1.0s]
  Iter 5: conv=0% loss=0.3187 [1.0s]
    Val ER: 79.4%
  Iter 6: conv=0% loss=0.2745 [1.0s]
  Iter 7: conv=0% loss=0.2309 [1.0s]
  Iter 8: conv=0% loss=0.2088 [1.0s]
  Iter 9: conv=0% loss=0.1881 [1.0s]
  Iter 10: conv=0% loss=0.1666 [1.0s]
    Val ER: 82.6%
  Iter 11: conv=0% loss=0.1539 [1.0s]
  Iter 12: conv=0% loss=0.1473 [1.0s]
  Iter 13: conv=0% loss=0.1341 [1.0s]
  Iter 14: conv=0% loss=0.1241 [1.0s]
  Iter 15: conv=0% loss=0.1229 [1.0s]
    Val ER: 84.6%
  Iter 16: conv=0% loss=0.1204 [1.0s]
  Iter 17: conv=0% loss=0.1161 [1.0s]
  Iter 18: conv=0% loss=0.1121 [1.0s]
  Iter 19: conv=0% loss=0.1112 [1.0s]
  Iter 20: conv=0% loss=0.1067 [1.0s]
    Val ER: 87.0%
  Iter 21: conv=0% loss=0.0982 [1.0s]
  Iter 22: conv=0% loss=0.0947 [1.0s]
  Iter 23: conv=0% loss=0.0920 [1.0s]
  Iter 24: conv=0% loss=0.0916 [1.0s]
  Iter 25: conv=0% loss=0.0888 [1.0s]
    Val ER: 88.2%
  Iter 26: conv=0% loss=0.0845 [1.0s]
  Iter 27: conv=0% loss=0.0838 [1.0s]
  Iter 28: conv=0% loss=0.0813 [1.0s]
  Iter 29: conv=0% loss=0.0819 [1.0s]
  Iter 30: conv=0% loss=0.0825 [1.0s]
    Val ER: 88.2%
  Iter 31: conv=0% loss=0.0779 [1.0s]
  Iter 32: conv=0% loss=0.0779 [1.0s]
  Iter 33: conv=0% loss=0.0761 [1.0s]
  Iter 34: conv=0% loss=0.0741 [1.0s]
  Iter 35: conv=0% loss=0.0733 [1.0s]
    Val ER: 89.2%
  Iter 36: conv=0% loss=0.0701 [1.0s]
  Iter 37: conv=0% loss=0.0697 [1.0s]
  Iter 38: conv=0% loss=0.0655 [1.0s]
  Iter 39: conv=0% loss=0.0670 [1.0s]
  Iter 40: conv=0% loss=0.0680 [1.0s]
    Val ER: 89.6%
  Iter 41: conv=0% loss=0.0642 [1.0s]
  Iter 42: conv=0% loss=0.0649 [1.0s]
  Iter 43: conv=0% loss=0.0622 [1.0s]
  Iter 44: conv=0% loss=0.0618 [1.0s]
  Iter 45: conv=0% loss=0.0601 [1.0s]
    Val ER: 90.2%
  Iter 46: conv=0% loss=0.0570 [1.0s]
  Iter 47: conv=0% loss=0.0563 [1.0s]
  Iter 48: conv=0% loss=0.0535 [1.0s]
  Iter 49: conv=0% loss=0.0522 [1.0s]
  Iter 50: conv=0% loss=0.0507 [1.0s]
    Val ER: 91.0%
  Iter 51: conv=0% loss=0.0489 [1.0s]
  Iter 52: conv=0% loss=0.0481 [1.0s]
  Iter 53: conv=0% loss=0.0474 [1.0s]
  Iter 54: conv=0% loss=0.0465 [1.0s]
  Iter 55: conv=0% loss=0.0454 [1.0s]
    Val ER: 91.5%
  Iter 56: conv=0% loss=0.0443 [1.0s]
  Iter 57: conv=0% loss=0.0436 [1.0s]
  Iter 58: conv=0% loss=0.0433 [1.0s]
  Iter 59: conv=0% loss=0.0427 [1.0s]
  Iter 60: conv=0% loss=0.0421 [1.0s]
    Val ER: 92.0%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.0s]
    Phase 1: 102.8s, 60 iter, ER=92.7%/92.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3714.9 val_ppl=882.5 acc=15.1% [3.8s] ★
  Epoch 2: train_ppl=492.3 val_ppl=552.0 acc=16.8% [3.9s] ★
  Epoch 3: train_ppl=258.5 val_ppl=468.0 acc=17.3% [3.9s] ★
  Epoch 4: train_ppl=151.5 val_ppl=461.9 acc=17.5% [4.0s] ★
  Epoch 5: train_ppl=89.4 val_ppl=513.1 acc=17.3% [4.0s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=461.9, acc=17.5%
    Phase 2: 19.6s, PPL=461.9, Acc=17.5%

[6/12] ODCM | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.5308 [2.1s]
  Iter 3: conv=0% loss=0.3825 [2.1s]
  Iter 4: conv=0% loss=0.3505 [2.1s]
  Iter 5: conv=0% loss=0.3216 [2.0s]
    Val ER: 79.4%
  Iter 6: conv=0% loss=0.2786 [2.0s]
  Iter 7: conv=0% loss=0.2343 [2.1s]
  Iter 8: conv=0% loss=0.2092 [2.0s]
  Iter 9: conv=0% loss=0.1887 [2.0s]
  Iter 10: conv=0% loss=0.1684 [2.1s]
    Val ER: 83.0%
  Iter 11: conv=0% loss=0.1570 [2.0s]
  Iter 12: conv=0% loss=0.1504 [2.0s]
  Iter 13: conv=0% loss=0.1370 [2.1s]
  Iter 14: conv=0% loss=0.1267 [2.0s]
  Iter 15: conv=0% loss=0.1243 [2.0s]
    Val ER: 84.5%
  Iter 16: conv=0% loss=0.1204 [2.1s]
  Iter 17: conv=0% loss=0.1161 [2.0s]
  Iter 18: conv=0% loss=0.1116 [2.0s]
  Iter 19: conv=0% loss=0.1090 [2.1s]
  Iter 20: conv=0% loss=0.1052 [2.0s]
    Val ER: 86.7%
  Iter 21: conv=0% loss=0.0978 [2.0s]
  Iter 22: conv=0% loss=0.0952 [2.1s]
  Iter 23: conv=0% loss=0.0914 [2.0s]
  Iter 24: conv=0% loss=0.0901 [2.0s]
  Iter 25: conv=0% loss=0.0884 [2.1s]
    Val ER: 88.2%
  Iter 26: conv=0% loss=0.0857 [2.0s]
  Iter 27: conv=0% loss=0.0876 [2.0s]
  Iter 28: conv=0% loss=0.0835 [2.1s]
  Iter 29: conv=0% loss=0.0843 [2.0s]
  Iter 30: conv=0% loss=0.0834 [2.0s]
    Val ER: 88.3%
  Iter 31: conv=0% loss=0.0799 [2.1s]
  Iter 32: conv=0% loss=0.0823 [2.0s]
  Iter 33: conv=0% loss=0.0785 [2.0s]
  Iter 34: conv=0% loss=0.0781 [2.1s]
  Iter 35: conv=0% loss=0.0755 [2.0s]
    Val ER: 89.2%
  Iter 36: conv=0% loss=0.0732 [2.0s]
  Iter 37: conv=0% loss=0.0736 [2.1s]
  Iter 38: conv=0% loss=0.0688 [2.1s]
  Iter 39: conv=0% loss=0.0681 [2.0s]
  Iter 40: conv=0% loss=0.0662 [2.1s]
    Val ER: 89.4%
  Iter 41: conv=0% loss=0.0651 [2.1s]
  Iter 42: conv=0% loss=0.0647 [2.0s]
  Iter 43: conv=0% loss=0.0614 [2.1s]
  Iter 44: conv=0% loss=0.0620 [2.1s]
  Iter 45: conv=0% loss=0.0606 [2.0s]
    Val ER: 90.3%
  Iter 46: conv=0% loss=0.0597 [2.1s]
  Iter 47: conv=0% loss=0.0585 [2.0s]
  Iter 48: conv=0% loss=0.0565 [2.0s]
  Iter 49: conv=0% loss=0.0567 [2.1s]
  Iter 50: conv=0% loss=0.0550 [2.0s]
    Val ER: 90.9%
  Iter 51: conv=0% loss=0.0529 [2.0s]
  Iter 52: conv=0% loss=0.0515 [2.1s]
  Iter 53: conv=0% loss=0.0512 [2.0s]
  Iter 54: conv=0% loss=0.0521 [2.0s]
  Iter 55: conv=0% loss=0.0520 [2.1s]
    Val ER: 91.5%
  Iter 56: conv=0% loss=0.0504 [2.0s]
  Iter 57: conv=0% loss=0.0489 [2.0s]
  Iter 58: conv=0% loss=0.0484 [2.1s]
  Iter 59: conv=0% loss=0.0481 [2.0s]
  Iter 60: conv=0% loss=0.0470 [2.0s]
    Val ER: 92.0%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.9s]
    Phase 1: 177.8s, 60 iter, ER=92.3%/92.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1714.5 val_ppl=583.4 acc=16.5% [7.0s] ★
  Epoch 2: train_ppl=359.7 val_ppl=410.1 acc=17.6% [7.2s] ★
  Epoch 3: train_ppl=204.4 val_ppl=368.8 acc=18.2% [7.3s] ★
  Epoch 4: train_ppl=127.8 val_ppl=383.3 acc=18.8% [7.3s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=368.8, acc=18.2%
    Phase 2: 28.8s, PPL=368.8, Acc=18.2%

======================================================================
Algorithm: SDL - Spectral Diversity Loss (ER直接最大化, 高コスト)
======================================================================

[7/12] SDL | ctx_dim=500 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.1359 [0.8s]
  Iter 3: conv=0% loss=-5.1980 [0.8s]
  Iter 4: conv=0% loss=-5.2463 [0.8s]
  Iter 5: conv=0% loss=-5.3103 [0.8s]
    Val ER: 82.5%
  Iter 6: conv=0% loss=-5.3609 [0.8s]
  Iter 7: conv=0% loss=-5.3784 [0.8s]
  Iter 8: conv=0% loss=-5.3779 [0.8s]
  Iter 9: conv=0% loss=-5.3914 [0.8s]
  Iter 10: conv=0% loss=-5.4153 [0.8s]
    Val ER: 85.0%
  Iter 11: conv=0% loss=-5.4239 [0.8s]
  Iter 12: conv=0% loss=-5.4267 [0.9s]
  Iter 13: conv=0% loss=-5.4381 [0.8s]
  Iter 14: conv=0% loss=-5.4425 [0.9s]
  Iter 15: conv=0% loss=-5.4414 [0.8s]
    Val ER: 87.5%
  Iter 16: conv=0% loss=-5.4528 [0.8s]
  Iter 17: conv=0% loss=-5.4590 [0.8s]
  Iter 18: conv=0% loss=-5.4583 [0.8s]
  Iter 19: conv=0% loss=-5.4714 [0.9s]
  Iter 20: conv=0% loss=-5.4652 [0.8s]
    Val ER: 91.6%
  Iter 21: conv=0% loss=-5.4632 [0.8s]
  Iter 22: conv=0% loss=-5.4795 [0.8s]
  Iter 23: conv=0% loss=-5.4696 [0.8s]
  Iter 24: conv=0% loss=-5.4759 [0.8s]
  Iter 25: conv=0% loss=-5.4849 [0.8s]
    Val ER: 94.2%
  Iter 26: conv=0% loss=-5.4780 [0.8s]
  Iter 27: conv=0% loss=-5.4900 [0.8s]
  Iter 28: conv=0% loss=-5.4839 [0.8s]
  Iter 29: conv=0% loss=-5.4858 [0.8s]
  Iter 30: conv=0% loss=-5.4895 [0.8s]
    Val ER: 95.3%
  Iter 31: conv=0% loss=-5.4843 [0.8s]
  Iter 32: conv=0% loss=-5.4871 [0.8s]
  Iter 33: conv=0% loss=-5.4839 [0.8s]
  Iter 34: conv=0% loss=-5.4862 [0.9s]
  Iter 35: conv=0% loss=-5.4863 [0.8s]
    Val ER: 95.9%
  Iter 36: conv=0% loss=-5.4829 [0.9s]
  Iter 37: conv=0% loss=-5.4837 [0.8s]
  Iter 38: conv=0% loss=-5.4842 [0.9s]
  Iter 39: conv=0% loss=-5.4852 [0.8s]
  Iter 40: conv=0% loss=-5.4855 [0.9s]
    Val ER: 96.4%
  Iter 41: conv=0% loss=-5.4855 [0.8s]
  Iter 42: conv=0% loss=-5.4864 [0.9s]
  Iter 43: conv=0% loss=-5.4861 [0.8s]
  Iter 44: conv=0% loss=-5.4860 [0.9s]
  Iter 45: conv=0% loss=-5.4870 [0.9s]
    Val ER: 96.6%
  Iter 46: conv=0% loss=-5.4881 [0.8s]
  Iter 47: conv=0% loss=-5.4894 [0.9s]
  Iter 48: conv=0% loss=-5.4899 [0.8s]
  Iter 49: conv=0% loss=-5.4907 [0.9s]
  Iter 50: conv=0% loss=-5.4922 [0.8s]
    Val ER: 96.8%
  Iter 51: conv=0% loss=-5.4932 [0.9s]
  Iter 52: conv=0% loss=-5.4940 [0.8s]
  Iter 53: conv=0% loss=-5.4955 [0.9s]
  Iter 54: conv=0% loss=-5.4970 [0.8s]
  Iter 55: conv=0% loss=-5.4981 [0.9s]
    Val ER: 96.9%
  Iter 56: conv=0% loss=-5.4990 [0.8s]
  Iter 57: conv=0% loss=-5.5000 [0.9s]
  Iter 58: conv=0% loss=-5.5014 [0.8s]
  Iter 59: conv=0% loss=-5.5026 [0.9s]
  Iter 60: conv=0% loss=-5.5028 [0.8s]
    Val ER: 97.1%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
    Phase 1: 88.6s, 60 iter, ER=98.0%/97.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=9271.7 val_ppl=1586.4 acc=13.5% [2.2s] ★
  Epoch 2: train_ppl=684.4 val_ppl=914.3 acc=15.1% [2.2s] ★
  Epoch 3: train_ppl=309.7 val_ppl=694.9 acc=16.0% [2.2s] ★
  Epoch 4: train_ppl=167.0 val_ppl=666.8 acc=16.4% [2.2s] ★
  Epoch 5: train_ppl=97.7 val_ppl=718.7 acc=16.5% [2.2s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=666.8, acc=16.4%
    Phase 2: 11.0s, PPL=666.8, Acc=16.4%

[8/12] SDL | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.1417 [1.7s]
  Iter 3: conv=0% loss=-5.2012 [1.7s]
  Iter 4: conv=0% loss=-5.2504 [1.7s]
  Iter 5: conv=0% loss=-5.3131 [1.7s]
    Val ER: 82.4%
  Iter 6: conv=0% loss=-5.3632 [1.7s]
  Iter 7: conv=0% loss=-5.3797 [1.7s]
  Iter 8: conv=0% loss=-5.3796 [1.7s]
  Iter 9: conv=0% loss=-5.3933 [1.7s]
  Iter 10: conv=0% loss=-5.4169 [1.7s]
    Val ER: 84.9%
  Iter 11: conv=0% loss=-5.4248 [1.7s]
  Iter 12: conv=0% loss=-5.4272 [1.7s]
  Iter 13: conv=0% loss=-5.4386 [1.7s]
  Iter 14: conv=0% loss=-5.4431 [1.7s]
  Iter 15: conv=0% loss=-5.4425 [1.7s]
    Val ER: 87.6%
  Iter 16: conv=0% loss=-5.4543 [1.7s]
  Iter 17: conv=0% loss=-5.4605 [1.7s]
  Iter 18: conv=0% loss=-5.4600 [1.7s]
  Iter 19: conv=0% loss=-5.4731 [1.7s]
  Iter 20: conv=0% loss=-5.4669 [1.7s]
    Val ER: 91.7%
  Iter 21: conv=0% loss=-5.4647 [1.7s]
  Iter 22: conv=0% loss=-5.4810 [1.7s]
  Iter 23: conv=0% loss=-5.4712 [1.7s]
  Iter 24: conv=0% loss=-5.4771 [1.7s]
  Iter 25: conv=0% loss=-5.4857 [1.7s]
    Val ER: 94.2%
  Iter 26: conv=0% loss=-5.4787 [1.7s]
  Iter 27: conv=0% loss=-5.4908 [1.7s]
  Iter 28: conv=0% loss=-5.4849 [1.7s]
  Iter 29: conv=0% loss=-5.4871 [1.7s]
  Iter 30: conv=0% loss=-5.4911 [1.7s]
    Val ER: 95.3%
  Iter 31: conv=0% loss=-5.4861 [1.7s]
  Iter 32: conv=0% loss=-5.4887 [1.7s]
  Iter 33: conv=0% loss=-5.4857 [1.7s]
  Iter 34: conv=0% loss=-5.4882 [1.7s]
  Iter 35: conv=0% loss=-5.4884 [1.7s]
    Val ER: 96.0%
  Iter 36: conv=0% loss=-5.4854 [1.7s]
  Iter 37: conv=0% loss=-5.4864 [1.7s]
  Iter 38: conv=0% loss=-5.4865 [1.7s]
  Iter 39: conv=0% loss=-5.4872 [1.7s]
  Iter 40: conv=0% loss=-5.4877 [1.7s]
    Val ER: 96.4%
  Iter 41: conv=0% loss=-5.4876 [1.7s]
  Iter 42: conv=0% loss=-5.4882 [1.7s]
  Iter 43: conv=0% loss=-5.4876 [1.7s]
  Iter 44: conv=0% loss=-5.4879 [1.7s]
  Iter 45: conv=0% loss=-5.4884 [1.7s]
    Val ER: 96.6%
  Iter 46: conv=0% loss=-5.4890 [1.7s]
  Iter 47: conv=0% loss=-5.4904 [1.7s]
  Iter 48: conv=0% loss=-5.4911 [1.7s]
  Iter 49: conv=0% loss=-5.4915 [1.7s]
  Iter 50: conv=0% loss=-5.4927 [1.7s]
    Val ER: 96.9%
  Iter 51: conv=0% loss=-5.4939 [1.7s]
  Iter 52: conv=0% loss=-5.4948 [1.7s]
  Iter 53: conv=0% loss=-5.4959 [1.7s]
  Iter 54: conv=0% loss=-5.4973 [1.7s]
  Iter 55: conv=0% loss=-5.4985 [1.7s]
    Val ER: 97.0%
  Iter 56: conv=0% loss=-5.4993 [1.7s]
  Iter 57: conv=0% loss=-5.5001 [1.7s]
  Iter 58: conv=0% loss=-5.5013 [1.7s]
  Iter 59: conv=0% loss=-5.5025 [1.7s]
  Iter 60: conv=0% loss=-5.5031 [1.7s]
    Val ER: 97.2%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.0s]
    Phase 1: 146.1s, 60 iter, ER=98.1%/97.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3789.2 val_ppl=913.9 acc=14.9% [3.9s] ★
  Epoch 2: train_ppl=513.5 val_ppl=573.3 acc=16.5% [3.9s] ★
  Epoch 3: train_ppl=267.5 val_ppl=489.9 acc=17.2% [3.9s] ★
  Epoch 4: train_ppl=155.8 val_ppl=489.5 acc=17.3% [4.0s] ★
  Epoch 5: train_ppl=92.1 val_ppl=552.9 acc=17.3% [3.9s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=489.5, acc=17.3%
    Phase 2: 19.7s, PPL=489.5, Acc=17.3%

[9/12] SDL | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.1404 [3.3s]
  Iter 3: conv=0% loss=-5.2009 [3.2s]
  Iter 4: conv=0% loss=-5.2491 [3.2s]
  Iter 5: conv=0% loss=-5.3122 [3.2s]
    Val ER: 82.5%
  Iter 6: conv=0% loss=-5.3632 [3.2s]
  Iter 7: conv=0% loss=-5.3791 [3.2s]
  Iter 8: conv=0% loss=-5.3777 [3.2s]
  Iter 9: conv=0% loss=-5.3910 [3.2s]
  Iter 10: conv=0% loss=-5.4153 [3.2s]
    Val ER: 85.0%
  Iter 11: conv=0% loss=-5.4234 [3.2s]
  Iter 12: conv=0% loss=-5.4260 [3.2s]
  Iter 13: conv=0% loss=-5.4369 [3.2s]
  Iter 14: conv=0% loss=-5.4401 [3.2s]
  Iter 15: conv=0% loss=-5.4395 [3.2s]
    Val ER: 87.5%
  Iter 16: conv=0% loss=-5.4519 [3.2s]
  Iter 17: conv=0% loss=-5.4571 [3.2s]
  Iter 18: conv=0% loss=-5.4565 [3.2s]
  Iter 19: conv=0% loss=-5.4702 [3.2s]
  Iter 20: conv=0% loss=-5.4636 [3.2s]
    Val ER: 91.8%
  Iter 21: conv=0% loss=-5.4618 [3.2s]
  Iter 22: conv=0% loss=-5.4788 [3.2s]
  Iter 23: conv=0% loss=-5.4679 [3.2s]
  Iter 24: conv=0% loss=-5.4741 [3.2s]
  Iter 25: conv=0% loss=-5.4831 [3.2s]
    Val ER: 94.2%
  Iter 26: conv=0% loss=-5.4755 [3.2s]
  Iter 27: conv=0% loss=-5.4880 [3.2s]
  Iter 28: conv=0% loss=-5.4818 [3.2s]
  Iter 29: conv=0% loss=-5.4832 [3.2s]
  Iter 30: conv=0% loss=-5.4878 [3.2s]
    Val ER: 95.3%
  Iter 31: conv=0% loss=-5.4829 [3.2s]
  Iter 32: conv=0% loss=-5.4854 [3.2s]
  Iter 33: conv=0% loss=-5.4826 [3.2s]
  Iter 34: conv=0% loss=-5.4850 [3.2s]
  Iter 35: conv=0% loss=-5.4855 [3.2s]
    Val ER: 95.9%
  Iter 36: conv=0% loss=-5.4822 [3.2s]
  Iter 37: conv=0% loss=-5.4830 [3.2s]
  Iter 38: conv=0% loss=-5.4839 [3.2s]
  Iter 39: conv=0% loss=-5.4847 [3.2s]
  Iter 40: conv=0% loss=-5.4849 [3.2s]
    Val ER: 96.3%
  Iter 41: conv=0% loss=-5.4845 [3.2s]
  Iter 42: conv=0% loss=-5.4852 [3.2s]
  Iter 43: conv=0% loss=-5.4851 [3.2s]
  Iter 44: conv=0% loss=-5.4851 [3.2s]
  Iter 45: conv=0% loss=-5.4857 [3.2s]
    Val ER: 96.6%
  Iter 46: conv=0% loss=-5.4864 [3.2s]
  Iter 47: conv=0% loss=-5.4878 [3.2s]
  Iter 48: conv=0% loss=-5.4883 [3.2s]
  Iter 49: conv=0% loss=-5.4887 [3.2s]
  Iter 50: conv=0% loss=-5.4899 [3.2s]
    Val ER: 96.9%
  Iter 51: conv=0% loss=-5.4910 [3.2s]
  Iter 52: conv=0% loss=-5.4918 [3.2s]
  Iter 53: conv=0% loss=-5.4930 [3.2s]
  Iter 54: conv=0% loss=-5.4946 [3.2s]
  Iter 55: conv=0% loss=-5.4957 [3.2s]
    Val ER: 97.0%
  Iter 56: conv=0% loss=-5.4965 [3.2s]
  Iter 57: conv=0% loss=-5.4973 [3.2s]
  Iter 58: conv=0% loss=-5.4984 [3.2s]
  Iter 59: conv=0% loss=-5.4995 [3.2s]
  Iter 60: conv=0% loss=-5.5000 [3.2s]
    Val ER: 97.2%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.9s]
    Phase 1: 246.8s, 60 iter, ER=98.0%/97.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1752.2 val_ppl=607.0 acc=16.1% [7.1s] ★
  Epoch 2: train_ppl=374.2 val_ppl=428.4 acc=17.2% [7.3s] ★
  Epoch 3: train_ppl=216.1 val_ppl=387.0 acc=17.8% [7.3s] ★
  Epoch 4: train_ppl=136.6 val_ppl=406.9 acc=18.1% [7.2s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=387.0, acc=17.8%
    Phase 2: 28.9s, PPL=387.0, Acc=17.8%

======================================================================
Algorithm: NUC - Nuclear Norm Maximization (核ノルム最大化, 高コスト)
======================================================================

[10/12] NUC | ctx_dim=500 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-3.6580 [0.9s]
  Iter 3: conv=0% loss=-4.1271 [0.8s]
  Iter 4: conv=0% loss=-3.8164 [0.8s]
  Iter 5: conv=0% loss=-3.7154 [0.8s]
    Val ER: 74.6%
  Iter 6: conv=0% loss=-4.1581 [0.9s]
  Iter 7: conv=0% loss=-4.4320 [0.8s]
  Iter 8: conv=0% loss=-4.4629 [0.9s]
  Iter 9: conv=0% loss=-4.6959 [0.9s]
  Iter 10: conv=0% loss=-4.8783 [0.8s]
    Val ER: 82.2%
  Iter 11: conv=0% loss=-4.9794 [0.8s]
  Iter 12: conv=0% loss=-5.1091 [0.9s]
  Iter 13: conv=0% loss=-5.1430 [0.8s]
  Iter 14: conv=0% loss=-5.1677 [0.9s]
  Iter 15: conv=0% loss=-5.2073 [0.9s]
    Val ER: 84.2%
  Iter 16: conv=0% loss=-5.1909 [0.8s]
  Iter 17: conv=0% loss=-5.3633 [0.8s]
  Iter 18: conv=0% loss=-5.4219 [0.9s]
  Iter 19: conv=0% loss=-5.4855 [0.8s]
  Iter 20: conv=0% loss=-5.5930 [0.8s]
    Val ER: 88.2%
  Iter 21: conv=0% loss=-5.5581 [0.8s]
  Iter 22: conv=0% loss=-5.7602 [0.8s]
  Iter 23: conv=0% loss=-5.7399 [0.8s]
  Iter 24: conv=0% loss=-5.8835 [0.9s]
  Iter 25: conv=0% loss=-5.8997 [0.8s]
    Val ER: 89.2%
  Iter 26: conv=0% loss=-5.9398 [0.8s]
  Iter 27: conv=0% loss=-6.0671 [0.9s]
  Iter 28: conv=0% loss=-6.0157 [0.8s]
  Iter 29: conv=0% loss=-6.1171 [0.8s]
  Iter 30: conv=0% loss=-6.0854 [0.8s]
    Val ER: 90.2%
  Iter 31: conv=0% loss=-6.1360 [0.8s]
  Iter 32: conv=0% loss=-6.1265 [0.8s]
  Iter 33: conv=0% loss=-6.1269 [0.8s]
  Iter 34: conv=0% loss=-6.1681 [0.8s]
  Iter 35: conv=0% loss=-6.1753 [0.8s]
    Val ER: 89.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
    Phase 1: 52.1s, 35 iter, ER=92.6%/90.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=9780.2 val_ppl=1678.3 acc=13.3% [2.2s] ★
  Epoch 2: train_ppl=743.6 val_ppl=947.2 acc=14.6% [2.2s] ★
  Epoch 3: train_ppl=343.7 val_ppl=721.3 acc=15.5% [2.3s] ★
  Epoch 4: train_ppl=188.5 val_ppl=694.1 acc=15.8% [2.3s] ★
  Epoch 5: train_ppl=110.1 val_ppl=749.8 acc=16.2% [2.3s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=694.1, acc=15.8%
    Phase 2: 11.2s, PPL=694.1, Acc=15.8%

[11/12] NUC | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-3.6548 [1.7s]
  Iter 3: conv=0% loss=-4.1214 [1.6s]
  Iter 4: conv=0% loss=-3.8156 [1.6s]
  Iter 5: conv=0% loss=-3.6957 [1.6s]
    Val ER: 74.6%
  Iter 6: conv=0% loss=-4.1362 [1.6s]
  Iter 7: conv=0% loss=-4.4155 [1.7s]
  Iter 8: conv=0% loss=-4.4456 [1.7s]
  Iter 9: conv=0% loss=-4.6744 [1.7s]
  Iter 10: conv=0% loss=-4.8746 [1.7s]
    Val ER: 82.3%
  Iter 11: conv=0% loss=-4.9722 [1.7s]
  Iter 12: conv=0% loss=-5.0818 [1.7s]
  Iter 13: conv=0% loss=-5.1255 [1.7s]
  Iter 14: conv=0% loss=-5.1644 [1.7s]
  Iter 15: conv=0% loss=-5.1974 [1.7s]
    Val ER: 84.3%
  Iter 16: conv=0% loss=-5.1788 [1.6s]
  Iter 17: conv=0% loss=-5.3426 [1.7s]
  Iter 18: conv=0% loss=-5.4125 [1.7s]
  Iter 19: conv=0% loss=-5.4725 [1.6s]
  Iter 20: conv=0% loss=-5.5810 [1.7s]
    Val ER: 88.4%
  Iter 21: conv=0% loss=-5.5610 [1.7s]
  Iter 22: conv=0% loss=-5.7445 [1.7s]
  Iter 23: conv=0% loss=-5.7275 [1.6s]
  Iter 24: conv=0% loss=-5.8507 [1.6s]
  Iter 25: conv=0% loss=-5.8825 [1.6s]
    Val ER: 89.2%
  Iter 26: conv=0% loss=-5.9236 [1.6s]
  Iter 27: conv=0% loss=-6.0195 [1.6s]
  Iter 28: conv=0% loss=-5.9689 [1.6s]
  Iter 29: conv=0% loss=-6.0694 [1.6s]
  Iter 30: conv=0% loss=-6.0437 [1.6s]
    Val ER: 90.1%
  Iter 31: conv=0% loss=-6.0869 [1.6s]
  Iter 32: conv=0% loss=-6.0780 [1.6s]
  Iter 33: conv=0% loss=-6.0832 [1.6s]
  Iter 34: conv=0% loss=-6.1222 [1.6s]
  Iter 35: conv=0% loss=-6.1278 [1.6s]
    Val ER: 90.1%
  Iter 36: conv=0% loss=-6.1490 [1.6s]
  Iter 37: conv=0% loss=-6.1645 [1.6s]
  Iter 38: conv=0% loss=-6.1719 [1.6s]
  Iter 39: conv=0% loss=-6.1695 [1.6s]
  Iter 40: conv=0% loss=-6.1813 [1.6s]
    Val ER: 88.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.0s]
    Phase 1: 95.0s, 40 iter, ER=92.6%/90.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3929.7 val_ppl=939.4 acc=14.7% [3.9s] ★
  Epoch 2: train_ppl=551.3 val_ppl=589.9 acc=15.9% [3.9s] ★
  Epoch 3: train_ppl=296.5 val_ppl=505.4 acc=16.6% [3.9s] ★
  Epoch 4: train_ppl=174.6 val_ppl=500.5 acc=16.7% [4.0s] ★
  Epoch 5: train_ppl=103.2 val_ppl=559.0 acc=16.5% [4.0s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=500.5, acc=16.7%
    Phase 2: 19.7s, PPL=500.5, Acc=16.7%

[12/12] NUC | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-3.6287 [3.2s]
  Iter 3: conv=0% loss=-4.0917 [3.2s]
  Iter 4: conv=0% loss=-3.7855 [3.2s]
  Iter 5: conv=0% loss=-3.6882 [3.2s]
    Val ER: 74.6%
  Iter 6: conv=0% loss=-4.1104 [3.2s]
  Iter 7: conv=0% loss=-4.3640 [3.3s]
  Iter 8: conv=0% loss=-4.3835 [3.3s]
  Iter 9: conv=0% loss=-4.6251 [3.2s]
  Iter 10: conv=0% loss=-4.8340 [3.2s]
    Val ER: 82.3%
  Iter 11: conv=0% loss=-4.9384 [3.3s]
  Iter 12: conv=0% loss=-5.0545 [3.2s]
  Iter 13: conv=0% loss=-5.0897 [3.2s]
  Iter 14: conv=0% loss=-5.1296 [3.2s]
  Iter 15: conv=0% loss=-5.1595 [3.2s]
    Val ER: 84.1%
  Iter 16: conv=0% loss=-5.1361 [3.2s]
  Iter 17: conv=0% loss=-5.3152 [3.2s]
  Iter 18: conv=0% loss=-5.3868 [3.2s]
  Iter 19: conv=0% loss=-5.4515 [3.2s]
  Iter 20: conv=0% loss=-5.5395 [3.2s]
    Val ER: 88.4%
  Iter 21: conv=0% loss=-5.5261 [3.2s]
  Iter 22: conv=0% loss=-5.7312 [3.2s]
  Iter 23: conv=0% loss=-5.6918 [3.2s]
  Iter 24: conv=0% loss=-5.8140 [3.1s]
  Iter 25: conv=0% loss=-5.8438 [3.2s]
    Val ER: 89.5%
  Iter 26: conv=0% loss=-5.8873 [3.1s]
  Iter 27: conv=0% loss=-5.9638 [3.1s]
  Iter 28: conv=0% loss=-5.9072 [3.1s]
  Iter 29: conv=0% loss=-6.0192 [3.1s]
  Iter 30: conv=0% loss=-5.9807 [3.1s]
    Val ER: 89.8%
  Iter 31: conv=0% loss=-6.0254 [3.1s]
  Iter 32: conv=0% loss=-6.0266 [3.1s]
  Iter 33: conv=0% loss=-6.0271 [3.1s]
  Iter 34: conv=0% loss=-6.0681 [3.1s]
  Iter 35: conv=0% loss=-6.0642 [3.1s]
    Val ER: 89.9%
  Iter 36: conv=0% loss=-6.0829 [3.1s]
  Iter 37: conv=0% loss=-6.0958 [3.1s]
  Iter 38: conv=0% loss=-6.0992 [3.1s]
  Iter 39: conv=0% loss=-6.1032 [3.1s]
  Iter 40: conv=0% loss=-6.1245 [3.1s]
    Val ER: 89.3%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.9s]
    Phase 1: 162.9s, 40 iter, ER=92.3%/89.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,335,504/57,906,748 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1813.9 val_ppl=638.2 acc=16.1% [7.1s] ★
  Epoch 2: train_ppl=405.0 val_ppl=453.4 acc=17.0% [7.3s] ★
  Epoch 3: train_ppl=236.0 val_ppl=410.0 acc=17.5% [7.3s] ★
  Epoch 4: train_ppl=148.1 val_ppl=428.4 acc=17.6% [7.2s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=410.0, acc=17.5%
    Phase 2: 28.8s, PPL=410.0, Acc=17.5%

==================================================================================================================================
FULL EXPERIMENT RESULTS (Phase 1 + Phase 2)
==================================================================================================================================
Algo   Samples     Tokens P1 Iter  Train ER  Val ER BestValER   T.PPL   V.PPL    Acc   Time
----------------------------------------------------------------------------------------------------------------------------------
MCDL        50     62,891      10     79.7%   78.8%     79.0%   181.5   568.0  17.4%    23s
MCDL       100    122,795      10     80.0%   78.8%     78.9%   170.7   422.2  18.4%    36s
MCDL       200    240,132      10     80.4%   79.3%     79.2%   141.2   353.4  19.1%    65s
ODCM        50     62,891      60     92.7%   92.4%     91.9%   164.3   636.4  16.3%    81s
ODCM       100    122,795      60     92.7%   92.4%     92.0%   151.5   461.9  17.5%   122s
ODCM       200    240,132      60     92.3%   92.4%     92.0%   204.4   368.8  18.2%   207s
SDL         50     62,891      60     98.0%   97.4%     97.1%   167.0   666.8  16.4%   100s
SDL        100    122,795      60     98.1%   97.4%     97.2%   155.8   489.5  17.3%   166s
SDL        200    240,132      60     98.0%   97.4%     97.2%   216.1   387.0  17.8%   276s
NUC         50     62,891      35     92.6%   90.5%     90.2%   188.5   694.1  15.8%    63s
NUC        100    122,795      40     92.6%   89.6%     90.1%   174.6   500.5  16.7%   115s
NUC        200    240,132      40     92.3%   89.9%     89.9%   236.0   410.0  17.5%   192s
==================================================================================================================================

====================================================================================================
SCALING LAW ANALYSIS (PPL = A × tokens^α)
====================================================================================================
Algorithm     α (PPL)            A       R² Best Val PPL   Best Acc
----------------------------------------------------------------------------------------------------
MCDL         -0.35404     2.78e+04   0.9793        353.4      19.1%
ODCM         -0.40713     5.63e+04   0.9898        368.8      18.2%
SDL          -0.40600     5.85e+04   0.9937        387.0      17.8%
NUC          -0.39290     5.22e+04   0.9805        410.0      17.5%
====================================================================================================

--- Ranking by α (more negative = better scaling) ---
  1. ODCM: α=-0.40713, PPL=368.8
  2. SDL: α=-0.40600, PPL=387.0
  3. NUC: α=-0.39290, PPL=410.0
  4. MCDL: α=-0.35404, PPL=353.4

--- Ranking by Val PPL (lower = better) ---
  1. MCDL: PPL=353.4, Acc=19.1%
  2. ODCM: PPL=368.8, Acc=18.2%
  3. SDL: PPL=387.0, Acc=17.8%
  4. NUC: PPL=410.0, Acc=17.5%

All results saved to: importants/logs/20251201_111633_diversity_full/all_results.json

Total time: 26.5 min

Experiment completed!
