======================================================================
DIVERSITY ALGORITHM FULL EXPERIMENT
(Phase 1 + Phase 2 + α Analysis)
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Algorithms: ['MCDL', 'ODCM', 'SDL', 'NUC']
Sample sizes: [50, 100, 200]
Context dim: 1000
Output: importants/logs/20251201_103540_diversity_full

Config:
  num_layers: 1
  dist_reg_weight: 0.9
  phase1_max_iterations: 60
  phase2_epochs: 20

======================================================================
Algorithm: MCDL - Mean-Centered Dispersion Loss (現行ベースライン)
======================================================================

[1/12] MCDL | ctx_dim=1000 | 50 samples
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 168kB/s]
config.json: 100% 665/665 [00:00<00:00, 5.27MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.68MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 1.71MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.13MB/s]
Loading training data...
  Loading 50 samples from UltraChat...
Generating train_sft split: 100% 207865/207865 [00:03<00:00, 53560.41 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 56623.69 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 73348.77 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 76279.23 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_50samples_full.pt
Loading validation data...
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
2025-12-01 10:35:57.571862: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 10:35:57.588495: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764585357.607278   93289 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764585357.613800   93289 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764585357.630772   93289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764585357.630803   93289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764585357.630806   93289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764585357.630809   93289 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 10:35:57.635694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.3492 [1.3s]
  Iter 3: conv=0% loss=-0.3529 [1.0s]
  Iter 4: conv=0% loss=-0.3158 [1.0s]
  Iter 5: conv=0% loss=-0.3082 [0.9s]
    Val ER: 62.5%
  Iter 6: conv=0% loss=-0.3396 [0.9s]
  Iter 7: conv=0% loss=-0.3620 [1.0s]
  Iter 8: conv=0% loss=-0.3784 [0.9s]
  Iter 9: conv=0% loss=-0.3994 [0.9s]
  Iter 10: conv=0% loss=-0.4100 [0.9s]
    Val ER: 60.4%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 18.1s, 10 iter, ER=63.7%/62.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=8958.6 val_ppl=1394.6 acc=13.0% [2.4s] ★
  Epoch 2: train_ppl=756.5 val_ppl=788.0 acc=15.3% [2.4s] ★
  Epoch 3: train_ppl=367.2 val_ppl=593.6 acc=16.3% [2.4s] ★
  Epoch 4: train_ppl=221.3 val_ppl=530.0 acc=16.9% [2.4s] ★
  Epoch 5: train_ppl=138.4 val_ppl=539.1 acc=17.5% [2.4s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=530.0, acc=16.9%
    Phase 2: 12.1s, PPL=530.0, Acc=16.9%

[2/12] MCDL | ctx_dim=1000 | 100 samples
Loading training data...
  Loading 100 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_100samples_full.pt
Loading validation data...
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.3466 [1.9s]
  Iter 3: conv=0% loss=-0.3505 [1.8s]
  Iter 4: conv=0% loss=-0.3130 [1.8s]
  Iter 5: conv=0% loss=-0.3049 [1.8s]
    Val ER: 62.3%
  Iter 6: conv=0% loss=-0.3363 [1.8s]
  Iter 7: conv=0% loss=-0.3588 [1.8s]
  Iter 8: conv=0% loss=-0.3759 [1.8s]
  Iter 9: conv=0% loss=-0.3976 [1.8s]
  Iter 10: conv=0% loss=-0.4075 [1.8s]
    Val ER: 60.1%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.5s]
    Phase 1: 28.4s, 10 iter, ER=63.9%/62.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3426.6 val_ppl=799.8 acc=14.9% [4.4s] ★
  Epoch 2: train_ppl=533.1 val_ppl=512.5 acc=16.7% [4.4s] ★
  Epoch 3: train_ppl=311.9 val_ppl=431.0 acc=17.7% [4.5s] ★
  Epoch 4: train_ppl=193.9 val_ppl=407.3 acc=18.4% [4.5s] ★
  Epoch 5: train_ppl=124.2 val_ppl=425.9 acc=18.4% [4.5s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=407.3, acc=18.4%
    Phase 2: 22.3s, PPL=407.3, Acc=18.4%

[3/12] MCDL | ctx_dim=1000 | 200 samples
Loading training data...
  Loading 200 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_200samples_full.pt
Loading validation data...
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.3449 [3.7s]
  Iter 3: conv=0% loss=-0.3490 [3.6s]
  Iter 4: conv=0% loss=-0.3114 [3.6s]
  Iter 5: conv=0% loss=-0.3046 [3.6s]
    Val ER: 62.7%
  Iter 6: conv=0% loss=-0.3359 [3.6s]
  Iter 7: conv=0% loss=-0.3557 [3.6s]
  Iter 8: conv=0% loss=-0.3715 [3.6s]
  Iter 9: conv=0% loss=-0.3933 [3.6s]
  Iter 10: conv=2% loss=-0.4036 [3.6s]
    Val ER: 61.3%
  → Val early stop: ER not improving for 1 checks
  Done: 2% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.6s]
    Phase 1: 49.8s, 10 iter, ER=64.8%/62.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1596.9 val_ppl=562.9 acc=16.1% [8.2s] ★
  Epoch 2: train_ppl=387.9 val_ppl=409.9 acc=17.9% [8.2s] ★
  Epoch 3: train_ppl=236.3 val_ppl=365.5 acc=18.7% [8.2s] ★
  Epoch 4: train_ppl=154.4 val_ppl=366.6 acc=19.2% [8.2s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=365.5, acc=18.7%
    Phase 2: 32.9s, PPL=365.5, Acc=18.7%

======================================================================
Algorithm: ODCM - Off-Diagonal Covariance Minimization (VICReg風, 推奨)
======================================================================

[4/12] ODCM | ctx_dim=1000 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6324 [1.0s]
  Iter 3: conv=0% loss=0.4680 [1.0s]
  Iter 4: conv=0% loss=0.4470 [1.0s]
  Iter 5: conv=0% loss=0.4338 [1.0s]
    Val ER: 69.2%
  Iter 6: conv=0% loss=0.4133 [1.0s]
  Iter 7: conv=0% loss=0.3773 [1.0s]
  Iter 8: conv=0% loss=0.3523 [1.0s]
  Iter 9: conv=0% loss=0.3406 [1.0s]
  Iter 10: conv=0% loss=0.3679 [1.0s]
    Val ER: 72.5%
  Iter 11: conv=0% loss=0.3885 [1.0s]
  Iter 12: conv=0% loss=0.3387 [1.0s]
  Iter 13: conv=0% loss=0.3012 [1.0s]
  Iter 14: conv=0% loss=0.2668 [1.0s]
  Iter 15: conv=0% loss=0.2454 [1.0s]
    Val ER: 76.3%
  Iter 16: conv=0% loss=0.2346 [1.0s]
  Iter 17: conv=0% loss=0.2260 [1.0s]
  Iter 18: conv=0% loss=0.2089 [1.0s]
  Iter 19: conv=0% loss=0.1971 [1.0s]
  Iter 20: conv=0% loss=0.2004 [1.0s]
    Val ER: 72.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.8s]
    Phase 1: 35.4s, 20 iter, ER=81.9%/76.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=10972.2 val_ppl=1594.7 acc=11.1% [2.5s] ★
  Epoch 2: train_ppl=695.3 val_ppl=868.1 acc=14.5% [2.6s] ★
  Epoch 3: train_ppl=331.8 val_ppl=694.5 acc=15.5% [2.6s] ★
  Epoch 4: train_ppl=186.8 val_ppl=664.4 acc=16.1% [2.6s] ★
  Epoch 5: train_ppl=106.0 val_ppl=712.0 acc=16.7% [2.6s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=664.4, acc=16.1%
    Phase 2: 12.8s, PPL=664.4, Acc=16.1%

[5/12] ODCM | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6205 [2.0s]
  Iter 3: conv=0% loss=0.4606 [2.0s]
  Iter 4: conv=0% loss=0.4417 [2.1s]
  Iter 5: conv=0% loss=0.4312 [2.0s]
    Val ER: 69.3%
  Iter 6: conv=0% loss=0.4111 [2.0s]
  Iter 7: conv=0% loss=0.3725 [1.9s]
  Iter 8: conv=0% loss=0.3459 [1.9s]
  Iter 9: conv=0% loss=0.3302 [1.9s]
  Iter 10: conv=0% loss=0.3443 [2.0s]
    Val ER: 72.6%
  Iter 11: conv=0% loss=0.3809 [2.0s]
  Iter 12: conv=0% loss=0.3399 [2.0s]
  Iter 13: conv=0% loss=0.3041 [2.0s]
  Iter 14: conv=0% loss=0.2741 [1.9s]
  Iter 15: conv=0% loss=0.2531 [1.9s]
    Val ER: 75.6%
  Iter 16: conv=0% loss=0.2433 [2.0s]
  Iter 17: conv=0% loss=0.2335 [1.9s]
  Iter 18: conv=0% loss=0.2111 [1.9s]
  Iter 19: conv=0% loss=0.1971 [2.0s]
  Iter 20: conv=0% loss=0.2054 [2.0s]
    Val ER: 75.1%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.6s]
    Phase 1: 58.7s, 20 iter, ER=81.9%/75.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3792.5 val_ppl=827.1 acc=13.9% [4.4s] ★
  Epoch 2: train_ppl=504.6 val_ppl=534.6 acc=15.9% [4.5s] ★
  Epoch 3: train_ppl=278.9 val_ppl=466.6 acc=17.3% [4.5s] ★
  Epoch 4: train_ppl=158.4 val_ppl=461.2 acc=17.7% [4.5s] ★
  Epoch 5: train_ppl=87.6 val_ppl=526.2 acc=17.6% [4.5s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=461.2, acc=17.7%
    Phase 2: 22.4s, PPL=461.2, Acc=17.7%

[6/12] ODCM | ctx_dim=1000 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6246 [3.9s]
  Iter 3: conv=0% loss=0.4701 [3.7s]
  Iter 4: conv=0% loss=0.4451 [3.6s]
  Iter 5: conv=0% loss=0.4314 [3.7s]
    Val ER: 68.9%
  Iter 6: conv=0% loss=0.4120 [3.6s]
  Iter 7: conv=0% loss=0.3799 [3.6s]
  Iter 8: conv=0% loss=0.3534 [3.6s]
  Iter 9: conv=0% loss=0.3424 [3.6s]
  Iter 10: conv=0% loss=0.3736 [3.7s]
    Val ER: 72.0%
  Iter 11: conv=0% loss=0.3983 [3.7s]
  Iter 12: conv=0% loss=0.3482 [3.7s]
  Iter 13: conv=0% loss=0.3088 [3.6s]
  Iter 14: conv=0% loss=0.2638 [3.7s]
  Iter 15: conv=0% loss=0.2436 [3.6s]
    Val ER: 75.8%
  Iter 16: conv=0% loss=0.2387 [3.6s]
  Iter 17: conv=0% loss=0.2330 [3.7s]
  Iter 18: conv=0% loss=0.2135 [3.6s]
  Iter 19: conv=0% loss=0.1996 [3.6s]
  Iter 20: conv=0% loss=0.2029 [3.6s]
    Val ER: 69.8%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.7s]
    Phase 1: 99.6s, 20 iter, ER=82.0%/75.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1696.7 val_ppl=573.7 acc=15.6% [8.1s] ★
  Epoch 2: train_ppl=362.5 val_ppl=433.8 acc=17.4% [8.2s] ★
  Epoch 3: train_ppl=205.1 val_ppl=408.0 acc=18.0% [8.2s] ★
  Epoch 4: train_ppl=118.9 val_ppl=433.1 acc=18.3% [8.1s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=408.0, acc=18.0%
    Phase 2: 32.7s, PPL=408.0, Acc=18.0%

======================================================================
Algorithm: SDL - Spectral Diversity Loss (ER直接最大化, 高コスト)
======================================================================

[7/12] SDL | ctx_dim=1000 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3876 [2.2s]
  Iter 3: conv=0% loss=-6.5439 [2.1s]
  Iter 4: conv=0% loss=-6.5300 [2.1s]
  Iter 5: conv=0% loss=-6.5493 [2.1s]
    Val ER: 74.1%
  Iter 6: conv=0% loss=-6.6266 [2.1s]
  Iter 7: conv=0% loss=-6.6449 [2.2s]
  Iter 8: conv=0% loss=-6.6195 [2.1s]
  Iter 9: conv=0% loss=-6.6345 [2.2s]
  Iter 10: conv=0% loss=-6.6492 [2.2s]
    Val ER: 72.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 28.4s, 10 iter, ER=80.6%/74.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=11905.2 val_ppl=1738.3 acc=11.3% [2.5s] ★
  Epoch 2: train_ppl=873.0 val_ppl=928.2 acc=13.7% [2.5s] ★
  Epoch 3: train_ppl=433.6 val_ppl=710.4 acc=14.9% [2.5s] ★
  Epoch 4: train_ppl=250.2 val_ppl=634.2 acc=15.7% [2.5s] ★
  Epoch 5: train_ppl=144.5 val_ppl=642.5 acc=16.1% [2.5s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=634.2, acc=15.7%
    Phase 2: 12.6s, PPL=634.2, Acc=15.7%

[8/12] SDL | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3965 [4.3s]
  Iter 3: conv=0% loss=-6.5480 [4.2s]
  Iter 4: conv=0% loss=-6.5338 [4.2s]
  Iter 5: conv=0% loss=-6.5525 [4.3s]
    Val ER: 74.0%
  Iter 6: conv=0% loss=-6.6285 [4.3s]
  Iter 7: conv=0% loss=-6.6470 [4.3s]
  Iter 8: conv=0% loss=-6.6224 [4.3s]
  Iter 9: conv=0% loss=-6.6361 [4.3s]
  Iter 10: conv=0% loss=-6.6499 [4.3s]
    Val ER: 73.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.6s]
    Phase 1: 50.4s, 10 iter, ER=80.7%/74.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=4139.1 val_ppl=947.4 acc=12.9% [4.4s] ★
  Epoch 2: train_ppl=585.3 val_ppl=594.9 acc=15.7% [4.4s] ★
  Epoch 3: train_ppl=324.4 val_ppl=507.4 acc=16.8% [4.4s] ★
  Epoch 4: train_ppl=188.9 val_ppl=498.1 acc=17.2% [4.4s] ★
  Epoch 5: train_ppl=112.4 val_ppl=549.5 acc=16.7% [4.4s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=498.1, acc=17.2%
    Phase 2: 22.2s, PPL=498.1, Acc=17.2%

[9/12] SDL | ctx_dim=1000 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3956 [8.4s]
  Iter 3: conv=0% loss=-6.5478 [8.4s]
  Iter 4: conv=0% loss=-6.5329 [8.4s]
  Iter 5: conv=0% loss=-6.5510 [8.4s]
    Val ER: 73.9%
  Iter 6: conv=0% loss=-6.6272 [8.4s]
  Iter 7: conv=0% loss=-6.6451 [8.0s]
  Iter 8: conv=0% loss=-6.6204 [8.0s]
  Iter 9: conv=0% loss=-6.6363 [8.0s]
  Iter 10: conv=0% loss=-6.6500 [8.1s]
    Val ER: 73.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.1s]
    Phase 1: 91.7s, 10 iter, ER=80.8%/73.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1821.1 val_ppl=631.8 acc=15.0% [8.2s] ★
  Epoch 2: train_ppl=406.4 val_ppl=471.1 acc=16.9% [8.2s] ★
  Epoch 3: train_ppl=233.9 val_ppl=442.1 acc=17.3% [8.2s] ★
  Epoch 4: train_ppl=144.2 val_ppl=469.8 acc=17.4% [8.2s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=442.1, acc=17.3%
    Phase 2: 32.8s, PPL=442.1, Acc=17.3%

======================================================================
Algorithm: NUC - Nuclear Norm Maximization (核ノルム最大化, 高コスト)
======================================================================

[10/12] NUC | ctx_dim=1000 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-7.1842 [2.2s]
  Iter 3: conv=0% loss=-8.1647 [2.1s]
  Iter 4: conv=0% loss=-7.3062 [2.1s]
  Iter 5: conv=0% loss=-6.9993 [2.1s]
    Val ER: 68.8%
  Iter 6: conv=0% loss=-7.6799 [2.1s]
  Iter 7: conv=0% loss=-8.1464 [2.2s]
  Iter 8: conv=0% loss=-8.1415 [2.2s]
  Iter 9: conv=0% loss=-8.5903 [2.2s]
  Iter 10: conv=0% loss=-8.8040 [2.1s]
    Val ER: 69.8%
  Iter 11: conv=0% loss=-9.0698 [2.1s]
  Iter 12: conv=0% loss=-9.2400 [2.1s]
  Iter 13: conv=0% loss=-9.0398 [2.1s]
  Iter 14: conv=0% loss=-9.1570 [2.2s]
  Iter 15: conv=0% loss=-8.6304 [2.2s]
    Val ER: 74.7%
  Iter 16: conv=0% loss=-9.2568 [2.2s]
  Iter 17: conv=0% loss=-8.8361 [2.2s]
  Iter 18: conv=0% loss=-9.7522 [2.1s]
  Iter 19: conv=0% loss=-10.1636 [2.1s]
  Iter 20: conv=0% loss=-9.7503 [2.1s]
    Val ER: 80.2%
  Iter 21: conv=0% loss=-11.0149 [2.1s]
  Iter 22: conv=0% loss=-10.0713 [2.1s]
  Iter 23: conv=0% loss=-11.7554 [2.1s]
  Iter 24: conv=0% loss=-11.2010 [2.1s]
  Iter 25: conv=0% loss=-11.8458 [2.1s]
    Val ER: 83.4%
  Iter 26: conv=0% loss=-12.1237 [2.1s]
  Iter 27: conv=0% loss=-12.1525 [2.1s]
  Iter 28: conv=0% loss=-12.7337 [2.1s]
  Iter 29: conv=0% loss=-12.6380 [2.1s]
  Iter 30: conv=0% loss=-12.8981 [2.1s]
    Val ER: 85.8%
  Iter 31: conv=0% loss=-12.9154 [2.1s]
  Iter 32: conv=0% loss=-13.1534 [2.1s]
  Iter 33: conv=0% loss=-13.1450 [2.1s]
  Iter 34: conv=0% loss=-13.2753 [2.1s]
  Iter 35: conv=0% loss=-13.1694 [2.1s]
    Val ER: 88.2%
  Iter 36: conv=0% loss=-13.5046 [2.1s]
  Iter 37: conv=0% loss=-13.5165 [2.1s]
  Iter 38: conv=0% loss=-13.7179 [2.1s]
  Iter 39: conv=0% loss=-13.6628 [2.1s]
  Iter 40: conv=0% loss=-13.8378 [2.1s]
    Val ER: 89.4%
  Iter 41: conv=0% loss=-13.8039 [2.1s]
  Iter 42: conv=0% loss=-13.9731 [2.1s]
  Iter 43: conv=0% loss=-13.9116 [2.1s]
  Iter 44: conv=0% loss=-14.0533 [2.1s]
  Iter 45: conv=0% loss=-14.0473 [2.1s]
    Val ER: 90.7%
  Iter 46: conv=0% loss=-14.1510 [2.1s]
  Iter 47: conv=0% loss=-14.1999 [2.1s]
  Iter 48: conv=0% loss=-14.2785 [2.1s]
  Iter 49: conv=0% loss=-14.3371 [2.1s]
  Iter 50: conv=0% loss=-14.3909 [2.1s]
    Val ER: 91.4%
  Iter 51: conv=0% loss=-14.4555 [2.1s]
  Iter 52: conv=0% loss=-14.4819 [2.1s]
  Iter 53: conv=0% loss=-14.5528 [2.2s]
  Iter 54: conv=0% loss=-14.5750 [2.1s]
  Iter 55: conv=0% loss=-14.6366 [2.1s]
    Val ER: 91.8%
  Iter 56: conv=0% loss=-14.6402 [2.1s]
  Iter 57: conv=0% loss=-14.6962 [2.1s]
  Iter 58: conv=0% loss=-14.7484 [2.1s]
  Iter 59: conv=0% loss=-14.7669 [2.1s]
  Iter 60: conv=0% loss=-14.7803 [2.1s]
    Val ER: 91.8%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.8s]
    Phase 1: 172.9s, 60 iter, ER=93.9%/91.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=9900.2 val_ppl=1642.4 acc=11.3% [2.5s] ★
  Epoch 2: train_ppl=712.0 val_ppl=903.1 acc=14.6% [2.5s] ★
  Epoch 3: train_ppl=313.7 val_ppl=714.3 acc=15.1% [2.5s] ★
  Epoch 4: train_ppl=154.8 val_ppl=738.2 acc=15.5% [2.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=714.3, acc=15.1%
    Phase 2: 10.2s, PPL=714.3, Acc=15.1%

[11/12] NUC | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-7.1881 [4.3s]
  Iter 3: conv=0% loss=-8.1497 [4.1s]
  Iter 4: conv=0% loss=-7.2807 [4.2s]
  Iter 5: conv=0% loss=-6.9703 [4.1s]
    Val ER: 68.6%
  Iter 6: conv=0% loss=-7.6504 [4.1s]
  Iter 7: conv=0% loss=-8.0888 [4.1s]
  Iter 8: conv=0% loss=-8.0864 [4.2s]
  Iter 9: conv=0% loss=-8.5331 [4.2s]
  Iter 10: conv=0% loss=-8.7656 [4.1s]
    Val ER: 69.9%
  Iter 11: conv=0% loss=-9.0537 [4.3s]
  Iter 12: conv=0% loss=-9.2280 [4.3s]
  Iter 13: conv=0% loss=-9.0472 [4.3s]
  Iter 14: conv=0% loss=-9.1787 [4.3s]
  Iter 15: conv=0% loss=-8.7062 [4.1s]
    Val ER: 74.8%
  Iter 16: conv=0% loss=-9.2863 [4.2s]
  Iter 17: conv=0% loss=-8.8811 [4.3s]
  Iter 18: conv=0% loss=-9.8177 [4.3s]
  Iter 19: conv=0% loss=-10.1967 [4.3s]
  Iter 20: conv=0% loss=-9.8763 [4.3s]
    Val ER: 80.3%
  Iter 21: conv=0% loss=-11.0636 [4.3s]
  Iter 22: conv=0% loss=-10.2242 [4.2s]
  Iter 23: conv=0% loss=-11.8225 [4.3s]
  Iter 24: conv=0% loss=-11.3041 [4.3s]
  Iter 25: conv=0% loss=-11.9334 [4.2s]
    Val ER: 83.5%
  Iter 26: conv=0% loss=-12.1220 [4.2s]
  Iter 27: conv=0% loss=-12.1857 [4.2s]
  Iter 28: conv=0% loss=-12.6729 [4.2s]
  Iter 29: conv=0% loss=-12.6015 [4.2s]
  Iter 30: conv=0% loss=-12.8611 [4.1s]
    Val ER: 85.7%
  Iter 31: conv=0% loss=-12.8117 [4.1s]
  Iter 32: conv=0% loss=-13.0850 [4.2s]
  Iter 33: conv=0% loss=-13.0043 [4.2s]
  Iter 34: conv=0% loss=-13.1854 [4.1s]
  Iter 35: conv=0% loss=-13.0370 [4.1s]
    Val ER: 88.1%
  Iter 36: conv=0% loss=-13.4242 [4.2s]
  Iter 37: conv=0% loss=-13.4104 [4.2s]
  Iter 38: conv=0% loss=-13.6237 [4.1s]
  Iter 39: conv=0% loss=-13.5133 [4.1s]
  Iter 40: conv=0% loss=-13.7312 [4.2s]
    Val ER: 89.6%
  Iter 41: conv=0% loss=-13.6985 [4.2s]
  Iter 42: conv=0% loss=-13.8956 [4.2s]
  Iter 43: conv=0% loss=-13.7951 [4.1s]
  Iter 44: conv=0% loss=-13.9232 [4.3s]
  Iter 45: conv=0% loss=-13.9251 [4.2s]
    Val ER: 90.7%
  Iter 46: conv=0% loss=-14.0237 [4.2s]
  Iter 47: conv=0% loss=-14.0789 [4.2s]
  Iter 48: conv=0% loss=-14.1435 [4.2s]
  Iter 49: conv=0% loss=-14.2220 [4.2s]
  Iter 50: conv=0% loss=-14.2887 [4.2s]
    Val ER: 91.5%
  Iter 51: conv=0% loss=-14.3632 [4.1s]
  Iter 52: conv=0% loss=-14.3861 [4.1s]
  Iter 53: conv=0% loss=-14.4671 [4.2s]
  Iter 54: conv=0% loss=-14.4780 [4.2s]
  Iter 55: conv=0% loss=-14.5534 [4.2s]
    Val ER: 91.5%
  Iter 56: conv=0% loss=-14.5448 [4.2s]
  Iter 57: conv=0% loss=-14.5809 [4.2s]
  Iter 58: conv=0% loss=-14.6418 [4.1s]
  Iter 59: conv=0% loss=-14.6850 [4.2s]
  Iter 60: conv=0% loss=-14.7048 [4.2s]
    Val ER: 91.9%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.6s]
    Phase 1: 306.4s, 60 iter, ER=93.8%/91.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3930.5 val_ppl=922.3 acc=14.1% [4.4s] ★
  Epoch 2: train_ppl=531.5 val_ppl=594.5 acc=15.8% [4.5s] ★
  Epoch 3: train_ppl=278.8 val_ppl=537.1 acc=16.2% [4.5s] ★
  Epoch 4: train_ppl=142.0 val_ppl=607.5 acc=15.9% [4.5s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=537.1, acc=16.2%
    Phase 2: 17.9s, PPL=537.1, Acc=16.2%

[12/12] NUC | ctx_dim=1000 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-7.1414 [8.3s]
  Iter 3: conv=0% loss=-8.1049 [8.1s]
  Iter 4: conv=0% loss=-7.2307 [8.0s]
  Iter 5: conv=0% loss=-6.9092 [7.9s]
    Val ER: 68.6%
  Iter 6: conv=0% loss=-7.5981 [8.0s]
  Iter 7: conv=0% loss=-8.0268 [8.0s]
  Iter 8: conv=0% loss=-8.0139 [8.0s]
  Iter 9: conv=0% loss=-8.4617 [7.9s]
  Iter 10: conv=0% loss=-8.6987 [8.0s]
    Val ER: 69.8%
  Iter 11: conv=0% loss=-8.9918 [7.9s]
  Iter 12: conv=0% loss=-9.1616 [8.1s]
  Iter 13: conv=0% loss=-8.9234 [7.9s]
  Iter 14: conv=0% loss=-9.0547 [8.0s]
  Iter 15: conv=0% loss=-8.5763 [7.9s]
    Val ER: 74.9%
  Iter 16: conv=0% loss=-9.1805 [8.0s]
  Iter 17: conv=0% loss=-8.7811 [7.9s]
  Iter 18: conv=0% loss=-9.6847 [8.0s]
  Iter 19: conv=0% loss=-10.0708 [7.9s]
  Iter 20: conv=0% loss=-9.7018 [7.9s]
    Val ER: 80.4%
  Iter 21: conv=0% loss=-10.9300 [7.8s]
  Iter 22: conv=0% loss=-10.1073 [7.9s]
  Iter 23: conv=0% loss=-11.6573 [7.8s]
  Iter 24: conv=0% loss=-11.2403 [7.8s]
  Iter 25: conv=0% loss=-11.8099 [7.7s]
    Val ER: 83.6%
  Iter 26: conv=0% loss=-12.1160 [7.8s]
  Iter 27: conv=0% loss=-12.1918 [7.7s]
  Iter 28: conv=0% loss=-12.6396 [7.8s]
  Iter 29: conv=0% loss=-12.5801 [7.6s]
  Iter 30: conv=0% loss=-12.8328 [7.8s]
    Val ER: 86.1%
  Iter 31: conv=0% loss=-12.7841 [7.6s]
  Iter 32: conv=0% loss=-13.0535 [7.8s]
  Iter 33: conv=0% loss=-12.9723 [7.7s]
  Iter 34: conv=0% loss=-13.1665 [7.8s]
  Iter 35: conv=0% loss=-13.0056 [7.6s]
    Val ER: 87.9%
  Iter 36: conv=0% loss=-13.3282 [7.8s]
  Iter 37: conv=0% loss=-13.3497 [7.6s]
  Iter 38: conv=0% loss=-13.5688 [7.8s]
  Iter 39: conv=0% loss=-13.4698 [7.7s]
  Iter 40: conv=0% loss=-13.6530 [7.8s]
    Val ER: 89.3%
  Iter 41: conv=0% loss=-13.6177 [7.7s]
  Iter 42: conv=0% loss=-13.7628 [7.8s]
  Iter 43: conv=0% loss=-13.7005 [7.7s]
  Iter 44: conv=0% loss=-13.8212 [7.8s]
  Iter 45: conv=0% loss=-13.8552 [7.7s]
    Val ER: 90.7%
  Iter 46: conv=0% loss=-13.9734 [7.8s]
  Iter 47: conv=0% loss=-14.0021 [7.7s]
  Iter 48: conv=0% loss=-13.9909 [7.8s]
  Iter 49: conv=0% loss=-14.0779 [7.7s]
  Iter 50: conv=0% loss=-14.1488 [7.8s]
    Val ER: 91.1%
  Iter 51: conv=0% loss=-14.2359 [7.7s]
  Iter 52: conv=0% loss=-14.2299 [7.8s]
  Iter 53: conv=0% loss=-14.3173 [7.7s]
  Iter 54: conv=0% loss=-14.3258 [7.8s]
  Iter 55: conv=0% loss=-14.4034 [7.7s]
    Val ER: 91.3%
  Iter 56: conv=0% loss=-14.4036 [7.8s]
  Iter 57: conv=0% loss=-14.4186 [7.7s]
  Iter 58: conv=0% loss=-14.4755 [7.8s]
  Iter 59: conv=0% loss=-14.5552 [7.7s]
  Iter 60: conv=0% loss=-14.5998 [7.8s]
    Val ER: 92.1%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.6s]
    Phase 1: 542.5s, 60 iter, ER=93.9%/92.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1789.0 val_ppl=627.2 acc=15.7% [8.2s] ★
  Epoch 2: train_ppl=405.1 val_ppl=472.8 acc=16.5% [8.2s] ★
  Epoch 3: train_ppl=226.7 val_ppl=462.3 acc=16.9% [8.2s] ★
  Epoch 4: train_ppl=120.4 val_ppl=580.9 acc=16.5% [8.2s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=462.3, acc=16.9%
    Phase 2: 32.8s, PPL=462.3, Acc=16.9%

==================================================================================================================================
FULL EXPERIMENT RESULTS (Phase 1 + Phase 2)
==================================================================================================================================
Algo   Samples     Tokens P1 Iter  Train ER  Val ER BestValER   T.PPL   V.PPL    Acc   Time
----------------------------------------------------------------------------------------------------------------------------------
MCDL        50     62,891      10     63.7%   61.4%     62.5%   221.3   530.0  16.9%    30s
MCDL       100    122,795      10     63.9%   61.2%     62.3%   193.9   407.3  18.4%    51s
MCDL       200    240,132      10     64.8%   62.3%     62.7%   236.3   365.5  18.7%    83s
ODCM        50     62,891      20     81.9%   73.7%     76.3%   186.8   664.4  16.1%    48s
ODCM       100    122,795      20     81.9%   75.8%     75.6%   158.4   461.2  17.7%    81s
ODCM       200    240,132      20     82.0%   70.6%     75.8%   205.1   408.0  18.0%   132s
SDL         50     62,891      10     80.6%   73.7%     74.1%   250.2   634.2  15.7%    41s
SDL        100    122,795      10     80.7%   73.8%     74.0%   188.9   498.1  17.2%    73s
SDL        200    240,132      10     80.8%   73.9%     73.9%   233.9   442.1  17.3%   124s
NUC         50     62,891      60     93.9%   92.3%     91.8%   313.7   714.3  15.1%   183s
NUC        100    122,795      60     93.8%   92.3%     91.9%   278.8   537.1  16.2%   324s
NUC        200    240,132      60     93.9%   92.5%     92.1%   226.7   462.3  16.9%   575s
==================================================================================================================================

====================================================================================================
SCALING LAW ANALYSIS (PPL = A × tokens^α)
====================================================================================================
Algorithm     α (PPL)            A       R² Best Val PPL   Best Acc
----------------------------------------------------------------------------------------------------
MCDL         -0.27730     1.11e+04   0.9448        365.5      18.7%
ODCM         -0.36377     3.55e+04   0.9235        408.0      18.0%
SDL          -0.26926     1.22e+04   0.9629        442.1      17.3%
NUC          -0.32477     2.53e+04   0.9687        462.3      16.9%
====================================================================================================

--- Ranking by α (more negative = better scaling) ---
  1. ODCM: α=-0.36377, PPL=408.0
  2. NUC: α=-0.32477, PPL=462.3
  3. MCDL: α=-0.27730, PPL=365.5
  4. SDL: α=-0.26926, PPL=442.1

--- Ranking by Val PPL (lower = better) ---
  1. MCDL: PPL=365.5, Acc=18.7%
  2. ODCM: PPL=408.0, Acc=18.0%
  3. SDL: PPL=442.1, Acc=17.3%
  4. NUC: PPL=462.3, Acc=16.9%

All results saved to: importants/logs/20251201_103540_diversity_full/all_results.json

Total time: 32.1 min

Experiment completed!
