From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
Already up to date.
======================================================================
OACD ALGORITHM TEST
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Algorithms: ['OACD']
Sample sizes: [50, 100, 200]
Context dim: 500
dist_reg_weight: 0.5
Output: importants/logs/20251201_130520_oacd_test
======================================================================

======================================================================
Algorithm: OACD - Origin-Anchored Centroid Dispersion (原点固定重心分散)
======================================================================

  [OACD] Samples: 50
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
2025-12-01 13:05:21.439100: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 13:05:21.454348: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764594321.474580   14621 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764594321.481061   14621 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764594321.497256   14621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764594321.497285   14621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764594321.497289   14621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764594321.497293   14621 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 13:05:21.502172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] OACD: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=6.0821 [0.7s]
  Iter 3: conv=0% loss=6.8540 [0.3s]
  Iter 4: conv=0% loss=5.4998 [0.2s]
  Iter 5: conv=0% loss=3.7139 [0.2s]
    Val ER: 82.0%
  Iter 6: conv=0% loss=2.3828 [0.3s]
  Iter 7: conv=0% loss=1.7730 [0.2s]
  Iter 8: conv=1% loss=1.6060 [0.2s]
  Iter 9: conv=1% loss=1.5607 [0.2s]
  Iter 10: conv=2% loss=1.5231 [0.2s]
    Val ER: 80.3%
  → Val early stop: ER not improving for 1 checks
  Done: 2% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.4s]
    Phase 1: 9.2s, 10 iter, ER=83.4%/82.0%, Conv=1.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=173521040.0 val_ppl=293299.4 acc=3.9% [1.7s] ★
  Epoch 2: train_ppl=25549.6 val_ppl=7322.1 acc=9.1% [1.8s] ★
  Epoch 3: train_ppl=2997.0 val_ppl=2686.9 acc=12.1% [1.8s] ★
  Epoch 4: train_ppl=1181.2 val_ppl=1699.7 acc=13.8% [1.8s] ★
  Epoch 5: train_ppl=705.0 val_ppl=1300.8 acc=14.7% [1.8s] ★
  Epoch 6: train_ppl=487.7 val_ppl=1077.1 acc=15.5% [1.8s] ★
  Epoch 7: train_ppl=370.9 val_ppl=938.7 acc=15.9% [1.8s] ★
  Epoch 8: train_ppl=291.5 val_ppl=840.8 acc=16.3% [1.8s] ★
  Epoch 9: train_ppl=238.7 val_ppl=769.1 acc=16.7% [1.8s] ★
  Epoch 10: train_ppl=202.7 val_ppl=715.6 acc=16.9% [1.8s] ★
  Epoch 11: train_ppl=172.0 val_ppl=676.2 acc=17.1% [1.8s] ★
  Epoch 12: train_ppl=149.4 val_ppl=644.3 acc=17.2% [1.8s] ★
  Epoch 13: train_ppl=130.8 val_ppl=618.9 acc=17.5% [1.8s] ★
  Epoch 14: train_ppl=116.0 val_ppl=601.0 acc=17.6% [1.8s] ★
  Epoch 15: train_ppl=105.3 val_ppl=588.7 acc=17.7% [1.8s] ★
  Epoch 16: train_ppl=94.0 val_ppl=582.9 acc=17.7% [1.8s] ★
  Epoch 17: train_ppl=85.3 val_ppl=577.4 acc=17.6% [1.8s] ★
  Epoch 18: train_ppl=79.8 val_ppl=572.7 acc=17.9% [1.8s] ★
  Epoch 19: train_ppl=71.0 val_ppl=571.0 acc=17.8% [1.8s] ★
  Epoch 20: train_ppl=64.7 val_ppl=573.3 acc=17.9% [1.8s]
  → Early stop at epoch 20
  Best: epoch 19, ppl=571.0, acc=17.8%
    Phase 2: 35.4s, PPL=571.0, Acc=17.8%

  [OACD] Samples: 100
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] OACD: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=6.0989 [0.4s]
  Iter 3: conv=0% loss=6.8597 [0.4s]
  Iter 4: conv=0% loss=5.5027 [0.4s]
  Iter 5: conv=0% loss=3.7222 [0.4s]
    Val ER: 82.1%
  Iter 6: conv=0% loss=2.3839 [0.4s]
  Iter 7: conv=0% loss=1.7598 [0.4s]
  Iter 8: conv=0% loss=1.5841 [0.4s]
  Iter 9: conv=1% loss=1.5369 [0.4s]
  Iter 10: conv=1% loss=1.5018 [0.4s]
    Val ER: 80.2%
  → Val early stop: ER not improving for 1 checks
  Done: 1% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 11.8s, 10 iter, ER=83.7%/82.1%, Conv=1.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3424310.0 val_ppl=8822.4 acc=9.0% [3.0s] ★
  Epoch 2: train_ppl=3264.7 val_ppl=1746.1 acc=13.8% [3.1s] ★
  Epoch 3: train_ppl=1075.2 val_ppl=1055.0 acc=15.6% [3.1s] ★
  Epoch 4: train_ppl=636.3 val_ppl=795.1 acc=16.5% [3.1s] ★
  Epoch 5: train_ppl=448.6 val_ppl=656.0 acc=17.0% [3.1s] ★
  Epoch 6: train_ppl=341.8 val_ppl=569.8 acc=17.4% [3.1s] ★
  Epoch 7: train_ppl=276.2 val_ppl=510.9 acc=17.7% [3.1s] ★
  Epoch 8: train_ppl=230.0 val_ppl=470.5 acc=18.0% [3.1s] ★
  Epoch 9: train_ppl=197.0 val_ppl=442.0 acc=18.2% [3.1s] ★
  Epoch 10: train_ppl=171.8 val_ppl=421.6 acc=18.4% [3.1s] ★
  Epoch 11: train_ppl=152.0 val_ppl=407.0 acc=18.6% [3.1s] ★
  Epoch 12: train_ppl=135.9 val_ppl=396.6 acc=18.8% [3.1s] ★
  Epoch 13: train_ppl=122.6 val_ppl=389.6 acc=18.9% [3.1s] ★
  Epoch 14: train_ppl=111.4 val_ppl=385.2 acc=19.0% [3.1s] ★
  Epoch 15: train_ppl=101.7 val_ppl=383.0 acc=19.2% [3.1s] ★
  Epoch 16: train_ppl=93.4 val_ppl=382.8 acc=19.4% [3.1s] ★
  Epoch 17: train_ppl=86.0 val_ppl=384.2 acc=19.4% [3.1s]
  → Early stop at epoch 17
  Best: epoch 16, ppl=382.8, acc=19.4%
    Phase 2: 52.5s, PPL=382.8, Acc=19.4%

  [OACD] Samples: 200
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] OACD: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=6.1226 [0.9s]
  Iter 3: conv=0% loss=6.8974 [0.8s]
  Iter 4: conv=0% loss=5.5547 [0.7s]
  Iter 5: conv=0% loss=3.7865 [0.7s]
    Val ER: 82.0%
  Iter 6: conv=0% loss=2.4424 [0.8s]
  Iter 7: conv=0% loss=1.7986 [0.8s]
  Iter 8: conv=0% loss=1.6068 [0.8s]
  Iter 9: conv=1% loss=1.5526 [0.8s]
  Iter 10: conv=1% loss=1.5175 [0.7s]
    Val ER: 80.2%
  → Val early stop: ER not improving for 1 checks
  Done: 1% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.3s]
    Phase 1: 17.7s, 10 iter, ER=83.7%/82.0%, Conv=1.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=127354.2 val_ppl=1760.7 acc=13.7% [5.6s] ★
  Epoch 2: train_ppl=1080.3 val_ppl=781.9 acc=16.2% [5.6s] ★
  Epoch 3: train_ppl=556.0 val_ppl=555.1 acc=17.4% [5.7s] ★
  Epoch 4: train_ppl=379.3 val_ppl=451.8 acc=17.8% [5.7s] ★
  Epoch 5: train_ppl=293.2 val_ppl=392.8 acc=18.3% [5.7s] ★
  Epoch 6: train_ppl=237.7 val_ppl=357.0 acc=18.7% [5.7s] ★
  Epoch 7: train_ppl=202.1 val_ppl=333.5 acc=19.0% [5.7s] ★
  Epoch 8: train_ppl=176.3 val_ppl=317.4 acc=19.4% [5.7s] ★
  Epoch 9: train_ppl=156.7 val_ppl=306.4 acc=19.7% [5.7s] ★
  Epoch 10: train_ppl=141.0 val_ppl=299.0 acc=19.9% [5.7s] ★
  Epoch 11: train_ppl=128.3 val_ppl=294.0 acc=20.1% [5.7s] ★
  Epoch 12: train_ppl=117.5 val_ppl=291.1 acc=20.2% [5.6s] ★
  Epoch 13: train_ppl=108.4 val_ppl=289.6 acc=20.2% [5.6s] ★
  Epoch 14: train_ppl=100.5 val_ppl=289.6 acc=20.2% [5.6s] ★
  Epoch 15: train_ppl=93.7 val_ppl=290.7 acc=20.2% [5.6s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=289.6, acc=20.2%
    Phase 2: 84.8s, PPL=289.6, Acc=20.2%

  [OACD] Scaling Law:
    α = -0.5067
    A = 1.51e+05
    R² = 0.9894

======================================================================
SUMMARY
======================================================================

Algorithm  Samples    Tokens       Val PPL    Acc      ER%      Conv%    Iter  
--------------------------------------------------------------------------------
OACD       50         62,891       571.0      17.8     81.2     1.9      10    
OACD       100        122,795      382.8      19.4     81.2     1.0      10    
OACD       200        240,132      289.6      20.2     81.3     1.1      10    

--------------------------------------------------------------------------------

Algorithm  α          A            R²        
--------------------------------------------------
OACD       -0.5067    1.51e+05     0.9894    

======================================================================
DONE
======================================================================
