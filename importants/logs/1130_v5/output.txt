HEAD is now at 7bb1343 Add configurable FFN with factory pattern
======================================================================
ALPHA SCALING EXPERIMENT (Data Amount Dependency)
======================================================================
Configurations: 1
Sample sizes: [50, 100, 200, 400, 800]
Window size: 4 points
Number of windows: 2
Multiplier: 2.0x
Total experiments: 5
Output: importants/logs/1130_v5
Device: cuda (NVIDIA L4, 23.8GB)

Configurations:
  1. 1L_1085d_1tok
Device: cuda (NVIDIA L4, 22.2GB)

======================================================================
Config: 1L_1085d_1tok
  num_layers=1, context_dim=1085, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 1085d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
2025-11-30 12:29:14.296796: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 12:29:14.312559: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764505754.333062   31254 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764505754.339587   31254 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764505754.356076   31254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764505754.356102   31254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764505754.356105   31254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764505754.356108   31254 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-30 12:29:14.361054: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2709 [1.1s]
  Iter 3: conv=0% loss=-0.2843 [0.7s]
  Iter 4: conv=0% loss=-0.3136 [0.6s]
  Iter 5: conv=0% loss=-0.3373 [0.6s]
  Iter 6: conv=0% loss=-0.3538 [0.6s]
  Iter 7: conv=0% loss=-0.3645 [0.6s]
  Iter 8: conv=0% loss=-0.3718 [0.6s]
  Iter 9: conv=0% loss=-0.3780 [0.6s]
  Iter 10: conv=0% loss=-0.3841 [0.6s]
  Iter 11: conv=1% loss=-0.3902 [0.6s]
  Iter 12: conv=1% loss=-0.3961 [0.6s]
  Iter 13: conv=1% loss=-0.4014 [0.6s]
  Iter 14: conv=2% loss=-0.4061 [0.6s]
  Iter 15: conv=2% loss=-0.4103 [0.6s]
  Iter 16: conv=2% loss=-0.4140 [0.6s]
  Iter 17: conv=2% loss=-0.4173 [0.6s]
  Iter 18: conv=3% loss=-0.4203 [0.6s]
  Iter 19: conv=3% loss=-0.4231 [0.6s]
  Iter 20: conv=4% loss=-0.4258 [0.6s]
  Iter 21: conv=6% loss=-0.4282 [0.6s]
  Iter 22: conv=7% loss=-0.4304 [0.6s]
  Iter 23: conv=8% loss=-0.4323 [0.6s]
  Iter 24: conv=9% loss=-0.4339 [0.6s]
  Iter 25: conv=11% loss=-0.4353 [0.6s]
  Iter 26: conv=13% loss=-0.4368 [0.7s]
  Iter 27: conv=15% loss=-0.4382 [0.6s]
  Iter 28: conv=18% loss=-0.4396 [0.6s]
  Iter 29: conv=21% loss=-0.4410 [0.6s]
  Iter 30: conv=25% loss=-0.4423 [0.6s]
  Iter 31: conv=28% loss=-0.4435 [0.6s]
  Iter 32: conv=32% loss=-0.4447 [0.7s]
  Iter 33: conv=36% loss=-0.4458 [0.6s]
  Iter 34: conv=40% loss=-0.4468 [0.6s]
  Iter 35: conv=43% loss=-0.4479 [0.6s]
  Iter 36: conv=46% loss=-0.4489 [0.6s]
  Iter 37: conv=49% loss=-0.4500 [0.6s]
  Iter 38: conv=52% loss=-0.4510 [0.6s]
  Iter 39: conv=55% loss=-0.4520 [0.6s]
  Iter 40: conv=57% loss=-0.4530 [0.6s]
  Iter 41: conv=60% loss=-0.4540 [0.6s]
  Iter 42: conv=62% loss=-0.4550 [0.6s]
  Iter 43: conv=64% loss=-0.4560 [0.6s]
  Iter 44: conv=65% loss=-0.4570 [0.6s]
  Iter 45: conv=67% loss=-0.4580 [0.6s]
  Iter 46: conv=69% loss=-0.4589 [0.6s]
  Iter 47: conv=70% loss=-0.4599 [0.7s]
  Iter 48: conv=72% loss=-0.4608 [0.6s]
  Iter 49: conv=73% loss=-0.4618 [0.6s]
  Iter 50: conv=74% loss=-0.4628 [0.6s]
  Iter 51: conv=75% loss=-0.4637 [0.6s]
  Iter 52: conv=76% loss=-0.4647 [0.6s]
  Iter 53: conv=77% loss=-0.4656 [0.6s]
  Iter 54: conv=78% loss=-0.4666 [0.6s]
  Iter 55: conv=79% loss=-0.4675 [0.6s]
  Iter 56: conv=80% loss=-0.4684 [0.7s]
  Iter 57: conv=81% loss=-0.4694 [0.7s]
  Iter 58: conv=81% loss=-0.4703 [0.7s]
  Iter 59: conv=82% loss=-0.4713 [0.6s]
  Iter 60: conv=82% loss=-0.4722 [0.7s]
  Done: 82% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
  ER: train=71.7%, val=71.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,426,944/42,038,080 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=2631292.5 val_ppl=24848.6 acc=7.9% [1.7s] ★
  Epoch 2: train_ppl=7150.7 val_ppl=4264.5 acc=9.8% [1.7s] ★
  Epoch 3: train_ppl=1934.1 val_ppl=2147.8 acc=13.0% [1.8s] ★
  Epoch 4: train_ppl=942.4 val_ppl=1482.2 acc=14.0% [1.8s] ★
  Epoch 5: train_ppl=581.7 val_ppl=1162.1 acc=14.9% [1.8s] ★
  Epoch 6: train_ppl=421.2 val_ppl=984.3 acc=15.5% [1.8s] ★
  Epoch 7: train_ppl=322.1 val_ppl=865.5 acc=16.0% [1.8s] ★
  Epoch 8: train_ppl=261.2 val_ppl=787.0 acc=16.5% [1.8s] ★
  Epoch 9: train_ppl=214.1 val_ppl=726.2 acc=16.8% [1.8s] ★
  Epoch 10: train_ppl=182.3 val_ppl=686.0 acc=17.1% [1.8s] ★
  Epoch 11: train_ppl=154.6 val_ppl=653.0 acc=17.4% [1.8s] ★
  Epoch 12: train_ppl=135.0 val_ppl=632.1 acc=17.6% [1.8s] ★
  Epoch 13: train_ppl=117.0 val_ppl=614.0 acc=17.8% [1.8s] ★
  Epoch 14: train_ppl=103.7 val_ppl=605.8 acc=17.8% [1.8s] ★
  Epoch 15: train_ppl=91.1 val_ppl=598.6 acc=17.8% [1.8s] ★
  Epoch 16: train_ppl=81.5 val_ppl=598.6 acc=17.9% [1.8s]
  → Early stop at epoch 16
  Best: epoch 15, ppl=598.6, acc=17.8%
  Result: PPL=598.6, Acc=17.8%
    → PPL: 598.6, Acc: 17.8%, ER: 71.0%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 1085d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2682 [1.4s]
  Iter 3: conv=0% loss=-0.2811 [1.2s]
  Iter 4: conv=0% loss=-0.3103 [1.2s]
  Iter 5: conv=0% loss=-0.3338 [1.2s]
  Iter 6: conv=0% loss=-0.3503 [1.2s]
  Iter 7: conv=0% loss=-0.3611 [1.2s]
  Iter 8: conv=0% loss=-0.3685 [1.2s]
  Iter 9: conv=0% loss=-0.3746 [1.2s]
  Iter 10: conv=0% loss=-0.3807 [1.2s]
  Iter 11: conv=0% loss=-0.3869 [1.2s]
  Iter 12: conv=0% loss=-0.3929 [1.2s]
  Iter 13: conv=0% loss=-0.3983 [1.2s]
  Iter 14: conv=1% loss=-0.4031 [1.2s]
  Iter 15: conv=1% loss=-0.4072 [1.2s]
  Iter 16: conv=1% loss=-0.4110 [1.2s]
  Iter 17: conv=1% loss=-0.4143 [1.2s]
  Iter 18: conv=1% loss=-0.4174 [1.2s]
  Iter 19: conv=2% loss=-0.4202 [1.2s]
  Iter 20: conv=3% loss=-0.4229 [1.2s]
  Iter 21: conv=4% loss=-0.4253 [1.2s]
  Iter 22: conv=6% loss=-0.4275 [1.2s]
  Iter 23: conv=7% loss=-0.4293 [1.2s]
  Iter 24: conv=8% loss=-0.4309 [1.2s]
  Iter 25: conv=10% loss=-0.4324 [1.2s]
  Iter 26: conv=12% loss=-0.4338 [1.2s]
  Iter 27: conv=15% loss=-0.4352 [1.2s]
  Iter 28: conv=18% loss=-0.4366 [1.2s]
  Iter 29: conv=22% loss=-0.4380 [1.2s]
  Iter 30: conv=26% loss=-0.4393 [1.2s]
  Iter 31: conv=30% loss=-0.4405 [1.2s]
  Iter 32: conv=34% loss=-0.4416 [1.2s]
  Iter 33: conv=38% loss=-0.4427 [1.2s]
  Iter 34: conv=42% loss=-0.4438 [1.2s]
  Iter 35: conv=45% loss=-0.4448 [1.2s]
  Iter 36: conv=48% loss=-0.4459 [1.2s]
  Iter 37: conv=51% loss=-0.4469 [1.2s]
  Iter 38: conv=54% loss=-0.4479 [1.2s]
  Iter 39: conv=57% loss=-0.4489 [1.2s]
  Iter 40: conv=59% loss=-0.4499 [1.2s]
  Iter 41: conv=61% loss=-0.4509 [1.2s]
  Iter 42: conv=64% loss=-0.4519 [1.2s]
  Iter 43: conv=65% loss=-0.4528 [1.2s]
  Iter 44: conv=67% loss=-0.4538 [1.2s]
  Iter 45: conv=68% loss=-0.4548 [1.2s]
  Iter 46: conv=70% loss=-0.4558 [1.2s]
  Iter 47: conv=72% loss=-0.4567 [1.2s]
  Iter 48: conv=73% loss=-0.4577 [1.2s]
  Iter 49: conv=74% loss=-0.4586 [1.2s]
  Iter 50: conv=75% loss=-0.4596 [1.2s]
  Iter 51: conv=76% loss=-0.4605 [1.2s]
  Iter 52: conv=77% loss=-0.4615 [1.2s]
  Iter 53: conv=78% loss=-0.4624 [1.2s]
  Iter 54: conv=79% loss=-0.4634 [1.2s]
  Iter 55: conv=80% loss=-0.4643 [1.2s]
  Iter 56: conv=81% loss=-0.4652 [1.2s]
  Iter 57: conv=82% loss=-0.4662 [1.2s]
  Iter 58: conv=82% loss=-0.4671 [1.2s]
  Iter 59: conv=83% loss=-0.4681 [1.2s]
  Iter 60: conv=83% loss=-0.4690 [1.2s]
  Done: 83% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.0s]
  ER: train=71.9%, val=70.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,426,944/42,038,080 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=181569.0 val_ppl=4507.1 acc=10.4% [3.2s] ★
  Epoch 2: train_ppl=2058.6 val_ppl=1510.4 acc=14.3% [3.2s] ★
  Epoch 3: train_ppl=890.4 val_ppl=956.8 acc=15.5% [3.2s] ★
  Epoch 4: train_ppl=552.2 val_ppl=741.7 acc=16.4% [3.2s] ★
  Epoch 5: train_ppl=406.3 val_ppl=620.1 acc=17.2% [3.3s] ★
  Epoch 6: train_ppl=312.4 val_ppl=547.4 acc=17.7% [3.3s] ★
  Epoch 7: train_ppl=253.8 val_ppl=499.2 acc=17.9% [3.2s] ★
  Epoch 8: train_ppl=212.0 val_ppl=466.2 acc=18.2% [3.2s] ★
  Epoch 9: train_ppl=180.8 val_ppl=443.0 acc=18.5% [3.2s] ★
  Epoch 10: train_ppl=156.3 val_ppl=427.2 acc=18.8% [3.2s] ★
  Epoch 11: train_ppl=136.7 val_ppl=416.7 acc=19.0% [3.2s] ★
  Epoch 12: train_ppl=120.6 val_ppl=410.3 acc=19.2% [3.2s] ★
  Epoch 13: train_ppl=107.2 val_ppl=407.2 acc=19.4% [3.2s] ★
  Epoch 14: train_ppl=95.9 val_ppl=406.9 acc=19.6% [3.2s] ★
  Epoch 15: train_ppl=86.2 val_ppl=408.8 acc=19.7% [3.2s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=406.9, acc=19.6%
  Result: PPL=406.9, Acc=19.6%
    → PPL: 406.9, Acc: 19.6%, ER: 70.8%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 1085d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2662 [2.6s]
  Iter 3: conv=0% loss=-0.2795 [2.2s]
  Iter 4: conv=0% loss=-0.3085 [2.2s]
  Iter 5: conv=0% loss=-0.3317 [2.2s]
  Iter 6: conv=0% loss=-0.3480 [2.2s]
  Iter 7: conv=0% loss=-0.3587 [2.2s]
  Iter 8: conv=0% loss=-0.3660 [2.2s]
  Iter 9: conv=0% loss=-0.3720 [2.2s]
  Iter 10: conv=0% loss=-0.3780 [2.2s]
  Iter 11: conv=1% loss=-0.3840 [2.2s]
  Iter 12: conv=1% loss=-0.3898 [2.2s]
  Iter 13: conv=1% loss=-0.3951 [2.3s]
  Iter 14: conv=1% loss=-0.3997 [2.2s]
  Iter 15: conv=1% loss=-0.4038 [2.2s]
  Iter 16: conv=2% loss=-0.4074 [2.2s]
  Iter 17: conv=2% loss=-0.4106 [2.2s]
  Iter 18: conv=2% loss=-0.4136 [2.2s]
  Iter 19: conv=2% loss=-0.4164 [2.2s]
  Iter 20: conv=3% loss=-0.4192 [2.3s]
  Iter 21: conv=4% loss=-0.4219 [2.2s]
  Iter 22: conv=6% loss=-0.4242 [2.2s]
  Iter 23: conv=7% loss=-0.4263 [2.2s]
  Iter 24: conv=8% loss=-0.4280 [2.2s]
  Iter 25: conv=9% loss=-0.4295 [2.2s]
  Iter 26: conv=11% loss=-0.4309 [2.2s]
  Iter 27: conv=12% loss=-0.4324 [2.2s]
  Iter 28: conv=14% loss=-0.4339 [2.2s]
  Iter 29: conv=17% loss=-0.4353 [2.2s]
  Iter 30: conv=19% loss=-0.4366 [2.2s]
  Iter 31: conv=22% loss=-0.4379 [2.2s]
  Iter 32: conv=25% loss=-0.4391 [2.2s]
  Iter 33: conv=29% loss=-0.4402 [2.2s]
  Iter 34: conv=32% loss=-0.4412 [2.2s]
  Iter 35: conv=36% loss=-0.4423 [2.2s]
  Iter 36: conv=39% loss=-0.4434 [2.2s]
  Iter 37: conv=42% loss=-0.4444 [2.2s]
  Iter 38: conv=45% loss=-0.4455 [2.3s]
  Iter 39: conv=48% loss=-0.4465 [2.2s]
  Iter 40: conv=50% loss=-0.4475 [2.3s]
  Iter 41: conv=53% loss=-0.4484 [2.2s]
  Iter 42: conv=55% loss=-0.4494 [2.2s]
  Iter 43: conv=58% loss=-0.4504 [2.2s]
  Iter 44: conv=60% loss=-0.4514 [2.2s]
  Iter 45: conv=62% loss=-0.4524 [2.2s]
  Iter 46: conv=64% loss=-0.4533 [2.2s]
  Iter 47: conv=66% loss=-0.4543 [2.2s]
  Iter 48: conv=67% loss=-0.4552 [2.2s]
  Iter 49: conv=69% loss=-0.4562 [2.2s]
  Iter 50: conv=71% loss=-0.4571 [2.2s]
  Iter 51: conv=72% loss=-0.4581 [2.2s]
  Iter 52: conv=73% loss=-0.4590 [2.2s]
  Iter 53: conv=75% loss=-0.4600 [2.3s]
  Iter 54: conv=76% loss=-0.4609 [2.2s]
  Iter 55: conv=77% loss=-0.4618 [2.2s]
  Iter 56: conv=78% loss=-0.4628 [2.2s]
  Iter 57: conv=79% loss=-0.4637 [2.2s]
  Iter 58: conv=80% loss=-0.4646 [2.2s]
  Iter 59: conv=81% loss=-0.4656 [2.2s]
  Iter 60: conv=81% loss=-0.4665 [2.2s]
  Done: 81% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.1s]
  ER: train=71.9%, val=70.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,426,944/42,038,080 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=24126.3 val_ppl=1529.9 acc=14.0% [5.9s] ★
  Epoch 2: train_ppl=959.0 val_ppl=747.5 acc=16.6% [5.9s] ★
  Epoch 3: train_ppl=520.2 val_ppl=535.0 acc=17.4% [5.9s] ★
  Epoch 4: train_ppl=363.1 val_ppl=441.1 acc=18.3% [5.9s] ★
  Epoch 5: train_ppl=281.0 val_ppl=387.8 acc=18.9% [5.9s] ★
  Epoch 6: train_ppl=229.3 val_ppl=355.2 acc=19.3% [5.8s] ★
  Epoch 7: train_ppl=193.6 val_ppl=333.3 acc=19.8% [5.8s] ★
  Epoch 8: train_ppl=167.0 val_ppl=319.1 acc=20.1% [5.8s] ★
  Epoch 9: train_ppl=146.7 val_ppl=309.5 acc=20.4% [5.8s] ★
  Epoch 10: train_ppl=130.3 val_ppl=303.5 acc=20.5% [5.8s] ★
  Epoch 11: train_ppl=116.8 val_ppl=300.1 acc=20.6% [5.8s] ★
  Epoch 12: train_ppl=105.5 val_ppl=298.8 acc=20.8% [5.8s] ★
  Epoch 13: train_ppl=95.9 val_ppl=299.3 acc=20.9% [5.8s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=298.8, acc=20.8%
  Result: PPL=298.8, Acc=20.8%
    → PPL: 298.8, Acc: 20.8%, ER: 70.8%

  --- 400 samples ---

--- Experiment: 400 samples, 1L, 1085d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_400samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 473429 tokens (400 samples)
  Val:   31024 tokens
  Data: 473,429 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 473,429 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2658 [4.6s]
  Iter 3: conv=0% loss=-0.2790 [4.3s]
  Iter 4: conv=0% loss=-0.3081 [4.3s]
  Iter 5: conv=0% loss=-0.3314 [4.4s]
  Iter 6: conv=0% loss=-0.3478 [4.3s]
  Iter 7: conv=0% loss=-0.3585 [4.3s]
  Iter 8: conv=0% loss=-0.3658 [4.3s]
  Iter 9: conv=0% loss=-0.3719 [4.3s]
  Iter 10: conv=0% loss=-0.3780 [4.3s]
  Iter 11: conv=0% loss=-0.3841 [4.3s]
  Iter 12: conv=1% loss=-0.3900 [4.3s]
  Iter 13: conv=1% loss=-0.3954 [4.3s]
  Iter 14: conv=1% loss=-0.4000 [4.3s]
  Iter 15: conv=1% loss=-0.4041 [4.3s]
  Iter 16: conv=2% loss=-0.4078 [4.2s]
  Iter 17: conv=2% loss=-0.4112 [4.3s]
  Iter 18: conv=2% loss=-0.4142 [4.3s]
  Iter 19: conv=3% loss=-0.4170 [4.3s]
  Iter 20: conv=4% loss=-0.4197 [4.3s]
  Iter 21: conv=5% loss=-0.4221 [4.3s]
  Iter 22: conv=6% loss=-0.4243 [4.3s]
  Iter 23: conv=7% loss=-0.4261 [4.3s]
  Iter 24: conv=8% loss=-0.4277 [4.3s]
  Iter 25: conv=10% loss=-0.4292 [4.3s]
  Iter 26: conv=12% loss=-0.4306 [4.3s]
  Iter 27: conv=15% loss=-0.4321 [4.3s]
  Iter 28: conv=17% loss=-0.4335 [4.3s]
  Iter 29: conv=21% loss=-0.4349 [4.3s]
  Iter 30: conv=24% loss=-0.4361 [4.4s]
  Iter 31: conv=28% loss=-0.4373 [4.3s]
  Iter 32: conv=32% loss=-0.4385 [4.3s]
  Iter 33: conv=35% loss=-0.4395 [4.3s]
  Iter 34: conv=39% loss=-0.4406 [4.3s]
  Iter 35: conv=43% loss=-0.4417 [4.3s]
  Iter 36: conv=46% loss=-0.4427 [4.3s]
  Iter 37: conv=49% loss=-0.4438 [4.3s]
  Iter 38: conv=51% loss=-0.4448 [4.3s]
  Iter 39: conv=54% loss=-0.4457 [4.3s]
  Iter 40: conv=57% loss=-0.4467 [4.3s]
  Iter 41: conv=59% loss=-0.4477 [4.3s]
  Iter 42: conv=61% loss=-0.4487 [4.3s]
  Iter 43: conv=63% loss=-0.4497 [4.3s]
  Iter 44: conv=65% loss=-0.4507 [4.3s]
  Iter 45: conv=66% loss=-0.4516 [4.3s]
  Iter 46: conv=68% loss=-0.4526 [4.3s]
  Iter 47: conv=69% loss=-0.4536 [4.4s]
  Iter 48: conv=71% loss=-0.4545 [4.3s]
  Iter 49: conv=72% loss=-0.4554 [4.3s]
  Iter 50: conv=73% loss=-0.4564 [4.3s]
  Iter 51: conv=75% loss=-0.4573 [4.3s]
  Iter 52: conv=75% loss=-0.4583 [4.3s]
  Iter 53: conv=76% loss=-0.4592 [4.3s]
  Iter 54: conv=77% loss=-0.4602 [4.3s]
  Iter 55: conv=78% loss=-0.4611 [4.3s]
  Iter 56: conv=79% loss=-0.4620 [4.3s]
  Iter 57: conv=80% loss=-0.4630 [4.3s]
  Iter 58: conv=81% loss=-0.4639 [4.3s]
  Iter 59: conv=82% loss=-0.4649 [4.3s]
  Iter 60: conv=82% loss=-0.4658 [4.3s]
  Done: 82% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.6s]
  ER: train=72.2%, val=70.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,426,944/42,038,080 parameters

[Phase 2] 473,429 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=5341.8 val_ppl=762.9 acc=16.2% [11.3s] ★
  Epoch 2: train_ppl=539.4 val_ppl=438.0 acc=18.1% [11.3s] ★
  Epoch 3: train_ppl=339.3 val_ppl=345.1 acc=19.1% [11.1s] ★
  Epoch 4: train_ppl=257.2 val_ppl=298.8 acc=19.9% [11.1s] ★
  Epoch 5: train_ppl=209.6 val_ppl=272.5 acc=20.5% [11.0s] ★
  Epoch 6: train_ppl=177.7 val_ppl=255.1 acc=20.9% [11.0s] ★
  Epoch 7: train_ppl=154.9 val_ppl=244.1 acc=21.4% [11.0s] ★
  Epoch 8: train_ppl=137.5 val_ppl=236.3 acc=21.6% [11.0s] ★
  Epoch 9: train_ppl=123.9 val_ppl=231.9 acc=21.9% [11.1s] ★
  Epoch 10: train_ppl=112.8 val_ppl=228.7 acc=22.1% [11.2s] ★
  Epoch 11: train_ppl=103.7 val_ppl=227.6 acc=22.1% [11.2s] ★
  Epoch 12: train_ppl=96.0 val_ppl=227.0 acc=22.2% [11.2s] ★
  Epoch 13: train_ppl=89.5 val_ppl=227.9 acc=22.3% [11.1s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=227.0, acc=22.2%
  Result: PPL=227.0, Acc=22.2%
    → PPL: 227.0, Acc: 22.2%, ER: 70.9%

  --- 800 samples ---

--- Experiment: 800 samples, 1L, 1085d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_800samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 948524 tokens (800 samples)
  Val:   31024 tokens
  Data: 948,524 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 948,524 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2654 [9.2s]
  Iter 3: conv=0% loss=-0.2783 [8.6s]
  Iter 4: conv=0% loss=-0.3074 [8.6s]
  Iter 5: conv=0% loss=-0.3307 [8.6s]
  Iter 6: conv=0% loss=-0.3470 [8.6s]
  Iter 7: conv=0% loss=-0.3578 [8.6s]
  Iter 8: conv=0% loss=-0.3651 [8.6s]
  Iter 9: conv=0% loss=-0.3712 [8.6s]
  Iter 10: conv=0% loss=-0.3773 [8.6s]
  Iter 11: conv=0% loss=-0.3835 [8.6s]
  Iter 12: conv=0% loss=-0.3895 [8.6s]
  Iter 13: conv=0% loss=-0.3948 [8.7s]
  Iter 14: conv=1% loss=-0.3996 [8.6s]
  Iter 15: conv=1% loss=-0.4037 [8.6s]
  Iter 16: conv=1% loss=-0.4074 [8.6s]
  Iter 17: conv=1% loss=-0.4108 [8.7s]
  Iter 18: conv=1% loss=-0.4139 [8.6s]
  Iter 19: conv=2% loss=-0.4167 [8.6s]
  Iter 20: conv=3% loss=-0.4194 [8.7s]
  Iter 21: conv=4% loss=-0.4218 [8.6s]
  Iter 22: conv=5% loss=-0.4240 [8.6s]
  Iter 23: conv=6% loss=-0.4258 [8.7s]
  Iter 24: conv=8% loss=-0.4274 [8.6s]
  Iter 25: conv=9% loss=-0.4289 [8.6s]
  Iter 26: conv=12% loss=-0.4303 [8.5s]
  Iter 27: conv=14% loss=-0.4318 [8.6s]
  Iter 28: conv=17% loss=-0.4332 [8.6s]
  Iter 29: conv=20% loss=-0.4346 [8.5s]
  Iter 30: conv=24% loss=-0.4358 [8.7s]
  Iter 31: conv=28% loss=-0.4370 [8.7s]
  Iter 32: conv=32% loss=-0.4381 [8.6s]
  Iter 33: conv=36% loss=-0.4392 [8.6s]
  Iter 34: conv=40% loss=-0.4403 [8.8s]
  Iter 35: conv=43% loss=-0.4414 [8.7s]
  Iter 36: conv=46% loss=-0.4424 [8.6s]
  Iter 37: conv=49% loss=-0.4434 [8.7s]
  Iter 38: conv=52% loss=-0.4444 [8.6s]
  Iter 39: conv=55% loss=-0.4454 [8.6s]
  Iter 40: conv=57% loss=-0.4464 [8.7s]
  Iter 41: conv=60% loss=-0.4474 [8.7s]
  Iter 42: conv=62% loss=-0.4484 [8.7s]
  Iter 43: conv=64% loss=-0.4493 [8.6s]
  Iter 44: conv=65% loss=-0.4503 [8.6s]
  Iter 45: conv=67% loss=-0.4513 [8.7s]
  Iter 46: conv=69% loss=-0.4523 [8.7s]
  Iter 47: conv=70% loss=-0.4532 [8.7s]
  Iter 48: conv=72% loss=-0.4542 [8.7s]
  Iter 49: conv=73% loss=-0.4551 [8.7s]
  Iter 50: conv=74% loss=-0.4561 [8.7s]
  Iter 51: conv=75% loss=-0.4570 [8.7s]
  Iter 52: conv=76% loss=-0.4579 [8.6s]
  Iter 53: conv=77% loss=-0.4589 [8.6s]
  Iter 54: conv=78% loss=-0.4598 [8.6s]
  Iter 55: conv=79% loss=-0.4607 [8.7s]
  Iter 56: conv=80% loss=-0.4617 [8.6s]
  Iter 57: conv=81% loss=-0.4626 [8.6s]
  Iter 58: conv=82% loss=-0.4636 [8.7s]
  Iter 59: conv=82% loss=-0.4645 [8.6s]
  Iter 60: conv=83% loss=-0.4654 [8.6s]
  Done: 83% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [6.4s]
  ER: train=72.2%, val=70.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,426,944/42,038,080 parameters

[Phase 2] 948,524 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1799.0 val_ppl=435.5 acc=18.3% [22.3s] ★
  Epoch 2: train_ppl=345.9 val_ppl=294.6 acc=20.1% [21.9s] ★
  Epoch 3: train_ppl=243.5 val_ppl=245.3 acc=21.2% [21.6s] ★
  Epoch 4: train_ppl=195.1 val_ppl=220.8 acc=22.1% [21.7s] ★
  Epoch 5: train_ppl=166.1 val_ppl=206.1 acc=22.7% [22.0s] ★
  Epoch 6: train_ppl=146.6 val_ppl=196.8 acc=23.1% [22.0s] ★
  Epoch 7: train_ppl=132.4 val_ppl=190.6 acc=23.3% [21.9s] ★
  Epoch 8: train_ppl=121.5 val_ppl=186.3 acc=23.6% [21.8s] ★
  Epoch 9: train_ppl=112.9 val_ppl=183.5 acc=23.8% [21.8s] ★
  Epoch 10: train_ppl=105.8 val_ppl=181.7 acc=24.0% [21.9s] ★
  Epoch 11: train_ppl=100.0 val_ppl=180.6 acc=24.0% [22.0s] ★
  Epoch 12: train_ppl=95.0 val_ppl=180.1 acc=24.0% [21.9s] ★
  Epoch 13: train_ppl=90.7 val_ppl=180.0 acc=24.1% [21.8s] ★
  Epoch 14: train_ppl=86.9 val_ppl=180.3 acc=24.1% [21.9s]
  → Early stop at epoch 14
  Best: epoch 13, ppl=180.0, acc=24.1%
  Result: PPL=180.0, Acc=24.1%
    → PPL: 180.0, Acc: 24.1%, ER: 70.8%

====================================================================================================
RESULTS SUMMARY
====================================================================================================
Config                   α          A     R²     PPL    Acc   T.PPL    ER Iter
----------------------------------------------------------------------------------------------------
1L_1085d_1tok      -0.4404   7.34e+04  0.989   180.0  24.1%    90.7 70.8%   60

====================================================================================================
DETAILED RESULTS (All sample sizes)
====================================================================================================
Config             Samples     Tokens P1 Iter Train ER  Val ER   T.PPL   V.PPL    Acc
----------------------------------------------------------------------------------------------------
1L_1085d_1tok           50     62,891      60    71.7%   71.0%    91.1   598.6  17.8%
1L_1085d_1tok          100    122,795      60    71.9%   70.8%    95.9   406.9  19.6%
1L_1085d_1tok          200    240,132      60    71.9%   70.8%   105.5   298.8  20.8%
1L_1085d_1tok          400    473,429      60    72.2%   70.9%    96.0   227.0  22.2%
1L_1085d_1tok          800    948,524      60    72.2%   70.8%    90.7   180.0  24.1%

Results saved to: importants/logs/1130_v5

====================================================================================================
ALPHA PROGRESSION ANALYSIS (Sliding Window)
Window size: 4 points
====================================================================================================

1L_1085d_1tok:
--------------------------------------------------------------------------------
  Window              Samples                    Tokens          α            A       R²
--------------------------------------------------------------------------------
       1               50-400            62,891-473,429   -0.47815     1.14e+05   0.9934
       2              100-800           122,795-948,524   -0.39931     4.29e+04   0.9950

  α change: -0.47815 → -0.39931 (Δ = +0.07884, +16.49%)
  Trend: ↑ DEGRADING (α becoming less negative = worse scaling)

Alpha progression saved to: importants/logs/1130_v5/alpha_progression.json

Total time: 28.3 min
