# Sample Size Scaling Law Analysis

**Date**: 2025-12-03
**Context Dim**: 256 (fixed)
**Device**: NVIDIA L4 (22.2GB)

## 実験結果サマリー

| Samples | Tokens | Val PPL | Val Acc | Effective Rank | Time |
|---------|--------|---------|---------|----------------|------|
| 100 | 122,795 | 321.4 | 19.1% | 68.4% | 121.9s |
| 200 | 240,132 | 253.1 | 19.9% | 68.1% | 195.0s |
| 400 | 473,429 | 186.0 | 21.4% | 68.9% | 345.0s |
| 800 | 948,524 | 155.8 | 22.6% | 68.1% | 720.7s |
| 1600 | 1,920,992 | **134.7** | **23.6%** | 67.0% | 1415.9s |

## スケーリング則（Power Law）

### モデル

```
PPL = A × samples^(-a)
```

対数変換すると線形関係になる:
```
log(PPL) = log(A) - a × log(samples)
```

### 全データフィット（100-1600 samples）

| Parameter | Value |
|-----------|-------|
| **a (scaling exponent)** | **0.3209** |
| **A (constant)** | **1365.92** |
| **R²** | **0.9812** |

### 区間ごとの分析

| 区間 | a (exponent) | A (constant) | R² |
|------|--------------|--------------|-----|
| 100-400 | 0.3945 | 2000.40 | 0.9947 |
| 200-800 | 0.3500 | 1581.96 | 0.9763 |
| **400-1600** | **0.2328** | **746.31** | **0.9968** |

**観察**: サンプル数が増えると、スケーリング係数 a が減少している（0.39 → 0.23）。
これは、大規模データになると改善率が徐々に低下することを示唆している。

## 予測精度

| Samples | Actual PPL | Predicted PPL | Error |
|---------|------------|---------------|-------|
| 100 | 321.4 | 311.6 | -3.1% |
| 200 | 253.1 | 249.4 | -1.4% |
| 400 | 186.0 | 199.7 | +7.4% |
| 800 | 155.8 | 159.9 | +2.6% |
| 1600 | 134.7 | 128.0 | -5.0% |

全体的に ±7% 以内の誤差でフィットしている。

## 外挿予測

全データフィット（a=0.3209, A=1365.92）を使用:

| Samples | Predicted PPL |
|---------|---------------|
| 3,200 | ~102.5 |
| 6,400 | ~82.0 |
| 10,000 | ~71.1 |
| 50,000 | ~42.4 |
| 100,000 | ~33.9 |

**注意**: 外挿予測は参考値。実際のスケーリングは、
- モデル容量の限界
- データの質（多様性）
- 正則化効果

などの影響を受けるため、大規模データでは異なる挙動を示す可能性がある。

## グラフ

### Linear Scale
![Linear Scale](sample_size_scaling_analysis.png)

### Log-Log Scale with Intervals
![Intervals](sample_size_scaling_by_interval.png)

## 考察

1. **高いR² (0.98)**: PPLはサンプルサイズに対してべき乗則に従っている

2. **減衰するスケーリング係数**:
   - 小規模データ（100-400）: a ≈ 0.39
   - 大規模データ（400-1600）: a ≈ 0.23
   - データを増やすほど改善率が低下する傾向

3. **Effective Rank の安定性**:
   - サンプルサイズに関わらず ER ≈ 67-69% で安定
   - Context Block の表現力は十分に活用されている

4. **実用的な閾値**:
   - PPL < 200 を達成するには: ~400 samples（~47万 tokens）以上
   - PPL < 150 を達成するには: ~1000 samples（~100万 tokens）以上
   - PPL < 100 を達成するには: ~3500 samples（~400万 tokens）以上（予測）

## 結論

- サンプルサイズの増加はPPL改善に効果的
- ただし、収穫逓減の法則が適用される（スケーリング係数の減衰）
- 現在のアーキテクチャ（context_dim=256）では、数千〜1万サンプルで PPL ~70-100 程度が見込まれる
