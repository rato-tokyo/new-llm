From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
Already up to date.
======================================================================
DIVERSITY ALGORITHM FULL EXPERIMENT
(Phase 1 + Phase 2 + α Analysis)
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Algorithms: ['MCDL', 'ODCM', 'SDL', 'NUC']
Sample sizes: [50, 100, 200]
Context dim: 1000
Output: importants/logs/20251201_105710_diversity_full

Config:
  num_layers: 1
  dist_reg_weight: 0.9
  phase1_max_iterations: 60
  phase2_epochs: 20

======================================================================
Algorithm: MCDL - Mean-Centered Dispersion Loss (現行ベースライン)
======================================================================

[1/12] MCDL | ctx_dim=1000 | 50 samples
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 181kB/s]
config.json: 100% 665/665 [00:00<00:00, 4.70MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.44MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 1.06MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.15MB/s]
Loading training data...
  Loading 50 samples from UltraChat...
README.md: 3.90kB [00:00, 17.8MB/s]
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:02<00:00, 121MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:01<00:00, 190MB/s] 
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:01<00:00, 230MB/s]
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:00<00:00, 81.9MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:01<00:00, 165MB/s]
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:01<00:00, 227MB/s]  
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:01<00:00, 193MB/s]
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:00<00:00, 89.9MB/s]
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 50702.68 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 57321.33 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 72632.72 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 70057.37 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_50samples_full.pt
Loading validation data...
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
2025-12-01 10:57:43.334529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 10:57:43.351285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764586663.370344     713 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764586663.377078     713 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764586663.393775     713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764586663.393805     713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764586663.393807     713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764586663.393810     713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 10:57:43.398794: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 5.81MB/s]
model.safetensors: 100% 548M/548M [00:00<00:00, 594MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2143 [2.1s]
  Iter 3: conv=0% loss=-0.1503 [1.0s]
  Iter 4: conv=0% loss=-0.1565 [0.9s]
  Iter 5: conv=0% loss=-0.2163 [0.9s]
    Val ER: 69.6%
  Iter 6: conv=0% loss=-0.2632 [0.9s]
  Iter 7: conv=0% loss=-0.2958 [0.9s]
  Iter 8: conv=0% loss=-0.3289 [0.9s]
  Iter 9: conv=0% loss=-0.3515 [0.9s]
  Iter 10: conv=1% loss=-0.3646 [0.9s]
    Val ER: 66.2%
  → Val early stop: ER not improving for 1 checks
  Done: 1% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 18.9s, 10 iter, ER=69.7%/69.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=8440.1 val_ppl=1370.5 acc=13.6% [2.4s] ★
  Epoch 2: train_ppl=714.1 val_ppl=768.7 acc=15.3% [2.4s] ★
  Epoch 3: train_ppl=346.6 val_ppl=580.5 acc=16.3% [2.4s] ★
  Epoch 4: train_ppl=203.5 val_ppl=532.6 acc=16.9% [2.4s] ★
  Epoch 5: train_ppl=131.4 val_ppl=538.3 acc=17.9% [2.4s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=532.6, acc=16.9%
    Phase 2: 12.1s, PPL=532.6, Acc=16.9%

[2/12] MCDL | ctx_dim=1000 | 100 samples
Loading training data...
  Loading 100 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_100samples_full.pt
Loading validation data...
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2119 [1.9s]
  Iter 3: conv=0% loss=-0.1484 [1.8s]
  Iter 4: conv=0% loss=-0.1531 [1.8s]
  Iter 5: conv=0% loss=-0.2128 [1.8s]
    Val ER: 69.6%
  Iter 6: conv=0% loss=-0.2599 [1.8s]
  Iter 7: conv=0% loss=-0.2931 [1.8s]
  Iter 8: conv=0% loss=-0.3262 [1.8s]
  Iter 9: conv=0% loss=-0.3488 [1.8s]
  Iter 10: conv=0% loss=-0.3616 [1.8s]
    Val ER: 65.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.5s]
    Phase 1: 28.5s, 10 iter, ER=69.9%/69.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3308.7 val_ppl=784.5 acc=14.8% [4.3s] ★
  Epoch 2: train_ppl=513.2 val_ppl=505.9 acc=16.8% [4.3s] ★
  Epoch 3: train_ppl=292.9 val_ppl=426.3 acc=18.1% [4.4s] ★
  Epoch 4: train_ppl=183.4 val_ppl=409.9 acc=18.5% [4.4s] ★
  Epoch 5: train_ppl=114.1 val_ppl=431.8 acc=18.5% [4.4s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=409.9, acc=18.5%
    Phase 2: 21.8s, PPL=409.9, Acc=18.5%

[3/12] MCDL | ctx_dim=1000 | 200 samples
Loading training data...
  Loading 200 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_200samples_full.pt
Loading validation data...
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2104 [3.7s]
  Iter 3: conv=0% loss=-0.1467 [3.6s]
  Iter 4: conv=0% loss=-0.1524 [3.6s]
  Iter 5: conv=0% loss=-0.2114 [3.6s]
    Val ER: 69.5%
  Iter 6: conv=0% loss=-0.2575 [3.6s]
  Iter 7: conv=0% loss=-0.2878 [3.6s]
  Iter 8: conv=0% loss=-0.3207 [3.6s]
  Iter 9: conv=0% loss=-0.3434 [3.6s]
  Iter 10: conv=2% loss=-0.3576 [3.6s]
    Val ER: 66.9%
  → Val early stop: ER not improving for 1 checks
  Done: 2% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.1s]
    Phase 1: 50.2s, 10 iter, ER=70.6%/69.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1550.0 val_ppl=548.5 acc=16.1% [8.3s] ★
  Epoch 2: train_ppl=372.9 val_ppl=403.9 acc=18.1% [8.3s] ★
  Epoch 3: train_ppl=225.0 val_ppl=371.0 acc=18.9% [8.4s] ★
  Epoch 4: train_ppl=145.5 val_ppl=377.0 acc=19.4% [8.4s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=371.0, acc=18.9%
    Phase 2: 33.4s, PPL=371.0, Acc=18.9%

======================================================================
Algorithm: ODCM - Off-Diagonal Covariance Minimization (VICReg風, 推奨)
======================================================================

[4/12] ODCM | ctx_dim=1000 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6691 [1.0s]
  Iter 3: conv=0% loss=0.5124 [1.0s]
  Iter 4: conv=0% loss=0.4806 [1.0s]
  Iter 5: conv=0% loss=0.4419 [1.0s]
    Val ER: 72.2%
  Iter 6: conv=0% loss=0.3952 [1.0s]
  Iter 7: conv=0% loss=0.3543 [1.0s]
  Iter 8: conv=0% loss=0.3219 [1.0s]
  Iter 9: conv=0% loss=0.2972 [1.0s]
  Iter 10: conv=0% loss=0.2847 [1.0s]
    Val ER: 76.4%
  Iter 11: conv=0% loss=0.2927 [1.0s]
  Iter 12: conv=0% loss=0.2902 [1.0s]
  Iter 13: conv=0% loss=0.2439 [1.0s]
  Iter 14: conv=0% loss=0.2075 [1.0s]
  Iter 15: conv=0% loss=0.1864 [1.0s]
    Val ER: 80.3%
  Iter 16: conv=0% loss=0.1741 [1.0s]
  Iter 17: conv=0% loss=0.1686 [1.0s]
  Iter 18: conv=0% loss=0.1594 [1.0s]
  Iter 19: conv=0% loss=0.1582 [1.0s]
  Iter 20: conv=0% loss=0.1627 [1.0s]
    Val ER: 73.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.8s]
    Phase 1: 35.5s, 20 iter, ER=83.9%/80.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=10773.6 val_ppl=1906.2 acc=9.9% [2.6s] ★
  Epoch 2: train_ppl=654.2 val_ppl=972.3 acc=14.3% [2.6s] ★
  Epoch 3: train_ppl=291.2 val_ppl=821.4 acc=14.3% [2.6s] ★
  Epoch 4: train_ppl=153.7 val_ppl=841.4 acc=14.8% [2.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=821.4, acc=14.3%
    Phase 2: 10.5s, PPL=821.4, Acc=14.3%

[5/12] ODCM | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6584 [2.0s]
  Iter 3: conv=0% loss=0.5068 [2.0s]
  Iter 4: conv=0% loss=0.4790 [2.0s]
  Iter 5: conv=0% loss=0.4389 [1.9s]
    Val ER: 72.3%
  Iter 6: conv=0% loss=0.3915 [2.0s]
  Iter 7: conv=0% loss=0.3493 [1.9s]
  Iter 8: conv=0% loss=0.3163 [1.9s]
  Iter 9: conv=0% loss=0.2921 [1.9s]
  Iter 10: conv=0% loss=0.2742 [2.0s]
    Val ER: 76.2%
  Iter 11: conv=0% loss=0.2792 [1.9s]
  Iter 12: conv=0% loss=0.2871 [1.9s]
  Iter 13: conv=0% loss=0.2478 [1.9s]
  Iter 14: conv=0% loss=0.2045 [1.9s]
  Iter 15: conv=0% loss=0.1825 [1.9s]
    Val ER: 80.1%
  Iter 16: conv=0% loss=0.1727 [1.9s]
  Iter 17: conv=0% loss=0.1684 [1.9s]
  Iter 18: conv=0% loss=0.1571 [1.9s]
  Iter 19: conv=0% loss=0.1546 [2.0s]
  Iter 20: conv=0% loss=0.1578 [1.9s]
    Val ER: 70.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.6s]
    Phase 1: 58.5s, 20 iter, ER=84.4%/80.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3632.5 val_ppl=895.4 acc=14.3% [4.6s] ★
  Epoch 2: train_ppl=452.8 val_ppl=590.7 acc=15.9% [4.6s] ★
  Epoch 3: train_ppl=237.4 val_ppl=533.1 acc=16.5% [4.6s] ★
  Epoch 4: train_ppl=127.6 val_ppl=569.0 acc=15.8% [4.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=533.1, acc=16.5%
    Phase 2: 18.5s, PPL=533.1, Acc=16.5%

[6/12] ODCM | ctx_dim=1000 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6622 [4.0s]
  Iter 3: conv=0% loss=0.5133 [3.8s]
  Iter 4: conv=0% loss=0.4774 [3.7s]
  Iter 5: conv=0% loss=0.4400 [3.8s]
    Val ER: 71.7%
  Iter 6: conv=0% loss=0.3951 [3.8s]
  Iter 7: conv=0% loss=0.3553 [3.8s]
  Iter 8: conv=0% loss=0.3205 [3.8s]
  Iter 9: conv=0% loss=0.2950 [3.8s]
  Iter 10: conv=0% loss=0.2837 [3.8s]
    Val ER: 76.2%
  Iter 11: conv=0% loss=0.2916 [3.8s]
  Iter 12: conv=0% loss=0.2887 [3.8s]
  Iter 13: conv=0% loss=0.2443 [3.8s]
  Iter 14: conv=0% loss=0.2089 [3.8s]
  Iter 15: conv=0% loss=0.1892 [3.8s]
    Val ER: 80.3%
  Iter 16: conv=0% loss=0.1775 [3.8s]
  Iter 17: conv=0% loss=0.1721 [3.8s]
  Iter 18: conv=0% loss=0.1629 [3.8s]
  Iter 19: conv=0% loss=0.1608 [3.8s]
  Iter 20: conv=0% loss=0.1657 [3.8s]
    Val ER: 74.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.1s]
    Phase 1: 102.8s, 20 iter, ER=84.0%/80.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1660.2 val_ppl=564.3 acc=16.2% [8.5s] ★
  Epoch 2: train_ppl=336.6 val_ppl=445.2 acc=17.5% [8.5s] ★
  Epoch 3: train_ppl=184.8 val_ppl=430.6 acc=17.8% [8.5s] ★
  Epoch 4: train_ppl=104.0 val_ppl=493.0 acc=17.8% [8.5s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=430.6, acc=17.8%
    Phase 2: 34.1s, PPL=430.6, Acc=17.8%

======================================================================
Algorithm: SDL - Spectral Diversity Loss (ER直接最大化, 高コスト)
======================================================================

[7/12] SDL | ctx_dim=1000 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.6489 [2.2s]
  Iter 3: conv=0% loss=-5.7080 [2.2s]
  Iter 4: conv=0% loss=-5.7759 [2.1s]
  Iter 5: conv=0% loss=-5.8396 [2.2s]
    Val ER: 79.3%
  Iter 6: conv=0% loss=-5.8951 [2.2s]
  Iter 7: conv=0% loss=-5.9108 [2.2s]
  Iter 8: conv=0% loss=-5.9030 [2.1s]
  Iter 9: conv=0% loss=-5.9087 [2.2s]
  Iter 10: conv=0% loss=-5.9424 [2.2s]
    Val ER: 80.0%
  Iter 11: conv=0% loss=-5.9547 [2.2s]
  Iter 12: conv=0% loss=-5.9594 [2.2s]
  Iter 13: conv=0% loss=-5.9643 [2.2s]
  Iter 14: conv=0% loss=-5.9601 [2.2s]
  Iter 15: conv=0% loss=-5.9617 [2.2s]
    Val ER: 80.9%
  Iter 16: conv=0% loss=-5.9778 [2.2s]
  Iter 17: conv=0% loss=-5.9672 [2.2s]
  Iter 18: conv=0% loss=-5.9666 [2.2s]
  Iter 19: conv=0% loss=-5.9910 [2.2s]
  Iter 20: conv=0% loss=-5.9908 [2.2s]
    Val ER: 88.1%
  Iter 21: conv=0% loss=-5.9985 [2.2s]
  Iter 22: conv=0% loss=-6.0257 [2.2s]
  Iter 23: conv=0% loss=-6.0232 [2.2s]
  Iter 24: conv=0% loss=-6.0317 [2.2s]
  Iter 25: conv=0% loss=-6.0468 [2.2s]
    Val ER: 90.9%
  Iter 26: conv=0% loss=-6.0451 [2.2s]
  Iter 27: conv=0% loss=-6.0513 [2.2s]
  Iter 28: conv=0% loss=-6.0528 [2.2s]
  Iter 29: conv=0% loss=-6.0530 [2.2s]
  Iter 30: conv=0% loss=-6.0529 [2.2s]
    Val ER: 93.0%
  Iter 31: conv=0% loss=-6.0487 [2.2s]
  Iter 32: conv=0% loss=-6.0454 [2.2s]
  Iter 33: conv=0% loss=-6.0427 [2.2s]
  Iter 34: conv=0% loss=-6.0419 [2.2s]
  Iter 35: conv=0% loss=-6.0384 [2.2s]
    Val ER: 93.7%
  Iter 36: conv=0% loss=-6.0337 [2.2s]
  Iter 37: conv=0% loss=-6.0330 [2.2s]
  Iter 38: conv=0% loss=-6.0338 [2.2s]
  Iter 39: conv=0% loss=-6.0313 [2.2s]
  Iter 40: conv=0% loss=-6.0276 [2.1s]
    Val ER: 94.1%
  Iter 41: conv=0% loss=-6.0274 [2.2s]
  Iter 42: conv=0% loss=-6.0297 [2.2s]
  Iter 43: conv=0% loss=-6.0306 [2.2s]
  Iter 44: conv=0% loss=-6.0298 [2.1s]
  Iter 45: conv=0% loss=-6.0308 [2.2s]
    Val ER: 95.1%
  Iter 46: conv=0% loss=-6.0326 [2.2s]
  Iter 47: conv=0% loss=-6.0324 [2.2s]
  Iter 48: conv=0% loss=-6.0308 [2.1s]
  Iter 49: conv=0% loss=-6.0317 [2.2s]
  Iter 50: conv=0% loss=-6.0338 [2.1s]
    Val ER: 94.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.8s]
    Phase 1: 147.2s, 50 iter, ER=96.1%/95.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=9699.1 val_ppl=1575.8 acc=11.5% [2.6s] ★
  Epoch 2: train_ppl=663.8 val_ppl=848.4 acc=14.7% [2.6s] ★
  Epoch 3: train_ppl=287.2 val_ppl=675.5 acc=15.6% [2.6s] ★
  Epoch 4: train_ppl=141.0 val_ppl=706.2 acc=15.8% [2.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=675.5, acc=15.6%
    Phase 2: 10.5s, PPL=675.5, Acc=15.6%

[8/12] SDL | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.6568 [4.3s]
  Iter 3: conv=0% loss=-5.7120 [4.2s]
  Iter 4: conv=0% loss=-5.7790 [4.2s]
  Iter 5: conv=0% loss=-5.8431 [4.2s]
    Val ER: 79.2%
  Iter 6: conv=0% loss=-5.8975 [4.3s]
  Iter 7: conv=0% loss=-5.9135 [4.2s]
  Iter 8: conv=0% loss=-5.9057 [4.2s]
  Iter 9: conv=0% loss=-5.9109 [4.2s]
  Iter 10: conv=0% loss=-5.9439 [4.2s]
    Val ER: 80.0%
  Iter 11: conv=0% loss=-5.9567 [4.3s]
  Iter 12: conv=0% loss=-5.9623 [4.2s]
  Iter 13: conv=0% loss=-5.9667 [4.2s]
  Iter 14: conv=0% loss=-5.9631 [4.2s]
  Iter 15: conv=0% loss=-5.9669 [4.2s]
    Val ER: 81.0%
  Iter 16: conv=0% loss=-5.9825 [4.2s]
  Iter 17: conv=0% loss=-5.9725 [4.2s]
  Iter 18: conv=0% loss=-5.9735 [4.2s]
  Iter 19: conv=0% loss=-5.9962 [4.2s]
  Iter 20: conv=0% loss=-5.9966 [4.2s]
    Val ER: 88.2%
  Iter 21: conv=0% loss=-6.0055 [4.2s]
  Iter 22: conv=0% loss=-6.0296 [4.2s]
  Iter 23: conv=0% loss=-6.0274 [4.2s]
  Iter 24: conv=0% loss=-6.0363 [4.2s]
  Iter 25: conv=0% loss=-6.0498 [4.2s]
    Val ER: 91.2%
  Iter 26: conv=0% loss=-6.0477 [4.2s]
  Iter 27: conv=0% loss=-6.0540 [4.2s]
  Iter 28: conv=0% loss=-6.0549 [4.2s]
  Iter 29: conv=0% loss=-6.0546 [4.2s]
  Iter 30: conv=0% loss=-6.0541 [4.2s]
    Val ER: 93.2%
  Iter 31: conv=0% loss=-6.0498 [4.2s]
  Iter 32: conv=0% loss=-6.0465 [4.2s]
  Iter 33: conv=0% loss=-6.0434 [4.2s]
  Iter 34: conv=0% loss=-6.0423 [4.2s]
  Iter 35: conv=0% loss=-6.0387 [4.2s]
    Val ER: 93.8%
  Iter 36: conv=0% loss=-6.0343 [4.2s]
  Iter 37: conv=0% loss=-6.0336 [4.2s]
  Iter 38: conv=0% loss=-6.0339 [4.2s]
  Iter 39: conv=0% loss=-6.0317 [4.2s]
  Iter 40: conv=0% loss=-6.0285 [4.2s]
    Val ER: 94.2%
  Iter 41: conv=0% loss=-6.0282 [4.2s]
  Iter 42: conv=0% loss=-6.0304 [4.2s]
  Iter 43: conv=0% loss=-6.0312 [4.2s]
  Iter 44: conv=0% loss=-6.0306 [4.2s]
  Iter 45: conv=0% loss=-6.0312 [4.2s]
    Val ER: 95.1%
  Iter 46: conv=0% loss=-6.0329 [4.2s]
  Iter 47: conv=0% loss=-6.0326 [4.2s]
  Iter 48: conv=0% loss=-6.0314 [4.2s]
  Iter 49: conv=0% loss=-6.0323 [4.2s]
  Iter 50: conv=0% loss=-6.0340 [4.2s]
    Val ER: 95.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.6s]
    Phase 1: 256.2s, 50 iter, ER=96.2%/95.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3779.3 val_ppl=868.9 acc=14.2% [4.6s] ★
  Epoch 2: train_ppl=490.2 val_ppl=566.6 acc=15.9% [4.6s] ★
  Epoch 3: train_ppl=247.8 val_ppl=511.5 acc=16.6% [4.6s] ★
  Epoch 4: train_ppl=125.6 val_ppl=584.0 acc=16.0% [4.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=511.5, acc=16.6%
    Phase 2: 18.4s, PPL=511.5, Acc=16.6%

[9/12] SDL | ctx_dim=1000 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.6561 [8.3s]
  Iter 3: conv=0% loss=-5.7118 [8.2s]
  Iter 4: conv=0% loss=-5.7785 [8.2s]
  Iter 5: conv=0% loss=-5.8415 [8.2s]
    Val ER: 79.2%
  Iter 6: conv=0% loss=-5.8951 [8.2s]
  Iter 7: conv=0% loss=-5.9109 [8.3s]
  Iter 8: conv=0% loss=-5.9032 [8.2s]
  Iter 9: conv=0% loss=-5.9089 [8.2s]
  Iter 10: conv=0% loss=-5.9419 [8.2s]
    Val ER: 80.0%
  Iter 11: conv=0% loss=-5.9552 [8.2s]
  Iter 12: conv=0% loss=-5.9607 [8.2s]
  Iter 13: conv=0% loss=-5.9649 [8.2s]
  Iter 14: conv=0% loss=-5.9609 [8.2s]
  Iter 15: conv=0% loss=-5.9657 [8.2s]
    Val ER: 80.9%
  Iter 16: conv=0% loss=-5.9799 [8.3s]
  Iter 17: conv=0% loss=-5.9681 [8.2s]
  Iter 18: conv=0% loss=-5.9702 [8.1s]
  Iter 19: conv=0% loss=-5.9930 [8.3s]
  Iter 20: conv=0% loss=-5.9923 [8.2s]
    Val ER: 88.2%
  Iter 21: conv=0% loss=-6.0029 [8.2s]
  Iter 22: conv=0% loss=-6.0278 [8.2s]
  Iter 23: conv=0% loss=-6.0244 [8.2s]
  Iter 24: conv=0% loss=-6.0344 [8.2s]
  Iter 25: conv=0% loss=-6.0476 [8.2s]
    Val ER: 91.2%
  Iter 26: conv=0% loss=-6.0451 [8.1s]
  Iter 27: conv=0% loss=-6.0521 [8.2s]
  Iter 28: conv=0% loss=-6.0528 [8.2s]
  Iter 29: conv=0% loss=-6.0523 [8.1s]
  Iter 30: conv=0% loss=-6.0517 [8.2s]
    Val ER: 93.2%
  Iter 31: conv=0% loss=-6.0474 [8.2s]
  Iter 32: conv=0% loss=-6.0443 [8.1s]
  Iter 33: conv=0% loss=-6.0411 [8.1s]
  Iter 34: conv=0% loss=-6.0401 [8.2s]
  Iter 35: conv=0% loss=-6.0363 [8.1s]
    Val ER: 93.8%
  Iter 36: conv=0% loss=-6.0321 [8.1s]
  Iter 37: conv=0% loss=-6.0318 [8.2s]
  Iter 38: conv=0% loss=-6.0320 [8.1s]
  Iter 39: conv=0% loss=-6.0299 [8.1s]
  Iter 40: conv=0% loss=-6.0272 [8.1s]
    Val ER: 94.2%
  Iter 41: conv=0% loss=-6.0269 [8.1s]
  Iter 42: conv=0% loss=-6.0287 [8.1s]
  Iter 43: conv=0% loss=-6.0295 [8.2s]
  Iter 44: conv=0% loss=-6.0290 [8.1s]
  Iter 45: conv=0% loss=-6.0298 [8.1s]
    Val ER: 95.0%
  Iter 46: conv=0% loss=-6.0315 [8.2s]
  Iter 47: conv=0% loss=-6.0314 [8.1s]
  Iter 48: conv=0% loss=-6.0302 [8.1s]
  Iter 49: conv=0% loss=-6.0309 [8.1s]
  Iter 50: conv=0% loss=-6.0326 [8.1s]
    Val ER: 94.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.1s]
    Phase 1: 468.8s, 50 iter, ER=96.1%/95.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1706.2 val_ppl=597.6 acc=15.9% [8.5s] ★
  Epoch 2: train_ppl=372.9 val_ppl=453.8 acc=16.6% [8.5s] ★
  Epoch 3: train_ppl=202.5 val_ppl=440.6 acc=17.4% [8.5s] ★
  Epoch 4: train_ppl=108.7 val_ppl=546.6 acc=17.0% [8.4s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=440.6, acc=17.4%
    Phase 2: 34.0s, PPL=440.6, Acc=17.4%

======================================================================
Algorithm: NUC - Nuclear Norm Maximization (核ノルム最大化, 高コスト)
======================================================================

[10/12] NUC | ctx_dim=1000 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3658 [2.2s]
  Iter 3: conv=0% loss=-7.1703 [2.1s]
  Iter 4: conv=0% loss=-6.4475 [2.2s]
  Iter 5: conv=0% loss=-6.2345 [2.1s]
    Val ER: 69.1%
  Iter 6: conv=0% loss=-6.8628 [2.2s]
  Iter 7: conv=0% loss=-7.2966 [2.2s]
  Iter 8: conv=0% loss=-7.3104 [2.2s]
  Iter 9: conv=0% loss=-7.7079 [2.1s]
  Iter 10: conv=0% loss=-7.9072 [2.2s]
    Val ER: 70.2%
  Iter 11: conv=0% loss=-8.1477 [2.2s]
  Iter 12: conv=0% loss=-8.3355 [2.2s]
  Iter 13: conv=0% loss=-8.1493 [2.2s]
  Iter 14: conv=0% loss=-8.2423 [2.2s]
  Iter 15: conv=0% loss=-7.7559 [2.2s]
    Val ER: 75.1%
  Iter 16: conv=0% loss=-8.2806 [2.2s]
  Iter 17: conv=0% loss=-7.9896 [2.2s]
  Iter 18: conv=0% loss=-8.6230 [2.2s]
  Iter 19: conv=0% loss=-9.1326 [2.2s]
  Iter 20: conv=0% loss=-8.4201 [2.2s]
    Val ER: 80.3%
  Iter 21: conv=0% loss=-9.7579 [2.2s]
  Iter 22: conv=0% loss=-8.9383 [2.2s]
  Iter 23: conv=0% loss=-10.4064 [2.1s]
  Iter 24: conv=0% loss=-10.0386 [2.1s]
  Iter 25: conv=0% loss=-10.3344 [2.1s]
    Val ER: 83.7%
  Iter 26: conv=0% loss=-10.7891 [2.1s]
  Iter 27: conv=0% loss=-10.6264 [2.1s]
  Iter 28: conv=0% loss=-11.3010 [2.1s]
  Iter 29: conv=0% loss=-11.1824 [2.1s]
  Iter 30: conv=0% loss=-11.4144 [2.1s]
    Val ER: 85.9%
  Iter 31: conv=0% loss=-11.4399 [2.1s]
  Iter 32: conv=0% loss=-11.6995 [2.1s]
  Iter 33: conv=0% loss=-11.6782 [2.1s]
  Iter 34: conv=0% loss=-11.8396 [2.1s]
  Iter 35: conv=0% loss=-11.6848 [2.1s]
    Val ER: 88.5%
  Iter 36: conv=0% loss=-12.0250 [2.1s]
  Iter 37: conv=0% loss=-11.9956 [2.1s]
  Iter 38: conv=0% loss=-12.2167 [2.1s]
  Iter 39: conv=0% loss=-12.1279 [2.1s]
  Iter 40: conv=0% loss=-12.3156 [2.1s]
    Val ER: 89.7%
  Iter 41: conv=0% loss=-12.2631 [2.1s]
  Iter 42: conv=0% loss=-12.4476 [2.2s]
  Iter 43: conv=0% loss=-12.3783 [2.1s]
  Iter 44: conv=0% loss=-12.5117 [2.2s]
  Iter 45: conv=0% loss=-12.4987 [2.1s]
    Val ER: 91.0%
  Iter 46: conv=0% loss=-12.5723 [2.2s]
  Iter 47: conv=0% loss=-12.6066 [2.1s]
  Iter 48: conv=0% loss=-12.6605 [2.2s]
  Iter 49: conv=0% loss=-12.7338 [2.1s]
  Iter 50: conv=0% loss=-12.7748 [2.1s]
    Val ER: 91.5%
  Iter 51: conv=0% loss=-12.8461 [2.1s]
  Iter 52: conv=0% loss=-12.8562 [2.1s]
  Iter 53: conv=0% loss=-12.9362 [2.1s]
  Iter 54: conv=0% loss=-12.9387 [2.1s]
  Iter 55: conv=0% loss=-12.9891 [2.1s]
    Val ER: 91.8%
  Iter 56: conv=0% loss=-12.9821 [2.1s]
  Iter 57: conv=0% loss=-13.0381 [2.1s]
  Iter 58: conv=0% loss=-13.0751 [2.1s]
  Iter 59: conv=0% loss=-13.0841 [2.1s]
  Iter 60: conv=0% loss=-13.0996 [2.1s]
    Val ER: 92.3%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 170.7s, 60 iter, ER=93.8%/92.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=9915.1 val_ppl=1669.8 acc=11.7% [2.6s] ★
  Epoch 2: train_ppl=719.9 val_ppl=914.9 acc=14.4% [2.6s] ★
  Epoch 3: train_ppl=319.2 val_ppl=720.1 acc=15.1% [2.6s] ★
  Epoch 4: train_ppl=156.6 val_ppl=747.1 acc=15.5% [2.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=720.1, acc=15.1%
    Phase 2: 10.5s, PPL=720.1, Acc=15.1%

[11/12] NUC | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3693 [4.2s]
  Iter 3: conv=0% loss=-7.1570 [4.2s]
  Iter 4: conv=0% loss=-6.4243 [4.3s]
  Iter 5: conv=0% loss=-6.2081 [4.3s]
    Val ER: 68.9%
  Iter 6: conv=0% loss=-6.8355 [4.2s]
  Iter 7: conv=0% loss=-7.2461 [4.3s]
  Iter 8: conv=0% loss=-7.2609 [4.3s]
  Iter 9: conv=0% loss=-7.6568 [4.3s]
  Iter 10: conv=0% loss=-7.8724 [4.2s]
    Val ER: 70.3%
  Iter 11: conv=0% loss=-8.1313 [4.2s]
  Iter 12: conv=0% loss=-8.3174 [4.3s]
  Iter 13: conv=0% loss=-8.1464 [4.3s]
  Iter 14: conv=0% loss=-8.2540 [4.2s]
  Iter 15: conv=0% loss=-7.8029 [4.2s]
    Val ER: 75.1%
  Iter 16: conv=0% loss=-8.3025 [4.3s]
  Iter 17: conv=0% loss=-8.0094 [4.2s]
  Iter 18: conv=0% loss=-8.6628 [4.2s]
  Iter 19: conv=0% loss=-9.1510 [4.2s]
  Iter 20: conv=0% loss=-8.5276 [4.3s]
    Val ER: 80.5%
  Iter 21: conv=0% loss=-9.8082 [4.2s]
  Iter 22: conv=0% loss=-9.0576 [4.2s]
  Iter 23: conv=0% loss=-10.4661 [4.2s]
  Iter 24: conv=0% loss=-10.1225 [4.1s]
  Iter 25: conv=0% loss=-10.4251 [4.2s]
    Val ER: 83.8%
  Iter 26: conv=0% loss=-10.7838 [4.1s]
  Iter 27: conv=0% loss=-10.6523 [4.1s]
  Iter 28: conv=0% loss=-11.2688 [4.1s]
  Iter 29: conv=0% loss=-11.1402 [4.1s]
  Iter 30: conv=0% loss=-11.4120 [4.1s]
    Val ER: 86.0%
  Iter 31: conv=0% loss=-11.3722 [4.2s]
  Iter 32: conv=0% loss=-11.6376 [4.2s]
  Iter 33: conv=0% loss=-11.5750 [4.1s]
  Iter 34: conv=0% loss=-11.7583 [4.1s]
  Iter 35: conv=0% loss=-11.5836 [4.1s]
    Val ER: 88.4%
  Iter 36: conv=0% loss=-11.9312 [4.1s]
  Iter 37: conv=0% loss=-11.8997 [4.1s]
  Iter 38: conv=0% loss=-12.1452 [4.1s]
  Iter 39: conv=0% loss=-12.0243 [4.1s]
  Iter 40: conv=0% loss=-12.1987 [4.2s]
    Val ER: 89.8%
  Iter 41: conv=0% loss=-12.1559 [4.2s]
  Iter 42: conv=0% loss=-12.3694 [4.1s]
  Iter 43: conv=0% loss=-12.3009 [4.1s]
  Iter 44: conv=0% loss=-12.3862 [4.2s]
  Iter 45: conv=0% loss=-12.3747 [4.1s]
    Val ER: 91.0%
  Iter 46: conv=0% loss=-12.4546 [4.1s]
  Iter 47: conv=0% loss=-12.5460 [4.1s]
  Iter 48: conv=0% loss=-12.5799 [4.2s]
  Iter 49: conv=0% loss=-12.6511 [4.2s]
  Iter 50: conv=0% loss=-12.6743 [4.1s]
    Val ER: 91.7%
  Iter 51: conv=0% loss=-12.7537 [4.1s]
  Iter 52: conv=0% loss=-12.7634 [4.2s]
  Iter 53: conv=0% loss=-12.8467 [4.2s]
  Iter 54: conv=0% loss=-12.8382 [4.1s]
  Iter 55: conv=0% loss=-12.9204 [4.1s]
    Val ER: 91.9%
  Iter 56: conv=0% loss=-12.9266 [4.2s]
  Iter 57: conv=0% loss=-12.9518 [4.2s]
  Iter 58: conv=0% loss=-12.9858 [4.1s]
  Iter 59: conv=0% loss=-13.0067 [4.1s]
  Iter 60: conv=0% loss=-13.0285 [4.2s]
    Val ER: 92.4%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.5s]
    Phase 1: 305.8s, 60 iter, ER=93.8%/92.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3899.4 val_ppl=906.3 acc=14.2% [4.6s] ★
  Epoch 2: train_ppl=525.4 val_ppl=589.9 acc=15.9% [4.6s] ★
  Epoch 3: train_ppl=274.6 val_ppl=536.6 acc=16.3% [4.6s] ★
  Epoch 4: train_ppl=138.9 val_ppl=614.5 acc=15.9% [4.6s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=536.6, acc=16.3%
    Phase 2: 18.4s, PPL=536.6, Acc=16.3%

[12/12] NUC | ctx_dim=1000 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3272 [8.2s]
  Iter 3: conv=0% loss=-7.1168 [8.2s]
  Iter 4: conv=0% loss=-6.3785 [8.2s]
  Iter 5: conv=0% loss=-6.1535 [8.1s]
    Val ER: 68.9%
  Iter 6: conv=0% loss=-6.7874 [8.2s]
  Iter 7: conv=0% loss=-7.1881 [8.1s]
  Iter 8: conv=0% loss=-7.1946 [8.2s]
  Iter 9: conv=0% loss=-7.5853 [8.1s]
  Iter 10: conv=0% loss=-7.8045 [8.2s]
    Val ER: 70.3%
  Iter 11: conv=0% loss=-8.0706 [8.1s]
  Iter 12: conv=0% loss=-8.2518 [8.3s]
  Iter 13: conv=0% loss=-8.0254 [8.1s]
  Iter 14: conv=0% loss=-8.1285 [8.2s]
  Iter 15: conv=0% loss=-7.6796 [8.1s]
    Val ER: 75.2%
  Iter 16: conv=0% loss=-8.1992 [8.2s]
  Iter 17: conv=0% loss=-7.9092 [8.0s]
  Iter 18: conv=0% loss=-8.5378 [8.2s]
  Iter 19: conv=0% loss=-9.0189 [8.1s]
  Iter 20: conv=0% loss=-8.3693 [8.1s]
    Val ER: 80.5%
  Iter 21: conv=0% loss=-9.6685 [8.0s]
  Iter 22: conv=0% loss=-8.9497 [8.1s]
  Iter 23: conv=0% loss=-10.2989 [8.0s]
  Iter 24: conv=0% loss=-10.0643 [8.0s]
  Iter 25: conv=0% loss=-10.3511 [7.9s]
    Val ER: 83.9%
  Iter 26: conv=0% loss=-10.7830 [8.1s]
  Iter 27: conv=0% loss=-10.6845 [7.9s]
  Iter 28: conv=0% loss=-11.2414 [8.0s]
  Iter 29: conv=0% loss=-11.1167 [7.8s]
  Iter 30: conv=0% loss=-11.3981 [7.9s]
    Val ER: 86.3%
  Iter 31: conv=0% loss=-11.3377 [7.9s]
  Iter 32: conv=0% loss=-11.6086 [8.0s]
  Iter 33: conv=0% loss=-11.5227 [7.8s]
  Iter 34: conv=0% loss=-11.7334 [7.9s]
  Iter 35: conv=0% loss=-11.5616 [7.8s]
    Val ER: 88.4%
  Iter 36: conv=0% loss=-11.8768 [7.9s]
  Iter 37: conv=0% loss=-11.8542 [7.8s]
  Iter 38: conv=0% loss=-12.0871 [8.0s]
  Iter 39: conv=0% loss=-11.9896 [7.8s]
  Iter 40: conv=0% loss=-12.1430 [8.0s]
    Val ER: 89.6%
  Iter 41: conv=0% loss=-12.0987 [7.9s]
  Iter 42: conv=0% loss=-12.2409 [8.0s]
  Iter 43: conv=0% loss=-12.2056 [7.9s]
  Iter 44: conv=0% loss=-12.2961 [8.0s]
  Iter 45: conv=0% loss=-12.3123 [7.9s]
    Val ER: 90.8%
  Iter 46: conv=0% loss=-12.3837 [8.0s]
  Iter 47: conv=0% loss=-12.4444 [7.9s]
  Iter 48: conv=0% loss=-12.4400 [8.0s]
  Iter 49: conv=0% loss=-12.5266 [7.9s]
  Iter 50: conv=0% loss=-12.5588 [8.0s]
    Val ER: 91.3%
  Iter 51: conv=0% loss=-12.6261 [7.9s]
  Iter 52: conv=0% loss=-12.6285 [8.1s]
  Iter 53: conv=0% loss=-12.7207 [7.9s]
  Iter 54: conv=0% loss=-12.7195 [8.0s]
  Iter 55: conv=0% loss=-12.7843 [7.9s]
    Val ER: 92.0%
  Iter 56: conv=0% loss=-12.8188 [8.0s]
  Iter 57: conv=0% loss=-12.8526 [7.9s]
  Iter 58: conv=0% loss=-12.8847 [8.0s]
  Iter 59: conv=0% loss=-12.9181 [7.9s]
  Iter 60: conv=0% loss=-12.9517 [8.0s]
    Val ER: 92.5%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.1s]
    Phase 1: 553.5s, 60 iter, ER=94.0%/92.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,945,504/76,128,248 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=1785.8 val_ppl=630.7 acc=15.9% [8.4s] ★
  Epoch 2: train_ppl=404.6 val_ppl=472.3 acc=16.6% [8.5s] ★
  Epoch 3: train_ppl=226.4 val_ppl=457.8 acc=17.2% [8.4s] ★
  Epoch 4: train_ppl=119.7 val_ppl=570.0 acc=16.5% [8.4s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=457.8, acc=17.2%
    Phase 2: 33.7s, PPL=457.8, Acc=17.2%

==================================================================================================================================
FULL EXPERIMENT RESULTS (Phase 1 + Phase 2)
==================================================================================================================================
Algo   Samples     Tokens P1 Iter  Train ER  Val ER BestValER   T.PPL   V.PPL    Acc   Time
----------------------------------------------------------------------------------------------------------------------------------
MCDL        50     62,891      10     69.7%   67.3%     69.6%   203.5   532.6  16.9%    31s
MCDL       100    122,795      10     69.9%   67.1%     69.6%   183.4   409.9  18.5%    50s
MCDL       200    240,132      10     70.6%   67.9%     69.5%   225.0   371.0  18.9%    84s
ODCM        50     62,891      20     83.9%   73.8%     80.3%   291.2   821.4  14.3%    46s
ODCM       100    122,795      20     84.4%   71.7%     80.1%   237.4   533.1  16.5%    77s
ODCM       200    240,132      20     84.0%   75.6%     80.3%   184.8   430.6  17.8%   137s
SDL         50     62,891      50     96.1%   95.1%     95.1%   287.2   675.5  15.6%   158s
SDL        100    122,795      50     96.2%   95.2%     95.1%   247.8   511.5  16.6%   275s
SDL        200    240,132      50     96.1%   95.1%     95.0%   202.5   440.6  17.4%   503s
NUC         50     62,891      60     93.8%   92.6%     92.3%   319.2   720.1  15.1%   181s
NUC        100    122,795      60     93.8%   92.8%     92.4%   274.6   536.6  16.3%   324s
NUC        200    240,132      60     94.0%   92.9%     92.5%   226.4   457.8  17.2%   587s
==================================================================================================================================

====================================================================================================
SCALING LAW ANALYSIS (PPL = A × tokens^α)
====================================================================================================
Algorithm     α (PPL)            A       R² Best Val PPL   Best Acc
----------------------------------------------------------------------------------------------------
MCDL         -0.26984     1.02e+04   0.9370        371.0      18.9%
ODCM         -0.48196     1.63e+05   0.9629        430.6      17.8%
SDL          -0.31886     2.24e+04   0.9704        440.6      17.4%
NUC          -0.33814     2.95e+04   0.9710        457.8      17.2%
====================================================================================================

--- Ranking by α (more negative = better scaling) ---
  1. ODCM: α=-0.48196, PPL=430.6
  2. NUC: α=-0.33814, PPL=457.8
  3. SDL: α=-0.31886, PPL=440.6
  4. MCDL: α=-0.26984, PPL=371.0

--- Ranking by Val PPL (lower = better) ---
  1. MCDL: PPL=371.0, Acc=18.9%
  2. ODCM: PPL=430.6, Acc=17.8%
  3. SDL: PPL=440.6, Acc=17.4%
  4. NUC: PPL=457.8, Acc=17.2%

All results saved to: importants/logs/20251201_105710_diversity_full/all_results.json

Total time: 44.2 min

Experiment completed!
