Already up to date.
======================================================================
SCALING LAW EXPERIMENT
======================================================================
Configurations: 9
Sample sizes: [50, 100, 200, 500]
Total experiments: 36
Output: results/scaling_20251130_031816
Device: cuda (NVIDIA L4, 23.8GB)

Configurations:
  1. 1L_768d_1tok
  2. 2L_768d_1tok
  3. 3L_768d_1tok
  4. 1L_768d_2tok
  5. 2L_768d_2tok
  6. 3L_768d_2tok
  7. 1L_768d_3tok
  8. 2L_768d_3tok
  9. 3L_768d_3tok
Device: cuda (NVIDIA L4, 22.2GB)

======================================================================
Config: 1L_768d_1tok
  num_layers=1, context_dim=768, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 768d, 1tok ---
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 173kB/s]
config.json: 100% 665/665 [00:00<00:00, 4.77MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.44MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 1.08MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.19MB/s]
Loading training data...
  Loading 50 samples from UltraChat...
README.md: 3.90kB [00:00, 23.3MB/s]
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:01<00:00, 125MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:03<00:00, 78.6MB/s]
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:01<00:00, 217MB/s]
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:01<00:00, 76.6MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:01<00:00, 205MB/s]
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:01<00:00, 225MB/s]
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:01<00:00, 240MB/s]
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:01<00:00, 79.2MB/s]
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 50024.54 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 55254.78 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 70874.31 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 73855.10 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_50samples_full.pt
Loading validation data...
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
2025-11-30 03:18:52.275604: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 03:18:52.291986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764472732.310941    5779 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764472732.317343    5779 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764472732.333967    5779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764472732.333993    5779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764472732.333996    5779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764472732.333999    5779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-30 03:18:52.338898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 4.88MB/s]
model.safetensors: 100% 548M/548M [00:01<00:00, 536MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0204 [1.5s]
  Iter 3: conv=0% loss=-0.0642 [0.5s]
  Iter 4: conv=0% loss=-0.1076 [0.5s]
  Iter 5: conv=0% loss=-0.1350 [0.4s]
  Iter 6: conv=0% loss=-0.1546 [0.4s]
  Iter 7: conv=0% loss=-0.1694 [0.5s]
  Iter 8: conv=1% loss=-0.1818 [0.5s]
  Iter 9: conv=1% loss=-0.1924 [0.5s]
  Iter 10: conv=1% loss=-0.2017 [0.5s]
  Iter 11: conv=2% loss=-0.2098 [0.5s]
  Iter 12: conv=2% loss=-0.2169 [0.5s]
  Iter 13: conv=3% loss=-0.2231 [0.5s]
  Iter 14: conv=3% loss=-0.2282 [0.5s]
  Iter 15: conv=4% loss=-0.2325 [0.5s]
  Iter 16: conv=5% loss=-0.2359 [0.5s]
  Iter 17: conv=6% loss=-0.2388 [0.5s]
  Iter 18: conv=8% loss=-0.2412 [0.5s]
  Iter 19: conv=10% loss=-0.2432 [0.4s]
  Iter 20: conv=13% loss=-0.2449 [0.4s]
  Iter 21: conv=16% loss=-0.2464 [0.4s]
  Iter 22: conv=21% loss=-0.2476 [0.5s]
  Iter 23: conv=26% loss=-0.2487 [0.5s]
  Iter 24: conv=31% loss=-0.2497 [0.5s]
  Iter 25: conv=37% loss=-0.2506 [0.4s]
  Iter 26: conv=42% loss=-0.2515 [0.5s]
  Iter 27: conv=47% loss=-0.2523 [0.5s]
  Iter 28: conv=50% loss=-0.2530 [0.4s]
  Iter 29: conv=54% loss=-0.2538 [0.5s]
  Iter 30: conv=57% loss=-0.2545 [0.5s]
  Iter 31: conv=60% loss=-0.2551 [0.5s]
  Iter 32: conv=63% loss=-0.2558 [0.4s]
  Iter 33: conv=65% loss=-0.2564 [0.4s]
  Iter 34: conv=67% loss=-0.2571 [0.5s]
  Iter 35: conv=69% loss=-0.2577 [0.5s]
  Iter 36: conv=71% loss=-0.2583 [0.5s]
  Iter 37: conv=72% loss=-0.2590 [0.5s]
  Iter 38: conv=73% loss=-0.2596 [0.5s]
  Iter 39: conv=74% loss=-0.2602 [0.5s]
  Iter 40: conv=76% loss=-0.2608 [0.5s]
  Done: 76% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.4s]
  ER: train=77.9%, val=77.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=11618601.0 val_ppl=52371.0 acc=7.3% [1.7s] ★
  Epoch 2: train_ppl=11870.3 val_ppl=5397.0 acc=9.6% [1.8s] ★
  Epoch 3: train_ppl=2383.2 val_ppl=2318.2 acc=13.6% [1.8s] ★
  Epoch 4: train_ppl=1013.0 val_ppl=1529.6 acc=14.4% [1.8s] ★
  Epoch 5: train_ppl=584.4 val_ppl=1188.1 acc=15.5% [1.8s] ★
  Epoch 6: train_ppl=411.0 val_ppl=995.9 acc=16.1% [1.8s] ★
  Epoch 7: train_ppl=316.0 val_ppl=877.6 acc=16.4% [1.8s] ★
  Epoch 8: train_ppl=248.1 val_ppl=790.3 acc=16.8% [1.8s] ★
  Epoch 9: train_ppl=204.5 val_ppl=729.7 acc=17.1% [1.8s] ★
  Epoch 10: train_ppl=170.6 val_ppl=683.9 acc=17.3% [1.8s] ★
  Best: epoch 10, ppl=683.9, acc=17.3%
  Result: PPL=683.9, Acc=17.3%
    → PPL: 683.9, Acc: 17.3%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading 100 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_100samples_full.pt
Loading validation data...
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0218 [1.0s]
  Iter 3: conv=0% loss=-0.0648 [0.9s]
  Iter 4: conv=0% loss=-0.1083 [0.9s]
  Iter 5: conv=0% loss=-0.1358 [0.9s]
  Iter 6: conv=0% loss=-0.1555 [0.9s]
  Iter 7: conv=0% loss=-0.1703 [0.9s]
  Iter 8: conv=0% loss=-0.1827 [0.9s]
  Iter 9: conv=0% loss=-0.1935 [0.9s]
  Iter 10: conv=1% loss=-0.2028 [0.9s]
  Iter 11: conv=1% loss=-0.2110 [0.9s]
  Iter 12: conv=1% loss=-0.2183 [0.9s]
  Iter 13: conv=1% loss=-0.2245 [0.9s]
  Iter 14: conv=2% loss=-0.2298 [0.9s]
  Iter 15: conv=2% loss=-0.2341 [0.9s]
  Iter 16: conv=3% loss=-0.2376 [0.9s]
  Iter 17: conv=5% loss=-0.2405 [0.9s]
  Iter 18: conv=6% loss=-0.2430 [0.9s]
  Iter 19: conv=9% loss=-0.2450 [0.9s]
  Iter 20: conv=12% loss=-0.2468 [0.9s]
  Iter 21: conv=15% loss=-0.2482 [0.9s]
  Iter 22: conv=20% loss=-0.2495 [0.9s]
  Iter 23: conv=25% loss=-0.2507 [0.9s]
  Iter 24: conv=31% loss=-0.2517 [0.9s]
  Iter 25: conv=36% loss=-0.2526 [0.9s]
  Iter 26: conv=42% loss=-0.2534 [0.9s]
  Iter 27: conv=46% loss=-0.2542 [0.9s]
  Iter 28: conv=50% loss=-0.2550 [0.9s]
  Iter 29: conv=54% loss=-0.2557 [0.9s]
  Iter 30: conv=57% loss=-0.2564 [0.9s]
  Iter 31: conv=60% loss=-0.2571 [0.9s]
  Iter 32: conv=62% loss=-0.2578 [0.9s]
  Iter 33: conv=65% loss=-0.2584 [0.9s]
  Iter 34: conv=66% loss=-0.2591 [0.9s]
  Iter 35: conv=68% loss=-0.2597 [0.9s]
  Iter 36: conv=70% loss=-0.2603 [0.9s]
  Iter 37: conv=71% loss=-0.2610 [0.9s]
  Iter 38: conv=73% loss=-0.2616 [0.9s]
  Iter 39: conv=74% loss=-0.2622 [0.9s]
  Iter 40: conv=75% loss=-0.2628 [0.9s]
  Done: 75% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.9s]
  ER: train=78.3%, val=77.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=492971.6 val_ppl=5738.8 acc=9.5% [3.2s] ★
  Epoch 2: train_ppl=2324.6 val_ppl=1528.3 acc=14.6% [3.2s] ★
  Epoch 3: train_ppl=893.7 val_ppl=959.4 acc=16.1% [3.2s] ★
  Epoch 4: train_ppl=551.7 val_ppl=732.2 acc=16.9% [3.3s] ★
  Epoch 5: train_ppl=387.6 val_ppl=614.2 acc=17.4% [3.3s] ★
  Epoch 6: train_ppl=299.7 val_ppl=540.0 acc=17.9% [3.3s] ★
  Epoch 7: train_ppl=242.7 val_ppl=491.1 acc=18.4% [3.3s] ★
  Epoch 8: train_ppl=202.9 val_ppl=457.1 acc=18.7% [3.3s] ★
  Epoch 9: train_ppl=173.6 val_ppl=433.1 acc=18.8% [3.3s] ★
  Epoch 10: train_ppl=150.8 val_ppl=415.8 acc=19.1% [3.2s] ★
  Best: epoch 10, ppl=415.8, acc=19.1%
  Result: PPL=415.8, Acc=19.1%
    → PPL: 415.8, Acc: 19.1%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading 200 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_200samples_full.pt
Loading validation data...
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0190 [1.8s]
  Iter 3: conv=0% loss=-0.0622 [1.6s]
  Iter 4: conv=0% loss=-0.1059 [1.5s]
  Iter 5: conv=0% loss=-0.1331 [1.6s]
  Iter 6: conv=0% loss=-0.1523 [1.6s]
  Iter 7: conv=0% loss=-0.1669 [1.5s]
  Iter 8: conv=0% loss=-0.1790 [1.6s]
  Iter 9: conv=1% loss=-0.1894 [1.6s]
  Iter 10: conv=1% loss=-0.1986 [1.6s]
  Iter 11: conv=1% loss=-0.2067 [1.6s]
  Iter 12: conv=2% loss=-0.2140 [1.6s]
  Iter 13: conv=2% loss=-0.2202 [1.6s]
  Iter 14: conv=2% loss=-0.2254 [1.6s]
  Iter 15: conv=3% loss=-0.2298 [1.6s]
  Iter 16: conv=3% loss=-0.2334 [1.6s]
  Iter 17: conv=5% loss=-0.2364 [1.6s]
  Iter 18: conv=6% loss=-0.2390 [1.6s]
  Iter 19: conv=7% loss=-0.2412 [1.6s]
  Iter 20: conv=9% loss=-0.2431 [1.6s]
  Iter 21: conv=11% loss=-0.2447 [1.6s]
  Iter 22: conv=14% loss=-0.2461 [1.6s]
  Iter 23: conv=18% loss=-0.2473 [1.6s]
  Iter 24: conv=22% loss=-0.2484 [1.6s]
  Iter 25: conv=26% loss=-0.2494 [1.6s]
  Iter 26: conv=30% loss=-0.2503 [1.6s]
  Iter 27: conv=34% loss=-0.2512 [1.6s]
  Iter 28: conv=38% loss=-0.2520 [1.6s]
  Iter 29: conv=41% loss=-0.2528 [1.6s]
  Iter 30: conv=45% loss=-0.2535 [1.6s]
  Iter 31: conv=48% loss=-0.2542 [1.6s]
  Iter 32: conv=51% loss=-0.2549 [1.6s]
  Iter 33: conv=54% loss=-0.2556 [1.6s]
  Iter 34: conv=56% loss=-0.2562 [1.6s]
  Iter 35: conv=59% loss=-0.2569 [1.6s]
  Iter 36: conv=60% loss=-0.2575 [1.6s]
  Iter 37: conv=62% loss=-0.2581 [1.6s]
  Iter 38: conv=64% loss=-0.2587 [1.6s]
  Iter 39: conv=66% loss=-0.2594 [1.6s]
  Iter 40: conv=67% loss=-0.2600 [1.6s]
  Done: 67% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.7s]
  ER: train=78.4%, val=77.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=44860.4 val_ppl=1592.4 acc=14.0% [6.0s] ★
  Epoch 2: train_ppl=959.4 val_ppl=738.1 acc=16.5% [6.1s] ★
  Epoch 3: train_ppl=515.6 val_ppl=529.9 acc=17.6% [6.1s] ★
  Epoch 4: train_ppl=354.7 val_ppl=436.4 acc=18.3% [6.0s] ★
  Epoch 5: train_ppl=273.7 val_ppl=383.5 acc=18.9% [6.0s] ★
  Epoch 6: train_ppl=223.9 val_ppl=349.8 acc=19.3% [5.9s] ★
  Epoch 7: train_ppl=189.7 val_ppl=327.5 acc=19.6% [5.9s] ★
  Epoch 8: train_ppl=163.9 val_ppl=312.2 acc=19.9% [5.9s] ★
  Epoch 9: train_ppl=144.4 val_ppl=301.9 acc=20.0% [5.8s] ★
  Epoch 10: train_ppl=129.0 val_ppl=294.5 acc=20.1% [5.8s] ★
  Best: epoch 10, ppl=294.5, acc=20.1%
  Result: PPL=294.5, Acc=20.1%
    → PPL: 294.5, Acc: 20.1%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading 500 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_500samples_full.pt
Loading validation data...
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0188 [4.1s]
  Iter 3: conv=0% loss=-0.0619 [3.8s]
  Iter 4: conv=0% loss=-0.1054 [3.8s]
  Iter 5: conv=0% loss=-0.1327 [3.8s]
  Iter 6: conv=0% loss=-0.1522 [3.9s]
  Iter 7: conv=0% loss=-0.1670 [3.8s]
  Iter 8: conv=1% loss=-0.1793 [3.8s]
  Iter 9: conv=1% loss=-0.1900 [3.8s]
  Iter 10: conv=1% loss=-0.1993 [3.8s]
  Iter 11: conv=1% loss=-0.2075 [3.8s]
  Iter 12: conv=1% loss=-0.2147 [3.8s]
  Iter 13: conv=2% loss=-0.2209 [3.8s]
  Iter 14: conv=2% loss=-0.2261 [3.8s]
  Iter 15: conv=3% loss=-0.2304 [3.8s]
  Iter 16: conv=4% loss=-0.2340 [3.8s]
  Iter 17: conv=5% loss=-0.2369 [3.8s]
  Iter 18: conv=7% loss=-0.2393 [3.8s]
  Iter 19: conv=9% loss=-0.2413 [3.8s]
  Iter 20: conv=11% loss=-0.2431 [3.8s]
  Iter 21: conv=15% loss=-0.2445 [3.8s]
  Iter 22: conv=20% loss=-0.2458 [3.8s]
  Iter 23: conv=25% loss=-0.2469 [3.8s]
  Iter 24: conv=30% loss=-0.2479 [3.8s]
  Iter 25: conv=35% loss=-0.2488 [3.8s]
  Iter 26: conv=40% loss=-0.2497 [3.8s]
  Iter 27: conv=44% loss=-0.2505 [3.8s]
  Iter 28: conv=48% loss=-0.2512 [3.8s]
  Iter 29: conv=52% loss=-0.2520 [3.8s]
  Iter 30: conv=55% loss=-0.2527 [3.9s]
  Iter 31: conv=58% loss=-0.2533 [3.8s]
  Iter 32: conv=60% loss=-0.2540 [3.8s]
  Iter 33: conv=63% loss=-0.2546 [3.8s]
  Iter 34: conv=65% loss=-0.2553 [3.8s]
  Iter 35: conv=67% loss=-0.2559 [3.8s]
  Iter 36: conv=68% loss=-0.2565 [3.8s]
  Iter 37: conv=70% loss=-0.2571 [3.8s]
  Iter 38: conv=71% loss=-0.2577 [3.8s]
  Iter 39: conv=72% loss=-0.2584 [3.8s]
  Iter 40: conv=73% loss=-0.2590 [3.8s]
  Done: 73% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.7s]
  ER: train=78.6%, val=77.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=4566.4 val_ppl=594.2 acc=17.5% [14.4s] ★
  Epoch 2: train_ppl=445.7 val_ppl=362.3 acc=19.2% [14.3s] ★
  Epoch 3: train_ppl=289.8 val_ppl=290.5 acc=20.0% [13.9s] ★
  Epoch 4: train_ppl=223.9 val_ppl=254.9 acc=20.7% [13.7s] ★
  Epoch 5: train_ppl=185.4 val_ppl=233.7 acc=21.2% [13.7s] ★
  Epoch 6: train_ppl=160.1 val_ppl=219.9 acc=21.6% [13.8s] ★
  Epoch 7: train_ppl=141.9 val_ppl=210.9 acc=21.9% [13.9s] ★
  Epoch 8: train_ppl=128.0 val_ppl=204.8 acc=22.2% [14.0s] ★
  Epoch 9: train_ppl=117.2 val_ppl=200.7 acc=22.5% [14.0s] ★
  Epoch 10: train_ppl=108.3 val_ppl=198.2 acc=22.6% [13.9s] ★
  Best: epoch 10, ppl=198.2, acc=22.6%
  Result: PPL=198.2, Acc=22.6%
    → PPL: 198.2, Acc: 22.6%, ER: 0.0%

======================================================================
Config: 2L_768d_1tok
  num_layers=2, context_dim=768, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 2L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0239 [0.5s]
  Iter 3: conv=0% loss=-0.0994 [0.4s]
  Iter 4: conv=0% loss=-0.1561 [0.5s]
  Iter 5: conv=0% loss=-0.1882 [0.4s]
  Iter 6: conv=0% loss=-0.2104 [0.5s]
  Iter 7: conv=2% loss=-0.2244 [0.4s]
  Iter 8: conv=5% loss=-0.2333 [0.4s]
  Iter 9: conv=12% loss=-0.2389 [0.5s]
  Iter 10: conv=31% loss=-0.2425 [0.4s]
  Iter 11: conv=61% loss=-0.2450 [0.5s]
  Iter 12: conv=84% loss=-0.2467 [0.4s]
  Iter 13: conv=94% loss=-0.2479 [0.5s]
  Iter 14: conv=97% loss=-0.2489 [0.4s]
  Iter 15: conv=99% loss=-0.2497 [0.5s]
  Iter 16: conv=99% loss=-0.2504 [0.4s]
  → Converged at iter 16
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
  ER: train=79.6%, val=79.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,365,440/43,326,720 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=26650.4 val_ppl=3366.7 acc=10.2% [1.9s] ★
  Epoch 2: train_ppl=1256.1 val_ppl=1361.4 acc=14.3% [1.9s] ★
  Epoch 3: train_ppl=485.3 val_ppl=918.7 acc=15.4% [1.9s] ★
  Epoch 4: train_ppl=282.2 val_ppl=756.5 acc=16.2% [1.9s] ★
  Epoch 5: train_ppl=195.7 val_ppl=686.3 acc=16.8% [2.0s] ★
  Epoch 6: train_ppl=143.6 val_ppl=655.0 acc=17.2% [1.9s] ★
  Epoch 7: train_ppl=109.4 val_ppl=647.2 acc=17.5% [1.9s] ★
  Epoch 8: train_ppl=85.3 val_ppl=653.6 acc=17.6% [2.0s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=647.2, acc=17.5%
  Result: PPL=647.2, Acc=17.5%
    → PPL: 647.2, Acc: 17.5%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 2L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0255 [1.2s]
  Iter 3: conv=0% loss=-0.1006 [1.1s]
  Iter 4: conv=0% loss=-0.1573 [1.0s]
  Iter 5: conv=0% loss=-0.1895 [1.0s]
  Iter 6: conv=0% loss=-0.2119 [1.0s]
  Iter 7: conv=1% loss=-0.2262 [1.0s]
  Iter 8: conv=3% loss=-0.2351 [1.0s]
  Iter 9: conv=10% loss=-0.2408 [1.0s]
  Iter 10: conv=29% loss=-0.2445 [1.0s]
  Iter 11: conv=61% loss=-0.2470 [1.0s]
  Iter 12: conv=84% loss=-0.2487 [1.1s]
  Iter 13: conv=94% loss=-0.2499 [1.0s]
  Iter 14: conv=97% loss=-0.2509 [1.0s]
  Iter 15: conv=99% loss=-0.2517 [1.0s]
  Iter 16: conv=99% loss=-0.2524 [1.0s]
  → Converged at iter 16
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.2s]
  ER: train=80.1%, val=79.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,365,440/43,326,720 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=8515.8 val_ppl=1409.4 acc=14.0% [3.4s] ★
  Epoch 2: train_ppl=715.0 val_ppl=703.1 acc=16.4% [3.4s] ★
  Epoch 3: train_ppl=376.5 val_ppl=546.4 acc=17.5% [3.4s] ★
  Epoch 4: train_ppl=256.0 val_ppl=484.0 acc=18.1% [3.5s] ★
  Epoch 5: train_ppl=186.7 val_ppl=455.8 acc=18.5% [3.4s] ★
  Epoch 6: train_ppl=143.3 val_ppl=445.5 acc=19.0% [3.4s] ★
  Epoch 7: train_ppl=112.3 val_ppl=446.4 acc=19.2% [3.4s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=445.5, acc=19.0%
  Result: PPL=445.5, Acc=19.0%
    → PPL: 445.5, Acc: 19.0%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 2L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0226 [1.8s]
  Iter 3: conv=0% loss=-0.0983 [2.0s]
  Iter 4: conv=0% loss=-0.1542 [1.9s]
  Iter 5: conv=0% loss=-0.1866 [1.9s]
  Iter 6: conv=0% loss=-0.2086 [1.9s]
  Iter 7: conv=1% loss=-0.2226 [1.9s]
  Iter 8: conv=3% loss=-0.2315 [1.9s]
  Iter 9: conv=8% loss=-0.2372 [1.9s]
  Iter 10: conv=24% loss=-0.2410 [1.9s]
  Iter 11: conv=51% loss=-0.2436 [1.9s]
  Iter 12: conv=77% loss=-0.2454 [1.9s]
  Iter 13: conv=90% loss=-0.2468 [1.9s]
  Iter 14: conv=95% loss=-0.2478 [1.9s]
  Iter 15: conv=98% loss=-0.2487 [1.9s]
  Iter 16: conv=99% loss=-0.2494 [1.9s]
  Iter 17: conv=99% loss=-0.2502 [1.9s]
  → Converged at iter 17
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.4s]
  ER: train=80.3%, val=79.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,365,440/43,326,720 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=3125.5 val_ppl=719.8 acc=16.0% [6.2s] ★
  Epoch 2: train_ppl=471.0 val_ppl=462.9 acc=17.5% [6.3s] ★
  Epoch 3: train_ppl=293.3 val_ppl=388.5 acc=18.7% [6.3s] ★
  Epoch 4: train_ppl=211.8 val_ppl=354.6 acc=19.3% [6.3s] ★
  Epoch 5: train_ppl=161.3 val_ppl=337.8 acc=19.8% [6.3s] ★
  Epoch 6: train_ppl=128.5 val_ppl=331.5 acc=20.1% [6.2s] ★
  Epoch 7: train_ppl=104.1 val_ppl=333.2 acc=20.3% [6.1s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=331.5, acc=20.1%
  Result: PPL=331.5, Acc=20.1%
    → PPL: 331.5, Acc: 20.1%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 2L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0224 [5.2s]
  Iter 3: conv=0% loss=-0.0975 [4.6s]
  Iter 4: conv=0% loss=-0.1542 [4.5s]
  Iter 5: conv=0% loss=-0.1862 [4.5s]
  Iter 6: conv=0% loss=-0.2084 [4.5s]
  Iter 7: conv=1% loss=-0.2226 [4.6s]
  Iter 8: conv=4% loss=-0.2315 [4.5s]
  Iter 9: conv=10% loss=-0.2371 [4.5s]
  Iter 10: conv=29% loss=-0.2409 [4.5s]
  Iter 11: conv=60% loss=-0.2434 [4.5s]
  Iter 12: conv=83% loss=-0.2451 [4.6s]
  Iter 13: conv=93% loss=-0.2463 [4.5s]
  Iter 14: conv=97% loss=-0.2473 [4.5s]
  Iter 15: conv=99% loss=-0.2481 [4.5s]
  Iter 16: conv=99% loss=-0.2488 [4.5s]
  → Converged at iter 16
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.8s]
  ER: train=80.5%, val=79.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,365,440/43,326,720 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1121.5 val_ppl=397.5 acc=18.2% [14.8s] ★
  Epoch 2: train_ppl=315.2 val_ppl=290.9 acc=20.0% [14.8s] ★
  Epoch 3: train_ppl=216.1 val_ppl=253.1 acc=20.9% [14.7s] ★
  Epoch 4: train_ppl=165.5 val_ppl=234.8 acc=21.6% [14.5s] ★
  Epoch 5: train_ppl=133.7 val_ppl=225.7 acc=22.0% [14.5s] ★
  Epoch 6: train_ppl=111.7 val_ppl=222.2 acc=22.5% [14.6s] ★
  Epoch 7: train_ppl=95.6 val_ppl=222.4 acc=22.6% [14.8s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=222.2, acc=22.5%
  Result: PPL=222.2, Acc=22.5%
    → PPL: 222.2, Acc: 22.5%, ER: 0.0%

======================================================================
Config: 3L_768d_1tok
  num_layers=3, context_dim=768, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 3L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0259 [0.6s]
  Iter 3: conv=0% loss=-0.1242 [0.6s]
  Iter 4: conv=0% loss=-0.1799 [0.6s]
  Iter 5: conv=0% loss=-0.2112 [0.6s]
  Iter 6: conv=2% loss=-0.2295 [0.6s]
  Iter 7: conv=18% loss=-0.2384 [0.6s]
  Iter 8: conv=66% loss=-0.2433 [0.6s]
  Iter 9: conv=96% loss=-0.2460 [0.6s]
  Iter 10: conv=100% loss=-0.2477 [0.6s]
  → Converged at iter 10
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.8s]
  ER: train=79.0%, val=78.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 3,547,392/45,690,624 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=19813.7 val_ppl=2803.9 acc=12.3% [2.0s] ★
  Epoch 2: train_ppl=988.6 val_ppl=1229.5 acc=14.4% [2.0s] ★
  Epoch 3: train_ppl=356.0 val_ppl=871.7 acc=15.5% [2.1s] ★
  Epoch 4: train_ppl=194.3 val_ppl=759.9 acc=16.4% [2.1s] ★
  Epoch 5: train_ppl=124.0 val_ppl=722.5 acc=16.9% [2.1s] ★
  Epoch 6: train_ppl=86.0 val_ppl=722.7 acc=17.1% [2.1s]
  → Early stop at epoch 6
  Best: epoch 5, ppl=722.5, acc=16.9%
  Result: PPL=722.5, Acc=16.9%
    → PPL: 722.5, Acc: 16.9%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 3L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0275 [1.3s]
  Iter 3: conv=0% loss=-0.1256 [1.3s]
  Iter 4: conv=0% loss=-0.1812 [1.2s]
  Iter 5: conv=0% loss=-0.2128 [1.2s]
  Iter 6: conv=1% loss=-0.2313 [1.2s]
  Iter 7: conv=17% loss=-0.2403 [1.2s]
  Iter 8: conv=65% loss=-0.2452 [1.2s]
  Iter 9: conv=96% loss=-0.2480 [1.2s]
  Iter 10: conv=100% loss=-0.2497 [1.2s]
  → Converged at iter 10
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.6s]
  ER: train=79.5%, val=78.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 3,547,392/45,690,624 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=6849.6 val_ppl=1186.9 acc=14.5% [3.6s] ★
  Epoch 2: train_ppl=600.5 val_ppl=664.2 acc=16.5% [3.6s] ★
  Epoch 3: train_ppl=309.2 val_ppl=538.2 acc=17.5% [3.6s] ★
  Epoch 4: train_ppl=193.9 val_ppl=489.2 acc=18.0% [3.7s] ★
  Epoch 5: train_ppl=132.4 val_ppl=480.4 acc=18.4% [3.6s] ★
  Epoch 6: train_ppl=94.5 val_ppl=492.0 acc=18.5% [3.6s]
  → Early stop at epoch 6
  Best: epoch 5, ppl=480.4, acc=18.4%
  Result: PPL=480.4, Acc=18.4%
    → PPL: 480.4, Acc: 18.4%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 3L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0247 [2.4s]
  Iter 3: conv=0% loss=-0.1239 [2.2s]
  Iter 4: conv=0% loss=-0.1782 [2.1s]
  Iter 5: conv=0% loss=-0.2098 [2.2s]
  Iter 6: conv=2% loss=-0.2280 [2.1s]
  Iter 7: conv=15% loss=-0.2371 [2.1s]
  Iter 8: conv=60% loss=-0.2420 [2.1s]
  Iter 9: conv=95% loss=-0.2448 [2.1s]
  Iter 10: conv=100% loss=-0.2466 [2.1s]
  → Converged at iter 10
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.1s]
  ER: train=79.8%, val=78.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 3,547,392/45,690,624 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=2581.7 val_ppl=666.7 acc=15.8% [6.6s] ★
  Epoch 2: train_ppl=414.1 val_ppl=448.8 acc=17.7% [6.6s] ★
  Epoch 3: train_ppl=250.0 val_ppl=381.4 acc=18.6% [6.7s] ★
  Epoch 4: train_ppl=169.4 val_ppl=355.4 acc=19.1% [6.7s] ★
  Epoch 5: train_ppl=123.4 val_ppl=348.6 acc=19.4% [6.6s] ★
  Epoch 6: train_ppl=93.2 val_ppl=353.6 acc=19.6% [6.6s]
  → Early stop at epoch 6
  Best: epoch 5, ppl=348.6, acc=19.4%
  Result: PPL=348.6, Acc=19.4%
    → PPL: 348.6, Acc: 19.4%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 3L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0245 [5.8s]
  Iter 3: conv=0% loss=-0.1224 [5.5s]
  Iter 4: conv=0% loss=-0.1780 [5.3s]
  Iter 5: conv=0% loss=-0.2092 [5.3s]
  Iter 6: conv=1% loss=-0.2277 [5.3s]
  Iter 7: conv=17% loss=-0.2367 [5.3s]
  Iter 8: conv=64% loss=-0.2416 [5.3s]
  Iter 9: conv=96% loss=-0.2444 [5.3s]
  Iter 10: conv=100% loss=-0.2461 [5.3s]
  → Converged at iter 10
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [7.8s]
  ER: train=79.7%, val=78.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 3,547,392/45,690,624 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=984.5 val_ppl=372.5 acc=18.4% [15.7s] ★
  Epoch 2: train_ppl=287.6 val_ppl=280.5 acc=20.3% [15.5s] ★
  Epoch 3: train_ppl=190.8 val_ppl=250.2 acc=21.2% [15.3s] ★
  Epoch 4: train_ppl=140.9 val_ppl=237.8 acc=21.9% [15.2s] ★
  Epoch 5: train_ppl=109.8 val_ppl=236.7 acc=22.0% [15.3s] ★
  Epoch 6: train_ppl=88.3 val_ppl=241.2 acc=22.3% [15.4s]
  → Early stop at epoch 6
  Best: epoch 5, ppl=236.7, acc=22.0%
  Result: PPL=236.7, Acc=22.0%
    → PPL: 236.7, Acc: 22.0%, ER: 0.0%

======================================================================
Config: 1L_768d_2tok
  num_layers=1, context_dim=768, num_input_tokens=2
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0180 [2.5s]
  Iter 3: conv=0% loss=-0.1126 [2.5s]
  Iter 4: conv=0% loss=-0.1608 [2.5s]
  Iter 5: conv=0% loss=-0.1893 [2.5s]
  Iter 6: conv=0% loss=-0.2077 [2.5s]
  Iter 7: conv=1% loss=-0.2201 [2.5s]
  Iter 8: conv=2% loss=-0.2285 [2.6s]
  Iter 9: conv=4% loss=-0.2342 [2.5s]
  Iter 10: conv=6% loss=-0.2384 [2.6s]
  Iter 11: conv=13% loss=-0.2414 [2.6s]
  Iter 12: conv=27% loss=-0.2437 [2.6s]
  Iter 13: conv=51% loss=-0.2454 [2.5s]
  Iter 14: conv=75% loss=-0.2466 [2.6s]
  Iter 15: conv=89% loss=-0.2476 [2.5s]
  Iter 16: conv=95% loss=-0.2485 [2.5s]
  Iter 17: conv=97% loss=-0.2493 [2.5s]
  Iter 18: conv=98% loss=-0.2500 [2.6s]
  Iter 19: conv=99% loss=-0.2507 [2.5s]
  Iter 20: conv=99% loss=-0.2514 [2.5s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.4s]
  ER: train=83.5%, val=83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=33359.8 val_ppl=4041.7 acc=9.0% [1.9s] ★
  Epoch 2: train_ppl=1363.9 val_ppl=1461.1 acc=14.4% [1.9s] ★
  Epoch 3: train_ppl=445.1 val_ppl=969.4 acc=15.8% [1.9s] ★
  Epoch 4: train_ppl=251.8 val_ppl=765.5 acc=16.7% [2.0s] ★
  Epoch 5: train_ppl=165.0 val_ppl=667.6 acc=17.3% [2.0s] ★
  Epoch 6: train_ppl=118.7 val_ppl=614.7 acc=17.6% [2.0s] ★
  Epoch 7: train_ppl=89.3 val_ppl=591.3 acc=18.1% [2.0s] ★
  Epoch 8: train_ppl=67.9 val_ppl=600.0 acc=18.1% [2.0s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=591.3, acc=18.1%
  Result: PPL=591.3, Acc=18.1%
    → PPL: 591.3, Acc: 18.1%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0194 [5.1s]
  Iter 3: conv=0% loss=-0.1135 [5.3s]
  Iter 4: conv=0% loss=-0.1619 [5.2s]
  Iter 5: conv=0% loss=-0.1905 [5.2s]
  Iter 6: conv=0% loss=-0.2092 [5.2s]
  Iter 7: conv=1% loss=-0.2217 [5.2s]
  Iter 8: conv=1% loss=-0.2302 [5.2s]
  Iter 9: conv=2% loss=-0.2359 [5.1s]
  Iter 10: conv=5% loss=-0.2401 [5.2s]
  Iter 11: conv=11% loss=-0.2432 [5.2s]
  Iter 12: conv=25% loss=-0.2455 [5.1s]
  Iter 13: conv=50% loss=-0.2472 [5.1s]
  Iter 14: conv=75% loss=-0.2485 [5.2s]
  Iter 15: conv=89% loss=-0.2495 [5.2s]
  Iter 16: conv=95% loss=-0.2504 [5.2s]
  Iter 17: conv=97% loss=-0.2512 [5.2s]
  Iter 18: conv=98% loss=-0.2519 [5.2s]
  Iter 19: conv=99% loss=-0.2526 [5.1s]
  Iter 20: conv=99% loss=-0.2533 [5.2s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.0s]
  ER: train=83.9%, val=83.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=10312.9 val_ppl=1526.9 acc=13.7% [3.4s] ★
  Epoch 2: train_ppl=694.6 val_ppl=733.0 acc=16.8% [3.4s] ★
  Epoch 3: train_ppl=337.4 val_ppl=531.7 acc=17.9% [3.4s] ★
  Epoch 4: train_ppl=213.0 val_ppl=452.1 acc=18.5% [3.4s] ★
  Epoch 5: train_ppl=153.2 val_ppl=415.9 acc=19.0% [3.4s] ★
  Epoch 6: train_ppl=115.3 val_ppl=402.7 acc=19.5% [3.4s] ★
  Epoch 7: train_ppl=89.6 val_ppl=400.3 acc=19.8% [3.4s] ★
  Epoch 8: train_ppl=71.1 val_ppl=406.0 acc=19.9% [3.4s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=400.3, acc=19.8%
  Result: PPL=400.3, Acc=19.8%
    → PPL: 400.3, Acc: 19.8%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0167 [10.1s]
  Iter 3: conv=0% loss=-0.1108 [10.0s]
  Iter 4: conv=0% loss=-0.1592 [9.9s]
  Iter 5: conv=0% loss=-0.1875 [10.1s]
  Iter 6: conv=0% loss=-0.2060 [10.1s]
  Iter 7: conv=0% loss=-0.2184 [10.0s]
  Iter 8: conv=2% loss=-0.2268 [9.9s]
  Iter 9: conv=2% loss=-0.2325 [10.0s]
  Iter 10: conv=4% loss=-0.2367 [10.1s]
  Iter 11: conv=9% loss=-0.2398 [10.0s]
  Iter 12: conv=18% loss=-0.2421 [10.1s]
  Iter 13: conv=37% loss=-0.2438 [10.0s]
  Iter 14: conv=61% loss=-0.2452 [9.9s]
  Iter 15: conv=79% loss=-0.2463 [10.0s]
  Iter 16: conv=88% loss=-0.2472 [10.0s]
  Iter 17: conv=93% loss=-0.2481 [10.0s]
  Iter 18: conv=95% loss=-0.2488 [10.0s]
  Iter 19: conv=97% loss=-0.2496 [10.0s]
  Iter 20: conv=98% loss=-0.2502 [10.0s]
  Iter 21: conv=98% loss=-0.2509 [10.0s]
  Iter 22: conv=99% loss=-0.2516 [10.0s]
  Iter 23: conv=99% loss=-0.2522 [9.9s]
  Iter 24: conv=99% loss=-0.2528 [10.1s]
  → Converged at iter 24
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [9.6s]
  ER: train=84.0%, val=83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=3520.0 val_ppl=733.2 acc=16.8% [6.2s] ★
  Epoch 2: train_ppl=423.9 val_ppl=434.5 acc=18.6% [6.3s] ★
  Epoch 3: train_ppl=244.7 val_ppl=352.0 acc=19.4% [6.3s] ★
  Epoch 4: train_ppl=171.1 val_ppl=319.3 acc=19.9% [6.2s] ★
  Epoch 5: train_ppl=129.1 val_ppl=303.0 acc=20.4% [6.1s] ★
  Epoch 6: train_ppl=102.1 val_ppl=295.8 acc=20.5% [6.1s] ★
  Epoch 7: train_ppl=83.2 val_ppl=295.1 acc=20.7% [6.1s] ★
  Epoch 8: train_ppl=68.8 val_ppl=298.6 acc=20.8% [6.1s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=295.1, acc=20.7%
  Result: PPL=295.1, Acc=20.7%
    → PPL: 295.1, Acc: 20.7%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0165 [24.3s]
  Iter 3: conv=0% loss=-0.1107 [24.5s]
  Iter 4: conv=0% loss=-0.1588 [24.5s]
  Iter 5: conv=0% loss=-0.1872 [24.3s]
  Iter 6: conv=0% loss=-0.2057 [24.6s]
  Iter 7: conv=1% loss=-0.2182 [24.5s]
  Iter 8: conv=1% loss=-0.2267 [24.5s]
  Iter 9: conv=2% loss=-0.2324 [24.6s]
  Iter 10: conv=5% loss=-0.2366 [24.5s]
  Iter 11: conv=11% loss=-0.2398 [24.5s]
  Iter 12: conv=25% loss=-0.2421 [24.4s]
  Iter 13: conv=49% loss=-0.2437 [24.4s]
  Iter 14: conv=74% loss=-0.2450 [24.4s]
  Iter 15: conv=88% loss=-0.2460 [24.5s]
  Iter 16: conv=94% loss=-0.2469 [24.5s]
  Iter 17: conv=97% loss=-0.2477 [24.5s]
  Iter 18: conv=98% loss=-0.2484 [24.5s]
  Iter 19: conv=99% loss=-0.2491 [24.4s]
  Iter 20: conv=99% loss=-0.2497 [24.5s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [23.5s]
  ER: train=84.0%, val=83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1150.3 val_ppl=364.2 acc=19.1% [14.7s] ★
  Epoch 2: train_ppl=265.6 val_ppl=256.6 acc=20.8% [14.6s] ★
  Epoch 3: train_ppl=176.7 val_ppl=220.2 acc=21.8% [14.4s] ★
  Epoch 4: train_ppl=133.4 val_ppl=202.8 acc=22.4% [14.3s] ★
  Epoch 5: train_ppl=107.5 val_ppl=194.1 acc=22.9% [14.3s] ★
  Epoch 6: train_ppl=90.1 val_ppl=190.3 acc=23.2% [14.4s] ★
  Epoch 7: train_ppl=77.6 val_ppl=189.4 acc=23.4% [14.5s] ★
  Epoch 8: train_ppl=68.0 val_ppl=190.8 acc=23.5% [14.6s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=189.4, acc=23.4%
  Result: PPL=189.4, Acc=23.4%
    → PPL: 189.4, Acc: 23.4%, ER: 0.0%

======================================================================
Config: 2L_768d_2tok
  num_layers=2, context_dim=768, num_input_tokens=2
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0217 [2.6s]
  Iter 3: conv=0% loss=-0.1534 [2.7s]
  Iter 4: conv=0% loss=-0.1995 [2.6s]
  Iter 5: conv=0% loss=-0.2246 [2.6s]
  Iter 6: conv=11% loss=-0.2364 [2.6s]
  Iter 7: conv=84% loss=-0.2421 [2.6s]
  Iter 8: conv=100% loss=-0.2454 [2.7s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.6s]
  ER: train=84.5%, val=84.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=10883.1 val_ppl=1677.0 acc=13.4% [2.1s] ★
  Epoch 2: train_ppl=649.3 val_ppl=869.5 acc=16.1% [2.2s] ★
  Epoch 3: train_ppl=250.7 val_ppl=686.6 acc=17.0% [2.2s] ★
  Epoch 4: train_ppl=134.0 val_ppl=622.3 acc=17.3% [2.2s] ★
  Epoch 5: train_ppl=81.2 val_ppl=629.4 acc=17.7% [2.2s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=622.3, acc=17.3%
  Result: PPL=622.3, Acc=17.3%
    → PPL: 622.3, Acc: 17.3%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0232 [5.3s]
  Iter 3: conv=0% loss=-0.1550 [5.4s]
  Iter 4: conv=0% loss=-0.2011 [5.4s]
  Iter 5: conv=0% loss=-0.2264 [5.4s]
  Iter 6: conv=10% loss=-0.2384 [5.4s]
  Iter 7: conv=82% loss=-0.2441 [5.4s]
  Iter 8: conv=100% loss=-0.2474 [5.4s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.4s]
  ER: train=85.0%, val=84.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=4201.7 val_ppl=867.4 acc=15.6% [3.8s] ★
  Epoch 2: train_ppl=444.1 val_ppl=496.9 acc=17.9% [3.9s] ★
  Epoch 3: train_ppl=214.7 val_ppl=412.6 acc=18.8% [3.9s] ★
  Epoch 4: train_ppl=127.6 val_ppl=391.9 acc=19.2% [3.9s] ★
  Epoch 5: train_ppl=79.5 val_ppl=405.6 acc=19.2% [3.8s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=391.9, acc=19.2%
  Result: PPL=391.9, Acc=19.2%
    → PPL: 391.9, Acc: 19.2%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0204 [10.7s]
  Iter 3: conv=0% loss=-0.1533 [10.8s]
  Iter 4: conv=0% loss=-0.1992 [10.5s]
  Iter 5: conv=0% loss=-0.2243 [10.5s]
  Iter 6: conv=10% loss=-0.2359 [10.6s]
  Iter 7: conv=84% loss=-0.2414 [10.6s]
  Iter 8: conv=100% loss=-0.2446 [10.6s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [10.5s]
  ER: train=85.3%, val=84.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1819.0 val_ppl=507.2 acc=17.3% [7.0s] ★
  Epoch 2: train_ppl=306.2 val_ppl=343.5 acc=19.1% [7.1s] ★
  Epoch 3: train_ppl=171.7 val_ppl=300.7 acc=20.0% [7.1s] ★
  Epoch 4: train_ppl=110.0 val_ppl=284.9 acc=20.5% [6.9s] ★
  Epoch 5: train_ppl=76.6 val_ppl=288.6 acc=20.8% [6.9s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=284.9, acc=20.5%
  Result: PPL=284.9, Acc=20.5%
    → PPL: 284.9, Acc: 20.5%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0201 [25.1s]
  Iter 3: conv=0% loss=-0.1517 [25.0s]
  Iter 4: conv=0% loss=-0.1978 [24.8s]
  Iter 5: conv=0% loss=-0.2228 [24.8s]
  Iter 6: conv=11% loss=-0.2347 [24.7s]
  Iter 7: conv=83% loss=-0.2405 [24.7s]
  Iter 8: conv=100% loss=-0.2438 [24.9s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [25.3s]
  ER: train=85.0%, val=84.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=746.6 val_ppl=294.8 acc=19.4% [16.6s] ★
  Epoch 2: train_ppl=214.9 val_ppl=221.7 acc=21.3% [16.3s] ★
  Epoch 3: train_ppl=134.9 val_ppl=200.9 acc=22.3% [16.1s] ★
  Epoch 4: train_ppl=97.2 val_ppl=197.0 acc=22.8% [16.0s] ★
  Epoch 5: train_ppl=74.9 val_ppl=203.0 acc=22.9% [16.1s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=197.0, acc=22.8%
  Result: PPL=197.0, Acc=22.8%
    → PPL: 197.0, Acc: 22.8%, ER: 0.0%

======================================================================
Config: 3L_768d_2tok
  num_layers=3, context_dim=768, num_input_tokens=2
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0225 [2.8s]
  Iter 3: conv=0% loss=-0.1630 [2.7s]
  Iter 4: conv=0% loss=-0.2079 [2.8s]
  Iter 5: conv=1% loss=-0.2305 [2.7s]
  Iter 6: conv=73% loss=-0.2401 [2.8s]
  Iter 7: conv=100% loss=-0.2447 [2.7s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.8s]
  ER: train=83.6%, val=83.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=16042.6 val_ppl=1677.6 acc=13.6% [2.4s] ★
  Epoch 2: train_ppl=652.8 val_ppl=857.3 acc=15.9% [2.4s] ★
  Epoch 3: train_ppl=216.7 val_ppl=651.1 acc=16.9% [2.4s] ★
  Epoch 4: train_ppl=101.0 val_ppl=661.9 acc=17.5% [2.5s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=651.1, acc=16.9%
  Result: PPL=651.1, Acc=16.9%
    → PPL: 651.1, Acc: 16.9%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0239 [5.6s]
  Iter 3: conv=0% loss=-0.1645 [5.6s]
  Iter 4: conv=0% loss=-0.2097 [5.6s]
  Iter 5: conv=1% loss=-0.2325 [5.5s]
  Iter 6: conv=74% loss=-0.2423 [5.5s]
  Iter 7: conv=100% loss=-0.2467 [5.4s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.6s]
  ER: train=84.1%, val=83.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=5078.8 val_ppl=872.3 acc=15.1% [4.3s] ★
  Epoch 2: train_ppl=414.0 val_ppl=468.1 acc=18.1% [4.3s] ★
  Epoch 3: train_ppl=177.1 val_ppl=400.8 acc=18.8% [4.3s] ★
  Epoch 4: train_ppl=94.6 val_ppl=417.1 acc=18.8% [4.3s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=400.8, acc=18.8%
  Result: PPL=400.8, Acc=18.8%
    → PPL: 400.8, Acc: 18.8%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0211 [10.7s]
  Iter 3: conv=0% loss=-0.1642 [10.9s]
  Iter 4: conv=0% loss=-0.2079 [10.8s]
  Iter 5: conv=1% loss=-0.2300 [10.8s]
  Iter 6: conv=73% loss=-0.2395 [10.8s]
  Iter 7: conv=100% loss=-0.2439 [10.7s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [11.2s]
  ER: train=84.3%, val=83.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1916.0 val_ppl=478.7 acc=16.8% [8.0s] ★
  Epoch 2: train_ppl=273.6 val_ppl=329.2 acc=18.9% [7.9s] ★
  Epoch 3: train_ppl=142.7 val_ppl=293.8 acc=20.2% [7.9s] ★
  Epoch 4: train_ppl=85.9 val_ppl=295.1 acc=20.4% [7.8s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=293.8, acc=20.2%
  Result: PPL=293.8, Acc=20.2%
    → PPL: 293.8, Acc: 20.2%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0209 [26.2s]
  Iter 3: conv=0% loss=-0.1613 [26.0s]
  Iter 4: conv=0% loss=-0.2061 [25.6s]
  Iter 5: conv=1% loss=-0.2287 [25.8s]
  Iter 6: conv=71% loss=-0.2385 [25.7s]
  Iter 7: conv=100% loss=-0.2431 [25.9s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [27.2s]
  ER: train=84.2%, val=83.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=723.6 val_ppl=270.1 acc=19.9% [18.5s] ★
  Epoch 2: train_ppl=194.4 val_ppl=206.7 acc=21.9% [18.2s] ★
  Epoch 3: train_ppl=118.6 val_ppl=194.6 acc=22.7% [18.1s] ★
  Epoch 4: train_ppl=83.3 val_ppl=199.3 acc=23.0% [18.1s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=194.6, acc=22.7%
  Result: PPL=194.6, Acc=22.7%
    → PPL: 194.6, Acc: 22.7%, ER: 0.0%

======================================================================
Config: 1L_768d_3tok
  num_layers=1, context_dim=768, num_input_tokens=3
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0182 [3.7s]
  Iter 3: conv=0% loss=-0.1346 [3.5s]
  Iter 4: conv=0% loss=-0.1835 [3.7s]
  Iter 5: conv=0% loss=-0.2091 [3.7s]
  Iter 6: conv=1% loss=-0.2240 [3.8s]
  Iter 7: conv=3% loss=-0.2331 [3.6s]
  Iter 8: conv=10% loss=-0.2387 [3.6s]
  Iter 9: conv=53% loss=-0.2423 [3.7s]
  Iter 10: conv=89% loss=-0.2447 [3.7s]
  Iter 11: conv=98% loss=-0.2463 [3.7s]
  Iter 12: conv=100% loss=-0.2475 [3.7s]
  → Converged at iter 12
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.7s]
  ER: train=86.5%, val=86.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 4,133,376/45,092,352 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=29904.0 val_ppl=3418.1 acc=10.2% [2.0s] ★
  Epoch 2: train_ppl=1289.3 val_ppl=1347.6 acc=14.0% [2.0s] ★
  Epoch 3: train_ppl=415.4 val_ppl=907.8 acc=15.5% [2.0s] ★
  Epoch 4: train_ppl=226.5 val_ppl=728.8 acc=16.4% [2.0s] ★
  Epoch 5: train_ppl=145.2 val_ppl=641.6 acc=17.0% [2.0s] ★
  Epoch 6: train_ppl=104.0 val_ppl=611.3 acc=17.6% [2.0s] ★
  Epoch 7: train_ppl=70.5 val_ppl=614.4 acc=17.4% [2.0s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=611.3, acc=17.6%
  Result: PPL=611.3, Acc=17.6%
    → PPL: 611.3, Acc: 17.6%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0197 [7.4s]
  Iter 3: conv=0% loss=-0.1357 [7.3s]
  Iter 4: conv=0% loss=-0.1848 [7.3s]
  Iter 5: conv=0% loss=-0.2105 [7.3s]
  Iter 6: conv=1% loss=-0.2256 [7.3s]
  Iter 7: conv=2% loss=-0.2349 [7.5s]
  Iter 8: conv=8% loss=-0.2405 [7.3s]
  Iter 9: conv=52% loss=-0.2441 [7.3s]
  Iter 10: conv=88% loss=-0.2465 [7.3s]
  Iter 11: conv=98% loss=-0.2482 [7.3s]
  Iter 12: conv=100% loss=-0.2494 [7.4s]
  → Converged at iter 12
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.2s]
  ER: train=86.8%, val=86.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 4,133,376/45,092,352 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=9367.4 val_ppl=1348.4 acc=14.0% [3.5s] ★
  Epoch 2: train_ppl=658.7 val_ppl=685.4 acc=16.6% [3.5s] ★
  Epoch 3: train_ppl=315.0 val_ppl=513.9 acc=17.7% [3.5s] ★
  Epoch 4: train_ppl=193.3 val_ppl=441.1 acc=18.4% [3.5s] ★
  Epoch 5: train_ppl=129.7 val_ppl=414.5 acc=18.8% [3.5s] ★
  Epoch 6: train_ppl=92.4 val_ppl=414.1 acc=19.0% [3.5s] ★
  Epoch 7: train_ppl=67.5 val_ppl=429.4 acc=19.0% [3.5s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=414.1, acc=19.0%
  Result: PPL=414.1, Acc=19.0%
    → PPL: 414.1, Acc: 19.0%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0170 [14.2s]
  Iter 3: conv=0% loss=-0.1333 [14.1s]
  Iter 4: conv=0% loss=-0.1821 [14.0s]
  Iter 5: conv=0% loss=-0.2073 [14.0s]
  Iter 6: conv=0% loss=-0.2223 [14.2s]
  Iter 7: conv=1% loss=-0.2316 [14.1s]
  Iter 8: conv=7% loss=-0.2372 [14.1s]
  Iter 9: conv=44% loss=-0.2409 [14.1s]
  Iter 10: conv=82% loss=-0.2433 [14.1s]
  Iter 11: conv=96% loss=-0.2450 [14.1s]
  Iter 12: conv=100% loss=-0.2462 [14.2s]
  → Converged at iter 12
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [10.1s]
  ER: train=87.1%, val=86.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 4,133,376/45,092,352 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=3233.3 val_ppl=690.0 acc=16.4% [6.4s] ★
  Epoch 2: train_ppl=411.6 val_ppl=424.4 acc=18.2% [6.5s] ★
  Epoch 3: train_ppl=228.7 val_ppl=344.9 acc=19.3% [6.5s] ★
  Epoch 4: train_ppl=153.3 val_ppl=312.8 acc=19.9% [6.4s] ★
  Epoch 5: train_ppl=109.5 val_ppl=301.6 acc=20.2% [6.3s] ★
  Epoch 6: train_ppl=82.6 val_ppl=302.5 acc=20.5% [6.3s]
  → Early stop at epoch 6
  Best: epoch 5, ppl=301.6, acc=20.2%
  Result: PPL=301.6, Acc=20.2%
    → PPL: 301.6, Acc: 20.2%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0168 [34.8s]
  Iter 3: conv=0% loss=-0.1329 [34.6s]
  Iter 4: conv=0% loss=-0.1816 [34.6s]
  Iter 5: conv=0% loss=-0.2070 [34.5s]
  Iter 6: conv=1% loss=-0.2220 [34.5s]
  Iter 7: conv=2% loss=-0.2313 [34.5s]
  Iter 8: conv=8% loss=-0.2370 [34.5s]
  Iter 9: conv=50% loss=-0.2406 [34.7s]
  Iter 10: conv=88% loss=-0.2430 [34.6s]
  Iter 11: conv=98% loss=-0.2447 [34.8s]
  Iter 12: conv=100% loss=-0.2459 [34.6s]
  → Converged at iter 12
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [24.7s]
  ER: train=87.1%, val=86.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 4,133,376/45,092,352 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1100.6 val_ppl=351.7 acc=19.0% [15.3s] ★
  Epoch 2: train_ppl=257.3 val_ppl=249.2 acc=20.8% [15.0s] ★
  Epoch 3: train_ppl=164.0 val_ppl=216.1 acc=21.8% [14.8s] ★
  Epoch 4: train_ppl=118.9 val_ppl=201.5 acc=22.5% [14.8s] ★
  Epoch 5: train_ppl=92.6 val_ppl=196.0 acc=22.8% [14.8s] ★
  Epoch 6: train_ppl=75.1 val_ppl=195.9 acc=22.9% [14.8s] ★
  Epoch 7: train_ppl=62.7 val_ppl=199.7 acc=22.9% [15.0s]
  → Early stop at epoch 7
  Best: epoch 6, ppl=195.9, acc=22.9%
  Result: PPL=195.9, Acc=22.9%
    → PPL: 195.9, Acc: 22.9%, ER: 0.0%

======================================================================
Config: 2L_768d_3tok
  num_layers=2, context_dim=768, num_input_tokens=3
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 2L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0205 [3.8s]
  Iter 3: conv=0% loss=-0.1675 [3.8s]
  Iter 4: conv=0% loss=-0.2121 [3.8s]
  Iter 5: conv=2% loss=-0.2319 [3.8s]
  Iter 6: conv=88% loss=-0.2407 [3.8s]
  Iter 7: conv=100% loss=-0.2448 [3.9s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.9s]
  ER: train=86.8%, val=86.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 11,217,408/54,537,984 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=12436.2 val_ppl=1709.0 acc=13.4% [2.4s] ★
  Epoch 2: train_ppl=684.6 val_ppl=861.2 acc=15.7% [2.4s] ★
  Epoch 3: train_ppl=250.9 val_ppl=653.9 acc=16.7% [2.4s] ★
  Epoch 4: train_ppl=112.2 val_ppl=608.7 acc=17.3% [2.4s] ★
  Epoch 5: train_ppl=57.3 val_ppl=685.5 acc=17.2% [2.4s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=608.7, acc=17.3%
  Result: PPL=608.7, Acc=17.3%
    → PPL: 608.7, Acc: 17.3%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 2L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0220 [7.7s]
  Iter 3: conv=0% loss=-0.1697 [7.6s]
  Iter 4: conv=0% loss=-0.2138 [7.7s]
  Iter 5: conv=3% loss=-0.2341 [7.6s]
  Iter 6: conv=86% loss=-0.2428 [7.5s]
  Iter 7: conv=100% loss=-0.2469 [7.5s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.7s]
  ER: train=87.2%, val=86.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 11,217,408/54,537,984 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=4508.8 val_ppl=833.5 acc=15.5% [4.1s] ★
  Epoch 2: train_ppl=426.9 val_ppl=481.0 acc=17.6% [4.2s] ★
  Epoch 3: train_ppl=186.9 val_ppl=402.1 acc=18.6% [4.3s] ★
  Epoch 4: train_ppl=95.0 val_ppl=408.8 acc=18.8% [4.2s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=402.1, acc=18.6%
  Result: PPL=402.1, Acc=18.6%
    → PPL: 402.1, Acc: 18.6%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 2L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0191 [14.9s]
  Iter 3: conv=0% loss=-0.1682 [14.8s]
  Iter 4: conv=0% loss=-0.2119 [14.7s]
  Iter 5: conv=2% loss=-0.2316 [14.6s]
  Iter 6: conv=87% loss=-0.2400 [14.7s]
  Iter 7: conv=100% loss=-0.2440 [14.7s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [11.0s]
  ER: train=87.3%, val=87.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 11,217,408/54,537,984 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1825.7 val_ppl=481.5 acc=17.3% [7.7s] ★
  Epoch 2: train_ppl=282.3 val_ppl=322.4 acc=19.5% [7.8s] ★
  Epoch 3: train_ppl=142.9 val_ppl=285.7 acc=20.3% [7.7s] ★
  Epoch 4: train_ppl=81.9 val_ppl=288.7 acc=20.7% [7.7s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=285.7, acc=20.3%
  Result: PPL=285.7, Acc=20.3%
    → PPL: 285.7, Acc: 20.3%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 2L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0189 [35.9s]
  Iter 3: conv=0% loss=-0.1665 [35.3s]
  Iter 4: conv=0% loss=-0.2106 [35.3s]
  Iter 5: conv=3% loss=-0.2304 [35.3s]
  Iter 6: conv=87% loss=-0.2392 [35.3s]
  Iter 7: conv=100% loss=-0.2433 [35.2s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [27.0s]
  ER: train=87.2%, val=86.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 11,217,408/54,537,984 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=731.5 val_ppl=280.2 acc=19.8% [18.3s] ★
  Epoch 2: train_ppl=195.0 val_ppl=212.4 acc=21.8% [18.0s] ★
  Epoch 3: train_ppl=113.5 val_ppl=198.1 acc=22.6% [17.8s] ★
  Epoch 4: train_ppl=76.1 val_ppl=205.6 acc=22.7% [17.7s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=198.1, acc=22.6%
  Result: PPL=198.1, Acc=22.6%
    → PPL: 198.1, Acc: 22.6%, ER: 0.0%

======================================================================
Config: 3L_768d_3tok
  num_layers=3, context_dim=768, num_input_tokens=3
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 3L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0182 [3.8s]
  Iter 3: conv=0% loss=-0.1667 [3.8s]
  Iter 4: conv=0% loss=-0.2113 [3.8s]
  Iter 5: conv=10% loss=-0.2325 [3.8s]
  Iter 6: conv=99% loss=-0.2413 [3.8s]
  → Converged at iter 6
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.0s]
  ER: train=85.7%, val=86.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,777,152/63,459,328 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=19525.4 val_ppl=1849.3 acc=13.1% [2.8s] ★
  Epoch 2: train_ppl=750.2 val_ppl=891.2 acc=15.8% [2.8s] ★
  Epoch 3: train_ppl=207.5 val_ppl=650.7 acc=17.0% [2.8s] ★
  Epoch 4: train_ppl=80.8 val_ppl=726.0 acc=17.4% [2.8s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=650.7, acc=17.0%
  Result: PPL=650.7, Acc=17.0%
    → PPL: 650.7, Acc: 17.0%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 3L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0195 [7.8s]
  Iter 3: conv=0% loss=-0.1688 [7.7s]
  Iter 4: conv=0% loss=-0.2133 [7.7s]
  Iter 5: conv=10% loss=-0.2345 [7.6s]
  Iter 6: conv=98% loss=-0.2433 [7.6s]
  Iter 7: conv=100% loss=-0.2475 [7.6s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [6.0s]
  ER: train=86.6%, val=86.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,777,152/63,459,328 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=5729.0 val_ppl=863.0 acc=14.9% [5.0s] ★
  Epoch 2: train_ppl=401.0 val_ppl=461.5 acc=17.7% [5.0s] ★
  Epoch 3: train_ppl=145.5 val_ppl=412.0 acc=18.9% [5.0s] ★
  Epoch 4: train_ppl=65.5 val_ppl=447.6 acc=19.0% [5.0s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=412.0, acc=18.9%
  Result: PPL=412.0, Acc=18.9%
    → PPL: 412.0, Acc: 18.9%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 3L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0168 [15.0s]
  Iter 3: conv=0% loss=-0.1686 [15.3s]
  Iter 4: conv=0% loss=-0.2114 [15.2s]
  Iter 5: conv=10% loss=-0.2319 [15.2s]
  Iter 6: conv=99% loss=-0.2406 [15.1s]
  Iter 7: conv=100% loss=-0.2447 [15.2s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [11.9s]
  ER: train=86.8%, val=86.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,777,152/63,459,328 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=2045.6 val_ppl=477.1 acc=17.4% [9.2s] ★
  Epoch 2: train_ppl=249.6 val_ppl=308.6 acc=19.2% [9.2s] ★
  Epoch 3: train_ppl=113.8 val_ppl=297.0 acc=20.6% [9.1s] ★
  Epoch 4: train_ppl=60.1 val_ppl=331.1 acc=20.8% [9.0s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=297.0, acc=20.6%
  Result: PPL=297.0, Acc=20.6%
    → PPL: 297.0, Acc: 20.6%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 3L, 768d, 3tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 3
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0166 [36.7s]
  Iter 3: conv=0% loss=-0.1656 [36.4s]
  Iter 4: conv=0% loss=-0.2098 [36.5s]
  Iter 5: conv=10% loss=-0.2311 [36.3s]
  Iter 6: conv=99% loss=-0.2398 [36.3s]
  Iter 7: conv=100% loss=-0.2439 [36.3s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [29.0s]
  ER: train=86.6%, val=86.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 17,777,152/63,459,328 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=720.0 val_ppl=255.2 acc=20.4% [21.7s] ★
  Epoch 2: train_ppl=175.0 val_ppl=199.7 acc=22.1% [21.2s] ★
  Epoch 3: train_ppl=97.7 val_ppl=193.7 acc=23.0% [20.9s] ★
  Epoch 4: train_ppl=62.8 val_ppl=207.2 acc=23.1% [21.1s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=193.7, acc=23.0%
  Result: PPL=193.7, Acc=23.0%
    → PPL: 193.7, Acc: 23.0%, ER: 0.0%

======================================================================
RESULTS SUMMARY
======================================================================
Config                           α            A       R²   Best PPL   Best Acc
----------------------------------------------------------------------
1L_768d_1tok               -0.5460     2.67e+05   0.9842      198.2     22.6%
2L_768d_1tok               -0.4734     1.18e+05   0.9969      222.2     22.5%
3L_768d_1tok               -0.4948     1.65e+05   0.9933      236.7     22.0%
1L_768d_2tok               -0.5035     1.51e+05   0.9978      189.4     23.4%
2L_768d_2tok               -0.5070     1.59e+05   0.9843      197.0     22.8%
3L_768d_2tok               -0.5300     2.14e+05   0.9864      194.6     22.7%
1L_768d_3tok               -0.5046     1.58e+05   0.9977      195.9     22.9%
2L_768d_3tok               -0.4994     1.45e+05   0.9904      198.1     22.6%
3L_768d_3tok               -0.5346     2.28e+05   0.9920      193.7     23.0%

Results saved to: results/scaling_20251130_031816

Total time: 96.9 min
