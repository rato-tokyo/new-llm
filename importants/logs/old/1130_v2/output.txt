remote: Enumerating objects: 26, done.
remote: Counting objects: 100% (26/26), done.
remote: Compressing objects: 100% (14/14), done.
remote: Total 21 (delta 9), reused 19 (delta 7), pack-reused 0 (from 0)
Unpacking objects: 100% (21/21), 213.52 KiB | 477.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
   54fed8c..77dde8a  main       -> origin/main
Updating 54fed8c..77dde8a
Fast-forward
 config.py                                          |   8 +-
 importants/context_dim_analysis.png                | Bin 0 -> 105232 bytes
 importants/context_dim_scaling.png                 | Bin 0 -> 127846 bytes
 .../experiment-results-20251130-context-dim.md     | 193 ++++++
 .../logs/1130_context_dim/detailed_results.json    | 114 ++++
 importants/logs/1130_context_dim/metadata.json     |  22 +
 importants/logs/1130_context_dim/output.txt        | 742 +++++++++++++++++++++
 importants/logs/1130_context_dim/summary.json      |  30 +
 scripts/plot_context_dim_scaling.py                | 294 ++++++++
 9 files changed, 1399 insertions(+), 4 deletions(-)
 create mode 100644 importants/context_dim_analysis.png
 create mode 100644 importants/context_dim_scaling.png
 create mode 100644 importants/experiment-results-20251130-context-dim.md
 create mode 100644 importants/logs/1130_context_dim/detailed_results.json
 create mode 100644 importants/logs/1130_context_dim/metadata.json
 create mode 100644 importants/logs/1130_context_dim/output.txt
 create mode 100644 importants/logs/1130_context_dim/summary.json
 create mode 100644 scripts/plot_context_dim_scaling.py
======================================================================
SCALING LAW EXPERIMENT
======================================================================
Configurations: 3
Sample sizes: [50, 100, 200, 500]
Total experiments: 12
Output: results/scaling_20251130_063415
Device: cuda (NVIDIA L4, 23.8GB)

Configurations:
  1. 1L_768d_1tok
  2. 1L_1200d_1tok
  3. 1L_1537d_1tok
Device: cuda (NVIDIA L4, 22.2GB)

======================================================================
Config: 1L_768d_1tok
  num_layers=1, context_dim=768, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
2025-11-30 06:34:17.580452: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-30 06:34:17.596091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764484457.617926   54929 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764484457.624569   54929 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764484457.641255   54929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764484457.641288   54929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764484457.641291   54929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764484457.641293   54929 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-30 06:34:17.646147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1474 [0.9s]
  Iter 3: conv=0% loss=-0.1696 [0.5s]
  Iter 4: conv=0% loss=-0.1968 [0.5s]
  Iter 5: conv=0% loss=-0.2165 [0.4s]
  Iter 6: conv=0% loss=-0.2309 [0.5s]
  Iter 7: conv=1% loss=-0.2407 [0.4s]
  Iter 8: conv=1% loss=-0.2477 [0.5s]
  Iter 9: conv=2% loss=-0.2533 [0.5s]
  Iter 10: conv=3% loss=-0.2585 [0.5s]
  Iter 11: conv=3% loss=-0.2634 [0.5s]
  Iter 12: conv=4% loss=-0.2680 [0.4s]
  Iter 13: conv=5% loss=-0.2720 [0.5s]
  Iter 14: conv=7% loss=-0.2752 [0.5s]
  Iter 15: conv=11% loss=-0.2778 [0.4s]
  Iter 16: conv=20% loss=-0.2799 [0.5s]
  Iter 17: conv=34% loss=-0.2818 [0.5s]
  Iter 18: conv=54% loss=-0.2835 [0.5s]
  Iter 19: conv=72% loss=-0.2850 [0.5s]
  Iter 20: conv=86% loss=-0.2864 [0.5s]
  Iter 21: conv=94% loss=-0.2875 [0.4s]
  Iter 22: conv=98% loss=-0.2884 [0.5s]
  Iter 23: conv=99% loss=-0.2893 [0.5s]
  → Converged at iter 23
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.4s]
  ER: train=79.7%, val=79.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=16560864.0 val_ppl=59385.9 acc=7.0% [1.8s] ★
  Epoch 2: train_ppl=13253.8 val_ppl=5613.1 acc=9.4% [1.8s] ★
  Epoch 3: train_ppl=2469.1 val_ppl=2370.9 acc=13.5% [1.8s] ★
  Epoch 4: train_ppl=1013.2 val_ppl=1545.6 acc=14.4% [1.8s] ★
  Epoch 5: train_ppl=580.0 val_ppl=1192.6 acc=15.4% [1.8s] ★
  Epoch 6: train_ppl=403.1 val_ppl=1001.5 acc=15.9% [1.8s] ★
  Epoch 7: train_ppl=303.7 val_ppl=877.6 acc=16.4% [1.8s] ★
  Epoch 8: train_ppl=239.8 val_ppl=791.1 acc=16.8% [1.8s] ★
  Epoch 9: train_ppl=195.6 val_ppl=729.0 acc=17.1% [1.8s] ★
  Epoch 10: train_ppl=163.8 val_ppl=683.1 acc=17.3% [1.8s] ★
  Epoch 11: train_ppl=139.4 val_ppl=649.1 acc=17.6% [1.9s] ★
  Epoch 12: train_ppl=120.5 val_ppl=623.9 acc=17.8% [1.9s] ★
  Epoch 13: train_ppl=105.2 val_ppl=605.3 acc=17.9% [1.9s] ★
  Epoch 14: train_ppl=92.7 val_ppl=591.9 acc=18.0% [1.9s] ★
  Epoch 15: train_ppl=82.2 val_ppl=582.9 acc=18.2% [1.9s] ★
  Epoch 16: train_ppl=73.4 val_ppl=577.9 acc=18.3% [1.9s] ★
  Epoch 17: train_ppl=65.9 val_ppl=576.0 acc=18.3% [1.9s] ★
  Epoch 18: train_ppl=59.3 val_ppl=577.3 acc=18.3% [1.9s]
  → Early stop at epoch 18
  Best: epoch 17, ppl=576.0, acc=18.3%
  Result: PPL=576.0, Acc=18.3%
    → PPL: 576.0, Acc: 18.3%, ER: 79.3%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1490 [0.9s]
  Iter 3: conv=0% loss=-0.1706 [0.9s]
  Iter 4: conv=0% loss=-0.1979 [0.9s]
  Iter 5: conv=0% loss=-0.2178 [0.9s]
  Iter 6: conv=0% loss=-0.2323 [0.9s]
  Iter 7: conv=1% loss=-0.2422 [0.9s]
  Iter 8: conv=1% loss=-0.2493 [0.9s]
  Iter 9: conv=1% loss=-0.2549 [0.9s]
  Iter 10: conv=2% loss=-0.2601 [0.9s]
  Iter 11: conv=2% loss=-0.2651 [0.9s]
  Iter 12: conv=2% loss=-0.2698 [0.9s]
  Iter 13: conv=3% loss=-0.2738 [0.9s]
  Iter 14: conv=5% loss=-0.2771 [0.9s]
  Iter 15: conv=9% loss=-0.2797 [0.9s]
  Iter 16: conv=18% loss=-0.2819 [0.9s]
  Iter 17: conv=33% loss=-0.2838 [0.9s]
  Iter 18: conv=53% loss=-0.2855 [0.9s]
  Iter 19: conv=72% loss=-0.2871 [0.9s]
  Iter 20: conv=86% loss=-0.2884 [0.9s]
  Iter 21: conv=94% loss=-0.2895 [0.9s]
  Iter 22: conv=98% loss=-0.2904 [0.9s]
  Iter 23: conv=99% loss=-0.2913 [0.9s]
  → Converged at iter 23
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.8s]
  ER: train=79.9%, val=79.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=633403.4 val_ppl=6084.5 acc=9.4% [3.2s] ★
  Epoch 2: train_ppl=2430.1 val_ppl=1549.6 acc=14.4% [3.3s] ★
  Epoch 3: train_ppl=910.0 val_ppl=961.9 acc=16.0% [3.3s] ★
  Epoch 4: train_ppl=544.9 val_ppl=734.7 acc=17.0% [3.3s] ★
  Epoch 5: train_ppl=383.3 val_ppl=613.0 acc=17.6% [3.3s] ★
  Epoch 6: train_ppl=293.3 val_ppl=537.8 acc=18.1% [3.3s] ★
  Epoch 7: train_ppl=236.6 val_ppl=487.8 acc=18.4% [3.3s] ★
  Epoch 8: train_ppl=196.8 val_ppl=453.8 acc=18.6% [3.3s] ★
  Epoch 9: train_ppl=167.8 val_ppl=429.9 acc=19.0% [3.2s] ★
  Epoch 10: train_ppl=145.6 val_ppl=412.7 acc=19.2% [3.3s] ★
  Epoch 11: train_ppl=127.9 val_ppl=400.8 acc=19.4% [3.2s] ★
  Epoch 12: train_ppl=113.5 val_ppl=393.0 acc=19.6% [3.2s] ★
  Epoch 13: train_ppl=101.5 val_ppl=388.3 acc=19.6% [3.2s] ★
  Epoch 14: train_ppl=91.3 val_ppl=386.5 acc=19.7% [3.2s] ★
  Epoch 15: train_ppl=82.7 val_ppl=386.7 acc=19.8% [3.2s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=386.5, acc=19.7%
  Result: PPL=386.5, Acc=19.7%
    → PPL: 386.5, Acc: 19.7%, ER: 79.2%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1459 [1.7s]
  Iter 3: conv=0% loss=-0.1673 [1.5s]
  Iter 4: conv=0% loss=-0.1944 [1.5s]
  Iter 5: conv=0% loss=-0.2138 [1.6s]
  Iter 6: conv=0% loss=-0.2279 [1.5s]
  Iter 7: conv=0% loss=-0.2376 [1.5s]
  Iter 8: conv=1% loss=-0.2446 [1.6s]
  Iter 9: conv=1% loss=-0.2502 [1.5s]
  Iter 10: conv=2% loss=-0.2553 [1.6s]
  Iter 11: conv=2% loss=-0.2602 [1.6s]
  Iter 12: conv=3% loss=-0.2648 [1.5s]
  Iter 13: conv=3% loss=-0.2689 [1.6s]
  Iter 14: conv=5% loss=-0.2722 [1.5s]
  Iter 15: conv=8% loss=-0.2749 [1.5s]
  Iter 16: conv=13% loss=-0.2771 [1.5s]
  Iter 17: conv=22% loss=-0.2790 [1.5s]
  Iter 18: conv=36% loss=-0.2808 [1.5s]
  Iter 19: conv=52% loss=-0.2826 [1.5s]
  Iter 20: conv=69% loss=-0.2841 [1.5s]
  Iter 21: conv=82% loss=-0.2854 [1.5s]
  Iter 22: conv=91% loss=-0.2865 [1.6s]
  Iter 23: conv=96% loss=-0.2875 [1.5s]
  Iter 24: conv=99% loss=-0.2885 [1.6s]
  Iter 25: conv=100% loss=-0.2894 [1.5s]
  → Converged at iter 25
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.5s]
  ER: train=80.0%, val=79.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=53247.3 val_ppl=1637.8 acc=13.8% [5.9s] ★
  Epoch 2: train_ppl=978.9 val_ppl=750.3 acc=16.5% [6.0s] ★
  Epoch 3: train_ppl=514.0 val_ppl=532.7 acc=17.7% [6.1s] ★
  Epoch 4: train_ppl=348.5 val_ppl=436.6 acc=18.3% [6.0s] ★
  Epoch 5: train_ppl=267.1 val_ppl=382.4 acc=18.9% [6.0s] ★
  Epoch 6: train_ppl=218.4 val_ppl=348.8 acc=19.3% [6.0s] ★
  Epoch 7: train_ppl=184.3 val_ppl=326.6 acc=19.6% [6.0s] ★
  Epoch 8: train_ppl=159.9 val_ppl=311.4 acc=19.9% [5.9s] ★
  Epoch 9: train_ppl=140.7 val_ppl=301.2 acc=20.0% [5.9s] ★
  Epoch 10: train_ppl=125.5 val_ppl=294.4 acc=20.1% [5.9s] ★
  Epoch 11: train_ppl=113.1 val_ppl=289.6 acc=20.3% [5.8s] ★
  Epoch 12: train_ppl=102.6 val_ppl=287.3 acc=20.4% [5.8s] ★
  Epoch 13: train_ppl=94.0 val_ppl=286.4 acc=20.4% [5.8s] ★
  Epoch 14: train_ppl=86.3 val_ppl=287.1 acc=20.6% [5.9s]
  → Early stop at epoch 14
  Best: epoch 13, ppl=286.4, acc=20.4%
  Result: PPL=286.4, Acc=20.4%
    → PPL: 286.4, Acc: 20.4%, ER: 79.1%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 768d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1456 [4.0s]
  Iter 3: conv=0% loss=-0.1674 [3.8s]
  Iter 4: conv=0% loss=-0.1945 [3.8s]
  Iter 5: conv=0% loss=-0.2143 [3.8s]
  Iter 6: conv=0% loss=-0.2287 [3.8s]
  Iter 7: conv=1% loss=-0.2385 [3.8s]
  Iter 8: conv=1% loss=-0.2456 [3.8s]
  Iter 9: conv=1% loss=-0.2512 [3.8s]
  Iter 10: conv=2% loss=-0.2564 [3.8s]
  Iter 11: conv=2% loss=-0.2613 [3.8s]
  Iter 12: conv=3% loss=-0.2660 [3.8s]
  Iter 13: conv=3% loss=-0.2700 [3.8s]
  Iter 14: conv=5% loss=-0.2733 [3.8s]
  Iter 15: conv=10% loss=-0.2759 [3.8s]
  Iter 16: conv=18% loss=-0.2781 [3.8s]
  Iter 17: conv=33% loss=-0.2799 [3.8s]
  Iter 18: conv=53% loss=-0.2816 [3.8s]
  Iter 19: conv=72% loss=-0.2832 [3.8s]
  Iter 20: conv=86% loss=-0.2845 [3.8s]
  Iter 21: conv=94% loss=-0.2856 [3.8s]
  Iter 22: conv=98% loss=-0.2865 [3.8s]
  Iter 23: conv=99% loss=-0.2874 [3.8s]
  → Converged at iter 23
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [3.3s]
  ER: train=80.3%, val=79.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,183,488/40,962,816 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=4885.8 val_ppl=594.5 acc=17.5% [14.2s] ★
  Epoch 2: train_ppl=442.2 val_ppl=358.7 acc=19.2% [14.1s] ★
  Epoch 3: train_ppl=285.6 val_ppl=287.3 acc=20.1% [13.9s] ★
  Epoch 4: train_ppl=220.5 val_ppl=252.5 acc=20.6% [13.8s] ★
  Epoch 5: train_ppl=182.9 val_ppl=232.0 acc=21.0% [13.7s] ★
  Epoch 6: train_ppl=157.9 val_ppl=218.8 acc=21.5% [13.8s] ★
  Epoch 7: train_ppl=139.9 val_ppl=210.0 acc=21.8% [13.9s] ★
  Epoch 8: train_ppl=126.3 val_ppl=204.2 acc=22.1% [14.0s] ★
  Epoch 9: train_ppl=115.5 val_ppl=200.4 acc=22.4% [14.0s] ★
  Epoch 10: train_ppl=106.8 val_ppl=198.1 acc=22.5% [13.9s] ★
  Epoch 11: train_ppl=99.5 val_ppl=196.8 acc=22.7% [13.9s] ★
  Epoch 12: train_ppl=93.4 val_ppl=196.3 acc=22.8% [13.8s] ★
  Epoch 13: train_ppl=88.2 val_ppl=196.4 acc=22.9% [13.8s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=196.3, acc=22.8%
  Result: PPL=196.3, Acc=22.8%
    → PPL: 196.3, Acc: 22.8%, ER: 79.2%

======================================================================
Config: 1L_1200d_1tok
  num_layers=1, context_dim=1200, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 1200d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2061 [0.6s]
  Iter 3: conv=0% loss=-0.2164 [0.5s]
  Iter 4: conv=0% loss=-0.2449 [0.6s]
  Iter 5: conv=0% loss=-0.2661 [0.5s]
  Iter 6: conv=0% loss=-0.2806 [0.6s]
  Iter 7: conv=0% loss=-0.2906 [0.5s]
  Iter 8: conv=1% loss=-0.2983 [0.6s]
  Iter 9: conv=1% loss=-0.3049 [0.5s]
  Iter 10: conv=1% loss=-0.3112 [0.6s]
  Iter 11: conv=2% loss=-0.3175 [0.5s]
  Iter 12: conv=2% loss=-0.3236 [0.6s]
  Iter 13: conv=3% loss=-0.3291 [0.5s]
  Iter 14: conv=3% loss=-0.3340 [0.6s]
  Iter 15: conv=4% loss=-0.3382 [0.5s]
  Iter 16: conv=5% loss=-0.3419 [0.7s]
  Iter 17: conv=6% loss=-0.3454 [0.7s]
  Iter 18: conv=9% loss=-0.3485 [0.5s]
  Iter 19: conv=13% loss=-0.3514 [0.6s]
  Iter 20: conv=17% loss=-0.3540 [0.5s]
  Iter 21: conv=25% loss=-0.3563 [0.6s]
  Iter 22: conv=36% loss=-0.3583 [0.5s]
  Iter 23: conv=51% loss=-0.3600 [0.6s]
  Iter 24: conv=68% loss=-0.3614 [0.7s]
  Iter 25: conv=82% loss=-0.3627 [0.5s]
  Iter 26: conv=91% loss=-0.3640 [0.6s]
  Iter 27: conv=96% loss=-0.3652 [0.5s]
  Iter 28: conv=98% loss=-0.3664 [0.6s]
  Iter 29: conv=99% loss=-0.3676 [0.5s]
  → Converged at iter 29
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.5s]
  ER: train=76.2%, val=75.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,515,264/42,477,840 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3225953.5 val_ppl=29275.1 acc=8.2% [1.9s] ★
  Epoch 2: train_ppl=7420.1 val_ppl=4165.2 acc=9.6% [1.9s] ★
  Epoch 3: train_ppl=1574.8 val_ppl=1955.0 acc=13.4% [1.9s] ★
  Epoch 4: train_ppl=762.8 val_ppl=1363.8 acc=14.2% [1.9s] ★
  Epoch 5: train_ppl=513.0 val_ppl=1085.9 acc=15.3% [1.9s] ★
  Epoch 6: train_ppl=350.3 val_ppl=920.9 acc=15.7% [1.9s] ★
  Epoch 7: train_ppl=272.3 val_ppl=819.9 acc=16.4% [1.9s] ★
  Epoch 8: train_ppl=212.9 val_ppl=746.3 acc=16.8% [1.9s] ★
  Epoch 9: train_ppl=177.1 val_ppl=698.2 acc=17.0% [1.9s] ★
  Epoch 10: train_ppl=146.7 val_ppl=660.4 acc=17.2% [1.9s] ★
  Epoch 11: train_ppl=126.2 val_ppl=637.4 acc=17.5% [1.9s] ★
  Epoch 12: train_ppl=107.4 val_ppl=618.4 acc=17.7% [1.9s] ★
  Epoch 13: train_ppl=94.1 val_ppl=609.1 acc=17.9% [1.9s] ★
  Epoch 14: train_ppl=81.4 val_ppl=601.4 acc=17.9% [1.9s] ★
  Epoch 15: train_ppl=72.1 val_ppl=601.9 acc=18.0% [1.9s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=601.4, acc=17.9%
  Result: PPL=601.4, Acc=17.9%
    → PPL: 601.4, Acc: 17.9%, ER: 75.7%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 1200d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2081 [1.5s]
  Iter 3: conv=0% loss=-0.2180 [1.5s]
  Iter 4: conv=0% loss=-0.2465 [1.5s]
  Iter 5: conv=0% loss=-0.2678 [1.5s]
  Iter 6: conv=0% loss=-0.2824 [1.5s]
  Iter 7: conv=0% loss=-0.2924 [1.6s]
  Iter 8: conv=0% loss=-0.3001 [1.5s]
  Iter 9: conv=0% loss=-0.3068 [1.5s]
  Iter 10: conv=1% loss=-0.3132 [1.5s]
  Iter 11: conv=1% loss=-0.3195 [1.5s]
  Iter 12: conv=1% loss=-0.3256 [1.5s]
  Iter 13: conv=1% loss=-0.3312 [1.5s]
  Iter 14: conv=2% loss=-0.3362 [1.5s]
  Iter 15: conv=2% loss=-0.3405 [1.6s]
  Iter 16: conv=3% loss=-0.3443 [1.6s]
  Iter 17: conv=4% loss=-0.3478 [1.5s]
  Iter 18: conv=7% loss=-0.3510 [1.5s]
  Iter 19: conv=10% loss=-0.3539 [1.5s]
  Iter 20: conv=15% loss=-0.3566 [1.5s]
  Iter 21: conv=23% loss=-0.3589 [1.5s]
  Iter 22: conv=35% loss=-0.3609 [1.6s]
  Iter 23: conv=51% loss=-0.3626 [1.6s]
  Iter 24: conv=68% loss=-0.3640 [1.5s]
  Iter 25: conv=82% loss=-0.3653 [1.5s]
  Iter 26: conv=91% loss=-0.3666 [1.6s]
  Iter 27: conv=96% loss=-0.3678 [1.6s]
  Iter 28: conv=98% loss=-0.3691 [1.6s]
  Iter 29: conv=99% loss=-0.3702 [1.6s]
  → Converged at iter 29
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.2s]
  ER: train=76.4%, val=75.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,515,264/42,477,840 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=216287.5 val_ppl=4458.6 acc=9.6% [3.3s] ★
  Epoch 2: train_ppl=1908.1 val_ppl=1373.4 acc=14.6% [3.3s] ★
  Epoch 3: train_ppl=807.1 val_ppl=882.7 acc=16.0% [3.3s] ★
  Epoch 4: train_ppl=487.2 val_ppl=691.8 acc=16.8% [3.4s] ★
  Epoch 5: train_ppl=352.2 val_ppl=585.6 acc=17.5% [3.3s] ★
  Epoch 6: train_ppl=271.0 val_ppl=520.9 acc=18.0% [3.3s] ★
  Epoch 7: train_ppl=218.5 val_ppl=478.4 acc=18.4% [3.3s] ★
  Epoch 8: train_ppl=181.3 val_ppl=449.4 acc=18.7% [3.3s] ★
  Epoch 9: train_ppl=153.3 val_ppl=429.5 acc=19.0% [3.3s] ★
  Epoch 10: train_ppl=131.5 val_ppl=416.7 acc=19.2% [3.3s] ★
  Epoch 11: train_ppl=114.1 val_ppl=408.9 acc=19.4% [3.3s] ★
  Epoch 12: train_ppl=99.8 val_ppl=405.1 acc=19.6% [3.3s] ★
  Epoch 13: train_ppl=88.0 val_ppl=404.9 acc=19.7% [3.2s] ★
  Epoch 14: train_ppl=78.1 val_ppl=407.6 acc=19.7% [3.2s]
  → Early stop at epoch 14
  Best: epoch 13, ppl=404.9, acc=19.7%
  Result: PPL=404.9, Acc=19.7%
    → PPL: 404.9, Acc: 19.7%, ER: 75.7%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 1200d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2043 [2.9s]
  Iter 3: conv=0% loss=-0.2140 [3.0s]
  Iter 4: conv=0% loss=-0.2421 [3.0s]
  Iter 5: conv=0% loss=-0.2631 [3.0s]
  Iter 6: conv=0% loss=-0.2772 [3.0s]
  Iter 7: conv=0% loss=-0.2869 [3.0s]
  Iter 8: conv=1% loss=-0.2943 [3.1s]
  Iter 9: conv=1% loss=-0.3007 [3.0s]
  Iter 10: conv=1% loss=-0.3068 [3.0s]
  Iter 11: conv=1% loss=-0.3127 [3.0s]
  Iter 12: conv=2% loss=-0.3185 [3.0s]
  Iter 13: conv=2% loss=-0.3239 [3.0s]
  Iter 14: conv=2% loss=-0.3287 [3.0s]
  Iter 15: conv=3% loss=-0.3329 [3.0s]
  Iter 16: conv=3% loss=-0.3368 [3.1s]
  Iter 17: conv=4% loss=-0.3403 [3.0s]
  Iter 18: conv=5% loss=-0.3436 [3.0s]
  Iter 19: conv=7% loss=-0.3466 [3.0s]
  Iter 20: conv=11% loss=-0.3494 [3.0s]
  Iter 21: conv=15% loss=-0.3520 [3.0s]
  Iter 22: conv=20% loss=-0.3543 [3.0s]
  Iter 23: conv=28% loss=-0.3564 [3.0s]
  Iter 24: conv=39% loss=-0.3581 [3.0s]
  Iter 25: conv=54% loss=-0.3596 [3.0s]
  Iter 26: conv=69% loss=-0.3611 [3.0s]
  Iter 27: conv=81% loss=-0.3624 [3.0s]
  Iter 28: conv=90% loss=-0.3638 [3.0s]
  Iter 29: conv=95% loss=-0.3650 [3.0s]
  Iter 30: conv=98% loss=-0.3662 [3.0s]
  Iter 31: conv=99% loss=-0.3673 [3.0s]
  → Converged at iter 31
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.2s]
  ER: train=76.4%, val=75.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,515,264/42,477,840 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=26674.7 val_ppl=1467.8 acc=14.2% [6.1s] ★
  Epoch 2: train_ppl=898.0 val_ppl=708.7 acc=16.6% [6.1s] ★
  Epoch 3: train_ppl=479.2 val_ppl=512.9 acc=17.8% [6.1s] ★
  Epoch 4: train_ppl=332.9 val_ppl=423.3 acc=18.5% [6.0s] ★
  Epoch 5: train_ppl=253.6 val_ppl=373.4 acc=19.0% [6.0s] ★
  Epoch 6: train_ppl=205.4 val_ppl=342.9 acc=19.4% [5.9s] ★
  Epoch 7: train_ppl=171.9 val_ppl=323.5 acc=19.8% [5.9s] ★
  Epoch 8: train_ppl=147.3 val_ppl=311.0 acc=20.1% [5.9s] ★
  Epoch 9: train_ppl=128.3 val_ppl=303.4 acc=20.3% [5.9s] ★
  Epoch 10: train_ppl=113.3 val_ppl=299.2 acc=20.5% [5.9s] ★
  Epoch 11: train_ppl=100.9 val_ppl=297.5 acc=20.6% [5.9s] ★
  Epoch 12: train_ppl=90.5 val_ppl=298.1 acc=20.5% [5.9s]
  → Early stop at epoch 12
  Best: epoch 11, ppl=297.5, acc=20.6%
  Result: PPL=297.5, Acc=20.6%
    → PPL: 297.5, Acc: 20.6%, ER: 75.4%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 1200d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2040 [7.1s]
  Iter 3: conv=0% loss=-0.2139 [7.3s]
  Iter 4: conv=0% loss=-0.2422 [7.3s]
  Iter 5: conv=0% loss=-0.2634 [7.3s]
  Iter 6: conv=0% loss=-0.2778 [7.3s]
  Iter 7: conv=0% loss=-0.2879 [7.3s]
  Iter 8: conv=1% loss=-0.2956 [7.3s]
  Iter 9: conv=1% loss=-0.3022 [7.4s]
  Iter 10: conv=1% loss=-0.3085 [7.3s]
  Iter 11: conv=1% loss=-0.3148 [7.3s]
  Iter 12: conv=1% loss=-0.3209 [7.4s]
  Iter 13: conv=2% loss=-0.3265 [7.3s]
  Iter 14: conv=2% loss=-0.3314 [7.3s]
  Iter 15: conv=2% loss=-0.3357 [7.3s]
  Iter 16: conv=3% loss=-0.3395 [7.3s]
  Iter 17: conv=4% loss=-0.3429 [7.3s]
  Iter 18: conv=7% loss=-0.3461 [7.3s]
  Iter 19: conv=10% loss=-0.3491 [7.3s]
  Iter 20: conv=15% loss=-0.3517 [7.3s]
  Iter 21: conv=23% loss=-0.3540 [7.3s]
  Iter 22: conv=35% loss=-0.3560 [7.3s]
  Iter 23: conv=51% loss=-0.3577 [7.4s]
  Iter 24: conv=68% loss=-0.3591 [7.3s]
  Iter 25: conv=82% loss=-0.3604 [7.4s]
  Iter 26: conv=91% loss=-0.3617 [7.3s]
  Iter 27: conv=96% loss=-0.3629 [7.3s]
  Iter 28: conv=98% loss=-0.3642 [7.3s]
  Iter 29: conv=99% loss=-0.3653 [7.3s]
  → Converged at iter 29
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.5s]
  ER: train=76.9%, val=75.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,515,264/42,477,840 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3624.8 val_ppl=575.6 acc=17.4% [14.5s] ★
  Epoch 2: train_ppl=429.1 val_ppl=354.9 acc=19.2% [14.2s] ★
  Epoch 3: train_ppl=279.3 val_ppl=284.9 acc=20.4% [14.0s] ★
  Epoch 4: train_ppl=213.8 val_ppl=251.1 acc=21.0% [13.8s] ★
  Epoch 5: train_ppl=175.6 val_ppl=231.6 acc=21.6% [13.8s] ★
  Epoch 6: train_ppl=150.1 val_ppl=219.1 acc=22.0% [13.9s] ★
  Epoch 7: train_ppl=131.7 val_ppl=211.1 acc=22.4% [14.0s] ★
  Epoch 8: train_ppl=117.6 val_ppl=206.0 acc=22.7% [14.1s] ★
  Epoch 9: train_ppl=106.5 val_ppl=202.9 acc=22.8% [14.1s] ★
  Epoch 10: train_ppl=97.5 val_ppl=201.5 acc=22.9% [14.0s] ★
  Epoch 11: train_ppl=90.1 val_ppl=201.1 acc=23.0% [14.0s] ★
  Epoch 12: train_ppl=83.8 val_ppl=201.5 acc=23.1% [14.0s]
  → Early stop at epoch 12
  Best: epoch 11, ppl=201.1, acc=23.0%
  Result: PPL=201.1, Acc=23.0%
    → PPL: 201.1, Acc: 23.0%, ER: 75.6%

======================================================================
Config: 1L_1537d_1tok
  num_layers=1, context_dim=1537, num_input_tokens=1
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 1537d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2446 [1.0s]
  Iter 3: conv=0% loss=-0.2496 [1.1s]
  Iter 4: conv=0% loss=-0.2774 [1.1s]
  Iter 5: conv=0% loss=-0.2996 [1.1s]
  Iter 6: conv=0% loss=-0.3156 [1.1s]
  Iter 7: conv=0% loss=-0.3260 [1.1s]
  Iter 8: conv=1% loss=-0.3336 [1.1s]
  Iter 9: conv=1% loss=-0.3401 [1.1s]
  Iter 10: conv=1% loss=-0.3468 [1.1s]
  Iter 11: conv=1% loss=-0.3537 [1.1s]
  Iter 12: conv=2% loss=-0.3604 [1.1s]
  Iter 13: conv=2% loss=-0.3668 [1.1s]
  Iter 14: conv=3% loss=-0.3724 [1.1s]
  Iter 15: conv=3% loss=-0.3775 [1.1s]
  Iter 16: conv=4% loss=-0.3821 [1.1s]
  Iter 17: conv=4% loss=-0.3865 [1.1s]
  Iter 18: conv=6% loss=-0.3906 [1.1s]
  Iter 19: conv=8% loss=-0.3943 [1.1s]
  Iter 20: conv=11% loss=-0.3978 [1.1s]
  Iter 21: conv=15% loss=-0.4009 [1.1s]
  Iter 22: conv=20% loss=-0.4036 [1.1s]
  Iter 23: conv=28% loss=-0.4060 [1.1s]
  Iter 24: conv=40% loss=-0.4081 [1.1s]
  Iter 25: conv=54% loss=-0.4099 [1.1s]
  Iter 26: conv=69% loss=-0.4116 [1.1s]
  Iter 27: conv=81% loss=-0.4131 [1.1s]
  Iter 28: conv=90% loss=-0.4145 [1.1s]
  Iter 29: conv=95% loss=-0.4158 [1.1s]
  Iter 30: conv=97% loss=-0.4170 [1.1s]
  Iter 31: conv=99% loss=-0.4181 [1.1s]
  Iter 32: conv=99% loss=-0.4192 [1.1s]
  → Converged at iter 32
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
  ER: train=73.4%, val=72.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,774,080/43,918,852 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=789728.1 val_ppl=15802.3 acc=8.7% [1.9s] ★
  Epoch 2: train_ppl=4897.8 val_ppl=3444.1 acc=10.2% [1.9s] ★
  Epoch 3: train_ppl=1419.0 val_ppl=1860.0 acc=13.1% [1.9s] ★
  Epoch 4: train_ppl=809.9 val_ppl=1357.4 acc=14.3% [1.9s] ★
  Epoch 5: train_ppl=507.4 val_ppl=1088.1 acc=15.0% [1.9s] ★
  Epoch 6: train_ppl=367.4 val_ppl=930.3 acc=15.8% [1.9s] ★
  Epoch 7: train_ppl=276.6 val_ppl=818.3 acc=16.4% [1.9s] ★
  Epoch 8: train_ppl=222.3 val_ppl=744.7 acc=16.6% [1.9s] ★
  Epoch 9: train_ppl=179.5 val_ppl=687.4 acc=16.9% [1.9s] ★
  Epoch 10: train_ppl=150.3 val_ppl=649.8 acc=17.0% [1.9s] ★
  Epoch 11: train_ppl=125.7 val_ppl=621.2 acc=17.3% [1.9s] ★
  Epoch 12: train_ppl=106.8 val_ppl=603.0 acc=17.4% [1.9s] ★
  Epoch 13: train_ppl=92.1 val_ppl=592.6 acc=17.5% [1.9s] ★
  Epoch 14: train_ppl=79.3 val_ppl=588.6 acc=17.6% [1.9s] ★
  Epoch 15: train_ppl=69.2 val_ppl=590.7 acc=17.7% [1.9s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=588.6, acc=17.6%
  Result: PPL=588.6, Acc=17.6%
    → PPL: 588.6, Acc: 17.6%, ER: 72.8%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 1537d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2467 [2.0s]
  Iter 3: conv=0% loss=-0.2514 [2.0s]
  Iter 4: conv=0% loss=-0.2793 [2.0s]
  Iter 5: conv=0% loss=-0.3015 [2.0s]
  Iter 6: conv=0% loss=-0.3176 [2.0s]
  Iter 7: conv=0% loss=-0.3279 [2.0s]
  Iter 8: conv=0% loss=-0.3355 [2.0s]
  Iter 9: conv=0% loss=-0.3421 [2.0s]
  Iter 10: conv=1% loss=-0.3488 [2.0s]
  Iter 11: conv=1% loss=-0.3557 [2.0s]
  Iter 12: conv=1% loss=-0.3626 [2.0s]
  Iter 13: conv=1% loss=-0.3690 [2.0s]
  Iter 14: conv=1% loss=-0.3747 [2.0s]
  Iter 15: conv=2% loss=-0.3798 [2.0s]
  Iter 16: conv=2% loss=-0.3845 [2.0s]
  Iter 17: conv=2% loss=-0.3889 [2.0s]
  Iter 18: conv=3% loss=-0.3931 [2.0s]
  Iter 19: conv=5% loss=-0.3969 [2.0s]
  Iter 20: conv=8% loss=-0.4004 [2.0s]
  Iter 21: conv=12% loss=-0.4036 [2.0s]
  Iter 22: conv=18% loss=-0.4064 [2.0s]
  Iter 23: conv=26% loss=-0.4088 [2.0s]
  Iter 24: conv=38% loss=-0.4109 [2.0s]
  Iter 25: conv=53% loss=-0.4128 [2.0s]
  Iter 26: conv=69% loss=-0.4145 [2.0s]
  Iter 27: conv=81% loss=-0.4160 [2.0s]
  Iter 28: conv=89% loss=-0.4174 [2.0s]
  Iter 29: conv=95% loss=-0.4187 [2.0s]
  Iter 30: conv=97% loss=-0.4199 [2.0s]
  Iter 31: conv=99% loss=-0.4210 [2.0s]
  Iter 32: conv=100% loss=-0.4221 [2.0s]
  → Converged at iter 32
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.4s]
  ER: train=73.6%, val=72.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,774,080/43,918,852 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=84937.2 val_ppl=3577.9 acc=10.5% [3.3s] ★
  Epoch 2: train_ppl=1723.8 val_ppl=1329.6 acc=14.5% [3.4s] ★
  Epoch 3: train_ppl=790.6 val_ppl=885.4 acc=16.1% [3.4s] ★
  Epoch 4: train_ppl=499.5 val_ppl=691.2 acc=16.8% [3.4s] ★
  Epoch 5: train_ppl=354.9 val_ppl=582.0 acc=17.2% [3.4s] ★
  Epoch 6: train_ppl=274.1 val_ppl=513.4 acc=17.7% [3.4s] ★
  Epoch 7: train_ppl=219.4 val_ppl=469.1 acc=18.0% [3.3s] ★
  Epoch 8: train_ppl=181.0 val_ppl=439.9 acc=18.3% [3.3s] ★
  Epoch 9: train_ppl=152.2 val_ppl=421.2 acc=18.6% [3.3s] ★
  Epoch 10: train_ppl=129.7 val_ppl=409.7 acc=18.8% [3.3s] ★
  Epoch 11: train_ppl=111.7 val_ppl=403.6 acc=19.0% [3.3s] ★
  Epoch 12: train_ppl=97.0 val_ppl=401.6 acc=19.1% [3.3s] ★
  Epoch 13: train_ppl=84.7 val_ppl=403.6 acc=19.3% [3.3s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=401.6, acc=19.1%
  Result: PPL=401.6, Acc=19.1%
    → PPL: 401.6, Acc: 19.1%, ER: 72.9%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 1537d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2424 [3.8s]
  Iter 3: conv=0% loss=-0.2469 [4.0s]
  Iter 4: conv=0% loss=-0.2742 [3.9s]
  Iter 5: conv=0% loss=-0.2961 [3.9s]
  Iter 6: conv=0% loss=-0.3117 [3.9s]
  Iter 7: conv=0% loss=-0.3217 [3.9s]
  Iter 8: conv=1% loss=-0.3289 [3.9s]
  Iter 9: conv=0% loss=-0.3349 [3.9s]
  Iter 10: conv=1% loss=-0.3412 [3.9s]
  Iter 11: conv=1% loss=-0.3477 [3.9s]
  Iter 12: conv=1% loss=-0.3542 [3.9s]
  Iter 13: conv=2% loss=-0.3603 [3.9s]
  Iter 14: conv=2% loss=-0.3658 [3.9s]
  Iter 15: conv=2% loss=-0.3707 [3.9s]
  Iter 16: conv=3% loss=-0.3753 [3.9s]
  Iter 17: conv=3% loss=-0.3796 [3.9s]
  Iter 18: conv=4% loss=-0.3838 [3.9s]
  Iter 19: conv=5% loss=-0.3879 [3.9s]
  Iter 20: conv=6% loss=-0.3917 [3.9s]
  Iter 21: conv=9% loss=-0.3952 [3.9s]
  Iter 22: conv=13% loss=-0.3984 [3.9s]
  Iter 23: conv=17% loss=-0.4011 [3.9s]
  Iter 24: conv=22% loss=-0.4035 [3.9s]
  Iter 25: conv=30% loss=-0.4057 [3.9s]
  Iter 26: conv=41% loss=-0.4076 [3.9s]
  Iter 27: conv=54% loss=-0.4094 [3.9s]
  Iter 28: conv=67% loss=-0.4111 [3.9s]
  Iter 29: conv=78% loss=-0.4127 [3.9s]
  Iter 30: conv=87% loss=-0.4141 [3.9s]
  Iter 31: conv=93% loss=-0.4154 [3.9s]
  Iter 32: conv=96% loss=-0.4167 [3.9s]
  Iter 33: conv=98% loss=-0.4178 [3.9s]
  Iter 34: conv=99% loss=-0.4188 [3.9s]
  → Converged at iter 34
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.7s]
  ER: train=73.6%, val=72.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,774,080/43,918,852 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=15512.1 val_ppl=1413.7 acc=14.5% [6.1s] ★
  Epoch 2: train_ppl=882.4 val_ppl=707.2 acc=16.6% [6.2s] ★
  Epoch 3: train_ppl=484.5 val_ppl=510.4 acc=17.6% [6.2s] ★
  Epoch 4: train_ppl=334.8 val_ppl=421.2 acc=18.3% [6.1s] ★
  Epoch 5: train_ppl=257.3 val_ppl=372.2 acc=18.9% [6.0s] ★
  Epoch 6: train_ppl=207.4 val_ppl=342.5 acc=19.4% [6.0s] ★
  Epoch 7: train_ppl=172.6 val_ppl=323.4 acc=19.8% [6.0s] ★
  Epoch 8: train_ppl=147.0 val_ppl=311.5 acc=19.9% [6.0s] ★
  Epoch 9: train_ppl=127.0 val_ppl=304.8 acc=20.1% [6.0s] ★
  Epoch 10: train_ppl=111.1 val_ppl=301.9 acc=20.4% [6.0s] ★
  Epoch 11: train_ppl=98.1 val_ppl=301.8 acc=20.4% [6.0s] ★
  Epoch 12: train_ppl=87.4 val_ppl=304.1 acc=20.4% [6.0s]
  → Early stop at epoch 12
  Best: epoch 11, ppl=301.8, acc=20.4%
  Result: PPL=301.8, Acc=20.4%
    → PPL: 301.8, Acc: 20.4%, ER: 72.4%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 1537d, 1tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2420 [9.3s]
  Iter 3: conv=0% loss=-0.2468 [9.6s]
  Iter 4: conv=0% loss=-0.2744 [9.5s]
  Iter 5: conv=0% loss=-0.2965 [9.6s]
  Iter 6: conv=0% loss=-0.3124 [9.6s]
  Iter 7: conv=0% loss=-0.3227 [9.6s]
  Iter 8: conv=1% loss=-0.3302 [9.6s]
  Iter 9: conv=1% loss=-0.3367 [9.6s]
  Iter 10: conv=1% loss=-0.3433 [9.5s]
  Iter 11: conv=1% loss=-0.3502 [9.6s]
  Iter 12: conv=1% loss=-0.3570 [9.5s]
  Iter 13: conv=1% loss=-0.3633 [9.6s]
  Iter 14: conv=2% loss=-0.3690 [9.5s]
  Iter 15: conv=2% loss=-0.3742 [9.6s]
  Iter 16: conv=2% loss=-0.3789 [9.5s]
  Iter 17: conv=3% loss=-0.3833 [9.6s]
  Iter 18: conv=3% loss=-0.3874 [9.5s]
  Iter 19: conv=5% loss=-0.3913 [9.5s]
  Iter 20: conv=8% loss=-0.3948 [9.6s]
  Iter 21: conv=12% loss=-0.3980 [9.5s]
  Iter 22: conv=17% loss=-0.4008 [9.6s]
  Iter 23: conv=25% loss=-0.4032 [9.6s]
  Iter 24: conv=37% loss=-0.4053 [9.5s]
  Iter 25: conv=52% loss=-0.4072 [9.6s]
  Iter 26: conv=67% loss=-0.4089 [9.5s]
  Iter 27: conv=80% loss=-0.4104 [9.5s]
  Iter 28: conv=89% loss=-0.4118 [9.6s]
  Iter 29: conv=94% loss=-0.4131 [9.6s]
  Iter 30: conv=97% loss=-0.4144 [9.6s]
  Iter 31: conv=99% loss=-0.4155 [9.5s]
  Iter 32: conv=99% loss=-0.4166 [9.5s]
  → Converged at iter 32
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [6.6s]
  ER: train=74.1%, val=72.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,774,080/43,918,852 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=2857.9 val_ppl=576.3 acc=17.5% [14.4s] ★
  Epoch 2: train_ppl=422.2 val_ppl=349.7 acc=19.3% [14.3s] ★
  Epoch 3: train_ppl=276.1 val_ppl=281.2 acc=20.4% [14.2s] ★
  Epoch 4: train_ppl=210.9 val_ppl=248.5 acc=21.2% [14.0s] ★
  Epoch 5: train_ppl=172.6 val_ppl=229.5 acc=21.8% [14.0s] ★
  Epoch 6: train_ppl=146.7 val_ppl=217.6 acc=22.2% [14.1s] ★
  Epoch 7: train_ppl=128.0 val_ppl=210.1 acc=22.5% [14.2s] ★
  Epoch 8: train_ppl=113.8 val_ppl=205.7 acc=22.7% [14.3s] ★
  Epoch 9: train_ppl=102.5 val_ppl=203.2 acc=22.8% [14.3s] ★
  Epoch 10: train_ppl=93.5 val_ppl=202.3 acc=22.9% [14.2s] ★
  Epoch 11: train_ppl=85.9 val_ppl=202.5 acc=23.0% [14.1s]
  → Early stop at epoch 11
  Best: epoch 10, ppl=202.3, acc=22.9%
  Result: PPL=202.3, Acc=22.9%
    → PPL: 202.3, Acc: 22.9%, ER: 72.6%

====================================================================================================
RESULTS SUMMARY
====================================================================================================
Config                   α          A     R²     PPL    Acc   T.PPL    ER Iter
----------------------------------------------------------------------------------------------------
1L_768d_1tok       -0.4761   1.07e+05  0.993   196.3  22.8%    93.4 79.2%   23
1L_1200d_1tok      -0.4853   1.24e+05  0.995   201.1  23.0%    90.1 75.6%   29
1L_1537d_1tok      -0.4717   1.05e+05  0.996   202.3  22.9%    93.5 72.6%   32

====================================================================================================
DETAILED RESULTS (All sample sizes)
====================================================================================================
Config             Samples     Tokens P1 Iter Train ER  Val ER   T.PPL   V.PPL    Acc
----------------------------------------------------------------------------------------------------
1L_768d_1tok            50     62,891      23    79.7%   79.3%    65.9   576.0  18.3%
1L_768d_1tok           100    122,795      23    79.9%   79.2%    91.3   386.5  19.7%
1L_768d_1tok           200    240,132      25    80.0%   79.1%    94.0   286.4  20.4%
1L_768d_1tok           500    587,970      23    80.3%   79.2%    93.4   196.3  22.8%
1L_1200d_1tok           50     62,891      29    76.2%   75.7%    81.4   601.4  17.9%
1L_1200d_1tok          100    122,795      29    76.4%   75.7%    88.0   404.9  19.7%
1L_1200d_1tok          200    240,132      31    76.4%   75.4%   100.9   297.5  20.6%
1L_1200d_1tok          500    587,970      29    76.9%   75.6%    90.1   201.1  23.0%
1L_1537d_1tok           50     62,891      32    73.4%   72.8%    79.3   588.6  17.6%
1L_1537d_1tok          100    122,795      32    73.6%   72.9%    97.0   401.6  19.1%
1L_1537d_1tok          200    240,132      34    73.6%   72.4%    98.1   301.8  20.4%
1L_1537d_1tok          500    587,970      32    74.1%   72.6%    93.5   202.3  22.9%

Results saved to: results/scaling_20251130_063415

Total time: 36.4 min
