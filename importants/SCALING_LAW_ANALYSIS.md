# Scaling Law Analysis

**最終更新**: 2025-12-03

---

## 結論サマリー

### ベストフィットモデル: 指数減衰

```
PPL = PPL_min + A × exp(-b × n^c)
```

| context_dim | PPL_min | 現在PPL (1600samples) | 理論限界との差 |
|-------------|---------|----------------------|---------------|
| 256 | 132.5 | 134.7 | 2.2 |
| 320 | 122.8 | 132.6 | 9.8 |

**重要な発見**:
1. **指数減衰モデルが最良**（AIC=16.15）
2. **dim=256はほぼ理論限界に到達**（差2.2）
3. **context_dim増加の効果は限定的**（25%増で7-9%のPPL_min改善）

---

## 1. 実験データ

### context_dim=256

| Samples | Tokens | Val PPL | Val Acc | ER% |
|---------|--------|---------|---------|-----|
| 100 | 122,795 | 321.4 | 19.1% | 68.4% |
| 200 | 240,132 | 253.1 | 19.9% | 68.1% |
| 400 | 473,429 | 186.0 | 21.4% | 68.9% |
| 800 | 948,524 | 155.8 | 22.6% | 68.1% |
| 1600 | 1,920,992 | 134.7 | 23.6% | 67.0% |

### context_dim=320

| Samples | Tokens | Val PPL | Val Acc | ER% |
|---------|--------|---------|---------|-----|
| 100 | 122,795 | 319.9 | 18.9% | 63.6% |
| 200 | 240,132 | 250.1 | 20.1% | 65.5% |
| 400 | 473,429 | 189.0 | 21.5% | 65.8% |
| 800 | 948,524 | 156.0 | 22.6% | 65.2% |
| 1600 | 1,920,992 | 132.6 | 23.4% | 65.8% |

---

## 2. モデル比較

### 検討した3つのモデル

| Model | 式 | パラメータ数 |
|-------|-----|------------|
| 単純べき乗則 | `PPL = A × n^(-a)` | 2 |
| 飽和モデル | `PPL = PPL_min + A × n^(-a)` | 3 |
| 指数減衰 | `PPL = PPL_min + A × exp(-b × n^c)` | 4 |

### AIC比較（両データセット平均）

| Model | Average AIC | 順位 |
|-------|-------------|------|
| **指数減衰** | **16.15** | **1位** |
| 飽和モデル | 19.02 | 2位 |
| 単純べき乗則 | 24.11 | 3位 |

### 詳細フィット結果

#### context_dim=256

| Model | R² | RSS | AIC |
|-------|-----|-----|-----|
| 単純べき乗則 | 0.9847 | 358.88 | 25.37 |
| 飽和モデル | 0.9952 | 111.86 | 21.54 |
| **指数減衰** | **0.9982** | **41.17** | **18.54** |

#### context_dim=320

| Model | R² | RSS | AIC |
|-------|-----|-----|-----|
| 単純べき乗則 | 0.9906 | 216.83 | 22.85 |
| 飽和モデル | 0.9982 | 40.76 | 16.49 |
| **指数減衰** | **0.9993** | **15.79** | **13.75** |

---

## 3. 指数減衰モデルのパラメータ

```
PPL = PPL_min + A × exp(-b × n^c)
```

| Parameter | dim=256 | dim=320 | 変化 |
|-----------|---------|---------|------|
| **PPL_min** | **132.5** | **122.8** | **-7.3%** |
| A | 473.4 | 763.1 | +61.2% |
| b | 0.0576 | 0.1995 | +246% |
| c | 0.6007 | 0.4155 | -30.8% |

### 予測精度（dim=256）

| Samples | Actual | Predicted | Error |
|---------|--------|-----------|-------|
| 100 | 321.4 | 322.0 | +0.2% |
| 200 | 253.1 | 250.6 | -1.0% |
| 400 | 186.0 | 190.2 | +2.2% |
| 800 | 155.8 | 151.9 | -2.5% |
| 1600 | 134.7 | 136.2 | +1.1% |

全ての点で ±2.5% 以内の誤差。

---

## 4. context_dim比較

### PPL差

| Samples | dim=256 | dim=320 | Diff |
|---------|---------|---------|------|
| 100 | 321.4 | 319.9 | -0.5% |
| 200 | 253.1 | 250.1 | -1.2% |
| 400 | 186.0 | 189.0 | +1.6% |
| 800 | 155.8 | 156.0 | +0.1% |
| 1600 | 134.7 | 132.6 | -1.6% |
| **Average** | - | - | **-0.3%** |

**結論**: 現時点のPPL差はほぼなし（平均-0.3%）

### Effective Rank

| dim | ER% | 絶対値 |
|-----|-----|--------|
| 256 | 68.1% | 174 |
| 320 | 65.2% | 209 |

追加した64次元のうち、約35次元（55%）しか活用されていない。

---

## 5. 外挿予測

### 指数減衰モデル

| Samples | dim=256 | dim=320 |
|---------|---------|---------|
| 3,200 | 132.7 | 125.0 |
| 6,400 | 132.5 | 123.3 |
| ∞ | 132.5 | 122.8 |

### 飽和モデル（参考）

| Samples | dim=256 | dim=320 |
|---------|---------|---------|
| 3,200 | 115.7 | 113.6 |
| 6,400 | 104.4 | 101.4 |
| ∞ | 80.5 | 73.6 |

---

## 6. 実用的含意

### 現在のアーキテクチャの限界

1. **dim=256はほぼ理論限界**（PPL_min=132.5、現在134.7）
2. **データ増加の効果は限定的**
3. **context_dim増加も効率が低い**（25%増で7-9%改善）

### PPL < 100 達成の条件

- **飽和モデルが正しい場合**: データ増加で到達可能（〜1万samples）
- **指数減衰が正しい場合**: アーキテクチャ変更が必要
  - Multi-block（カスケード連結）
  - 層数増加
  - 学習方法の改善

---

## 7. 今後の検証

1. **3200, 6400 samples での実験**: モデル判別
2. **Multi-block アーキテクチャ**: 表現力増加の効果確認
3. **End-to-end学習**: Phase 1/2 分離の影響確認

---

## 分析スクリプト

```bash
# 最終モデル分析
python3 scripts/analyze_final_scaling_model.py

# context_dim比較
python3 scripts/analyze_context_dim_comparison.py

# スケーリングモデル比較
python3 scripts/analyze_scaling_models.py

# 単純べき乗則分析
python3 scripts/analyze_sample_size_scaling.py
```

---

*Last Updated: 2025-12-03*
