# Context Dim 探索実験

**最終更新**: 2025-12-03

---

## 結論サマリー

### 最適 context_dim

| サンプル数 | 最適 dim | Best PPL | ER% |
|-----------|----------|----------|-----|
| 100 | 300 | 327.3 | 65.9% |
| 200 | 300 | 253.5 | 66.2% |
| 1000 | 240 | 151.5 | 68.4% |
| 2000 | 400 | 130.0 | 61.4% |

**推奨**: context_dim = 300〜400

---

## 1. 2000サンプル実験（dim=100〜500, step=100）

**環境**: NVIDIA L4 (22.2GB)
**データ**: Train: 2,403,563 tokens, Val: 22,723 tokens

| dim | Val PPL | Val Acc | ER% | Phase 1 Iter | 総時間 |
|-----|---------|---------|-----|--------------|--------|
| 100 | 146.4 | 22.4% | 75.9% | 19 | 1653s |
| 200 | 133.5 | 23.3% | 70.9% | 23 | 1670s |
| 300 | 130.4 | 23.6% | 66.5% | 24 | 1700s |
| **400** | **130.0** | **24.0%** | 61.4% | 28 | 1874s |
| 500 | (切断) | - | 57.7% | 32 | - |

### 発見

1. **PPL改善は収益逓減**
   - dim 100→200: -12.9 PPL (大幅改善)
   - dim 200→300: -3.1 PPL
   - dim 300→400: -0.4 PPL (ほぼ横ばい)

2. **ER%は次元増加で低下**
   - dim=100: 75.9%
   - dim=500: 57.7%（約42%の次元が無駄）

3. **Phase 1収束時間は次元に比例**
   - dim=100: 88s
   - dim=500: 345s（約4倍）

---

## 2. 小規模サンプル実験（100/200サンプル）

### 100サンプル（dim刻み50）

| dim | PPL | Acc | ER% |
|-----|-----|-----|-----|
| 50 | 383.9 | 17.2% | 81.7% |
| 100 | 352.3 | 18.0% | 76.9% |
| 200 | 338.0 | 18.7% | 70.8% |
| **300** | **327.3** | **19.0%** | 65.9% |
| 400 | 330.4 | 19.2% | 61.9% |

### 200サンプル（dim刻み100）

| dim | PPL | Acc | ER% |
|-----|-----|-----|-----|
| 100 | 272.3 | 19.2% | 76.5% |
| 200 | 257.0 | 19.7% | 70.9% |
| **300** | **253.5** | **19.7%** | 66.2% |
| 400 | 258.0 | 19.8% | 61.3% |
| 500 | 258.3 | 19.9% | 54.7% |

### サンプル数2倍の効果

| dim | 100 samples | 200 samples | 改善率 |
|-----|-------------|-------------|--------|
| 100 | 352.3 | 272.3 | -22.7% |
| 200 | 338.0 | 257.0 | -24.0% |
| 300 | 327.3 | 253.5 | -22.5% |

**結論**: サンプル数2倍でPPL約22-24%改善

---

## 3. 1000サンプル細粒度探索（dim=200〜300, step=20）

| dim | Val PPL | Val Acc | ER% |
|-----|---------|---------|-----|
| 200 | 154.5 | 22.6% | 71.0% |
| 220 | 151.8 | 22.5% | 69.4% |
| **240** | **151.5** | **22.6%** | 68.4% |
| 260 | (切断) | - | - |

**観察**: dim=200→240で約3 PPL改善（小さい）

---

## 4. 傾向と推奨

### 最適 context_dim の傾向

```
サンプル数↑ → 最適dim↑（やや）
  100 samples: dim=300
  200 samples: dim=300
  2000 samples: dim=400
```

ただし、dim=300とdim=400の差は小さい（0.4 PPL）。

### Effective Rank と PPL のトレードオフ

```
dim↑ → ER%↓ → ある点で PPL 改善が頭打ち
```

- **ER% 65%以上**: 十分な次元活用
- **ER% 60%以下**: 次元が大きすぎる可能性

### 推奨設定

| 優先事項 | 推奨 dim | 理由 |
|---------|----------|------|
| **コスト効率** | 300 | PPL差小さく、計算時間短い |
| **最高性能** | 400 | 最良PPL（差は0.4のみ） |

### 避けるべき設定

- **dim=500以上**: PPL改善極小、ER%低下、計算コスト増

---

## 5. サンプル数とPPLの関係

| サンプル数 | dim=200 PPL | dim=300 PPL |
|-----------|-------------|-------------|
| 100 | 338.0 | 327.3 |
| 200 | 257.0 | 253.5 |
| 1000 | 154.5 | ~150 |
| 2000 | 133.5 | 130.4 |

**結論**: データ量増加がPPL改善に最も効果的

---

## 分析スクリプト

```bash
# context_dim探索
python3 scripts/context_dim_search.py -s 2000

# サンプルサイズ探索
python3 scripts/experiment_sample_size_search.py --start 100 --end 1600
```

---

*Last Updated: 2025-12-03*
