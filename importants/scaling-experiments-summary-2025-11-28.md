# スケーリング実験結果サマリー (2025-11-28)

## 概要

PPL（Perplexity）とトークン数の関係を調査し、スケーリング則を導出した実験のまとめ。

スケーリング則: `PPL = A × tokens^α`

**α（傾き）が重要**: αが大きいほど、データ量増加による改善効果が高い。

---

## 実験結果一覧

### 1. 11/27 スケーリング実験（3層モデル, num_input_tokens=1）

| 設定 | 値 |
|------|-----|
| モデル | 3層, context_dim=768 |
| num_input_tokens | **1** |
| 等差減少設計 | **なし** |
| パラメータ | 84.3M |

| Samples | Tokens | Val PPL | Val Acc |
|---------|--------|---------|---------|
| 50 | 3,495 | 7,152 | 10.0% |
| 100 | 7,466 | 3,360 | 7.4% |
| 200 | 15,832 | 2,439 | 9.1% |
| 500 | 39,814 | 1,171 | 13.8% |

**スケーリング則**: `PPL = 2,264,935 × tokens^(-0.7143)`

| 指標 | 値 |
|------|-----|
| **α** | **-0.7143** |
| R² | 0.9818 |
| 解釈 | 2倍のデータで38%PPL削減 |

---

### 2. 11/27 スケーリング実験（6層モデル, num_input_tokens=1）

| 設定 | 値 |
|------|-----|
| モデル | 6層, context_dim=768 |
| num_input_tokens | **1** |
| 等差減少設計 | **なし** |
| パラメータ | 91.4M |

| Samples | Tokens | Val PPL | Val Acc |
|---------|--------|---------|---------|
| 50 | 3,495 | 7,342 | 8.4% |
| 100 | 7,466 | 3,498 | 7.0% |
| 200 | 15,832 | 2,242 | 9.0% |
| 500 | 39,814 | 1,145 | 15.2% |

**スケーリング則**: `PPL = 3,022,654 × tokens^(-0.7463)`

| 指標 | 値 |
|------|-----|
| **α** | **-0.7463** |
| R² | 0.9908 |
| 解釈 | 2倍のデータで40%PPL削減 |

---

### 3. 11/28 比較実験（num_input_tokens=2, 等差減少なし）

| 実験 | Layers | context_dim | num_input_tokens | 等差減少 |
|------|--------|-------------|------------------|----------|
| A | 6 | 768 | **2** | なし |
| B | 3 | 1536 | **2** | なし |

| 実験 | Samples | Tokens | Val PPL | Val Acc |
|------|---------|--------|---------|---------|
| A | 500 | 86,925 | 554.58 | 16.0% |
| A | 1000 | 183,620 | 447.22 | 17.5% |
| B | 500 | 86,925 | 586.36 | 16.4% |
| B | 1000 | 183,620 | 439.79 | 18.1% |

**スケーリング則**:

| 実験 | α | A | 解釈 |
|------|-----|-----|------|
| A (6層/768dim) | **-0.287** | 14,504 | 2倍のデータで22%PPL削減 |
| B (3層/1536dim) | **-0.385** | 46,791 | 2倍のデータで30%PPL削減 |

---

## α値比較まとめ

| 実験 | num_input_tokens | 等差減少 | α値 | 解釈 |
|------|------------------|----------|-----|------|
| 3層/768dim (11/27) | **1** | なし | **-0.7143** | 2倍データで38%PPL削減 |
| 6層/768dim (11/27) | **1** | なし | **-0.7463** | 2倍データで40%PPL削減 |
| 6層/768dim (11/28) | **2** | なし | -0.287 | 2倍データで22%PPL削減 |
| 3層/1536dim (11/28) | **2** | なし | -0.385 | 2倍データで30%PPL削減 |

---

## 重要な発見

### 1. num_input_tokensの影響が極めて大きい

| num_input_tokens | 平均α | スケーリング効率 |
|------------------|-------|------------------|
| 1 | **-0.73** | 高い |
| 2 | -0.34 | 低い |

**結論**: num_input_tokens=1 の方がスケーリング効率が**約2.1倍**良い。

### 2. レイヤー数の影響

- 6層 > 3層（わずかにαが改善）
- ただし、num_input_tokensの影響に比べると小さい

### 3. context_dimの影響

- 3層/1536dim vs 6層/768dim の比較では、3層/1536dimがやや良いα
- ただし、これもnum_input_tokensの影響に比べると小さい

---

## PPL予測表（num_input_tokens=1, 6層モデル基準）

α = -0.7463 を使用:

| Train Tokens | 予測 PPL |
|--------------|----------|
| 10,000 | 4,500 |
| 50,000 | 1,000 |
| 100,000 | **561** |
| 500,000 | **169** |
| 1,000,000 | **101** |
| 10,000,000 | **18** |

---

## 関連ファイル

- [scaling_experiment_2025-11-27.md](scaling_experiment_2025-11-27.md) - 3層モデル詳細
- [scaling_experiment_layer6_2025-11-27.md](scaling_experiment_layer6_2025-11-27.md) - 6層モデル詳細
- [layer-context-comparison-2025-11-28.md](layer-context-comparison-2025-11-28.md) - num_input_tokens=2実験詳細

---

## 今後の実験予定

- num_input_tokens=1 + 等差減少設計 での実験（現在実行中）
- より大規模なデータでの検証（100万トークン以上）
