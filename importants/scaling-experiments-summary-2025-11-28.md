# スケーリング実験結果サマリー (2025-11-28)

## 概要

PPL（Perplexity）とトークン数の関係を調査し、スケーリング則を導出した実験のまとめ。

スケーリング則: `PPL = A × tokens^α`

**α（傾き）が重要**: αが大きいほど、データ量増加による改善効果が高い。

---

## 実験結果一覧

### 1. 11/27 スケーリング実験（3層モデル, num_input_tokens=1）

| 設定 | 値 |
|------|-----|
| モデル | 3層, context_dim=768 |
| num_input_tokens | **1** |
| 等差減少設計 | **なし** |
| パラメータ | 84.3M |

| Samples | Tokens | Val PPL | Val Acc |
|---------|--------|---------|---------|
| 50 | 3,495 | 7,152 | 10.0% |
| 100 | 7,466 | 3,360 | 7.4% |
| 200 | 15,832 | 2,439 | 9.1% |
| 500 | 39,814 | 1,171 | 13.8% |

**スケーリング則**: `PPL = 2,264,935 × tokens^(-0.7143)`

| 指標 | 値 |
|------|-----|
| **α** | **-0.7143** |
| R² | 0.9818 |
| 解釈 | 2倍のデータで38%PPL削減 |

---

### 2. 11/27 スケーリング実験（6層モデル, num_input_tokens=1）

| 設定 | 値 |
|------|-----|
| モデル | 6層, context_dim=768 |
| num_input_tokens | **1** |
| 等差減少設計 | **なし** |
| パラメータ | 91.4M |

| Samples | Tokens | Val PPL | Val Acc |
|---------|--------|---------|---------|
| 50 | 3,495 | 7,342 | 8.4% |
| 100 | 7,466 | 3,498 | 7.0% |
| 200 | 15,832 | 2,242 | 9.0% |
| 500 | 39,814 | 1,145 | 15.2% |

**スケーリング則**: `PPL = 3,022,654 × tokens^(-0.7463)`

| 指標 | 値 |
|------|-----|
| **α** | **-0.7463** |
| R² | 0.9908 |
| 解釈 | 2倍のデータで40%PPL削減 |

---

### 3. 11/28 比較実験 v1（num_input_tokens=2, 等差減少なし）

| 実験 | Layers | context_dim | num_input_tokens | 等差減少 |
|------|--------|-------------|------------------|----------|
| A | 6 | 768 | **2** | なし |
| B | 3 | 1536 | **2** | なし |

| 実験 | Samples | Tokens | Val PPL | Val Acc |
|------|---------|--------|---------|---------|
| A | 500 | 86,925 | 554.58 | 16.0% |
| A | 1000 | 183,620 | 447.22 | 17.5% |
| B | 500 | 86,925 | 586.36 | 16.4% |
| B | 1000 | 183,620 | 439.79 | 18.1% |

**スケーリング則**:

| 実験 | α | A | 解釈 |
|------|-----|-----|------|
| A (6層/768dim) | **-0.287** | 14,504 | 2倍のデータで22%PPL削減 |
| B (3層/1536dim) | **-0.385** | 46,791 | 2倍のデータで30%PPL削減 |

---

### 4. 11/28 比較実験 v2（num_input_tokens=1, 大規模データ）⭐ 最新

**実験条件**:
| 設定 | 値 |
|------|-----|
| GPU | NVIDIA L4 (23.8GB) |
| num_input_tokens | **1** |
| Embedding freeze | True |
| Phase 2 epochs | 10 (early stopping patience=1) |

| 実験 | Layers | context_dim | ContextBlock | TokenBlock |
|------|--------|-------------|--------------|------------|
| A | 6 | 768 | 15.39M | 8.57M |
| B | 3 | 1536 | 20.73M | 5.91M |

**結果**:

| 実験 | Samples | Tokens | Val PPL | Val Acc | Train ER | Val ER |
|------|---------|--------|---------|---------|----------|--------|
| A | 500 | 86,925 | 700.60 | 14.47% | 9.9% | 9.4% |
| A | 1000 | 183,620 | **562.93** | **15.21%** | 9.6% | 9.1% |
| B | 500 | 86,925 | 708.31 | 13.74% | 15.1% | 13.9% |
| B | 1000 | 183,620 | 570.76 | **15.31%** | 15.1% | 13.8% |

**スケーリング則**:

| 実験 | α | A | 解釈 |
|------|-----|-----|------|
| A (6層/768dim) | **-0.2926** | 19,519 | 2倍のデータで22%PPL削減 |
| B (3層/1536dim) | **-0.2887** | 18,890 | 2倍のデータで22%PPL削減 |

**重要な発見**:
- 両アーキテクチャでα値がほぼ同じ（-0.29前後）
- 6層モデルがわずかにスケーリング効率が良い（α=-0.2926 vs -0.2887）
- PPLは6層モデルが優位（562.93 vs 570.76）
- Accuracyは3層モデルがわずかに優位（15.31% vs 15.21%）

---

## α値比較まとめ

| 実験 | num_input_tokens | データ規模 | α値 | 解釈 |
|------|------------------|------------|-----|------|
| 3層/768dim (11/27) | **1** | 小規模(〜40k tok) | **-0.7143** | 2倍データで38%PPL削減 |
| 6層/768dim (11/27) | **1** | 小規模(〜40k tok) | **-0.7463** | 2倍データで40%PPL削減 |
| 6層/768dim (11/28 v1) | **2** | 中規模(〜180k tok) | -0.287 | 2倍データで22%PPL削減 |
| 3層/1536dim (11/28 v1) | **2** | 中規模(〜180k tok) | -0.385 | 2倍データで30%PPL削減 |
| 6層/768dim (11/28 v2) ⭐ | **1** | 中規模(〜180k tok) | **-0.2926** | 2倍データで22%PPL削減 |
| 3層/1536dim (11/28 v2) ⭐ | **1** | 中規模(〜180k tok) | **-0.2887** | 2倍データで22%PPL削減 |

---

## 重要な発見

### 1. データ規模によるα値の変化

⚠️ **重要**: 小規模データ（〜40k tok）と中規模データ（〜180k tok）でα値が大きく異なる

| データ規模 | num_input_tokens=1 α | 備考 |
|------------|---------------------|------|
| 小規模（〜40k tok） | **-0.72 〜 -0.75** | 11/27実験 |
| 中規模（〜180k tok） | **-0.29** | 11/28 v2実験 |

**考察**:
- 小規模データでは急激にPPLが改善（α大きい）
- 中規模になると改善が緩やかに（αが小さくなる）
- これは自然なスケーリング曲線の振る舞い

### 2. num_input_tokensの影響（同一データ規模での比較）

中規模データ（〜180k tok）での比較:

| num_input_tokens | α | 解釈 |
|------------------|-----|------|
| **1** | -0.29 | 標準的なスケーリング |
| **2** | -0.29〜-0.39 | ほぼ同等〜やや良い |

**結論**: 中規模データでは num_input_tokens による差は小さい

### 3. アーキテクチャの影響（最新実験 v2）

| アーキテクチャ | α | Val PPL (1000s) | Val Acc |
|----------------|-----|-----------------|---------|
| 6層/768dim | **-0.2926** | **562.93** | 15.21% |
| 3層/1536dim | -0.2887 | 570.76 | **15.31%** |

**結論**:
- スケーリング効率（α）はほぼ同等
- PPLは6層モデルが優位
- Accuracyは3層モデルがわずかに優位
- 差は誤差範囲内と考えられる

---

## PPL予測表（最新実験 v2基準）

### 6層/768dim モデル (α = -0.2926, A = 19,519)

| Train Tokens | 予測 PPL | 備考 |
|--------------|----------|------|
| 86,925 | 700.60 | 実測値 (500 samples) |
| 183,620 | 562.93 | 実測値 (1000 samples) |
| 500,000 | **383** | 予測 |
| 1,000,000 | **314** | 予測 |
| 5,000,000 | **213** | 予測 |
| 10,000,000 | **175** | 予測 |

### 3層/1536dim モデル (α = -0.2887, A = 18,890)

| Train Tokens | 予測 PPL | 備考 |
|--------------|----------|------|
| 86,925 | 708.31 | 実測値 (500 samples) |
| 183,620 | 570.76 | 実測値 (1000 samples) |
| 500,000 | **392** | 予測 |
| 1,000,000 | **323** | 予測 |
| 5,000,000 | **221** | 予測 |
| 10,000,000 | **182** | 予測 |

**注**: α ≈ -0.29 は従来のTransformerスケーリング則と比較すると控えめな値。大規模データでのさらなる実験が必要。

---

## 関連ファイル

- [scaling_experiment_2025-11-27.md](scaling_experiment_2025-11-27.md) - 3層モデル詳細
- [scaling_experiment_layer6_2025-11-27.md](scaling_experiment_layer6_2025-11-27.md) - 6層モデル詳細
- [layer-context-comparison-2025-11-28.md](layer-context-comparison-2025-11-28.md) - num_input_tokens=2実験詳細

---

## 今後の実験予定

- より大規模なデータでの検証（100万トークン以上）
- 等差減少設計（iteration数を減らす手法）の効果検証
- Effective Rank の向上実験（現在 9-15% と低い）
- Phase 2 キャッシュモードの高速化効果測定

---

## 実験環境

- **GPU**: NVIDIA L4 (23.8GB)
- **Framework**: PyTorch
- **Data**: UltraChat (HuggingFaceH4/ultrachat_200k)
- **Phase 1**: CVFP固定点学習（max 40 iterations, early stopping）
- **Phase 2**: Next-token prediction（10 epochs, early stopping patience=1）
