# Layer Asymmetric Experiment Results (2025-12-02)

## 概要

ContextBlockとTokenBlockのレイヤー数を独立して設定した4構成の比較実験。

**共通環境**:
- GPU: NVIDIA L4 (22.2GB)
- Samples: 2000
- Context dim: 500
- 訓練データ: 2,403,563 tokens
- 検証データ: 22,723 tokens

---

## 結果サマリー

| Config | Context | Token | Params | Phase1 Iter | Conv% | ER% | Val PPL | Val Acc | Total Time |
|--------|---------|-------|--------|-------------|-------|-----|---------|---------|------------|
| **C1T1** | 1 | 1 | 40.2M | 30 | 92% | 79.7% | **127.2** | **24.7%** | ~1200s |
| C2T1 | 2 | 1 | 40.8M | 14 | 93% | 79.6% | 138.7 | 23.2% | 1249s |
| C1T2 | 1 | 2 | 41.2M | 30 | 92% | 77.9% | 300.8 | 17.4% | 530s |
| C2T2 | 2 | 2 | 41.8M | 14 | 93% | 79.6% | 132.2 | 24.4% | 730s |

---

## 詳細分析

### 1. Phase 1（ContextBlock学習）

| Config | Iterations | Conv% | ER% | 収束速度 |
|--------|------------|-------|-----|----------|
| C1T1 | 30 | 92% | 79.7% | 遅い |
| **C2T1** | **14** | **93%** | 79.6% | **速い** |
| C1T2 | 30 | 92% | 77.9% | 遅い |
| **C2T2** | **14** | **93%** | 79.6% | **速い** |

**発見**:
- ContextBlockが**2層**の場合、収束が**2倍以上速い**（14 iter vs 30 iter）
- TokenBlockの層数はPhase 1の収束速度に影響しない（当然、Phase 1ではTokenBlock未使用）

### 2. Phase 2（TokenBlock学習）

| Config | Best Epoch | Train PPL | Val PPL | Val Acc | 学習パラメータ |
|--------|------------|-----------|---------|---------|---------------|
| **C1T1** | - | - | **127.2** | **24.7%** | ~976K |
| C2T1 | 18 | 93.1 | 138.7 | 23.2% | 978K |
| C1T2 | 3 | 135.8 | 300.8 | 17.4% | 1,954K |
| C2T2 | 9 | 79.7 | 132.2 | 24.4% | 1,954K |

**発見**:
- **C1T1が最良**: Val PPL 127.2、Acc 24.7%
- **C1T2が大幅に性能低下**: Val PPL 300.8、Acc 17.4%（最悪）
- TokenBlock 2層化は性能向上に繋がらない

### 3. パラメータ効率

| Config | Total Params | Phase 2 学習対象 | 効率（Acc/Params） |
|--------|-------------|-----------------|-------------------|
| **C1T1** | **40.2M** | 976K (2.4%) | **24.7% / 40.2M** |
| C2T1 | 40.8M | 978K (2.4%) | 23.2% / 40.8M |
| C1T2 | 41.2M | 1,954K (4.7%) | 17.4% / 41.2M |
| C2T2 | 41.8M | 1,954K (4.7%) | 24.4% / 41.8M |

---

## 重要な知見

### 1. C1T1（1層+1層）が最良

- **最高性能**: Val PPL 127.2、Val Acc 24.7%
- **最小パラメータ**: 40.2M
- シンプルな構成が最も効果的

### 2. 層を増やしても性能向上しない

| 比較 | PPL変化 | Acc変化 |
|------|--------|--------|
| C1T1 → C2T1 | 127.2 → 138.7 (+9%) | 24.7% → 23.2% (-1.5%) |
| C1T1 → C2T2 | 127.2 → 132.2 (+4%) | 24.7% → 24.4% (-0.3%) |
| C1T1 → C1T2 | 127.2 → 300.8 (+137%) | 24.7% → 17.4% (-7.3%) |

- **ContextBlock 2層化**: Phase 1収束は速くなるが、最終性能は低下
- **TokenBlock 2層化**: C1T2は大幅悪化、C2T2でも改善せず
- **G案（prev/current context）の効果は限定的**

### 3. C1T2は危険な組み合わせ

- ContextBlock 1層 + TokenBlock 2層は避けるべき
- TokenBlockだけ深くしても文脈表現が追いつかない

---

## 推奨構成

### 最良: C1T1（ContextBlock 1層 + TokenBlock 1層）

```python
context_layers = 1
token_layers = 1
# または
num_layers = 1
```

- Val PPL: **127.2**（最良）
- Val Acc: **24.7%**（最高）
- パラメータ: **40.2M**（最小）
- シンプルで効率的

### 代替: C2T2（ContextBlock 2層 + TokenBlock 2層）

```python
context_layers = 2
token_layers = 2
# または
num_layers = 2
```

- Val PPL: 132.2
- Val Acc: 24.4%
- Phase 1収束が速い（14 iter vs 30 iter）
- 収束速度を優先する場合に選択

### 非推奨

- **C1T2**: 性能大幅低下（PPL 300.8）- **絶対に避ける**
- **C2T1**: 性能が中途半端（PPL 138.7）

---

## 結論

1. **C1T1（1層+1層）が最良構成** - 性能・効率ともに最高
2. **層を増やしても性能は向上しない** - シンプルな方が良い
3. **ContextBlock層数 >= TokenBlock層数** が安定（C1T2は危険）
4. **G案の効果は限定的** - 2層でのprev/current差分活用は性能向上に繋がらず

---

## 設定変更

現在の`num_layers = 2`を`num_layers = 1`に変更することを推奨。

```python
# config.py
num_layers = 1  # C1T1が最良
```

---

*Last Updated: 2025-12-02*
