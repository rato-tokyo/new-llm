# Context Dim Search 実験結果（2000サンプル）

**実験日**: 2025-12-02/03
**サンプル数**: 2000（Train: 2,403,563 tokens, Val: 22,723 tokens）
**環境**: NVIDIA L4 (22.2GB)
**探索範囲**: dim=100〜500（step=100）

## 結果サマリー

| dim | Val PPL | Val Acc | ER% | Phase 1 Iter | Phase 1 Conv | Best Epoch | 総時間 |
|-----|---------|---------|-----|--------------|--------------|------------|--------|
| 100 | 146.4 | 22.4% | 75.9% | 19 | 91% | 19 | 1653s |
| 200 | 133.5 | 23.3% | 70.9% | 23 | 90% | 18 | 1670s |
| 300 | 130.4 | 23.6% | 66.5% | 24 | 90% | 18 | 1700s |
| 400 | **130.0** | **24.0%** | 61.4% | 28 | 91% | 19 | 1874s |
| 500 | (途中切断) | - | 57.7% | 32 | 93% | - | - |

## 主要な発見

### 1. PPL改善は収益逓減

```
dim 100→200: -12.9 PPL (大幅改善)
dim 200→300: -3.1 PPL
dim 300→400: -0.4 PPL (ほぼ横ばい)
```

**結論**: dim=300以降はPPL改善が急激に減速する。

### 2. Effective Rank（ER%）は次元増加で低下

| dim | ER% | 絶対値 |
|-----|-----|--------|
| 100 | 75.9% | 75.9/100 |
| 200 | 70.9% | 141.7/200 |
| 300 | 66.5% | 199.5/300 |
| 400 | 61.4% | 245.8/400 |
| 500 | 57.7% | 288.5/500 |

**結論**: 次元を増やしても「有効に使われる次元」の割合は減少する。dim=500では約42%の次元が効果的に使われていない。

### 3. Phase 1収束時間は次元に比例

| dim | Phase 1 Iter | Phase 1 時間 |
|-----|--------------|--------------|
| 100 | 19 | 87.9s |
| 200 | 23 | 154.1s |
| 300 | 24 | 180.4s |
| 400 | 28 | 265.3s |
| 500 | 32 | 344.9s |

**結論**: 大きな次元は収束に時間がかかる。dim=500はdim=100の約4倍。

### 4. Phase 2 Prep（キャッシュ収集）は470〜510秒で安定

チャンク処理により、GPUメモリは2GB程度で安定（以前は17GB+消費していた問題を解決済み）。

## 推奨設定

### 最適 context_dim: **300〜400**

**理由**:
1. PPL改善が最も効率的な範囲（dim=300で130.4、dim=400で130.0）
2. ER%が60%以上を維持（十分な次元活用）
3. 計算時間が妥当（Phase 1: 180〜265秒）

### dim=500以上は非推奨

**理由**:
1. PPL改善が極めて小さい（dim=400からの改善予測: 0.1未満）
2. ER%が57.7%に低下（43%の次元が無駄）
3. Phase 1収束に344秒以上かかる
4. パラメータ数増加によるオーバーフィッティングリスク

## 比較: 以前の実験（100/200サンプル）との違い

| サンプル数 | 最適dim | Best PPL |
|-----------|---------|----------|
| 100 | 300 | 256.7 |
| 200 | 300 | 196.4 |
| **2000** | **400** | **130.0** |

**観察**: サンプル数が増えると、より大きなdimが最適になる傾向。ただし、2000サンプルでもdim=300とdim=400の差は0.4 PPLのみ。

## 結論

1. **推奨設定**: `context_dim=300`（コスト効率最優先）または `context_dim=400`（最高性能）
2. **dim=500以上は避ける**: 計算コスト増加に対してPPL改善が見合わない
3. **ER%監視**: 60%以下に落ちたら次元が大きすぎる可能性

---

**注記**: dim=500の実験はPhase 2 Epoch 14で切断されたため、最終PPLは不明。ただし、Epoch 14時点でval_ppl=136.7であり、dim=400の最終値130.0を上回る可能性が高い。

---

## 追加実験: 1000サンプルでの細かい探索（dim=200〜300, step=20）

**実験日**: 2025-12-03
**サンプル数**: 1000（Train: 1,196,134 tokens, Val: 22,723 tokens）
**環境**: NVIDIA L4 (22.2GB)
**探索範囲**: dim=200〜300（step=20）

### 結果サマリー（1000サンプル）

| dim | Val PPL | Val Acc | ER% | Phase 1 Iter | Phase 1 Conv | Best Epoch | 総時間 |
|-----|---------|---------|-----|--------------|--------------|------------|--------|
| 200 | 154.5 | 22.6% | 71.0% | 25 | 92% | 20 | 888s |
| 220 | 151.8 | 22.5% | 69.4% | 25 | 92% | 20 | 891s |
| 240 | 151.5 | 22.6% | 68.4% | 24 | 92% | 21 | 918s |
| 260 | (途中切断) | - | - | 25+ | 91%+ | - | - |

### 観察

1. **PPL改善は緩やか**: dim=200→240で約3 PPL改善（154.5→151.5）
2. **ER%は安定して低下**: 71.0%→68.4%（20刻みで約1.3%ずつ低下）
3. **Phase 1収束は安定**: 24〜25イテレーションで92%程度に収束
4. **1000サンプルでのPPLは2000サンプルより高い**: dim=200で154.5（1000）vs 133.5（2000）

### 1000サンプル vs 2000サンプル比較

| dim | 1000サンプル PPL | 2000サンプル PPL | 差 |
|-----|-----------------|-----------------|-----|
| 200 | 154.5 | 133.5 | +21.0 |

**結論**: サンプル数を2倍にすると、PPLが約21ポイント改善する（dim=200の場合）。

### 暫定結論（1000サンプル実験）

- **最適dim**: dim=240付近が最も効率的（PPL=151.5、ER%=68.4%）
- **dim=200以下の探索価値**: ER%が高い（71%+）ため、計算効率を優先する場合は検討の余地あり
