# v5 vs v6 実験比較分析: Phase 1パラメータの影響

**実験日**: 2025-11-30
**比較対象**: 1130_v5 vs 1130_v6

## 実験条件の違い

| パラメータ | v5 | v6 | 変更 |
|-----------|-----|-----|------|
| `phase1_max_iterations` | 60 | 100 | +67% |
| `phase1_gradient_clip` | 1.0 | 2.0 | +100% |

**共通条件**:
- アーキテクチャ: 1L_1085d_1tok（1層、context_dim=1085、1トークン入力）
- サンプルサイズ: 50, 100, 200, 400, 800
- Phase 2: 20 epochs（同一条件）

---

## 結果サマリー

### 最終性能（800サンプル時）

| 指標 | v5 (60iter, clip=1.0) | v6 (100iter, clip=2.0) | 差分 |
|------|----------------------|------------------------|------|
| **Val PPL** | **180.0** | 187.4 | +7.4 (+4.1%) |
| **Val Acc** | **24.1%** | 23.9% | -0.2% |
| **α (スケーリング係数)** | **-0.4404** | -0.4218 | +0.019 |
| **Val ER** | 70.8% | 66.1% | -4.7% |
| **Train ER** | 72.2% | 67.7% | -4.5% |
| **Phase 1 収束率** | 83% | 95% | +12% |
| **実行時間** | 28.3分 | 39.8分 | +11.5分 (+41%) |

---

## 詳細分析

### 1. Effective Rank (ER) の変化

v6では収束率が高い（95%）が、ERは低下している。

| サンプル数 | v5 Val ER | v6 Val ER | 差分 |
|-----------|-----------|-----------|------|
| 50 | 71.0% | 66.9% | -4.1% |
| 100 | 70.8% | 66.4% | -4.4% |
| 200 | 70.8% | 66.7% | -4.1% |
| 400 | 70.9% | 66.4% | -4.5% |
| 800 | 70.8% | 66.1% | -4.7% |

**観察**: v6は全サンプルサイズで一貫してERが4-5%低い。

### 2. PPLの変化（全サンプルサイズ）

| サンプル数 | v5 Val PPL | v6 Val PPL | 差分 |
|-----------|------------|------------|------|
| 50 | 598.6 | 592.6 | -6.0 (-1.0%) |
| 100 | 406.9 | 409.8 | +2.9 (+0.7%) |
| 200 | 298.8 | 305.4 | +6.6 (+2.2%) |
| 400 | 227.0 | 234.7 | +7.7 (+3.4%) |
| 800 | **180.0** | 187.4 | +7.4 (+4.1%) |

**観察**: 50サンプルではv6がわずかに良いが、サンプル数が増えるとv5が優位。

### 3. αスケーリングの変化

| ウィンドウ | v5 α | v6 α | 差分 |
|-----------|------|------|------|
| Window 1 (50-400) | -0.478 | -0.457 | +0.021 |
| Window 2 (100-800) | -0.399 | -0.383 | +0.016 |
| **全体 α** | **-0.440** | -0.422 | +0.018 |

**観察**: v5の方がαが急峻（よりデータ効率が良い）。

### 4. Phase 1 収束の推移

**v5 (60 iterations, clip=1.0)**:
```
Iter 30: conv=25%, loss=-0.4423
Iter 40: conv=57%, loss=-0.4530
Iter 50: conv=74%, loss=-0.4628
Iter 60: conv=82%, loss=-0.4722  ← 終了
```

**v6 (100 iterations, clip=2.0)**:
```
Iter 30: conv=25%, loss=-0.4358
Iter 40: conv=57%, loss=-0.4464
Iter 50: conv=74%, loss=-0.4561
Iter 60: conv=83%, loss=-0.4654
...
Iter 100: conv=95%, loss=-0.5014  ← 終了
```

**観察**:
- 60iterまでは両者ほぼ同じloss推移
- v6は追加の40iterで収束率を95%まで上げたが、loss改善は-0.4722→-0.5014で限定的

---

## パラメータ変更の影響分析

### 1. max_iterations増加（60→100）の影響

**メリット**:
- 収束率が82%→95%に向上（+13%）
- より多くのトークンが固定点に収束

**デメリット**:
- ERが4-5%低下（多様性の喪失）
- Val PPLが悪化（+7.4）
- 実行時間が+41%増加

**考察**:
追加のイテレーションは「より多くのトークンを似たような固定点に収束させる」ことに成功したが、その結果として**多様性が失われ、表現力が低下**した。固定点学習では収束率の追求よりも適切なERの維持が重要。

### 2. gradient_clip増加（1.0→2.0）の影響

**理論的な影響**:
- より大きな勾配更新を許容
- 学習が高速化する可能性

**実際の影響**:
- 60iter時点のlossはv6の方がやや低い（-0.4654 vs -0.4722 at iter 60）
  - ただしv6はiter60なので同じiter数での比較ではない
- 最終的なPPLはv5が優位

**考察**:
gradient_clip=2.0は学習を不安定にしているわけではないが、明確なメリットも見られない。1.0は保守的だが安定した設定。

---

## 結論と推奨設定

### 主要な発見

1. **ERとPPLにはトレードオフがある**: 収束率を上げるとERが下がり、PPLが悪化
2. **60 iterationsで十分**: 追加のiterationは逆効果
3. **gradient_clip=1.0が適切**: 2.0への変更によるメリットなし

### ER-PPL相関

| 実験 | Val ER | Val PPL |
|------|--------|---------|
| v5 (800samples) | 70.8% | **180.0** |
| v6 (800samples) | 66.1% | 187.4 |
| v4 (800samples) | 70.8% | 180.0 |

**一貫した傾向**: Val ER ≈ 70%前後が最適で、ERが低下するとPPLが悪化。

### 推奨設定

```python
phase1_max_iterations = 60           # 100は過剰
phase1_gradient_clip = 1.0           # 業界標準で十分
```

### 今後の検討

1. **ERを維持しながら収束率を上げる方法**: dist_reg_weightの調整
2. **最適なイテレーション数の探索**: 40, 50, 60, 70での比較
3. **gradient_clip < 1.0の検証**: 0.5など、より保守的な設定

---

## 生データ

### v5 詳細結果
```
Config             Samples     Tokens P1 Iter Train ER  Val ER   T.PPL   V.PPL    Acc
1L_1085d_1tok           50     62,891      60    71.7%   71.0%    91.1   598.6  17.8%
1L_1085d_1tok          100    122,795      60    71.9%   70.8%    95.9   406.9  19.6%
1L_1085d_1tok          200    240,132      60    71.9%   70.8%   105.5   298.8  20.8%
1L_1085d_1tok          400    473,429      60    72.2%   70.9%    96.0   227.0  22.2%
1L_1085d_1tok          800    948,524      60    72.2%   70.8%    90.7   180.0  24.1%
```

### v6 詳細結果
```
Config             Samples     Tokens P1 Iter Train ER  Val ER   T.PPL   V.PPL    Acc
1L_1085d_1tok           50     62,891     100    67.6%   66.9%    88.0   592.6  17.8%
1L_1085d_1tok          100    122,795     100    67.6%   66.4%   103.0   409.8  19.4%
1L_1085d_1tok          200    240,132     100    67.9%   66.7%   112.0   305.4  20.5%
1L_1085d_1tok          400    473,429     100    67.9%   66.4%   102.4   234.7  21.8%
1L_1085d_1tok          800    948,524     100    67.7%   66.1%    97.0   187.4  23.9%
```

### Alpha Progression

**v5**:
- Window 1 (50-400): α = -0.478, R² = 0.993
- Window 2 (100-800): α = -0.399, R² = 0.995
- Δα = +0.079 (+16.5%)

**v6**:
- Window 1 (50-400): α = -0.457, R² = 0.993
- Window 2 (100-800): α = -0.383, R² = 0.996
- Δα = +0.074 (+16.1%)
