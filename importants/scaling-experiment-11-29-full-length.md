# スケーリング実験 2025-11-29（全長トークン化 + 11/27設定再現）

## 概要

11/27実験の設定（`token_input_all_layers=True`, `Embedding freeze=False`）を、全長トークン化（`truncation=False`）で再現した実験。

**スケーリング則**: `PPL = 75,687 × tokens^(-0.41)`

**α = -0.41** (2倍のデータで25% PPL削減)

**R² = 0.982** (高い適合度)

---

## 実験設定

| 設定 | 値 |
|------|-----|
| GPU | NVIDIA L4 (22.2GB) |
| モデル | 6層, context_dim=768, embed_dim=768 |
| パラメータ | 91.43M (うちPhase 2学習対象: 45.74M) |
| num_input_tokens | 1 |
| token_input_all_layers | **True** (11/27設定) |
| Embedding freeze | **False** (11/27設定) |
| トークン化 | **truncation=False (全長)** |
| 乱数シード | 42 |
| Phase 1 | max_iterations=40, early stopping |
| Phase 2 | epochs=10, patience=1 |

---

## 実験結果

### 実験1: 50サンプル

| フェーズ | 項目 | 値 |
|---------|------|-----|
| データ | 訓練トークン | 56,602 |
| データ | 検証トークン | 6,289 |
| Phase 1 | 収束イテレーション | 7/40 (early stop) |
| Phase 1 | 収束率 | 99.3% |
| Phase 1 | Train ER | 75.1% |
| Phase 1 | Val ER | 71.6% |
| Phase 1 | 所要時間 | 1.3min |
| Phase 2 | Best Epoch | 7/10 (early stop at 8) |
| Phase 2 | **Val PPL** | **799.42** |
| Phase 2 | **Val Acc** | **17.11%** |
| Phase 2 | 所要時間 | 1.4min |
| | キャッシュサイズ | 994.9MB + 110.5MB |

### 実験2: 100サンプル

| フェーズ | 項目 | 値 |
|---------|------|-----|
| データ | 訓練トークン | 110,516 |
| データ | 検証トークン | 12,279 |
| Phase 1 | 収束イテレーション | 7/40 (early stop) |
| Phase 1 | 収束率 | 99.3% |
| Phase 1 | Train ER | 75.6% |
| Phase 1 | Val ER | 73.0% |
| Phase 1 | 所要時間 | 2.5min |
| Phase 2 | Best Epoch | 6/10 (early stop at 7) |
| Phase 2 | **Val PPL** | **680.12** |
| Phase 2 | **Val Acc** | **17.77%** |
| Phase 2 | 所要時間 | 2.8min |
| | キャッシュサイズ | 1942.6MB + 215.8MB |

### 実験3: 200サンプル

| フェーズ | 項目 | 値 |
|---------|------|-----|
| データ | 訓練トークン | 216,119 |
| データ | 検証トークン | 24,013 |
| Phase 1 | 収束イテレーション | 7/40 (early stop) |
| Phase 1 | 収束率 | 99.2% |
| Phase 1 | Train ER | 76.3% |
| Phase 1 | Val ER | 73.7% |
| Phase 1 | 所要時間 | 4.9min |
| Phase 2 | Best Epoch | 4/10 (early stop at 5) |
| Phase 2 | **Val PPL** | **500.12** |
| Phase 2 | **Val Acc** | **20.81%** |
| Phase 2 | 所要時間 | 5.1min |
| | キャッシュサイズ | 3798.9MB + 422.1MB |

### 実験4: 500サンプル ✅ 成功

| フェーズ | 項目 | 値 |
|---------|------|-----|
| データ | 訓練トークン | 529,173 |
| データ | 検証トークン | 58,797 |
| Phase 1 | 収束イテレーション | 7/40 (early stop) |
| Phase 1 | 収束率 | 99.2% |
| Phase 1 | Train ER | 75.9% |
| Phase 1 | Val ER | 75.5% |
| Phase 1 | 所要時間 | 11.7min |
| Phase 2 | キャッシュ構築 | 627.2s |
| Phase 2 | キャッシュサイズ | 9301.9MB + 1033.5MB = 10.09GB |
| Phase 2 | バッチサイズ自動調整 | 8864 → 3171 |
| Phase 2 | Best Epoch | 2/10 (early stop at 3) |
| Phase 2 | **Val PPL** | **325.00** |
| Phase 2 | **Val Acc** | **18.76%** |
| Phase 2 | 所要時間 | 11.5min |
| | **合計時間** | **24.1min** |

---

## スケーリング則（全4実験）

```
PPL = A × tokens^α

α = -0.4109
A = 75,686.99
R² = 0.9820
p-value = 0.0090
```

| サンプル数 | トークン数 | Val PPL | Val Acc |
|-----------|-----------|---------|---------|
| 50 | 56,602 | 799.42 | 17.11% |
| 100 | 110,516 | 680.12 | 17.77% |
| 200 | 216,119 | 500.12 | 20.81% |
| 500 | 529,173 | 325.00 | 18.76% |

**解釈**: 2倍のデータで約25% PPL削減

### PPL予測（外挿）

| トークン数 | 予測PPL |
|-----------|---------|
| 1,000,000 | 259 |
| 2,000,000 | 195 |
| 5,000,000 | 134 |
| 10,000,000 | 101 |

---

## 11/27実験との比較

| 項目 | 11/27実験 | 本実験 (11/29) |
|------|----------|---------------|
| トークン化 | max_length=128 | truncation=False (全長) |
| tokens/sample | ~80 | ~1,058 (13倍) |
| α値 | **-0.75** | **-0.41** |
| 500samples時PPL | 1,145 | **325.00** |

**考察**:
- 全長トークン化により、同じサンプル数でもトークン数が13倍に増加
- α値が-0.75から-0.41に変化（異なるスケールでの測定）
- **絶対的なPPLは大幅改善**: 1,145 → 325（72%削減）
- 同じトークン数で比較すると、全長トークン化の方が効率的

---

## Phase 1 分析

### Effective Rank推移

| サンプル数 | Train ER | Val ER |
|-----------|----------|--------|
| 50 | 75.1% | 71.6% |
| 100 | 75.6% | 73.0% |
| 200 | 76.3% | 73.7% |
| 500 | 75.9% | 75.5% |

**観察**:
- Effective Rankは75-76%で安定
- データ量増加によるER低下は見られない
- 検証データのERも70-75%を維持
- 500サンプルでVal ERが向上（75.5%）

### 収束特性

- 全実験で7イテレーションで早期終了
- 収束率は99%以上を達成
- CVFPロスは安定して減少

---

## 技術的問題と解決策

### 500サンプルでのOOM問題（解決済み）

**問題**:
1. キャッシュサイズ: 10.09GB (GPU総容量の45%)
2. モデル + 勾配: 約11GB
3. backward時に約1.65GB追加で必要

**解決策（実装済み）**:
```python
# src/trainers/phase2.py に自動メモリ管理を追加
- キャッシュ構築後にGPUメモリ状態を確認
- 空きメモリからバッチサイズを自動計算
- backward用に3.5倍のメモリマージン
- 安全係数0.5 × 追加マージン0.5を適用
- バッチサイズ: 8864 → 3171に自動調整
```

### 評価時のOOM問題（解決済み）

**問題**:
- 58,797トークンを一度にforward
- logits (58797 × 50257 × 4bytes) = 11GB必要

**解決策**:
```python
# evaluateメソッドにバッチ処理を追加
- 評価時もバッチ分割（デフォルト8192トークン）
- backwardがないので訓練時より大きなバッチ可
```

---

## 結論

1. **500サンプル実験成功**: Val PPL 325.00達成
2. **スケーリング則確定**: α = -0.41 (R² = 0.98)
3. **メモリ管理改善**: 自動バッチサイズ調整でOOM回避
4. **全長トークン化の効果**: 同じサンプル数でより多くの情報を学習

---

## 次のステップ

1. **1000サンプル以上での検証**
   - α値の収束挙動を確認
   - より正確なスケーリング則の導出

2. **異なる設定での比較**
   - Embedding freeze=True での実験
   - token_input_all_layers=False での実験

3. **コードのリファクタリング**
   - メモリ管理の一元化
   - train/evaluate の共通バッチ処理

---

## 関連ファイル

- [scaling-experiments-summary-2025-11-28.md](scaling-experiments-summary-2025-11-28.md) - 過去実験のまとめ
- [scripts/unified_scaling_experiment.py](../scripts/unified_scaling_experiment.py) - 実験スクリプト
- [src/utils/memory.py](../src/utils/memory.py) - メモリ管理ユーティリティ

---

## 実験ログ

| 実験 | ログファイル | 開始時刻 |
|------|-------------|---------|
| 実験1-3 | `colab_1_3.txt` | 2025-11-29 03:12:47 |
| 実験4 | `colab_4.txt` | 2025-11-29 06:22:23 |
