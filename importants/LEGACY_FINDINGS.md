# Legacy Findings（過去の設計からの知見）

**最終更新**: 2025-12-03

このドキュメントは、過去の実験から得られた重要な知見をまとめたものです。
新しい設計との比較や、将来の改善の参考にしてください。

---

## 1. Embedding凍結の発見（2025-11-27）

**結論**: 小規模データでは**Embedding凍結が必須**

| 指標 | Embedding学習 | Embedding凍結 | 改善率 |
|------|--------------|--------------|--------|
| Val PPL | 1189 | **334** | **-72%** |
| Val Acc | 11.6% | **18.9%** | **+63%** |
| 学習パラメータ | 49.2M | **7.09M** | **-86%** |

**要因**:
- 過学習抑制（パラメータ/データ比が改善）
- GPT-2の意味表現を保持
- Weight Tyingとの相乗効果

---

## 2. レイヤー数の実験（2025-11-26）

**結論**: **1層が最適**。multi-layerは複雑化しても改善なし。

| レイヤー数 | Val PPL | Val Acc | 問題点 |
|-----------|---------|---------|--------|
| **1層** | **127.2** | **24.7%** | なし |
| 2層 | 132.2 | 24.4% | 微悪化 |
| 3層 | 774 | 16.2% | 過学習 |
| 6層 | 996 | 13.8% | 大幅悪化 |

**C1T2（Token層だけ深い）は性能崩壊**（PPL=300）

---

## 3. 多様性アルゴリズム比較（2025-12-01）

**結論**: **OACDを採用**（シンプルかつ効果的）

| Algorithm | Val ER% | 計算コスト | 特徴 |
|-----------|---------|-----------|------|
| SDL | 94.9% | 27.4ms | 最高ER、高コスト |
| NUC | 91.9% | 27.9ms | 高ER、高コスト |
| ODCM | 89.2% | 0.72ms | 高ER、低コスト |
| MCDL | 78.4% | 0.20ms | 現行、最速 |

**OACDはMCDLを改良したもの**。分散最大化+重心固定で安定。

### 削除されたアルゴリズム
- CVFP: 多様性損失のみで収束可能
- ODCM/SDL/NUC: OACDに統合

---

## 4. context_dim探索（1-block, 2025-11-30）

**結論**: 1-blockではcontext_dim=300-400が最適

| サンプル数 | 最適dim | Best PPL | ER% |
|-----------|---------|----------|-----|
| 100 | 300 | 327 | 66% |
| 200 | 300 | 254 | 66% |
| 1000 | 240 | 152 | 68% |
| 2000 | 400 | 130 | 61% |

**注意**: これは**1-block**の結果。現在は**2-block（cd=256×2）**が主流。

---

## 5. 過学習抑制の知見

### コンテキストノイズ（2025-11-26）

| 設定 | 継続改善エポック | Final Val Acc |
|------|-----------------|---------------|
| ノイズなし | 1 | 17.4% |
| **ノイズあり (0.1)** | **6** | **20.8%** |

**効果**: Train/Val乖離を12倍縮小

### 現在の対策
- Embedding凍結（最も効果的）
- Early Stopping 90%
- prev_context_steps（時系列情報活用）

---

## 6. スケーリング則の進化

### 初期（Power Law）
```
PPL = A × n^α
```
問題: 理論限界なし（非現実的）

### 中期（指数減衰）
```
PPL = PPL_min + A × exp(-b × n^c)
```
問題: パラメータ過多、外挿精度低い

### 現在（飽和モデル）
```
PPL = PPL_min + A × n^(-a)
```
利点: AIC最小、外挿精度高い（誤差1.2%）

---

## 7. 失敗した設計パターン

| パターン | 結果 | 理由 |
|---------|------|------|
| C1T2（Token層だけ深い） | PPL=300（崩壊） | Token層のみ深くすると過学習 |
| multi-layer | 悪化 | 複雑化しても改善なし |
| early_stopping=0.99 | 悪化 | 過収束 |
| interval=8（prev_context） | PPL+8% | 離れた履歴は無関係 |
| context_dim=1000（単体） | 非効率 | 500×2連結の方が効率的 |
| CVFP損失 | 不要 | 多様性損失のみで収束 |

---

## 8. 性能比較サマリー

### アーキテクチャ別（1600 samples）

| 構成 | Val PPL | Val Acc |
|------|---------|---------|
| 1-block (cd=256) | 134.7 | - |
| **2-block (p=0)** | **127.4** | **24.5%** |
| 2-block (p=1) | 118.2 | 25.3% |
| 2-block (p=2) | 114.7 | 25.7% |

### 理論限界値（PPL_min）

| 構成 | PPL_min | 改善方法 |
|------|---------|---------|
| 2-block (p=0) | 95.4 | baseline |
| 2-block (p=2) | 87.3 | prev_context |
| 次のステップ | <87 | アーキテクチャ変更必要 |

---

## 9. 今後の検討事項

1. **3-block, 4-block**: ブロック数増加で表現力向上？
2. **context_dim増加**: 256→320, 384 での検証
3. **prev_context_steps=3+**: さらに履歴を増やす効果
4. **新しい多様性損失**: ER向上の別アプローチ

---

*Last Updated: 2025-12-03*
