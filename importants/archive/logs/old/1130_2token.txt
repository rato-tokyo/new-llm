======================================================================
Config: 1L_768d_2tok
  num_layers=1, context_dim=768, num_input_tokens=2
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0180 [2.5s]
  Iter 3: conv=0% loss=-0.1126 [2.5s]
  Iter 4: conv=0% loss=-0.1608 [2.5s]
  Iter 5: conv=0% loss=-0.1893 [2.5s]
  Iter 6: conv=0% loss=-0.2077 [2.5s]
  Iter 7: conv=1% loss=-0.2201 [2.5s]
  Iter 8: conv=2% loss=-0.2285 [2.6s]
  Iter 9: conv=4% loss=-0.2342 [2.5s]
  Iter 10: conv=6% loss=-0.2384 [2.6s]
  Iter 11: conv=13% loss=-0.2414 [2.6s]
  Iter 12: conv=27% loss=-0.2437 [2.6s]
  Iter 13: conv=51% loss=-0.2454 [2.5s]
  Iter 14: conv=75% loss=-0.2466 [2.6s]
  Iter 15: conv=89% loss=-0.2476 [2.5s]
  Iter 16: conv=95% loss=-0.2485 [2.5s]
  Iter 17: conv=97% loss=-0.2493 [2.5s]
  Iter 18: conv=98% loss=-0.2500 [2.6s]
  Iter 19: conv=99% loss=-0.2507 [2.5s]
  Iter 20: conv=99% loss=-0.2514 [2.5s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.4s]
  ER: train=83.5%, val=83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=33359.8 val_ppl=4041.7 acc=9.0% [1.9s] ★
  Epoch 2: train_ppl=1363.9 val_ppl=1461.1 acc=14.4% [1.9s] ★
  Epoch 3: train_ppl=445.1 val_ppl=969.4 acc=15.8% [1.9s] ★
  Epoch 4: train_ppl=251.8 val_ppl=765.5 acc=16.7% [2.0s] ★
  Epoch 5: train_ppl=165.0 val_ppl=667.6 acc=17.3% [2.0s] ★
  Epoch 6: train_ppl=118.7 val_ppl=614.7 acc=17.6% [2.0s] ★
  Epoch 7: train_ppl=89.3 val_ppl=591.3 acc=18.1% [2.0s] ★
  Epoch 8: train_ppl=67.9 val_ppl=600.0 acc=18.1% [2.0s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=591.3, acc=18.1%
  Result: PPL=591.3, Acc=18.1%
    → PPL: 591.3, Acc: 18.1%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0194 [5.1s]
  Iter 3: conv=0% loss=-0.1135 [5.3s]
  Iter 4: conv=0% loss=-0.1619 [5.2s]
  Iter 5: conv=0% loss=-0.1905 [5.2s]
  Iter 6: conv=0% loss=-0.2092 [5.2s]
  Iter 7: conv=1% loss=-0.2217 [5.2s]
  Iter 8: conv=1% loss=-0.2302 [5.2s]
  Iter 9: conv=2% loss=-0.2359 [5.1s]
  Iter 10: conv=5% loss=-0.2401 [5.2s]
  Iter 11: conv=11% loss=-0.2432 [5.2s]
  Iter 12: conv=25% loss=-0.2455 [5.1s]
  Iter 13: conv=50% loss=-0.2472 [5.1s]
  Iter 14: conv=75% loss=-0.2485 [5.2s]
  Iter 15: conv=89% loss=-0.2495 [5.2s]
  Iter 16: conv=95% loss=-0.2504 [5.2s]
  Iter 17: conv=97% loss=-0.2512 [5.2s]
  Iter 18: conv=98% loss=-0.2519 [5.2s]
  Iter 19: conv=99% loss=-0.2526 [5.1s]
  Iter 20: conv=99% loss=-0.2533 [5.2s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.0s]
  ER: train=83.9%, val=83.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=10312.9 val_ppl=1526.9 acc=13.7% [3.4s] ★
  Epoch 2: train_ppl=694.6 val_ppl=733.0 acc=16.8% [3.4s] ★
  Epoch 3: train_ppl=337.4 val_ppl=531.7 acc=17.9% [3.4s] ★
  Epoch 4: train_ppl=213.0 val_ppl=452.1 acc=18.5% [3.4s] ★
  Epoch 5: train_ppl=153.2 val_ppl=415.9 acc=19.0% [3.4s] ★
  Epoch 6: train_ppl=115.3 val_ppl=402.7 acc=19.5% [3.4s] ★
  Epoch 7: train_ppl=89.6 val_ppl=400.3 acc=19.8% [3.4s] ★
  Epoch 8: train_ppl=71.1 val_ppl=406.0 acc=19.9% [3.4s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=400.3, acc=19.8%
  Result: PPL=400.3, Acc=19.8%
    → PPL: 400.3, Acc: 19.8%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0167 [10.1s]
  Iter 3: conv=0% loss=-0.1108 [10.0s]
  Iter 4: conv=0% loss=-0.1592 [9.9s]
  Iter 5: conv=0% loss=-0.1875 [10.1s]
  Iter 6: conv=0% loss=-0.2060 [10.1s]
  Iter 7: conv=0% loss=-0.2184 [10.0s]
  Iter 8: conv=2% loss=-0.2268 [9.9s]
  Iter 9: conv=2% loss=-0.2325 [10.0s]
  Iter 10: conv=4% loss=-0.2367 [10.1s]
  Iter 11: conv=9% loss=-0.2398 [10.0s]
  Iter 12: conv=18% loss=-0.2421 [10.1s]
  Iter 13: conv=37% loss=-0.2438 [10.0s]
  Iter 14: conv=61% loss=-0.2452 [9.9s]
  Iter 15: conv=79% loss=-0.2463 [10.0s]
  Iter 16: conv=88% loss=-0.2472 [10.0s]
  Iter 17: conv=93% loss=-0.2481 [10.0s]
  Iter 18: conv=95% loss=-0.2488 [10.0s]
  Iter 19: conv=97% loss=-0.2496 [10.0s]
  Iter 20: conv=98% loss=-0.2502 [10.0s]
  Iter 21: conv=98% loss=-0.2509 [10.0s]
  Iter 22: conv=99% loss=-0.2516 [10.0s]
  Iter 23: conv=99% loss=-0.2522 [9.9s]
  Iter 24: conv=99% loss=-0.2528 [10.1s]
  → Converged at iter 24
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [9.6s]
  ER: train=84.0%, val=83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=3520.0 val_ppl=733.2 acc=16.8% [6.2s] ★
  Epoch 2: train_ppl=423.9 val_ppl=434.5 acc=18.6% [6.3s] ★
  Epoch 3: train_ppl=244.7 val_ppl=352.0 acc=19.4% [6.3s] ★
  Epoch 4: train_ppl=171.1 val_ppl=319.3 acc=19.9% [6.2s] ★
  Epoch 5: train_ppl=129.1 val_ppl=303.0 acc=20.4% [6.1s] ★
  Epoch 6: train_ppl=102.1 val_ppl=295.8 acc=20.5% [6.1s] ★
  Epoch 7: train_ppl=83.2 val_ppl=295.1 acc=20.7% [6.1s] ★
  Epoch 8: train_ppl=68.8 val_ppl=298.6 acc=20.8% [6.1s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=295.1, acc=20.7%
  Result: PPL=295.1, Acc=20.7%
    → PPL: 295.1, Acc: 20.7%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 1L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0165 [24.3s]
  Iter 3: conv=0% loss=-0.1107 [24.5s]
  Iter 4: conv=0% loss=-0.1588 [24.5s]
  Iter 5: conv=0% loss=-0.1872 [24.3s]
  Iter 6: conv=0% loss=-0.2057 [24.6s]
  Iter 7: conv=1% loss=-0.2182 [24.5s]
  Iter 8: conv=1% loss=-0.2267 [24.5s]
  Iter 9: conv=2% loss=-0.2324 [24.6s]
  Iter 10: conv=5% loss=-0.2366 [24.5s]
  Iter 11: conv=11% loss=-0.2398 [24.5s]
  Iter 12: conv=25% loss=-0.2421 [24.4s]
  Iter 13: conv=49% loss=-0.2437 [24.4s]
  Iter 14: conv=74% loss=-0.2450 [24.4s]
  Iter 15: conv=88% loss=-0.2460 [24.5s]
  Iter 16: conv=94% loss=-0.2469 [24.5s]
  Iter 17: conv=97% loss=-0.2477 [24.5s]
  Iter 18: conv=98% loss=-0.2484 [24.5s]
  Iter 19: conv=99% loss=-0.2491 [24.4s]
  Iter 20: conv=99% loss=-0.2497 [24.5s]
  → Converged at iter 20
  Done: 99% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [23.5s]
  ER: train=84.0%, val=83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,953,728/43,322,880 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1150.3 val_ppl=364.2 acc=19.1% [14.7s] ★
  Epoch 2: train_ppl=265.6 val_ppl=256.6 acc=20.8% [14.6s] ★
  Epoch 3: train_ppl=176.7 val_ppl=220.2 acc=21.8% [14.4s] ★
  Epoch 4: train_ppl=133.4 val_ppl=202.8 acc=22.4% [14.3s] ★
  Epoch 5: train_ppl=107.5 val_ppl=194.1 acc=22.9% [14.3s] ★
  Epoch 6: train_ppl=90.1 val_ppl=190.3 acc=23.2% [14.4s] ★
  Epoch 7: train_ppl=77.6 val_ppl=189.4 acc=23.4% [14.5s] ★
  Epoch 8: train_ppl=68.0 val_ppl=190.8 acc=23.5% [14.6s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=189.4, acc=23.4%
  Result: PPL=189.4, Acc=23.4%
    → PPL: 189.4, Acc: 23.4%, ER: 0.0%

======================================================================
Config: 2L_768d_2tok
  num_layers=2, context_dim=768, num_input_tokens=2
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0217 [2.6s]
  Iter 3: conv=0% loss=-0.1534 [2.7s]
  Iter 4: conv=0% loss=-0.1995 [2.6s]
  Iter 5: conv=0% loss=-0.2246 [2.6s]
  Iter 6: conv=11% loss=-0.2364 [2.6s]
  Iter 7: conv=84% loss=-0.2421 [2.6s]
  Iter 8: conv=100% loss=-0.2454 [2.7s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.6s]
  ER: train=84.5%, val=84.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=10883.1 val_ppl=1677.0 acc=13.4% [2.1s] ★
  Epoch 2: train_ppl=649.3 val_ppl=869.5 acc=16.1% [2.2s] ★
  Epoch 3: train_ppl=250.7 val_ppl=686.6 acc=17.0% [2.2s] ★
  Epoch 4: train_ppl=134.0 val_ppl=622.3 acc=17.3% [2.2s] ★
  Epoch 5: train_ppl=81.2 val_ppl=629.4 acc=17.7% [2.2s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=622.3, acc=17.3%
  Result: PPL=622.3, Acc=17.3%
    → PPL: 622.3, Acc: 17.3%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0232 [5.3s]
  Iter 3: conv=0% loss=-0.1550 [5.4s]
  Iter 4: conv=0% loss=-0.2011 [5.4s]
  Iter 5: conv=0% loss=-0.2264 [5.4s]
  Iter 6: conv=10% loss=-0.2384 [5.4s]
  Iter 7: conv=82% loss=-0.2441 [5.4s]
  Iter 8: conv=100% loss=-0.2474 [5.4s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.4s]
  ER: train=85.0%, val=84.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=4201.7 val_ppl=867.4 acc=15.6% [3.8s] ★
  Epoch 2: train_ppl=444.1 val_ppl=496.9 acc=17.9% [3.9s] ★
  Epoch 3: train_ppl=214.7 val_ppl=412.6 acc=18.8% [3.9s] ★
  Epoch 4: train_ppl=127.6 val_ppl=391.9 acc=19.2% [3.9s] ★
  Epoch 5: train_ppl=79.5 val_ppl=405.6 acc=19.2% [3.8s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=391.9, acc=19.2%
  Result: PPL=391.9, Acc=19.2%
    → PPL: 391.9, Acc: 19.2%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0204 [10.7s]
  Iter 3: conv=0% loss=-0.1533 [10.8s]
  Iter 4: conv=0% loss=-0.1992 [10.5s]
  Iter 5: conv=0% loss=-0.2243 [10.5s]
  Iter 6: conv=10% loss=-0.2359 [10.6s]
  Iter 7: conv=84% loss=-0.2414 [10.6s]
  Iter 8: conv=100% loss=-0.2446 [10.6s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [10.5s]
  ER: train=85.3%, val=84.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1819.0 val_ppl=507.2 acc=17.3% [7.0s] ★
  Epoch 2: train_ppl=306.2 val_ppl=343.5 acc=19.1% [7.1s] ★
  Epoch 3: train_ppl=171.7 val_ppl=300.7 acc=20.0% [7.1s] ★
  Epoch 4: train_ppl=110.0 val_ppl=284.9 acc=20.5% [6.9s] ★
  Epoch 5: train_ppl=76.6 val_ppl=288.6 acc=20.8% [6.9s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=284.9, acc=20.5%
  Result: PPL=284.9, Acc=20.5%
    → PPL: 284.9, Acc: 20.5%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 2L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0201 [25.1s]
  Iter 3: conv=0% loss=-0.1517 [25.0s]
  Iter 4: conv=0% loss=-0.1978 [24.8s]
  Iter 5: conv=0% loss=-0.2228 [24.8s]
  Iter 6: conv=11% loss=-0.2347 [24.7s]
  Iter 7: conv=83% loss=-0.2405 [24.7s]
  Iter 8: conv=100% loss=-0.2438 [24.9s]
  → Converged at iter 8
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [25.3s]
  ER: train=85.0%, val=84.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 6,792,192/48,933,120 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=746.6 val_ppl=294.8 acc=19.4% [16.6s] ★
  Epoch 2: train_ppl=214.9 val_ppl=221.7 acc=21.3% [16.3s] ★
  Epoch 3: train_ppl=134.9 val_ppl=200.9 acc=22.3% [16.1s] ★
  Epoch 4: train_ppl=97.2 val_ppl=197.0 acc=22.8% [16.0s] ★
  Epoch 5: train_ppl=74.9 val_ppl=203.0 acc=22.9% [16.1s]
  → Early stop at epoch 5
  Best: epoch 4, ppl=197.0, acc=22.8%
  Result: PPL=197.0, Acc=22.8%
    → PPL: 197.0, Acc: 22.8%, ER: 0.0%

======================================================================
Config: 3L_768d_2tok
  num_layers=3, context_dim=768, num_input_tokens=2
======================================================================

  --- 50 samples ---

--- Experiment: 50 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
  Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 62,891 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0225 [2.8s]
  Iter 3: conv=0% loss=-0.1630 [2.7s]
  Iter 4: conv=0% loss=-0.2079 [2.8s]
  Iter 5: conv=1% loss=-0.2305 [2.7s]
  Iter 6: conv=73% loss=-0.2401 [2.8s]
  Iter 7: conv=100% loss=-0.2447 [2.7s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [2.8s]
  ER: train=83.6%, val=83.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=16042.6 val_ppl=1677.6 acc=13.6% [2.4s] ★
  Epoch 2: train_ppl=652.8 val_ppl=857.3 acc=15.9% [2.4s] ★
  Epoch 3: train_ppl=216.7 val_ppl=651.1 acc=16.9% [2.4s] ★
  Epoch 4: train_ppl=101.0 val_ppl=661.9 acc=17.5% [2.5s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=651.1, acc=16.9%
  Result: PPL=651.1, Acc=16.9%
    → PPL: 651.1, Acc: 16.9%, ER: 0.0%

  --- 100 samples ---

--- Experiment: 100 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
  Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 122,795 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0239 [5.6s]
  Iter 3: conv=0% loss=-0.1645 [5.6s]
  Iter 4: conv=0% loss=-0.2097 [5.6s]
  Iter 5: conv=1% loss=-0.2325 [5.5s]
  Iter 6: conv=74% loss=-0.2423 [5.5s]
  Iter 7: conv=100% loss=-0.2467 [5.4s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [5.6s]
  ER: train=84.1%, val=83.6%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=5078.8 val_ppl=872.3 acc=15.1% [4.3s] ★
  Epoch 2: train_ppl=414.0 val_ppl=468.1 acc=18.1% [4.3s] ★
  Epoch 3: train_ppl=177.1 val_ppl=400.8 acc=18.8% [4.3s] ★
  Epoch 4: train_ppl=94.6 val_ppl=417.1 acc=18.8% [4.3s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=400.8, acc=18.8%
  Result: PPL=400.8, Acc=18.8%
    → PPL: 400.8, Acc: 18.8%, ER: 0.0%

  --- 200 samples ---

--- Experiment: 200 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
  Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 240,132 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0211 [10.7s]
  Iter 3: conv=0% loss=-0.1642 [10.9s]
  Iter 4: conv=0% loss=-0.2079 [10.8s]
  Iter 5: conv=1% loss=-0.2300 [10.8s]
  Iter 6: conv=73% loss=-0.2395 [10.8s]
  Iter 7: conv=100% loss=-0.2439 [10.7s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [11.2s]
  ER: train=84.3%, val=83.8%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=1916.0 val_ppl=478.7 acc=16.8% [8.0s] ★
  Epoch 2: train_ppl=273.6 val_ppl=329.2 acc=18.9% [7.9s] ★
  Epoch 3: train_ppl=142.7 val_ppl=293.8 acc=20.2% [7.9s] ★
  Epoch 4: train_ppl=85.9 val_ppl=295.1 acc=20.4% [7.8s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=293.8, acc=20.2%
  Result: PPL=293.8, Acc=20.2%
    → PPL: 293.8, Acc: 20.2%, ER: 0.0%

  --- 500 samples ---

--- Experiment: 500 samples, 3L, 768d, 2tok ---
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   31024 tokens
  Data: 587,970 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] Train: 587,970 tokens, 40 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.0209 [26.2s]
  Iter 3: conv=0% loss=-0.1613 [26.0s]
  Iter 4: conv=0% loss=-0.2061 [25.6s]
  Iter 5: conv=1% loss=-0.2287 [25.8s]
  Iter 6: conv=71% loss=-0.2385 [25.7s]
  Iter 7: conv=100% loss=-0.2431 [25.9s]
  → Converged at iter 7
  Done: 100% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [27.2s]
  ER: train=84.2%, val=83.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 10,499,584/54,412,288 parameters

[Phase 2] 587,970 train / 31,024 val tokens, 10 epochs
  Epoch 1: train_ppl=723.6 val_ppl=270.1 acc=19.9% [18.5s] ★
  Epoch 2: train_ppl=194.4 val_ppl=206.7 acc=21.9% [18.2s] ★
  Epoch 3: train_ppl=118.6 val_ppl=194.6 acc=22.7% [18.1s] ★
  Epoch 4: train_ppl=83.3 val_ppl=199.3 acc=23.0% [18.1s]
  → Early stop at epoch 4
  Best: epoch 3, ppl=194.6, acc=22.7%
  Result: PPL=194.6, Acc=22.7%
    → PPL: 194.6, Acc: 22.7%, ER: 0.0%
