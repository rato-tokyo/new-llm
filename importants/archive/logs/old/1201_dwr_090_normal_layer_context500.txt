======================================================================
DIVERSITY ALGORITHM FULL EXPERIMENT
(Phase 1 + Phase 2 + α Analysis)
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Algorithms: ['MCDL', 'ODCM', 'SDL', 'NUC']
Sample sizes: [50, 100, 200]
Context dim: 500
Output: importants/logs/20251201_115147_diversity_full

Config:
  num_layers: 1
  dist_reg_weight: 0.9
  phase1_max_iterations: 60
  phase2_epochs: 20

======================================================================
Algorithm: MCDL - Mean-Centered Dispersion Loss (現行ベースライン)
======================================================================

[1/12] MCDL | ctx_dim=500 | 50 samples
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 178kB/s]
config.json: 100% 665/665 [00:00<00:00, 5.45MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 8.17MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 3.58MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 10.3MB/s]
Loading training data...
  Loading 50 samples from UltraChat...
README.md: 3.90kB [00:00, 15.8MB/s]
  Error: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 7d127772-e77b-4b24-b08a-fa27beb78dbd)')
Traceback (most recent call last):
  File "/content/new-llm/scripts/diversity_full_experiment.py", line 289, in run_all_experiments
    result = run_single_experiment(
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/content/new-llm/scripts/diversity_full_experiment.py", line 115, in run_single_experiment
    train_token_ids, val_token_ids = data_provider.load_data()
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/new-llm/src/providers/data/memory.py", line 52, in load_data
    self._train_token_ids, self._sample_order, self._sample_boundaries = self._load_train_data(tokenizer)
                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/content/new-llm/src/providers/data/memory.py", line 95, in _load_train_data
    dataset = load_dataset(
              ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/load.py", line 1392, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/load.py", line 1132, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/load.py", line 1031, in dataset_module_factory
    raise e1 from None
  File "/usr/local/lib/python3.12/dist-packages/datasets/load.py", line 1004, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/load.py", line 632, in get_module
    data_files = DataFilesDict.from_patterns(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/data_files.py", line 689, in from_patterns
    else DataFilesList.from_patterns(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/data_files.py", line 592, in from_patterns
    origin_metadata = _get_origin_metadata(data_files, download_config=download_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/data_files.py", line 506, in _get_origin_metadata
    return thread_map(
           ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py", line 69, in thread_map
    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/tqdm/contrib/concurrent.py", line 51, in _executor_map
    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/tqdm/std.py", line 1169, in __iter__
    for obj in iterable:
               ^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
  File "/usr/lib/python3.12/concurrent/futures/thread.py", line 59, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/datasets/data_files.py", line 485, in _get_single_origin_metadata
    resolved_path = fs.resolve_path(data_file)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_file_system.py", line 199, in resolve_path
    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_file_system.py", line 126, in _repo_and_revision_exist
    self._api.repo_info(
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py", line 2867, in repo_info
    return method(
           ^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py", line 2730, in dataset_info
    r = get_session().get(path, headers=headers, timeout=timeout, params=params)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py", line 95, in send
    return super().send(request, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/requests/adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: 7d127772-e77b-4b24-b08a-fa27beb78dbd)')

[2/12] MCDL | ctx_dim=500 | 100 samples
Loading training data...
  Loading 100 samples from UltraChat...
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:01<00:00, 199MB/s]
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:00<00:00, 278MB/s]
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:00<00:00, 256MB/s]
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:00<00:00, 115MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:00<00:00, 277MB/s]  
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:00<00:00, 302MB/s]
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:00<00:00, 252MB/s]
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:00<00:00, 142MB/s]
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 50029.12 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 54846.92 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 71432.62 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 75891.18 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_100samples_full.pt
Loading validation data...
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
2025-12-01 11:52:21.535357: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 11:52:21.551602: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764589941.570875    2425 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764589941.577503    2425 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764589941.594120    2425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764589941.594157    2425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764589941.594160    2425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764589941.594163    2425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 11:52:21.599213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 4.89MB/s]
model.safetensors: 100% 548M/548M [00:01<00:00, 534MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1517 [1.5s]
  Iter 3: conv=0% loss=-0.1819 [0.5s]
  Iter 4: conv=0% loss=-0.2089 [0.4s]
  Iter 5: conv=0% loss=-0.2294 [0.4s]
    Val ER: 82.5%
  Iter 6: conv=0% loss=-0.2448 [0.4s]
  Iter 7: conv=0% loss=-0.2551 [0.4s]
  Iter 8: conv=0% loss=-0.2618 [0.4s]
  Iter 9: conv=1% loss=-0.2665 [0.4s]
  Iter 10: conv=1% loss=-0.2705 [0.4s]
    Val ER: 81.6%
  → Val early stop: ER not improving for 1 checks
  Done: 1% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 13.4s, 10 iter, ER=84.2%/82.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=3497767.8 val_ppl=9489.2 acc=9.4% [2.9s] ★
  Epoch 2: train_ppl=3354.4 val_ppl=1779.6 acc=13.9% [2.9s] ★
  Epoch 3: train_ppl=1072.1 val_ppl=1064.4 acc=15.6% [2.9s] ★
  Epoch 4: train_ppl=627.0 val_ppl=799.7 acc=16.5% [3.0s] ★
  Epoch 5: train_ppl=442.2 val_ppl=658.9 acc=16.9% [3.0s] ★
  Epoch 6: train_ppl=335.4 val_ppl=571.7 acc=17.3% [3.0s] ★
  Epoch 7: train_ppl=271.1 val_ppl=512.5 acc=17.7% [3.0s] ★
  Epoch 8: train_ppl=225.4 val_ppl=471.9 acc=18.0% [3.0s] ★
  Epoch 9: train_ppl=193.0 val_ppl=442.9 acc=18.3% [3.0s] ★
  Epoch 10: train_ppl=168.1 val_ppl=422.4 acc=18.4% [3.0s] ★
  Epoch 11: train_ppl=148.7 val_ppl=407.8 acc=18.6% [3.0s] ★
  Epoch 12: train_ppl=133.0 val_ppl=397.5 acc=18.8% [3.0s] ★
  Epoch 13: train_ppl=119.9 val_ppl=390.4 acc=18.9% [3.0s] ★
  Epoch 14: train_ppl=108.9 val_ppl=386.1 acc=19.1% [3.1s] ★
  Epoch 15: train_ppl=99.5 val_ppl=384.0 acc=19.2% [3.1s] ★
  Epoch 16: train_ppl=91.2 val_ppl=383.9 acc=19.3% [3.1s] ★
  Epoch 17: train_ppl=84.1 val_ppl=385.3 acc=19.3% [3.1s]
  → Early stop at epoch 17
  Best: epoch 16, ppl=383.9, acc=19.3%
    Phase 2: 51.0s, PPL=383.9, Acc=19.3%

[3/12] MCDL | ctx_dim=500 | 200 samples
Loading training data...
  Loading 200 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_200samples_full.pt
Loading validation data...
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1503 [0.9s]
  Iter 3: conv=0% loss=-0.1806 [0.8s]
  Iter 4: conv=0% loss=-0.2073 [0.8s]
  Iter 5: conv=0% loss=-0.2277 [0.7s]
    Val ER: 82.5%
  Iter 6: conv=0% loss=-0.2430 [0.7s]
  Iter 7: conv=0% loss=-0.2533 [0.7s]
  Iter 8: conv=0% loss=-0.2600 [0.7s]
  Iter 9: conv=1% loss=-0.2648 [0.7s]
  Iter 10: conv=2% loss=-0.2687 [0.7s]
    Val ER: 81.7%
  → Val early stop: ER not improving for 1 checks
  Done: 2% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.4s]
    Phase 1: 17.9s, 10 iter, ER=84.2%/82.5%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=130975.9 val_ppl=1775.3 acc=14.0% [5.6s] ★
  Epoch 2: train_ppl=1060.4 val_ppl=780.2 acc=16.4% [5.7s] ★
  Epoch 3: train_ppl=551.0 val_ppl=552.9 acc=17.4% [5.8s] ★
  Epoch 4: train_ppl=372.7 val_ppl=451.1 acc=17.9% [5.9s] ★
  Epoch 5: train_ppl=290.4 val_ppl=392.2 acc=18.4% [5.9s] ★
  Epoch 6: train_ppl=234.8 val_ppl=357.0 acc=18.9% [5.9s] ★
  Epoch 7: train_ppl=201.0 val_ppl=332.9 acc=19.2% [5.9s] ★
  Epoch 8: train_ppl=174.3 val_ppl=317.1 acc=19.5% [5.9s] ★
  Epoch 9: train_ppl=154.8 val_ppl=305.9 acc=19.7% [5.8s] ★
  Epoch 10: train_ppl=139.2 val_ppl=298.5 acc=19.9% [5.9s] ★
  Epoch 11: train_ppl=126.5 val_ppl=293.5 acc=20.0% [5.8s] ★
  Epoch 12: train_ppl=115.9 val_ppl=290.5 acc=20.2% [5.8s] ★
  Epoch 13: train_ppl=106.8 val_ppl=289.1 acc=20.2% [5.8s] ★
  Epoch 14: train_ppl=99.0 val_ppl=289.1 acc=20.3% [5.8s] ★
  Epoch 15: train_ppl=92.3 val_ppl=290.0 acc=20.4% [5.8s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=289.1, acc=20.3%
    Phase 2: 87.3s, PPL=289.1, Acc=20.3%

======================================================================
Algorithm: ODCM - Off-Diagonal Covariance Minimization (VICReg風, 推奨)
======================================================================

[4/12] ODCM | ctx_dim=500 | 50 samples
Loading training data...
  Loading 50 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_50samples_full.pt
Loading validation data...
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.5252 [0.3s]
  Iter 3: conv=0% loss=0.3563 [0.2s]
  Iter 4: conv=0% loss=0.3066 [0.2s]
  Iter 5: conv=0% loss=0.2627 [0.2s]
    Val ER: 81.2%
  Iter 6: conv=0% loss=0.2250 [0.2s]
  Iter 7: conv=0% loss=0.1990 [0.2s]
  Iter 8: conv=0% loss=0.1835 [0.2s]
  Iter 9: conv=0% loss=0.1740 [0.2s]
  Iter 10: conv=0% loss=0.1674 [0.2s]
    Val ER: 81.4%
  Iter 11: conv=0% loss=0.1625 [0.2s]
  Iter 12: conv=0% loss=0.1589 [0.2s]
  Iter 13: conv=0% loss=0.1543 [0.2s]
  Iter 14: conv=0% loss=0.1484 [0.2s]
  Iter 15: conv=0% loss=0.1419 [0.2s]
    Val ER: 83.0%
  Iter 16: conv=0% loss=0.1363 [0.2s]
  Iter 17: conv=0% loss=0.1318 [0.2s]
  Iter 18: conv=0% loss=0.1283 [0.2s]
  Iter 19: conv=0% loss=0.1252 [0.2s]
  Iter 20: conv=0% loss=0.1217 [0.2s]
    Val ER: 83.3%
  Iter 21: conv=0% loss=0.1184 [0.2s]
  Iter 22: conv=0% loss=0.1157 [0.2s]
  Iter 23: conv=0% loss=0.1131 [0.2s]
  Iter 24: conv=0% loss=0.1103 [0.2s]
  Iter 25: conv=0% loss=0.1077 [0.2s]
    Val ER: 84.5%
  Iter 26: conv=0% loss=0.1056 [0.2s]
  Iter 27: conv=0% loss=0.1044 [0.2s]
  Iter 28: conv=0% loss=0.1032 [0.2s]
  Iter 29: conv=0% loss=0.1021 [0.2s]
  Iter 30: conv=0% loss=0.1012 [0.2s]
    Val ER: 84.9%
  Iter 31: conv=0% loss=0.1001 [0.2s]
  Iter 32: conv=0% loss=0.0984 [0.2s]
  Iter 33: conv=0% loss=0.0961 [0.2s]
  Iter 34: conv=0% loss=0.0936 [0.2s]
  Iter 35: conv=0% loss=0.0913 [0.2s]
    Val ER: 85.3%
  Iter 36: conv=0% loss=0.0897 [0.2s]
  Iter 37: conv=0% loss=0.0885 [0.2s]
  Iter 38: conv=0% loss=0.0875 [0.2s]
  Iter 39: conv=0% loss=0.0866 [0.2s]
  Iter 40: conv=0% loss=0.0854 [0.2s]
    Val ER: 85.9%
  Iter 41: conv=0% loss=0.0839 [0.2s]
  Iter 42: conv=0% loss=0.0827 [0.2s]
  Iter 43: conv=0% loss=0.0817 [0.2s]
  Iter 44: conv=0% loss=0.0806 [0.2s]
  Iter 45: conv=0% loss=0.0796 [0.2s]
    Val ER: 86.3%
  Iter 46: conv=0% loss=0.0787 [0.2s]
  Iter 47: conv=0% loss=0.0780 [0.3s]
  Iter 48: conv=0% loss=0.0774 [0.2s]
  Iter 49: conv=0% loss=0.0768 [0.2s]
  Iter 50: conv=0% loss=0.0764 [0.2s]
    Val ER: 86.7%
  Iter 51: conv=0% loss=0.0758 [0.2s]
  Iter 52: conv=0% loss=0.0748 [0.2s]
  Iter 53: conv=0% loss=0.0734 [0.2s]
  Iter 54: conv=0% loss=0.0720 [0.2s]
  Iter 55: conv=0% loss=0.0710 [0.2s]
    Val ER: 87.0%
  Iter 56: conv=0% loss=0.0704 [0.2s]
  Iter 57: conv=0% loss=0.0700 [0.2s]
  Iter 58: conv=0% loss=0.0696 [0.2s]
  Iter 59: conv=0% loss=0.0693 [0.2s]
  Iter 60: conv=0% loss=0.0691 [0.2s]
    Val ER: 87.3%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.4s]
    Phase 1: 50.7s, 60 iter, ER=89.1%/87.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=346930400.0 val_ppl=948260.4 acc=3.3% [1.8s] ★
  Epoch 2: train_ppl=46649.4 val_ppl=9627.5 acc=7.8% [1.8s] ★
  Epoch 3: train_ppl=3540.5 val_ppl=3149.7 acc=11.1% [1.8s] ★
  Epoch 4: train_ppl=1261.6 val_ppl=1920.1 acc=12.8% [1.9s] ★
  Epoch 5: train_ppl=703.5 val_ppl=1443.8 acc=14.0% [1.9s] ★
  Epoch 6: train_ppl=470.9 val_ppl=1190.1 acc=14.7% [1.9s] ★
  Epoch 7: train_ppl=347.0 val_ppl=1035.2 acc=15.2% [1.9s] ★
  Epoch 8: train_ppl=274.2 val_ppl=928.2 acc=15.7% [1.9s] ★
  Epoch 9: train_ppl=219.6 val_ppl=852.1 acc=15.9% [1.9s] ★
  Epoch 10: train_ppl=181.9 val_ppl=794.1 acc=16.1% [1.9s] ★
  Epoch 11: train_ppl=153.9 val_ppl=750.4 acc=16.4% [1.8s] ★
  Epoch 12: train_ppl=131.9 val_ppl=717.2 acc=16.5% [1.9s] ★
  Epoch 13: train_ppl=114.5 val_ppl=692.8 acc=16.7% [1.9s] ★
  Epoch 14: train_ppl=101.1 val_ppl=675.6 acc=16.7% [1.8s] ★
  Epoch 15: train_ppl=90.4 val_ppl=664.3 acc=16.9% [1.8s] ★
  Epoch 16: train_ppl=79.8 val_ppl=660.3 acc=16.9% [1.8s] ★
  Epoch 17: train_ppl=71.5 val_ppl=660.1 acc=16.9% [1.8s] ★
  Epoch 18: train_ppl=64.5 val_ppl=661.2 acc=16.9% [1.8s]
  → Early stop at epoch 18
  Best: epoch 17, ppl=660.1, acc=16.9%
    Phase 2: 33.3s, PPL=660.1, Acc=16.9%

[5/12] ODCM | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.5173 [0.5s]
  Iter 3: conv=0% loss=0.3507 [0.5s]
  Iter 4: conv=0% loss=0.3019 [0.5s]
  Iter 5: conv=0% loss=0.2575 [0.5s]
    Val ER: 81.3%
  Iter 6: conv=0% loss=0.2194 [0.5s]
  Iter 7: conv=0% loss=0.1933 [0.5s]
  Iter 8: conv=0% loss=0.1783 [0.5s]
  Iter 9: conv=0% loss=0.1695 [0.5s]
  Iter 10: conv=0% loss=0.1633 [0.5s]
    Val ER: 81.4%
  Iter 11: conv=0% loss=0.1586 [0.5s]
  Iter 12: conv=0% loss=0.1548 [0.5s]
  Iter 13: conv=0% loss=0.1503 [0.5s]
  Iter 14: conv=0% loss=0.1446 [0.5s]
  Iter 15: conv=0% loss=0.1384 [0.5s]
    Val ER: 83.2%
  Iter 16: conv=0% loss=0.1331 [0.5s]
  Iter 17: conv=0% loss=0.1289 [0.5s]
  Iter 18: conv=0% loss=0.1254 [0.5s]
  Iter 19: conv=0% loss=0.1220 [0.5s]
  Iter 20: conv=0% loss=0.1182 [0.5s]
    Val ER: 83.8%
  Iter 21: conv=0% loss=0.1148 [0.5s]
  Iter 22: conv=0% loss=0.1123 [0.5s]
  Iter 23: conv=0% loss=0.1099 [0.5s]
  Iter 24: conv=0% loss=0.1074 [0.5s]
  Iter 25: conv=0% loss=0.1051 [0.5s]
    Val ER: 84.8%
  Iter 26: conv=0% loss=0.1032 [0.5s]
  Iter 27: conv=0% loss=0.1019 [0.5s]
  Iter 28: conv=0% loss=0.1002 [0.5s]
  Iter 29: conv=0% loss=0.0981 [0.5s]
  Iter 30: conv=0% loss=0.0964 [0.5s]
    Val ER: 85.2%
  Iter 31: conv=0% loss=0.0949 [0.5s]
  Iter 32: conv=0% loss=0.0936 [0.5s]
  Iter 33: conv=0% loss=0.0919 [0.5s]
  Iter 34: conv=0% loss=0.0900 [0.5s]
  Iter 35: conv=0% loss=0.0881 [0.5s]
    Val ER: 85.7%
  Iter 36: conv=0% loss=0.0864 [0.5s]
  Iter 37: conv=0% loss=0.0849 [0.5s]
  Iter 38: conv=0% loss=0.0836 [0.5s]
  Iter 39: conv=0% loss=0.0823 [0.5s]
  Iter 40: conv=0% loss=0.0806 [0.5s]
    Val ER: 86.4%
  Iter 41: conv=0% loss=0.0787 [0.5s]
  Iter 42: conv=0% loss=0.0772 [0.5s]
  Iter 43: conv=0% loss=0.0761 [0.5s]
  Iter 44: conv=0% loss=0.0754 [0.5s]
  Iter 45: conv=0% loss=0.0750 [0.5s]
    Val ER: 86.5%
  Iter 46: conv=0% loss=0.0747 [0.5s]
  Iter 47: conv=0% loss=0.0741 [0.5s]
  Iter 48: conv=0% loss=0.0731 [0.5s]
  Iter 49: conv=0% loss=0.0721 [0.5s]
  Iter 50: conv=0% loss=0.0712 [0.5s]
    Val ER: 87.1%
  Iter 51: conv=0% loss=0.0704 [0.5s]
  Iter 52: conv=0% loss=0.0696 [0.5s]
  Iter 53: conv=0% loss=0.0688 [0.5s]
  Iter 54: conv=0% loss=0.0678 [0.5s]
  Iter 55: conv=0% loss=0.0668 [0.5s]
    Val ER: 87.6%
  Iter 56: conv=0% loss=0.0660 [0.5s]
  Iter 57: conv=0% loss=0.0654 [0.5s]
  Iter 58: conv=0% loss=0.0649 [0.5s]
  Iter 59: conv=0% loss=0.0644 [0.5s]
  Iter 60: conv=0% loss=0.0641 [0.5s]
    Val ER: 87.7%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 70.4s, 60 iter, ER=89.5%/87.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=7151223.5 val_ppl=11821.2 acc=8.6% [3.1s] ★
  Epoch 2: train_ppl=3663.3 val_ppl=1957.3 acc=13.2% [3.2s] ★
  Epoch 3: train_ppl=1110.9 val_ppl=1159.7 acc=15.0% [3.3s] ★
  Epoch 4: train_ppl=626.7 val_ppl=871.6 acc=15.9% [3.3s] ★
  Epoch 5: train_ppl=430.1 val_ppl=715.3 acc=16.6% [3.3s] ★
  Epoch 6: train_ppl=323.7 val_ppl=619.9 acc=17.0% [3.3s] ★
  Epoch 7: train_ppl=257.3 val_ppl=557.3 acc=17.4% [3.3s] ★
  Epoch 8: train_ppl=212.5 val_ppl=514.9 acc=17.6% [3.3s] ★
  Epoch 9: train_ppl=180.5 val_ppl=485.6 acc=17.8% [3.3s] ★
  Epoch 10: train_ppl=156.2 val_ppl=465.5 acc=18.0% [3.2s] ★
  Epoch 11: train_ppl=137.2 val_ppl=451.9 acc=18.2% [3.2s] ★
  Epoch 12: train_ppl=121.8 val_ppl=443.2 acc=18.2% [3.2s] ★
  Epoch 13: train_ppl=109.2 val_ppl=438.2 acc=18.3% [3.2s] ★
  Epoch 14: train_ppl=98.5 val_ppl=436.4 acc=18.4% [3.2s] ★
  Epoch 15: train_ppl=89.4 val_ppl=437.1 acc=18.5% [3.2s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=436.4, acc=18.4%
    Phase 2: 48.5s, PPL=436.4, Acc=18.4%

[6/12] ODCM | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.5192 [0.9s]
  Iter 3: conv=0% loss=0.3539 [0.9s]
  Iter 4: conv=0% loss=0.3057 [0.9s]
  Iter 5: conv=0% loss=0.2630 [0.9s]
    Val ER: 80.9%
  Iter 6: conv=0% loss=0.2260 [0.9s]
  Iter 7: conv=0% loss=0.2000 [0.9s]
  Iter 8: conv=0% loss=0.1846 [0.9s]
  Iter 9: conv=0% loss=0.1756 [0.9s]
  Iter 10: conv=0% loss=0.1691 [0.9s]
    Val ER: 81.2%
  Iter 11: conv=0% loss=0.1641 [0.9s]
  Iter 12: conv=0% loss=0.1604 [0.9s]
  Iter 13: conv=0% loss=0.1562 [0.9s]
  Iter 14: conv=0% loss=0.1507 [0.9s]
  Iter 15: conv=0% loss=0.1447 [0.9s]
    Val ER: 83.0%
  Iter 16: conv=0% loss=0.1390 [0.9s]
  Iter 17: conv=0% loss=0.1341 [0.9s]
  Iter 18: conv=0% loss=0.1298 [0.9s]
  Iter 19: conv=0% loss=0.1260 [0.9s]
  Iter 20: conv=0% loss=0.1225 [0.9s]
    Val ER: 83.5%
  Iter 21: conv=0% loss=0.1197 [0.9s]
  Iter 22: conv=0% loss=0.1174 [0.9s]
  Iter 23: conv=0% loss=0.1149 [0.9s]
  Iter 24: conv=0% loss=0.1120 [0.9s]
  Iter 25: conv=0% loss=0.1091 [0.9s]
    Val ER: 84.7%
  Iter 26: conv=0% loss=0.1068 [0.9s]
  Iter 27: conv=0% loss=0.1051 [0.9s]
  Iter 28: conv=0% loss=0.1037 [0.9s]
  Iter 29: conv=0% loss=0.1025 [0.9s]
  Iter 30: conv=0% loss=0.1014 [0.9s]
    Val ER: 84.8%
  Iter 31: conv=0% loss=0.1004 [0.9s]
  Iter 32: conv=0% loss=0.0990 [0.9s]
  Iter 33: conv=0% loss=0.0972 [0.9s]
  Iter 34: conv=0% loss=0.0951 [0.9s]
  Iter 35: conv=0% loss=0.0931 [0.9s]
    Val ER: 85.3%
  Iter 36: conv=0% loss=0.0917 [0.9s]
  Iter 37: conv=0% loss=0.0906 [0.9s]
  Iter 38: conv=0% loss=0.0898 [0.9s]
  Iter 39: conv=0% loss=0.0890 [0.9s]
  Iter 40: conv=0% loss=0.0880 [0.9s]
    Val ER: 86.0%
  Iter 41: conv=0% loss=0.0868 [0.9s]
  Iter 42: conv=0% loss=0.0856 [0.9s]
  Iter 43: conv=0% loss=0.0848 [0.9s]
  Iter 44: conv=0% loss=0.0843 [0.9s]
  Iter 45: conv=0% loss=0.0841 [0.9s]
    Val ER: 85.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.4s]
    Phase 1: 80.2s, 45 iter, ER=87.9%/86.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=184282.9 val_ppl=1904.9 acc=13.2% [5.9s] ★
  Epoch 2: train_ppl=1122.9 val_ppl=819.4 acc=15.9% [6.0s] ★
  Epoch 3: train_ppl=550.1 val_ppl=568.5 acc=17.0% [6.0s] ★
  Epoch 4: train_ppl=368.1 val_ppl=459.1 acc=17.6% [5.9s] ★
  Epoch 5: train_ppl=279.5 val_ppl=401.1 acc=18.1% [5.9s] ★
  Epoch 6: train_ppl=227.3 val_ppl=366.5 acc=18.5% [5.9s] ★
  Epoch 7: train_ppl=192.6 val_ppl=344.2 acc=18.9% [5.9s] ★
  Epoch 8: train_ppl=167.5 val_ppl=329.6 acc=19.1% [5.8s] ★
  Epoch 9: train_ppl=148.3 val_ppl=319.9 acc=19.3% [5.8s] ★
  Epoch 10: train_ppl=133.0 val_ppl=313.7 acc=19.4% [5.8s] ★
  Epoch 11: train_ppl=120.5 val_ppl=310.1 acc=19.5% [5.8s] ★
  Epoch 12: train_ppl=110.1 val_ppl=308.5 acc=19.6% [5.8s] ★
  Epoch 13: train_ppl=101.2 val_ppl=308.7 acc=19.6% [5.8s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=308.5, acc=19.6%
    Phase 2: 76.2s, PPL=308.5, Acc=19.6%

======================================================================
Algorithm: SDL - Spectral Diversity Loss (ER直接最大化, 高コスト)
======================================================================

[7/12] SDL | ctx_dim=500 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.1891 [0.5s]
  Iter 3: conv=0% loss=-5.2927 [0.5s]
  Iter 4: conv=0% loss=-5.3376 [0.5s]
  Iter 5: conv=0% loss=-5.3697 [0.5s]
    Val ER: 84.4%
  Iter 6: conv=0% loss=-5.3923 [0.5s]
  Iter 7: conv=0% loss=-5.4060 [0.5s]
  Iter 8: conv=0% loss=-5.4136 [0.5s]
  Iter 9: conv=0% loss=-5.4182 [0.5s]
  Iter 10: conv=0% loss=-5.4222 [0.5s]
    Val ER: 84.4%
  Iter 11: conv=0% loss=-5.4268 [0.5s]
  Iter 12: conv=0% loss=-5.4322 [0.5s]
  Iter 13: conv=0% loss=-5.4379 [0.5s]
  Iter 14: conv=0% loss=-5.4433 [0.5s]
  Iter 15: conv=0% loss=-5.4483 [0.5s]
    Val ER: 87.8%
  Iter 16: conv=0% loss=-5.4526 [0.5s]
  Iter 17: conv=0% loss=-5.4564 [0.5s]
  Iter 18: conv=0% loss=-5.4601 [0.5s]
  Iter 19: conv=0% loss=-5.4639 [0.5s]
  Iter 20: conv=0% loss=-5.4676 [0.5s]
    Val ER: 89.5%
  Iter 21: conv=0% loss=-5.4710 [0.5s]
  Iter 22: conv=0% loss=-5.4738 [0.5s]
  Iter 23: conv=0% loss=-5.4763 [0.5s]
  Iter 24: conv=0% loss=-5.4785 [0.5s]
  Iter 25: conv=0% loss=-5.4808 [0.5s]
    Val ER: 91.2%
  Iter 26: conv=0% loss=-5.4832 [0.5s]
  Iter 27: conv=0% loss=-5.4857 [0.5s]
  Iter 28: conv=0% loss=-5.4880 [0.5s]
  Iter 29: conv=0% loss=-5.4901 [0.5s]
  Iter 30: conv=0% loss=-5.4919 [0.5s]
    Val ER: 91.9%
  Iter 31: conv=0% loss=-5.4934 [0.5s]
  Iter 32: conv=0% loss=-5.4947 [0.5s]
  Iter 33: conv=0% loss=-5.4960 [0.5s]
  Iter 34: conv=0% loss=-5.4971 [0.5s]
  Iter 35: conv=0% loss=-5.4983 [0.5s]
    Val ER: 92.6%
  Iter 36: conv=0% loss=-5.4994 [0.5s]
  Iter 37: conv=0% loss=-5.5005 [0.5s]
  Iter 38: conv=0% loss=-5.5014 [0.5s]
  Iter 39: conv=0% loss=-5.5024 [0.5s]
  Iter 40: conv=0% loss=-5.5033 [0.5s]
    Val ER: 93.4%
  Iter 41: conv=0% loss=-5.5044 [0.5s]
  Iter 42: conv=0% loss=-5.5055 [0.5s]
  Iter 43: conv=0% loss=-5.5066 [0.5s]
  Iter 44: conv=0% loss=-5.5075 [0.5s]
  Iter 45: conv=0% loss=-5.5082 [0.5s]
    Val ER: 93.7%
  Iter 46: conv=0% loss=-5.5088 [0.5s]
  Iter 47: conv=0% loss=-5.5093 [0.5s]
  Iter 48: conv=0% loss=-5.5100 [0.5s]
  Iter 49: conv=0% loss=-5.5108 [0.5s]
  Iter 50: conv=0% loss=-5.5116 [0.5s]
    Val ER: 94.1%
  Iter 51: conv=0% loss=-5.5125 [0.5s]
  Iter 52: conv=0% loss=-5.5133 [0.5s]
  Iter 53: conv=0% loss=-5.5141 [0.5s]
  Iter 54: conv=0% loss=-5.5148 [0.5s]
  Iter 55: conv=0% loss=-5.5155 [0.5s]
    Val ER: 94.6%
  Iter 56: conv=0% loss=-5.5163 [0.5s]
  Iter 57: conv=0% loss=-5.5171 [0.5s]
  Iter 58: conv=0% loss=-5.5179 [0.5s]
  Iter 59: conv=0% loss=-5.5187 [0.5s]
  Iter 60: conv=0% loss=-5.5194 [0.5s]
    Val ER: 94.9%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.4s]
    Phase 1: 64.2s, 60 iter, ER=96.7%/94.9%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=674999872.0 val_ppl=1995478.4 acc=2.6% [1.8s] ★
  Epoch 2: train_ppl=85846.8 val_ppl=14729.8 acc=7.7% [1.9s] ★
  Epoch 3: train_ppl=4613.3 val_ppl=3907.0 acc=10.3% [1.9s] ★
  Epoch 4: train_ppl=1481.2 val_ppl=2238.2 acc=12.5% [1.9s] ★
  Epoch 5: train_ppl=785.5 val_ppl=1644.5 acc=13.8% [1.9s] ★
  Epoch 6: train_ppl=510.1 val_ppl=1346.6 acc=14.4% [1.9s] ★
  Epoch 7: train_ppl=368.0 val_ppl=1165.1 acc=14.8% [1.9s] ★
  Epoch 8: train_ppl=282.0 val_ppl=1042.0 acc=15.2% [1.9s] ★
  Epoch 9: train_ppl=224.9 val_ppl=954.4 acc=15.6% [1.9s] ★
  Epoch 10: train_ppl=184.3 val_ppl=889.3 acc=15.8% [1.9s] ★
  Epoch 11: train_ppl=154.0 val_ppl=840.5 acc=16.0% [1.9s] ★
  Epoch 12: train_ppl=131.2 val_ppl=805.1 acc=16.2% [1.9s] ★
  Epoch 13: train_ppl=114.4 val_ppl=779.3 acc=16.5% [1.9s] ★
  Epoch 14: train_ppl=98.8 val_ppl=764.1 acc=16.5% [1.8s] ★
  Epoch 15: train_ppl=86.7 val_ppl=754.6 acc=16.6% [1.8s] ★
  Epoch 16: train_ppl=76.6 val_ppl=746.7 acc=16.7% [1.9s] ★
  Epoch 17: train_ppl=67.9 val_ppl=745.6 acc=16.8% [1.9s] ★
  Epoch 18: train_ppl=61.6 val_ppl=748.9 acc=16.9% [1.9s]
  → Early stop at epoch 18
  Best: epoch 17, ppl=745.6, acc=16.8%
    Phase 2: 33.6s, PPL=745.6, Acc=16.8%

[8/12] SDL | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.1948 [1.1s]
  Iter 3: conv=0% loss=-5.2962 [1.1s]
  Iter 4: conv=0% loss=-5.3402 [1.1s]
  Iter 5: conv=0% loss=-5.3720 [1.1s]
    Val ER: 84.4%
  Iter 6: conv=0% loss=-5.3944 [1.1s]
  Iter 7: conv=0% loss=-5.4080 [1.1s]
  Iter 8: conv=0% loss=-5.4154 [1.1s]
  Iter 9: conv=0% loss=-5.4199 [1.1s]
  Iter 10: conv=0% loss=-5.4238 [1.1s]
    Val ER: 84.4%
  Iter 11: conv=0% loss=-5.4283 [1.1s]
  Iter 12: conv=0% loss=-5.4335 [1.1s]
  Iter 13: conv=0% loss=-5.4389 [1.1s]
  Iter 14: conv=0% loss=-5.4443 [1.1s]
  Iter 15: conv=0% loss=-5.4491 [1.1s]
    Val ER: 87.8%
  Iter 16: conv=0% loss=-5.4534 [1.1s]
  Iter 17: conv=0% loss=-5.4572 [1.1s]
  Iter 18: conv=0% loss=-5.4608 [1.1s]
  Iter 19: conv=0% loss=-5.4646 [1.1s]
  Iter 20: conv=0% loss=-5.4683 [1.1s]
    Val ER: 89.6%
  Iter 21: conv=0% loss=-5.4716 [1.1s]
  Iter 22: conv=0% loss=-5.4745 [1.1s]
  Iter 23: conv=0% loss=-5.4769 [1.1s]
  Iter 24: conv=0% loss=-5.4792 [1.0s]
  Iter 25: conv=0% loss=-5.4815 [1.1s]
    Val ER: 91.3%
  Iter 26: conv=0% loss=-5.4839 [1.1s]
  Iter 27: conv=0% loss=-5.4864 [1.1s]
  Iter 28: conv=0% loss=-5.4887 [1.1s]
  Iter 29: conv=0% loss=-5.4908 [1.1s]
  Iter 30: conv=0% loss=-5.4926 [1.1s]
    Val ER: 92.0%
  Iter 31: conv=0% loss=-5.4942 [1.1s]
  Iter 32: conv=0% loss=-5.4956 [1.1s]
  Iter 33: conv=0% loss=-5.4968 [1.1s]
  Iter 34: conv=0% loss=-5.4980 [1.1s]
  Iter 35: conv=0% loss=-5.4992 [1.1s]
    Val ER: 92.8%
  Iter 36: conv=0% loss=-5.5002 [1.1s]
  Iter 37: conv=0% loss=-5.5013 [1.1s]
  Iter 38: conv=0% loss=-5.5022 [1.1s]
  Iter 39: conv=0% loss=-5.5032 [1.1s]
  Iter 40: conv=0% loss=-5.5042 [1.1s]
    Val ER: 93.5%
  Iter 41: conv=0% loss=-5.5052 [1.1s]
  Iter 42: conv=0% loss=-5.5063 [1.1s]
  Iter 43: conv=0% loss=-5.5073 [1.1s]
  Iter 44: conv=0% loss=-5.5082 [1.1s]
  Iter 45: conv=0% loss=-5.5089 [1.1s]
    Val ER: 93.9%
  Iter 46: conv=0% loss=-5.5095 [1.1s]
  Iter 47: conv=0% loss=-5.5101 [1.1s]
  Iter 48: conv=0% loss=-5.5107 [1.1s]
  Iter 49: conv=0% loss=-5.5114 [1.1s]
  Iter 50: conv=0% loss=-5.5122 [1.1s]
    Val ER: 94.3%
  Iter 51: conv=0% loss=-5.5131 [1.0s]
  Iter 52: conv=0% loss=-5.5139 [1.1s]
  Iter 53: conv=0% loss=-5.5146 [1.1s]
  Iter 54: conv=0% loss=-5.5154 [1.1s]
  Iter 55: conv=0% loss=-5.5161 [1.1s]
    Val ER: 94.8%
  Iter 56: conv=0% loss=-5.5168 [1.1s]
  Iter 57: conv=0% loss=-5.5175 [1.1s]
  Iter 58: conv=0% loss=-5.5183 [1.1s]
  Iter 59: conv=0% loss=-5.5190 [1.1s]
  Iter 60: conv=0% loss=-5.5197 [1.1s]
    Val ER: 95.1%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.6s]
    Phase 1: 104.4s, 60 iter, ER=96.8%/95.1%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=13632992.0 val_ppl=18519.9 acc=7.5% [3.2s] ★
  Epoch 2: train_ppl=4767.2 val_ppl=2341.4 acc=12.4% [3.3s] ★
  Epoch 3: train_ppl=1260.2 val_ppl=1321.7 acc=14.6% [3.3s] ★
  Epoch 4: train_ppl=685.8 val_ppl=973.8 acc=15.5% [3.3s] ★
  Epoch 5: train_ppl=461.0 val_ppl=794.9 acc=16.0% [3.3s] ★
  Epoch 6: train_ppl=342.3 val_ppl=685.8 acc=16.5% [3.3s] ★
  Epoch 7: train_ppl=269.6 val_ppl=614.7 acc=16.9% [3.2s] ★
  Epoch 8: train_ppl=220.9 val_ppl=566.3 acc=17.2% [3.2s] ★
  Epoch 9: train_ppl=186.1 val_ppl=532.9 acc=17.3% [3.2s] ★
  Epoch 10: train_ppl=159.7 val_ppl=509.7 acc=17.5% [3.2s] ★
  Epoch 11: train_ppl=139.2 val_ppl=494.2 acc=17.6% [3.2s] ★
  Epoch 12: train_ppl=122.8 val_ppl=484.3 acc=17.8% [3.2s] ★
  Epoch 13: train_ppl=109.2 val_ppl=479.0 acc=17.9% [3.2s] ★
  Epoch 14: train_ppl=97.8 val_ppl=477.5 acc=17.9% [3.2s] ★
  Epoch 15: train_ppl=88.2 val_ppl=479.1 acc=18.0% [3.2s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=477.5, acc=17.9%
    Phase 2: 48.3s, PPL=477.5, Acc=17.9%

[9/12] SDL | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.1934 [2.0s]
  Iter 3: conv=0% loss=-5.2944 [2.1s]
  Iter 4: conv=0% loss=-5.3386 [2.0s]
  Iter 5: conv=0% loss=-5.3705 [2.0s]
    Val ER: 84.3%
  Iter 6: conv=0% loss=-5.3930 [2.1s]
  Iter 7: conv=0% loss=-5.4067 [2.0s]
  Iter 8: conv=0% loss=-5.4141 [2.0s]
  Iter 9: conv=0% loss=-5.4186 [2.0s]
  Iter 10: conv=0% loss=-5.4224 [2.0s]
    Val ER: 84.4%
  Iter 11: conv=0% loss=-5.4268 [2.0s]
  Iter 12: conv=0% loss=-5.4320 [2.0s]
  Iter 13: conv=0% loss=-5.4375 [2.0s]
  Iter 14: conv=0% loss=-5.4429 [2.0s]
  Iter 15: conv=0% loss=-5.4478 [2.0s]
    Val ER: 87.8%
  Iter 16: conv=0% loss=-5.4520 [2.0s]
  Iter 17: conv=0% loss=-5.4558 [2.0s]
  Iter 18: conv=0% loss=-5.4594 [2.0s]
  Iter 19: conv=0% loss=-5.4631 [2.0s]
  Iter 20: conv=0% loss=-5.4667 [2.0s]
    Val ER: 89.6%
  Iter 21: conv=0% loss=-5.4700 [2.0s]
  Iter 22: conv=0% loss=-5.4729 [2.0s]
  Iter 23: conv=0% loss=-5.4753 [2.0s]
  Iter 24: conv=0% loss=-5.4776 [2.0s]
  Iter 25: conv=0% loss=-5.4798 [2.0s]
    Val ER: 91.2%
  Iter 26: conv=0% loss=-5.4822 [2.0s]
  Iter 27: conv=0% loss=-5.4845 [2.0s]
  Iter 28: conv=0% loss=-5.4869 [2.0s]
  Iter 29: conv=0% loss=-5.4890 [2.0s]
  Iter 30: conv=0% loss=-5.4909 [2.0s]
    Val ER: 92.1%
  Iter 31: conv=0% loss=-5.4925 [2.0s]
  Iter 32: conv=0% loss=-5.4938 [2.0s]
  Iter 33: conv=0% loss=-5.4950 [2.0s]
  Iter 34: conv=0% loss=-5.4961 [2.0s]
  Iter 35: conv=0% loss=-5.4972 [2.0s]
    Val ER: 92.7%
  Iter 36: conv=0% loss=-5.4983 [2.0s]
  Iter 37: conv=0% loss=-5.4993 [2.0s]
  Iter 38: conv=0% loss=-5.5003 [2.0s]
  Iter 39: conv=0% loss=-5.5013 [2.0s]
  Iter 40: conv=0% loss=-5.5023 [2.0s]
    Val ER: 93.5%
  Iter 41: conv=0% loss=-5.5034 [2.0s]
  Iter 42: conv=0% loss=-5.5044 [2.0s]
  Iter 43: conv=0% loss=-5.5054 [2.0s]
  Iter 44: conv=0% loss=-5.5062 [2.0s]
  Iter 45: conv=0% loss=-5.5068 [2.0s]
    Val ER: 93.9%
  Iter 46: conv=0% loss=-5.5073 [2.0s]
  Iter 47: conv=0% loss=-5.5079 [2.0s]
  Iter 48: conv=0% loss=-5.5085 [2.0s]
  Iter 49: conv=0% loss=-5.5093 [2.0s]
  Iter 50: conv=0% loss=-5.5101 [2.0s]
    Val ER: 94.3%
  Iter 51: conv=0% loss=-5.5109 [2.0s]
  Iter 52: conv=0% loss=-5.5117 [2.0s]
  Iter 53: conv=0% loss=-5.5125 [2.0s]
  Iter 54: conv=0% loss=-5.5132 [2.0s]
  Iter 55: conv=0% loss=-5.5139 [2.0s]
    Val ER: 94.8%
  Iter 56: conv=0% loss=-5.5146 [2.0s]
  Iter 57: conv=0% loss=-5.5153 [2.0s]
  Iter 58: conv=0% loss=-5.5160 [2.0s]
  Iter 59: conv=0% loss=-5.5167 [2.0s]
  Iter 60: conv=0% loss=-5.5174 [2.0s]
    Val ER: 95.0%
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.4s]
    Phase 1: 170.6s, 60 iter, ER=96.7%/95.0%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=320543.2 val_ppl=2317.8 acc=12.7% [5.8s] ★
  Epoch 2: train_ppl=1300.1 val_ppl=945.9 acc=15.5% [5.9s] ★
  Epoch 3: train_ppl=615.0 val_ppl=654.6 acc=16.4% [5.9s] ★
  Epoch 4: train_ppl=402.9 val_ppl=524.6 acc=17.0% [5.9s] ★
  Epoch 5: train_ppl=301.0 val_ppl=454.3 acc=17.4% [5.9s] ★
  Epoch 6: train_ppl=241.6 val_ppl=412.0 acc=17.8% [5.8s] ★
  Epoch 7: train_ppl=202.8 val_ppl=385.1 acc=18.2% [5.9s] ★
  Epoch 8: train_ppl=175.0 val_ppl=367.5 acc=18.4% [5.8s] ★
  Epoch 9: train_ppl=153.9 val_ppl=356.2 acc=18.6% [5.8s] ★
  Epoch 10: train_ppl=137.2 val_ppl=349.1 acc=18.8% [5.8s] ★
  Epoch 11: train_ppl=123.5 val_ppl=345.2 acc=18.9% [5.8s] ★
  Epoch 12: train_ppl=112.1 val_ppl=343.8 acc=18.9% [5.8s] ★
  Epoch 13: train_ppl=102.5 val_ppl=344.5 acc=19.0% [5.8s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=343.8, acc=18.9%
    Phase 2: 75.8s, PPL=343.8, Acc=18.9%

======================================================================
Algorithm: NUC - Nuclear Norm Maximization (核ノルム最大化, 高コスト)
======================================================================

[10/12] NUC | ctx_dim=500 | 50 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_50samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 62891 tokens (50 samples)
  Val:   31024 tokens
    Data: 62,891 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 62,891 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-4.0792 [0.5s]
  Iter 3: conv=0% loss=-4.3545 [0.5s]
  Iter 4: conv=0% loss=-4.6019 [0.5s]
  Iter 5: conv=0% loss=-4.8755 [0.5s]
    Val ER: 83.2%
  Iter 6: conv=0% loss=-5.0858 [0.5s]
  Iter 7: conv=0% loss=-5.1994 [0.5s]
  Iter 8: conv=0% loss=-5.2454 [0.5s]
  Iter 9: conv=0% loss=-5.2686 [0.5s]
  Iter 10: conv=0% loss=-5.2925 [0.5s]
    Val ER: 83.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.3s]
    Phase 1: 11.5s, 10 iter, ER=86.4%/83.2%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 62,891 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=227770432.0 val_ppl=459666.1 acc=3.6% [1.8s] ★
  Epoch 2: train_ppl=32045.8 val_ppl=7816.7 acc=8.4% [1.8s] ★
  Epoch 3: train_ppl=3180.5 val_ppl=2832.2 acc=11.4% [1.8s] ★
  Epoch 4: train_ppl=1225.3 val_ppl=1784.0 acc=13.0% [1.8s] ★
  Epoch 5: train_ppl=709.5 val_ppl=1368.7 acc=14.0% [1.8s] ★
  Epoch 6: train_ppl=485.0 val_ppl=1131.9 acc=14.7% [1.8s] ★
  Epoch 7: train_ppl=368.9 val_ppl=988.3 acc=15.4% [1.8s] ★
  Epoch 8: train_ppl=286.2 val_ppl=888.5 acc=15.5% [1.8s] ★
  Epoch 9: train_ppl=232.1 val_ppl=811.4 acc=16.0% [1.8s] ★
  Epoch 10: train_ppl=192.8 val_ppl=754.6 acc=16.1% [1.9s] ★
  Epoch 11: train_ppl=163.4 val_ppl=711.1 acc=16.3% [1.9s] ★
  Epoch 12: train_ppl=143.0 val_ppl=678.6 acc=16.5% [1.9s] ★
  Epoch 13: train_ppl=123.4 val_ppl=660.2 acc=16.4% [1.9s] ★
  Epoch 14: train_ppl=108.5 val_ppl=638.3 acc=16.8% [1.9s] ★
  Epoch 15: train_ppl=97.5 val_ppl=627.2 acc=16.9% [1.9s] ★
  Epoch 16: train_ppl=86.5 val_ppl=627.4 acc=16.7% [1.9s]
  → Early stop at epoch 16
  Best: epoch 15, ppl=627.2, acc=16.9%
    Phase 2: 29.4s, PPL=627.2, Acc=16.9%

[11/12] NUC | ctx_dim=500 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
    Data: 122,795 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-4.0747 [1.0s]
  Iter 3: conv=0% loss=-4.3441 [1.0s]
  Iter 4: conv=0% loss=-4.5892 [1.0s]
  Iter 5: conv=0% loss=-4.8614 [1.1s]
    Val ER: 83.3%
  Iter 6: conv=0% loss=-5.0711 [1.1s]
  Iter 7: conv=0% loss=-5.1856 [1.1s]
  Iter 8: conv=0% loss=-5.2312 [1.0s]
  Iter 9: conv=0% loss=-5.2530 [1.1s]
  Iter 10: conv=0% loss=-5.2750 [1.1s]
    Val ER: 83.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [0.7s]
    Phase 1: 17.5s, 10 iter, ER=86.6%/83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 122,795 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=4554237.5 val_ppl=9158.6 acc=8.4% [3.2s] ★
  Epoch 2: train_ppl=3383.2 val_ppl=1849.5 acc=13.0% [3.2s] ★
  Epoch 3: train_ppl=1107.1 val_ppl=1109.7 acc=15.0% [3.2s] ★
  Epoch 4: train_ppl=646.4 val_ppl=838.2 acc=15.8% [3.3s] ★
  Epoch 5: train_ppl=447.0 val_ppl=690.3 acc=16.5% [3.3s] ★
  Epoch 6: train_ppl=338.6 val_ppl=597.3 acc=16.9% [3.3s] ★
  Epoch 7: train_ppl=270.0 val_ppl=535.1 acc=17.3% [3.3s] ★
  Epoch 8: train_ppl=223.3 val_ppl=493.0 acc=17.5% [3.3s] ★
  Epoch 9: train_ppl=189.9 val_ppl=464.2 acc=17.7% [3.2s] ★
  Epoch 10: train_ppl=164.7 val_ppl=444.3 acc=17.9% [3.2s] ★
  Epoch 11: train_ppl=145.0 val_ppl=430.7 acc=18.0% [3.2s] ★
  Epoch 12: train_ppl=129.1 val_ppl=421.8 acc=18.1% [3.2s] ★
  Epoch 13: train_ppl=115.9 val_ppl=416.6 acc=18.3% [3.2s] ★
  Epoch 14: train_ppl=104.8 val_ppl=414.3 acc=18.3% [3.2s] ★
  Epoch 15: train_ppl=95.3 val_ppl=414.5 acc=18.4% [3.2s]
  → Early stop at epoch 15
  Best: epoch 14, ppl=414.3, acc=18.3%
    Phase 2: 48.4s, PPL=414.3, Acc=18.3%

[12/12] NUC | ctx_dim=500 | 200 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_200samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 240132 tokens (200 samples)
  Val:   31024 tokens
    Data: 240,132 train, 31,024 val tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 240,132 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-4.0442 [2.0s]
  Iter 3: conv=0% loss=-4.3059 [2.0s]
  Iter 4: conv=0% loss=-4.5466 [2.0s]
  Iter 5: conv=0% loss=-4.8155 [2.0s]
    Val ER: 83.3%
  Iter 6: conv=0% loss=-5.0252 [2.0s]
  Iter 7: conv=0% loss=-5.1418 [2.0s]
  Iter 8: conv=0% loss=-5.1895 [2.0s]
  Iter 9: conv=0% loss=-5.2118 [2.0s]
  Iter 10: conv=0% loss=-5.2344 [2.0s]
    Val ER: 83.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [1.4s]
    Phase 1: 28.7s, 10 iter, ER=86.6%/83.3%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 977,664/40,210,540 parameters

[Phase 2] 240,132 train / 31,024 val tokens, 20 epochs
  Epoch 1: train_ppl=148094.0 val_ppl=1834.9 acc=13.3% [5.8s] ★
  Epoch 2: train_ppl=1117.1 val_ppl=823.4 acc=15.7% [5.9s] ★
  Epoch 3: train_ppl=559.3 val_ppl=580.9 acc=16.7% [5.8s] ★
  Epoch 4: train_ppl=379.0 val_ppl=469.9 acc=17.3% [5.9s] ★
  Epoch 5: train_ppl=286.2 val_ppl=409.3 acc=17.7% [5.9s] ★
  Epoch 6: train_ppl=232.4 val_ppl=372.6 acc=18.1% [5.9s] ★
  Epoch 7: train_ppl=196.5 val_ppl=349.3 acc=18.5% [5.9s] ★
  Epoch 8: train_ppl=170.9 val_ppl=334.2 acc=18.8% [5.8s] ★
  Epoch 9: train_ppl=151.4 val_ppl=324.2 acc=19.0% [5.8s] ★
  Epoch 10: train_ppl=135.8 val_ppl=317.9 acc=19.2% [5.8s] ★
  Epoch 11: train_ppl=123.2 val_ppl=314.3 acc=19.3% [5.8s] ★
  Epoch 12: train_ppl=112.6 val_ppl=312.7 acc=19.3% [5.8s] ★
  Epoch 13: train_ppl=103.6 val_ppl=312.9 acc=19.4% [5.8s]
  → Early stop at epoch 13
  Best: epoch 12, ppl=312.7, acc=19.3%
    Phase 2: 75.8s, PPL=312.7, Acc=19.3%

==================================================================================================================================
FULL EXPERIMENT RESULTS (Phase 1 + Phase 2)
==================================================================================================================================
Algo   Samples     Tokens P1 Iter  Train ER  Val ER BestValER   T.PPL   V.PPL    Acc   Time
----------------------------------------------------------------------------------------------------------------------------------
MCDL       100    122,795      10     84.2%   82.6%     82.5%    91.2   383.9  19.3%    64s
MCDL       200    240,132      10     84.2%   82.7%     82.5%    99.0   289.1  20.3%   105s
ODCM        50     62,891      60     89.1%   88.2%     87.3%    71.5   660.1  16.9%    84s
ODCM       100    122,795      60     89.5%   88.5%     87.7%    98.5   436.4  18.4%   119s
ODCM       200    240,132      45     87.9%   86.7%     86.0%   110.1   308.5  19.6%   156s
SDL         50     62,891      60     96.7%   95.8%     94.9%    67.9   745.6  16.8%    98s
SDL        100    122,795      60     96.8%   96.0%     95.1%    97.8   477.5  17.9%   153s
SDL        200    240,132      60     96.7%   95.9%     95.0%   112.1   343.8  18.9%   246s
NUC         50     62,891      10     86.4%   83.8%     83.2%    97.5   627.2  16.9%    41s
NUC        100    122,795      10     86.6%   83.9%     83.3%   104.8   414.3  18.3%    66s
NUC        200    240,132      10     86.6%   83.9%     83.3%   112.6   312.7  19.3%   105s
==================================================================================================================================

====================================================================================================
SCALING LAW ANALYSIS (PPL = A × tokens^α)
====================================================================================================
Algorithm     α (PPL)            A       R² Best Val PPL   Best Acc
----------------------------------------------------------------------------------------------------
MCDL         -0.42281     5.44e+04   1.0000        289.1      20.3%
ODCM         -0.56767     3.46e+05   0.9973        308.5      19.6%
SDL          -0.57776     4.33e+05   0.9923        343.8      18.9%
NUC          -0.51933     1.90e+05   0.9878        312.7      19.3%
====================================================================================================

--- Ranking by α (more negative = better scaling) ---
  1. SDL: α=-0.57776, PPL=343.8
  2. ODCM: α=-0.56767, PPL=308.5
  3. NUC: α=-0.51933, PPL=312.7
  4. MCDL: α=-0.42281, PPL=289.1

--- Ranking by Val PPL (lower = better) ---
  1. MCDL: PPL=289.1, Acc=20.3%
  2. ODCM: PPL=308.5, Acc=19.6%
  3. NUC: PPL=312.7, Acc=19.3%
  4. SDL: PPL=343.8, Acc=18.9%

All results saved to: importants/logs/20251201_115147_diversity_full/all_results.json

Total time: 23.1 min

Experiment completed!
