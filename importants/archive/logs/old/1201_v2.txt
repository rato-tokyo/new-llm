remote: Enumerating objects: 45, done.
remote: Counting objects: 100% (45/45), done.
remote: Compressing objects: 100% (9/9), done.
remote: Total 27 (delta 18), reused 27 (delta 18), pack-reused 0 (from 0)
Unpacking objects: 100% (27/27), 15.58 KiB | 1.73 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   104b94a..fc73ad6  main       -> origin/main
Updating 104b94a..fc73ad6
Fast-forward
 importants/cvfp_hypothesis_test_20251201.md        | 150 ++++
 .../diversity_algorithm_comparison_20251201.md     | 148 ++++
 importants/logs/1201.txt                           | 832 +++++++++++++++++++++
 scripts/cvfp_hypothesis_test.py                    |  30 +-
 scripts/diversity_algorithm_experiment.py          | 260 ++++---
 src/evaluation/__init__.py                         |   3 +-
 src/evaluation/metrics.py                          |  51 +-
 src/losses/__init__.py                             |  14 -
 src/losses/diversity.py                            | 227 ++----
 src/trainers/phase1/__init__.py                    |   3 +-
 src/trainers/phase1/base.py                        |   5 +-
 src/trainers/phase1/memory.py                      |  92 ++-
 12 files changed, 1427 insertions(+), 388 deletions(-)
 create mode 100644 importants/cvfp_hypothesis_test_20251201.md
 create mode 100644 importants/diversity_algorithm_comparison_20251201.md
 create mode 100644 importants/logs/1201.txt
======================================================================
DIVERSITY ALGORITHM COMPARISON EXPERIMENT
======================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Algorithms: ['MCDL', 'ODCM', 'SDL', 'NUC', 'WMSE']
Sample sizes: [100]
Context dims: [768, 1000]
Output: importants/logs/20251201_091225_diversity_comparison

Config:
  num_layers: 1
  dist_reg_weight: 0.9
  phase1_max_iterations: 60

######################################################################
# context_dim = 768
######################################################################

======================================================================
Algorithm: MCDL - Mean-Centered Dispersion Loss (現行ベースライン)
======================================================================

[1/10] MCDL | ctx_dim=768 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
2025-12-01 09:12:28.017755: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 09:12:28.033301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764580348.053677   72623 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764580348.060196   72623 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764580348.076880   72623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764580348.076909   72623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764580348.076911   72623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764580348.076914   72623 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 09:12:28.081750: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.1764 [1.8s]
  Iter 3: conv=0% loss=-0.1351 [1.5s]
  Iter 4: conv=0% loss=-0.1484 [1.3s]
  Iter 5: conv=0% loss=-0.2095 [1.3s]
    Val ER: 74.5%
  Iter 6: conv=0% loss=-0.2571 [1.4s]
  Iter 7: conv=0% loss=-0.2829 [1.3s]
  Iter 8: conv=0% loss=-0.3061 [1.3s]
  Iter 9: conv=0% loss=-0.3222 [1.3s]
  Iter 10: conv=2% loss=-0.3329 [1.3s]
    Val ER: 72.3%
  → Val early stop: ER not improving for 1 checks
  Done: 2% converged
  Train ER: 74.9%
  Val ER: 72.3%
  Time: 22.0s
  Loss calc: 1.26ms/iter

======================================================================
Algorithm: ODCM - Off-Diagonal Covariance Minimization (VICReg風, 推奨)
======================================================================

[2/10] ODCM | ctx_dim=768 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6160 [1.4s]
  Iter 3: conv=0% loss=0.4620 [1.3s]
  Iter 4: conv=0% loss=0.4338 [1.3s]
  Iter 5: conv=0% loss=0.3828 [1.4s]
    Val ER: 75.2%
  Iter 6: conv=0% loss=0.3405 [1.4s]
  Iter 7: conv=0% loss=0.2994 [1.4s]
  Iter 8: conv=0% loss=0.2718 [1.4s]
  Iter 9: conv=0% loss=0.2481 [1.4s]
  Iter 10: conv=0% loss=0.2269 [1.3s]
    Val ER: 79.6%
  Iter 11: conv=0% loss=0.2108 [1.4s]
  Iter 12: conv=0% loss=0.2005 [1.4s]
  Iter 13: conv=0% loss=0.1886 [1.4s]
  Iter 14: conv=0% loss=0.1688 [1.4s]
  Iter 15: conv=0% loss=0.1533 [1.4s]
    Val ER: 82.6%
  Iter 16: conv=0% loss=0.1460 [1.4s]
  Iter 17: conv=0% loss=0.1391 [1.4s]
  Iter 18: conv=0% loss=0.1334 [1.4s]
  Iter 19: conv=0% loss=0.1336 [1.4s]
  Iter 20: conv=0% loss=0.1330 [1.4s]
    Val ER: 84.3%
  Iter 21: conv=0% loss=0.1347 [1.4s]
  Iter 22: conv=0% loss=0.1317 [1.4s]
  Iter 23: conv=0% loss=0.1293 [1.4s]
  Iter 24: conv=0% loss=0.1246 [1.4s]
  Iter 25: conv=0% loss=0.1170 [1.4s]
    Val ER: 85.6%
  Iter 26: conv=0% loss=0.1157 [1.4s]
  Iter 27: conv=0% loss=0.1118 [1.4s]
  Iter 28: conv=0% loss=0.1094 [1.4s]
  Iter 29: conv=0% loss=0.1065 [1.4s]
  Iter 30: conv=0% loss=0.1061 [1.4s]
    Val ER: 85.9%
  Iter 31: conv=0% loss=0.1070 [1.4s]
  Iter 32: conv=0% loss=0.1060 [1.4s]
  Iter 33: conv=0% loss=0.1069 [1.4s]
  Iter 34: conv=0% loss=0.1031 [1.4s]
  Iter 35: conv=0% loss=0.1023 [1.4s]
    Val ER: 86.3%
  Iter 36: conv=0% loss=0.1029 [1.4s]
  Iter 37: conv=0% loss=0.1053 [1.4s]
  Iter 38: conv=0% loss=0.1076 [1.4s]
  Iter 39: conv=0% loss=0.1042 [1.4s]
  Iter 40: conv=0% loss=0.1084 [1.4s]
    Val ER: 87.0%
  Iter 41: conv=0% loss=0.1057 [1.4s]
  Iter 42: conv=0% loss=0.1007 [1.4s]
  Iter 43: conv=0% loss=0.0979 [1.4s]
  Iter 44: conv=0% loss=0.0897 [1.4s]
  Iter 45: conv=0% loss=0.0897 [1.4s]
    Val ER: 88.4%
  Iter 46: conv=0% loss=0.0837 [1.4s]
  Iter 47: conv=0% loss=0.0813 [1.4s]
  Iter 48: conv=0% loss=0.0805 [1.4s]
  Iter 49: conv=0% loss=0.0787 [1.4s]
  Iter 50: conv=0% loss=0.0781 [1.4s]
    Val ER: 88.9%
  Iter 51: conv=0% loss=0.0754 [1.4s]
  Iter 52: conv=0% loss=0.0740 [1.4s]
  Iter 53: conv=0% loss=0.0741 [1.4s]
  Iter 54: conv=0% loss=0.0738 [1.4s]
  Iter 55: conv=0% loss=0.0734 [1.4s]
    Val ER: 89.4%
  Iter 56: conv=0% loss=0.0722 [1.4s]
  Iter 57: conv=0% loss=0.0711 [1.4s]
  Iter 58: conv=0% loss=0.0700 [1.4s]
  Iter 59: conv=0% loss=0.0686 [1.4s]
  Iter 60: conv=0% loss=0.0676 [1.4s]
    Val ER: 90.0%
  Done: 0% converged
  Train ER: 90.7%
  Val ER: 90.0%
  Time: 133.0s
  Loss calc: 2.13ms/iter

======================================================================
Algorithm: SDL - Spectral Diversity Loss (ER直接最大化, 高コスト)
======================================================================

[3/10] SDL | ctx_dim=768 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.4539 [2.7s]
  Iter 3: conv=0% loss=-5.5244 [2.7s]
  Iter 4: conv=0% loss=-5.5933 [2.7s]
  Iter 5: conv=0% loss=-5.6561 [2.7s]
    Val ER: 80.4%
  Iter 6: conv=0% loss=-5.7016 [2.7s]
  Iter 7: conv=0% loss=-5.7235 [2.7s]
  Iter 8: conv=0% loss=-5.7270 [2.7s]
  Iter 9: conv=0% loss=-5.7331 [2.7s]
  Iter 10: conv=0% loss=-5.7543 [2.8s]
    Val ER: 82.0%
  Iter 11: conv=0% loss=-5.7695 [2.8s]
  Iter 12: conv=0% loss=-5.7731 [2.8s]
  Iter 13: conv=0% loss=-5.7759 [2.8s]
  Iter 14: conv=0% loss=-5.7730 [2.7s]
  Iter 15: conv=0% loss=-5.7746 [2.8s]
    Val ER: 83.2%
  Iter 16: conv=0% loss=-5.7881 [2.7s]
  Iter 17: conv=0% loss=-5.7880 [2.7s]
  Iter 18: conv=0% loss=-5.7910 [2.7s]
  Iter 19: conv=0% loss=-5.8119 [2.7s]
  Iter 20: conv=0% loss=-5.8225 [2.7s]
    Val ER: 90.9%
  Iter 21: conv=0% loss=-5.8302 [2.7s]
  Iter 22: conv=0% loss=-5.8453 [2.7s]
  Iter 23: conv=0% loss=-5.8484 [2.7s]
  Iter 24: conv=0% loss=-5.8506 [2.7s]
  Iter 25: conv=0% loss=-5.8551 [2.7s]
    Val ER: 94.0%
  Iter 26: conv=0% loss=-5.8496 [2.7s]
  Iter 27: conv=0% loss=-5.8475 [2.7s]
  Iter 28: conv=0% loss=-5.8410 [2.7s]
  Iter 29: conv=0% loss=-5.8359 [2.7s]
  Iter 30: conv=0% loss=-5.8313 [2.7s]
    Val ER: 94.6%
  Iter 31: conv=0% loss=-5.8246 [2.7s]
  Iter 32: conv=0% loss=-5.8222 [2.7s]
  Iter 33: conv=0% loss=-5.8173 [2.7s]
  Iter 34: conv=0% loss=-5.8144 [2.7s]
  Iter 35: conv=0% loss=-5.8128 [2.7s]
    Val ER: 94.9%
  Iter 36: conv=0% loss=-5.8110 [2.6s]
  Iter 37: conv=0% loss=-5.8098 [2.6s]
  Iter 38: conv=0% loss=-5.8096 [2.7s]
  Iter 39: conv=0% loss=-5.8103 [2.7s]
  Iter 40: conv=0% loss=-5.8105 [2.7s]
    Val ER: 95.0%
  Iter 41: conv=0% loss=-5.8119 [2.7s]
  Iter 42: conv=0% loss=-5.8142 [2.7s]
  Iter 43: conv=0% loss=-5.8151 [2.7s]
  Iter 44: conv=0% loss=-5.8159 [2.7s]
  Iter 45: conv=0% loss=-5.8162 [2.7s]
    Val ER: 95.4%
  Iter 46: conv=0% loss=-5.8168 [2.7s]
  Iter 47: conv=0% loss=-5.8192 [2.7s]
  Iter 48: conv=0% loss=-5.8204 [2.7s]
  Iter 49: conv=0% loss=-5.8217 [2.7s]
  Iter 50: conv=0% loss=-5.8237 [2.7s]
    Val ER: 95.5%
  Iter 51: conv=0% loss=-5.8241 [2.7s]
  Iter 52: conv=0% loss=-5.8248 [2.7s]
  Iter 53: conv=0% loss=-5.8260 [2.7s]
  Iter 54: conv=0% loss=-5.8278 [2.7s]
  Iter 55: conv=0% loss=-5.8292 [2.7s]
    Val ER: 95.6%
  Iter 56: conv=0% loss=-5.8300 [2.7s]
  Iter 57: conv=0% loss=-5.8317 [2.7s]
  Iter 58: conv=0% loss=-5.8321 [2.7s]
  Iter 59: conv=0% loss=-5.8328 [2.7s]
  Iter 60: conv=0% loss=-5.8330 [2.7s]
    Val ER: 96.0%
  Done: 0% converged
  Train ER: 96.7%
  Val ER: 96.0%
  Time: 210.2s
  Loss calc: 71.56ms/iter

======================================================================
Algorithm: NUC - Nuclear Norm Maximization (核ノルム最大化, 高コスト)
======================================================================

[4/10] NUC | ctx_dim=768 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.0924 [2.7s]
  Iter 3: conv=0% loss=-5.8678 [2.7s]
  Iter 4: conv=0% loss=-5.5236 [2.7s]
  Iter 5: conv=0% loss=-5.4053 [2.7s]
    Val ER: 72.9%
  Iter 6: conv=0% loss=-5.9361 [2.7s]
  Iter 7: conv=0% loss=-6.3072 [2.7s]
  Iter 8: conv=0% loss=-6.4112 [2.7s]
  Iter 9: conv=0% loss=-6.6290 [2.7s]
  Iter 10: conv=0% loss=-6.9360 [2.7s]
    Val ER: 77.5%
  Iter 11: conv=0% loss=-7.2223 [2.7s]
  Iter 12: conv=0% loss=-7.3128 [2.7s]
  Iter 13: conv=0% loss=-7.3141 [2.7s]
  Iter 14: conv=0% loss=-7.2502 [2.7s]
  Iter 15: conv=0% loss=-7.1501 [2.7s]
    Val ER: 77.3%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Train ER: 82.6%
  Val ER: 77.3%
  Time: 51.5s
  Loss calc: 72.46ms/iter

======================================================================
Algorithm: WMSE - Whitening MSE (白色化ベース)
======================================================================

[5/10] WMSE | ctx_dim=768 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] WMSE: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.3889 [1.4s]
  Iter 3: conv=0% loss=0.3831 [1.4s]
  Iter 4: conv=0% loss=0.3897 [1.4s]
  Iter 5: conv=0% loss=0.3434 [1.4s]
    Val ER: 71.5%
  Iter 6: conv=0% loss=0.2522 [1.4s]
  Iter 7: conv=0% loss=0.1878 [1.4s]
  Iter 8: conv=0% loss=0.1445 [1.4s]
  Iter 9: conv=0% loss=0.1213 [1.4s]
  Iter 10: conv=0% loss=0.1103 [1.4s]
    Val ER: 68.5%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Train ER: 73.6%
  Val ER: 68.5%
  Time: 21.9s
  Loss calc: 1.08ms/iter

######################################################################
# context_dim = 1000
######################################################################

======================================================================
Algorithm: MCDL - Mean-Centered Dispersion Loss (現行ベースライン)
======================================================================

[6/10] MCDL | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] MCDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-0.2119 [1.9s]
  Iter 3: conv=0% loss=-0.1484 [1.9s]
  Iter 4: conv=0% loss=-0.1531 [1.9s]
  Iter 5: conv=0% loss=-0.2128 [1.8s]
    Val ER: 69.6%
  Iter 6: conv=0% loss=-0.2599 [1.8s]
  Iter 7: conv=0% loss=-0.2931 [1.9s]
  Iter 8: conv=0% loss=-0.3262 [1.8s]
  Iter 9: conv=0% loss=-0.3488 [1.9s]
  Iter 10: conv=0% loss=-0.3616 [1.9s]
    Val ER: 65.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Train ER: 69.9%
  Val ER: 65.9%
  Time: 27.2s
  Loss calc: 1.06ms/iter

======================================================================
Algorithm: ODCM - Off-Diagonal Covariance Minimization (VICReg風, 推奨)
======================================================================

[7/10] ODCM | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] ODCM: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.6584 [1.9s]
  Iter 3: conv=0% loss=0.5068 [1.9s]
  Iter 4: conv=0% loss=0.4790 [1.9s]
  Iter 5: conv=0% loss=0.4389 [1.9s]
    Val ER: 72.3%
  Iter 6: conv=0% loss=0.3915 [1.9s]
  Iter 7: conv=0% loss=0.3493 [1.9s]
  Iter 8: conv=0% loss=0.3163 [1.9s]
  Iter 9: conv=0% loss=0.2921 [1.9s]
  Iter 10: conv=0% loss=0.2742 [1.9s]
    Val ER: 76.2%
  Iter 11: conv=0% loss=0.2792 [1.9s]
  Iter 12: conv=0% loss=0.2871 [1.9s]
  Iter 13: conv=0% loss=0.2478 [1.9s]
  Iter 14: conv=0% loss=0.2045 [1.9s]
  Iter 15: conv=0% loss=0.1825 [1.9s]
    Val ER: 80.1%
  Iter 16: conv=0% loss=0.1727 [1.9s]
  Iter 17: conv=0% loss=0.1684 [1.9s]
  Iter 18: conv=0% loss=0.1571 [1.9s]
  Iter 19: conv=0% loss=0.1546 [1.9s]
  Iter 20: conv=0% loss=0.1578 [1.9s]
    Val ER: 70.9%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Train ER: 84.4%
  Val ER: 70.9%
  Time: 55.4s
  Loss calc: 1.29ms/iter

======================================================================
Algorithm: SDL - Spectral Diversity Loss (ER直接最大化, 高コスト)
======================================================================

[8/10] SDL | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] SDL: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-5.6568 [4.1s]
  Iter 3: conv=0% loss=-5.7120 [4.1s]
  Iter 4: conv=0% loss=-5.7790 [4.1s]
  Iter 5: conv=0% loss=-5.8431 [4.1s]
    Val ER: 79.2%
  Iter 6: conv=0% loss=-5.8975 [4.2s]
  Iter 7: conv=0% loss=-5.9135 [4.1s]
  Iter 8: conv=0% loss=-5.9057 [4.1s]
  Iter 9: conv=0% loss=-5.9109 [4.1s]
  Iter 10: conv=0% loss=-5.9439 [4.1s]
    Val ER: 80.0%
  Iter 11: conv=0% loss=-5.9567 [4.1s]
  Iter 12: conv=0% loss=-5.9623 [4.1s]
  Iter 13: conv=0% loss=-5.9667 [4.1s]
  Iter 14: conv=0% loss=-5.9631 [4.1s]
  Iter 15: conv=0% loss=-5.9669 [4.1s]
    Val ER: 81.0%
  Iter 16: conv=0% loss=-5.9825 [4.1s]
  Iter 17: conv=0% loss=-5.9725 [4.1s]
  Iter 18: conv=0% loss=-5.9735 [4.1s]
  Iter 19: conv=0% loss=-5.9962 [4.1s]
  Iter 20: conv=0% loss=-5.9966 [4.1s]
    Val ER: 88.2%
  Iter 21: conv=0% loss=-6.0055 [4.1s]
  Iter 22: conv=0% loss=-6.0296 [4.1s]
  Iter 23: conv=0% loss=-6.0274 [4.1s]
  Iter 24: conv=0% loss=-6.0363 [4.1s]
  Iter 25: conv=0% loss=-6.0498 [4.1s]
    Val ER: 91.2%
  Iter 26: conv=0% loss=-6.0477 [4.1s]
  Iter 27: conv=0% loss=-6.0540 [4.1s]
  Iter 28: conv=0% loss=-6.0549 [4.1s]
  Iter 29: conv=0% loss=-6.0546 [4.1s]
  Iter 30: conv=0% loss=-6.0541 [4.1s]
    Val ER: 93.2%
  Iter 31: conv=0% loss=-6.0498 [4.1s]
  Iter 32: conv=0% loss=-6.0465 [4.1s]
  Iter 33: conv=0% loss=-6.0434 [4.1s]
  Iter 34: conv=0% loss=-6.0423 [4.1s]
  Iter 35: conv=0% loss=-6.0387 [4.1s]
    Val ER: 93.8%
  Iter 36: conv=0% loss=-6.0343 [4.1s]
  Iter 37: conv=0% loss=-6.0336 [4.1s]
  Iter 38: conv=0% loss=-6.0339 [4.1s]
  Iter 39: conv=0% loss=-6.0317 [4.1s]
  Iter 40: conv=0% loss=-6.0285 [4.1s]
    Val ER: 94.2%
  Iter 41: conv=0% loss=-6.0282 [4.1s]
  Iter 42: conv=0% loss=-6.0304 [4.1s]
  Iter 43: conv=0% loss=-6.0312 [4.1s]
  Iter 44: conv=0% loss=-6.0306 [4.1s]
  Iter 45: conv=0% loss=-6.0312 [4.1s]
    Val ER: 95.1%
  Iter 46: conv=0% loss=-6.0329 [4.1s]
  Iter 47: conv=0% loss=-6.0326 [4.1s]
  Iter 48: conv=0% loss=-6.0314 [4.1s]
  Iter 49: conv=0% loss=-6.0323 [4.1s]
  Iter 50: conv=0% loss=-6.0340 [4.1s]
    Val ER: 95.0%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Train ER: 96.2%
  Val ER: 95.0%
  Time: 248.3s
  Loss calc: 117.36ms/iter

======================================================================
Algorithm: NUC - Nuclear Norm Maximization (核ノルム最大化, 高コスト)
======================================================================

[9/10] NUC | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] NUC: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=-6.3693 [4.2s]
  Iter 3: conv=0% loss=-7.1570 [4.1s]
  Iter 4: conv=0% loss=-6.4243 [4.1s]
  Iter 5: conv=0% loss=-6.2081 [4.1s]
    Val ER: 68.9%
  Iter 6: conv=0% loss=-6.8355 [4.2s]
  Iter 7: conv=0% loss=-7.2461 [4.2s]
  Iter 8: conv=0% loss=-7.2609 [4.2s]
  Iter 9: conv=0% loss=-7.6568 [4.2s]
  Iter 10: conv=0% loss=-7.8724 [4.1s]
    Val ER: 70.3%
  Iter 11: conv=0% loss=-8.1313 [4.1s]
  Iter 12: conv=0% loss=-8.3174 [4.2s]
  Iter 13: conv=0% loss=-8.1464 [4.1s]
  Iter 14: conv=0% loss=-8.2540 [4.1s]
  Iter 15: conv=0% loss=-7.8029 [4.1s]
    Val ER: 75.1%
  Iter 16: conv=0% loss=-8.3025 [4.1s]
  Iter 17: conv=0% loss=-8.0094 [4.1s]
  Iter 18: conv=0% loss=-8.6628 [4.1s]
  Iter 19: conv=0% loss=-9.1510 [4.1s]
  Iter 20: conv=0% loss=-8.5276 [4.1s]
    Val ER: 80.5%
  Iter 21: conv=0% loss=-9.8082 [4.1s]
  Iter 22: conv=0% loss=-9.0576 [4.1s]
  Iter 23: conv=0% loss=-10.4661 [4.1s]
  Iter 24: conv=0% loss=-10.1225 [4.1s]
  Iter 25: conv=0% loss=-10.4251 [4.1s]
    Val ER: 83.8%
  Iter 26: conv=0% loss=-10.7838 [4.1s]
  Iter 27: conv=0% loss=-10.6523 [4.0s]
  Iter 28: conv=0% loss=-11.2688 [4.0s]
  Iter 29: conv=0% loss=-11.1402 [4.0s]
  Iter 30: conv=0% loss=-11.4120 [4.0s]
    Val ER: 86.0%
  Iter 31: conv=0% loss=-11.3722 [4.0s]
  Iter 32: conv=0% loss=-11.6376 [4.1s]
  Iter 33: conv=0% loss=-11.5750 [4.1s]
  Iter 34: conv=0% loss=-11.7583 [4.0s]
  Iter 35: conv=0% loss=-11.5836 [4.0s]
    Val ER: 88.4%
  Iter 36: conv=0% loss=-11.9312 [4.0s]
  Iter 37: conv=0% loss=-11.8997 [4.0s]
  Iter 38: conv=0% loss=-12.1452 [4.0s]
  Iter 39: conv=0% loss=-12.0243 [4.0s]
  Iter 40: conv=0% loss=-12.1987 [4.1s]
    Val ER: 89.8%
  Iter 41: conv=0% loss=-12.1559 [4.1s]
  Iter 42: conv=0% loss=-12.3694 [4.0s]
  Iter 43: conv=0% loss=-12.3009 [4.0s]
  Iter 44: conv=0% loss=-12.3862 [4.0s]
  Iter 45: conv=0% loss=-12.3747 [4.0s]
    Val ER: 91.0%
  Iter 46: conv=0% loss=-12.4546 [4.1s]
  Iter 47: conv=0% loss=-12.5460 [4.1s]
  Iter 48: conv=0% loss=-12.5799 [4.0s]
  Iter 49: conv=0% loss=-12.6511 [4.0s]
  Iter 50: conv=0% loss=-12.6743 [4.0s]
    Val ER: 91.7%
  Iter 51: conv=0% loss=-12.7537 [4.1s]
  Iter 52: conv=0% loss=-12.7634 [4.1s]
  Iter 53: conv=0% loss=-12.8467 [4.1s]
  Iter 54: conv=0% loss=-12.8382 [4.1s]
  Iter 55: conv=0% loss=-12.9204 [4.1s]
    Val ER: 91.9%
  Iter 56: conv=0% loss=-12.9266 [4.1s]
  Iter 57: conv=0% loss=-12.9518 [4.1s]
  Iter 58: conv=0% loss=-12.9858 [4.1s]
  Iter 59: conv=0% loss=-13.0067 [4.0s]
  Iter 60: conv=0% loss=-13.0285 [4.1s]
    Val ER: 92.4%
  Done: 0% converged
  Train ER: 93.8%
  Val ER: 92.4%
  Time: 297.5s
  Loss calc: 111.85ms/iter

======================================================================
Algorithm: WMSE - Whitening MSE (白色化ベース)
======================================================================

[10/10] WMSE | ctx_dim=1000 | 100 samples
Loading training data...
  Loading from cache: ./cache/ultrachat_100samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (31024 > 1024). Running this sequence through the model will result in indexing errors
  Train: 122795 tokens (100 samples)
  Val:   31024 tokens
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(1 layers) + TokenBlock(1 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters

[Phase 1] WMSE: 122,795 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=0.3967 [1.9s]
  Iter 3: conv=0% loss=0.4221 [1.9s]
  Iter 4: conv=0% loss=0.4382 [1.9s]
  Iter 5: conv=0% loss=0.4020 [1.9s]
    Val ER: 67.6%
  Iter 6: conv=0% loss=0.3228 [1.9s]
  Iter 7: conv=0% loss=0.2377 [1.9s]
  Iter 8: conv=0% loss=0.1810 [1.9s]
  Iter 9: conv=0% loss=0.1580 [1.9s]
  Iter 10: conv=0% loss=0.1569 [1.9s]
    Val ER: 64.3%
  → Val early stop: ER not improving for 1 checks
  Done: 0% converged
  Train ER: 70.2%
  Val ER: 64.3%
  Time: 27.6s
  Loss calc: 1.39ms/iter

===================================================================================================================
DIVERSITY ALGORITHM COMPARISON RESULTS
===================================================================================================================
Algorithm   ctx_dim  Samples     Tokens  Train ER%    Val ER%  Time(s)   Loss(ms)
-------------------------------------------------------------------------------------------------------------------
MCDL            768      100    122,795       74.9       72.3     22.0       1.26
ODCM            768      100    122,795       90.7       90.0    133.0       2.13
SDL             768      100    122,795       96.7       96.0    210.2      71.56
NUC             768      100    122,795       82.6       77.3     51.5      72.46
WMSE            768      100    122,795       73.6       68.5     21.9       1.08
MCDL           1000      100    122,795       69.9       65.9     27.2       1.06
ODCM           1000      100    122,795       84.4       70.9     55.4       1.29
SDL            1000      100    122,795       96.2       95.0    248.3     117.36
NUC            1000      100    122,795       93.8       92.4    297.5     111.85
WMSE           1000      100    122,795       70.2       64.3     27.6       1.39
===================================================================================================================

====================================================================================================
SUMMARY BY ALGORITHM (context_dim=768)
====================================================================================================
Algorithm   Avg Train ER%  Avg Val ER%  Avg Time(s)   Avg Loss(ms)
----------------------------------------------------------------------------------------------------
SDL                  96.7         96.0        210.2          71.56
ODCM                 90.7         90.0        133.0           2.13
NUC                  82.6         77.3         51.5          72.46
MCDL                 74.9         72.3         22.0           1.26
WMSE                 73.6         68.5         21.9           1.08

====================================================================================================
SUMMARY BY ALGORITHM (context_dim=1000)
====================================================================================================
Algorithm   Avg Train ER%  Avg Val ER%  Avg Time(s)   Avg Loss(ms)
----------------------------------------------------------------------------------------------------
SDL                  96.2         95.0        248.3         117.36
NUC                  93.8         92.4        297.5         111.85
ODCM                 84.4         70.9         55.4           1.29
MCDL                 69.9         65.9         27.2           1.06
WMSE                 70.2         64.3         27.6           1.39
(Sorted by Val ER% descending)

✓ Results saved to: importants/logs/20251201_091225_diversity_comparison/results.json

✅ Experiment completed!
