# Early Stopping Threshold (DWR) Comparison (2025-12-02)

## 概要

Phase 1のEarly Stopping閾値（収束率）を変えた実験比較。

- **dwr0.90**: `early_stopping_threshold = 0.90` (90%で停止)
- **dwr0.99**: `early_stopping_threshold = 0.99` (99%で停止)

**共通環境**:
- GPU: NVIDIA L4 (22.2GB)
- Samples: 2000
- Context dim: 500
- 訓練データ: 2,403,563 tokens
- 検証データ: 22,723 tokens

---

## 結果サマリー

### C1T1 比較

| DWR | Conv% | Iter | **Val ER%** | **Val PPL** | Val Acc | Phase1 Time |
|-----|-------|------|-------------|-------------|---------|-------------|
| 0.90 | 92% | 30 | **79.7%** | **127.2** | **24.7%** | ~300s |
| 0.99 | 99% | 41 | 76.1% | 235.1 | 18.5% | 430s |

**変化**: ER -3.6%, PPL +107.9 (+85%), Acc -6.2%

### C2T2 比較

| DWR | Conv% | Iter | **Val ER%** | **Val PPL** | Val Acc | Phase1 Time |
|-----|-------|------|-------------|-------------|---------|-------------|
| 0.90 | 93% | 14 | 79.6% | 132.2 | 24.4% | ~170s |
| 0.99 | 99% | 17 | **79.1%** | **133.0** | **24.5%** | 217s |

**変化**: ER -0.5%, PPL +0.8 (+0.6%), Acc +0.1%

---

## 重大な発見

### 1. C1T1はdwr0.99で大幅に悪化

```
dwr0.90: PPL 127.2, Acc 24.7%, ER 79.7%
dwr0.99: PPL 235.1, Acc 18.5%, ER 76.1%
         ↓
PPL +85%悪化、Acc -6.2%低下
```

**原因仮説**:
- C1T1は1層しかないため、過度な収束（99%）で**過学習**が発生
- 92%程度の「適度な収束」が1層モデルには最適

### 2. C2T2はdwr0.99でほぼ変化なし

```
dwr0.90: PPL 132.2, Acc 24.4%, ER 79.6%
dwr0.99: PPL 133.0, Acc 24.5%, ER 79.1%
         ↓
PPL +0.6%、Acc +0.1%（誤差範囲）
```

**原因仮説**:
- C2T2は2層あるため、より多くの反復に耐えられる
- 2層の冗長性が過学習を防いでいる

### 3. Val ERとVal PPLの相関

| Config | DWR | Val ER% | Val PPL | 傾向 |
|--------|-----|---------|---------|------|
| C1T1 | 0.90 | 79.7% | 127.2 | ER高 → PPL低（良） |
| C1T1 | 0.99 | 76.1% | 235.1 | ER低 → PPL高（悪） |
| C2T2 | 0.90 | 79.6% | 132.2 | - |
| C2T2 | 0.99 | 79.1% | 133.0 | - |

**相関分析**:
- **C1T1で強い相関**: ER 79.7% → PPL 127.2、ER 76.1% → PPL 235.1
- **C2T2では相関弱い**: ER 0.5%低下でもPPL +0.8のみ

**解釈**:
- **高いER（Effective Rank）= 多様な文脈表現**
- 1層モデルでは、ERが下がると**表現力不足**で性能が急落
- 2層モデルでは、層間の冗長性がER低下をカバー

---

## 全実験結果一覧

| Config | DWR | Iter | Conv% | Train ER% | Val ER% | Val PPL | Val Acc | Total Time |
|--------|-----|------|-------|-----------|---------|---------|---------|------------|
| C1T1 | 0.90 | 30 | 92% | - | **79.7%** | **127.2** | **24.7%** | ~1200s |
| C1T1 | 0.99 | 41 | 99% | 77.7% | 76.1% | 235.1 | 18.5% | 920s |
| C2T2 | 0.90 | 14 | 93% | - | 79.6% | 132.2 | 24.4% | ~730s |
| C2T2 | 0.99 | 17 | 99% | 80.7% | 79.1% | 133.0 | 24.5% | 768s |

---

## 結論

### 1. dwr0.90が最適（特にC1T1）

| 構成 | 推奨DWR | 理由 |
|------|---------|------|
| C1T1 | **0.90** | dwr0.99でPPL 85%悪化 |
| C2T2 | 0.90/0.99どちらでも可 | 差は誤差範囲 |

### 2. C1T1 + dwr0.90が最良

```
Config: C1T1, DWR: 0.90
Val PPL: 127.2（全条件中最良）
Val Acc: 24.7%（全条件中最高）
Val ER:  79.7%（全条件中最高）
```

### 3. ERとPPLの相関は構成依存

- **1層モデル（C1T1）**: ER ↔ PPL に強い相関
- **2層モデル（C2T2）**: 相関は弱い（層の冗長性がバッファ）

### 4. 設定変更推奨

```python
# config/phase1.py
early_stopping_threshold = 0.90  # 0.99から戻す（特にC1T1使用時）

# config.py
num_layers = 1  # C1T1が最良
```

---

## Phase 1詳細比較

### C1T1 収束曲線

**dwr0.90 (30 iter)**:
```
Iter 10: conv=0%  → Iter 20: conv=18% → Iter 30: conv=92%
収束速度: 緩やか、ER維持
```

**dwr0.99 (41 iter)**:
```
Iter 20: conv=33% → Iter 30: conv=92% → Iter 41: conv=99%
追加11回の反復でER低下（77.7% → 76.1%）
```

### C2T2 収束曲線

**dwr0.90 (14 iter)**:
```
Iter 5: conv=0% → Iter 10: conv=26% → Iter 14: conv=93%
高速収束
```

**dwr0.99 (17 iter)**:
```
Iter 10: conv=26% → Iter 16: conv=99% → Iter 17: conv=99%
追加3回のみ、ER維持（80.7%）
```

---

## 考察：なぜdwr0.99でC1T1だけ悪化するのか

### 仮説1: 過収束による多様性喪失

1層のContextBlockは表現力に限界があり、99%収束を目指すと：
- 同じ方向に「押し込まれる」コンテキストが増える
- 結果としてERが低下（79.7% → 76.1%）
- 多様性不足でPhase 2の学習が困難に

### 仮説2: 2層モデルの冗長性

2層のContextBlockは：
- Layer 1とLayer 2で異なる表現を学習可能
- 片方のERが下がっても、もう一方がカバー
- G案（prev/current context）により、時間方向の情報も補完

### 実験的検証案

```bash
# ER推移を詳細に記録する実験
python3 scripts/experiment.py --preset c1t1 --samples 2000 \
  --log-er-per-iteration  # 各イテレーションでのER記録（要実装）
```

---

*Last Updated: 2025-12-02*
