# CVFP仮説検証実験結果 (2025-12-01)

## 仮説

**「多様性損失のみ（dist_reg_weight=1.0、CVFPなし）で訓練しても、固定点に収束するのではないか？」**

## 結論

**✅ 仮説は正しい。全12アルゴリズムで固定点収束を確認。**

訓練後に追加で5回伝播したところ、**全アルゴリズムでCos Sim = 1.0、MSE = 0.0**となり、完全な固定点に収束していることが判明。

---

## 実験条件

| 項目 | 値 |
|------|-----|
| サンプル数 | 100 |
| トークン数 | 122,795 |
| num_layers | 1 |
| dist_reg_weight | **1.0（CVFPなし、多様性のみ）** |
| max_iterations | 60 |
| 追加伝播回数 | 5 |
| GPU | NVIDIA L4 |

---

## 結果サマリー（Val ER順）

| 順位 | アルゴリズム | Val ER% | Train ER% | 訓練中conv | 追加伝播後 | 時間(s) |
|:----:|:------------|--------:|----------:|-----------:|:----------:|--------:|
| 🥇1 | **ODCM** | **87.4%** | 88.9% | 0% | ✅ 固定点 | 135.1 |
| 🥈2 | **UNIF** | **81.3%** | 85.4% | 0% | ✅ 固定点 | 174.0 |
| 🥉3 | **NUC** | **76.8%** | 82.0% | 0% | ✅ 固定点 | 52.2 |
| 4 | WMSE | 75.7% | 76.8% | 0% | ✅ 固定点 | 156.0 |
| 5 | InfoNCE | 72.8% | 79.6% | 0% | ✅ 固定点 | 38.6 |
| 6 | SDL | 72.2% | 77.4% | 0% | ✅ 固定点 | 52.6 |
| 7 | UDEL | 66.3% | 73.0% | 0% | ✅ 固定点 | 21.5 |
| 8 | CTM | 64.6% | 67.9% | 0% | ✅ 固定点 | 22.0 |
| 9 | MCDL | 64.3% | 67.7% | 0% | ✅ 固定点 | 22.2 |
| 10 | DECORR | 53.2% | 58.1% | 72% | ✅ 固定点 | 33.4 |
| 11 | DUE | 26.1% | 32.3% | 40% | ✅ 固定点 | 21.6 |
| 12 | HSIC | 4.8% | 6.6% | 100% | ✅ 固定点 | 51.4 |

---

## 主要な発見

### 1. 全アルゴリズムで固定点収束

追加伝播の結果（全アルゴリズム共通）:

| Iter | MSE | Cos Sim | 状態 |
|------|-----|---------|------|
| 1 | 0.05〜1.7 | 0.16〜0.99 | 前回からの変化あり |
| 2 | **0.000000** | **1.0000** | 固定点到達 |
| 3 | 0.000000 | 1.0000 | 固定点維持 |
| 4 | 0.000000 | 1.0000 | 固定点維持 |
| 5 | 0.000000 | 1.0000 | 固定点維持 |

**たった1回の追加伝播で完全な固定点に到達。**

### 2. 訓練中の収束率0%は正常

`dist_reg_weight=1.0`では：
- CVFP損失 = 0（完全に無視）
- 訓練中の収束率 = 0%（CVFP閾値で判定しているため）
- **しかし実際には固定点に収束している**

これは収束率の計算がCVFP損失ベースであるため。多様性損失のみでも固定点学習は暗黙的に行われている。

### 3. ODCMが最高のER達成

| アルゴリズム | Val ER (CVFPあり, 0.9) | Val ER (CVFPなし, 1.0) | 差分 |
|-------------|----------------------:|----------------------:|-----:|
| **ODCM** | 89.2% | **87.4%** | -1.8% |
| SDL | 94.9% | 72.2% | -22.7% |
| NUC | 91.9% | 76.8% | -15.1% |
| MCDL | 78.4% | 64.3% | -14.1% |

**ODCMはCVFPなしでも高いERを維持。**

### 4. CVFP損失の効果

| 設定 | ODCM Val ER | 備考 |
|------|-------------|------|
| dist_reg_weight=0.9 | 89.2% | CVFP 10%あり |
| dist_reg_weight=1.0 | 87.4% | CVFPなし |

CVFPを10%加えると**+1.8%のER改善**。効果は限定的だが存在する。

---

## 理論的考察

### なぜ多様性損失のみで固定点に収束するのか？

1. **並列処理方式の特性**:
   - Token i の処理に `previous_contexts[i-1]` を使用
   - イテレーションを重ねるごとに、前回のcontextが現在の入力に近づく
   - 結果として `f(x) = x` の固定点方程式を満たす

2. **暗黙的な固定点誘導**:
   - 多様性損失は「異なるトークンが異なるcontextを持つ」ことを促す
   - しかし同じトークンに対しては一貫したcontextを出力するよう学習
   - これが暗黙的に固定点学習として機能

3. **ContextBlockの収縮性**:
   - 1レイヤーの場合、ContextBlockは収縮写像に近い性質を持つ可能性
   - 任意の初期値から同じ固定点に収束

---

## 結論と推奨

### 発見

1. **CVFPは必須ではない** - 多様性損失のみで固定点に収束可能
2. **CVFPの効果は限定的** - +1.8%のER改善（ODCMの場合）
3. **ODCMが最も安定** - CVFPの有無に関わらず高いER

### 推奨

1. **ODCM採用** - 現行MCDLより23%高いER（87.4% vs 64.3%）
2. **dist_reg_weight=0.9維持** - 若干のER改善効果あり
3. **収束率表示の改善検討** - 現在の表示は誤解を招く可能性

---

## 実験詳細データ

### 追加伝播時の初回変化量

| アルゴリズム | Iter 1 MSE | Iter 1 Cos Sim | 解釈 |
|-------------|----------:|---------------:|------|
| HSIC | 0.001894 | 0.9990 | ほぼ固定点 |
| DUE | 0.053820 | 0.9734 | 小変化 |
| DECORR | 0.168665 | 0.9157 | 小変化 |
| WMSE | 0.612441 | 0.6942 | 中変化 |
| MCDL | 0.844752 | 0.5902 | 中変化 |
| CTM | 0.862291 | 0.5818 | 中変化 |
| UDEL | 0.924976 | 0.5523 | 中変化 |
| InfoNCE | 1.312058 | 0.3507 | 大変化 |
| NUC | 1.462194 | 0.3052 | 大変化 |
| ODCM | 1.505579 | 0.3388 | 大変化 |
| SDL | 1.532922 | 0.2385 | 大変化 |
| UNIF | 1.740085 | 0.1592 | 最大変化 |

初回の変化量が大きくても、2回目以降は全て完全収束。
