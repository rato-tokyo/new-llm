# Scaling Experiment Report - Layer 6 (2025-11-27)

## 実験概要

Colab L4 GPUを使用したサンプル数スケーリング実験（6層モデル）。
訓練トークン数とValidation PPLの関係を調査し、スケーリング則を導出。

## 実験環境

- **GPU**: NVIDIA L4 (23.8GB)
- **モデル**: E案アーキテクチャ (ContextBlock + TokenBlock)
  - ContextBlock: 6 layers
  - TokenBlock: 6 layers
  - context_dim: 768
- **パラメータ数**: 91,429,969 (全体), 45,739,345 (Phase 2学習対象)
- **Phase 1設定**:
  - max_iterations: 10
  - learning_rate: 0.002
  - dist_reg_weight: 0.5
- **Phase 2設定**:
  - epochs: 10
  - batch_size: 512
  - patience: 2 (early stopping)

## 実験結果

| Samples | Train Tokens | Val Tokens | Val PPL | Val Acc | Train ER% | Val ER% | Time |
|---------|-------------|------------|---------|---------|-----------|---------|------|
| 50 | 3,495 | 690 | 7,341.91 | 8.42% | 72.8% | 49.2% | 1.8min |
| 100 | 7,466 | 690 | 3,498.24 | 6.97% | 75.8% | 49.0% | 2.5min |
| 200 | 15,832 | 690 | 2,242.03 | 9.00% | 76.1% | 48.8% | 5.1min |
| 500 | 39,814 | 690 | 1,145.23 | 15.24% | 75.9% | 48.5% | 12.6min |

**Total experiment time**: 22.1 minutes

## スケーリング則

### PPL vs Train Tokens

PPLはべき乗則（Power Law）に従う：

```
PPL = A × N^(-α)
```

### 線形回帰分析（scipy.stats.linregress使用）

対数変換したデータで線形回帰を実施：

| Tokens (N) | log(N) | PPL | log(PPL) |
|------------|--------|-----|----------|
| 3,495 | 8.1591 | 7,341.91 | 8.9014 |
| 7,466 | 8.9181 | 3,498.24 | 8.1600 |
| 15,832 | 9.6698 | 2,242.03 | 7.7151 |
| 39,814 | 10.5920 | 1,145.23 | 7.0434 |

### 回帰結果

| 統計量 | 値 |
|--------|-----|
| **傾き (α)** | **0.7463** |
| **切片 log(A)** | 14.9216 |
| **A** | 3,022,654 |
| **R²** | **0.9908** |
| **p-value** | 0.0046 |
| **標準誤差** | 0.0508 |

### 導出された数式

```
log(PPL) = 14.92 - 0.7463 × log(N)
```

または：

```
PPL = 3,022,654 × N^(-0.7463)
```

**スケーリング指数 α = 0.7463**

### モデルの適合度

| Tokens | 実測PPL | 予測PPL | 誤差 |
|--------|---------|---------|------|
| 3,495 | 7,341.91 | 6,852.55 | -6.67% |
| 7,466 | 3,498.24 | 3,888.98 | +11.17% |
| 15,832 | 2,242.03 | 2,219.23 | -1.02% |
| 39,814 | 1,145.23 | 1,115.07 | -2.63% |

## PPL予測表

| Train Tokens | 予測 PPL | 備考 |
|--------------|----------|------|
| 3,495 | 6,853 | 実測: 7,342 |
| 7,466 | 3,889 | 実測: 3,498 |
| 15,832 | 2,219 | 実測: 2,242 |
| 39,814 | 1,115 | 実測: 1,145 |
| 100,000 | **561** | 予測 |
| 500,000 | **169** | 予測 |
| 1,000,000 | **101** | 予測 |
| 10,000,000 | **18** | 予測 |

## Layer 3 vs Layer 6 比較

### スケーリング則の比較

| 項目 | Layer 3 | Layer 6 | 差分 |
|------|---------|---------|------|
| スケーリング指数 α | 0.7143 | 0.7463 | +4.5% |
| 係数 A | 2,264,935 | 3,022,654 | +33.4% |
| R² | 0.9818 | 0.9908 | +0.9% |
| パラメータ数 | 84.3M | 91.4M | +8.4% |

### 同一トークン数でのPPL比較

| Tokens | Layer 3 PPL | Layer 6 PPL | Layer 6/Layer 3 |
|--------|-------------|-------------|-----------------|
| 3,495 | 6,668 | 6,853 | 1.03x |
| 7,466 | 3,877 | 3,889 | 1.00x |
| 15,832 | 2,266 | 2,219 | 0.98x |
| 39,814 | 1,173 | 1,115 | 0.95x |
| 100,000 | 607 | 561 | 0.92x |
| 1,000,000 | 117 | 101 | 0.86x |

**結論**: データ量が増えるほど、Layer 6の方がPPLが低くなる傾向がある。

## 分析に使用したPythonコード

```python
import numpy as np
from scipy import stats

# 実験データ
tokens = np.array([3495, 7466, 15832, 39814])
ppl = np.array([7341.91, 3498.24, 2242.03, 1145.23])

# 対数変換
log_tokens = np.log(tokens)
log_ppl = np.log(ppl)

# 線形回帰: log(PPL) = a + b * log(N)
slope, intercept, r_value, p_value, std_err = stats.linregress(log_tokens, log_ppl)

# 結果
# slope = -0.7463 (α)
# intercept = 14.9216 (log(A))
# A = exp(14.9216) = 3,022,654
# R² = 0.9908
```

## 観察事項

### Phase 1 (CVFP Learning)
- 全サンプル数で**7イテレーション**で早期停止（収束率 > 95%）
- Train Effective Rank: 72.8% - 76.1%（安定）
- Val Effective Rank: 48.5% - 49.2%（Layer 3の56-58%より低い）
- 検証データはすべて**CONVERGED**状態

### Phase 2 (Next-Token Prediction)
- 全実験で**Epoch 1が最良**（early stoppingでEpoch 3-4で停止）
- 過学習傾向が強い（TrainとValの乖離が大きい）
- サンプル数増加でVal Accが改善（8.42% → 15.24%）

### Layer 3との違い
1. **Effective Rank**: Layer 6はLayer 3より低い（~49% vs ~56%）
   - 深いネットワークで情報が圧縮されている可能性
2. **スケーリング指数**: Layer 6の方が急峻（0.7463 vs 0.7143）
   - データ量増加による恩恵が大きい
3. **処理時間**: Layer 6は約1.3倍遅い（22.1min vs 17.2min）

### 問題点・課題
1. **Val Accの不規則性**: 100サンプルで6.97%と最低値
   - early stoppingが早すぎた可能性
2. **過学習**: Train Lossは下がるがVal Lossが上昇
   - 正則化の強化が必要かもしれない
3. **Val ER低下**: Layer 6ではVal ERがLayer 3より約7%低い
   - 深層モデルでの多様性確保に課題

## 結論

1. **スケーリング則の確認**: PPLはトークン数のべき乗に反比例（α=0.7463, R²=0.9908）
2. **Layer 6 vs Layer 3**: 大規模データではLayer 6が有利（PPL 5-14%低下）
3. **実用的なPPL達成には大量データが必要**: PPL<100には約100万トークン以上が必要
4. **過学習対策が次の課題**: 特にPhase 2での正則化強化が必要
