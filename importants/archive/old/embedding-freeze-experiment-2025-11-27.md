# Embedding凍結実験結果 (2025-11-27)

## 実験概要

**目的**: GPT-2事前学習Embeddingを凍結した場合の性能比較

**仮説**: Embedding凍結により過学習を抑制し、汎化性能が向上する

**結果**: **仮説が支持された** - Embedding凍結により大幅な性能向上

---

## 実験条件

| 項目 | 設定 |
|------|------|
| GPU | NVIDIA L4 (23.8GB) |
| アーキテクチャ | E案 (ContextBlock + TokenBlock) |
| レイヤー数 | 6 |
| Weight Tying | 有効 |
| Phase 1 | dist_reg_weight=0.5, max_iterations=10 |
| Phase 2 | epochs=10, batch_size=512, lr=0.002, patience=2 |
| Embedding | **凍結** (GPT-2 pretrained) |

---

## 500サンプル実験

### データ

| 項目 | 値 |
|------|-----|
| サンプル数 | 500 (train) + 50 (val) |
| 訓練トークン | 251,208 |
| 検証トークン | 25,121 |

### Phase 1結果

| 指標 | 値 |
|------|-----|
| 収束イテレーション | 7/10 (Early Stop) |
| 収束トークン | 250,715/251,208 (99.8%) |
| Effective Rank | **76.3%** |
| 処理時間 | 3.9分 |

### Phase 2結果

| Epoch | Train Loss | Train PPL | Val Loss | Val PPL | Val Acc |
|-------|-----------|-----------|----------|---------|---------|
| 1 | 6.3691 | 583.51 | 5.9604 | 387.75 | 17.52% |
| **2** | **5.4994** | **244.55** | **5.8121** | **334.31** | **18.88%** |
| 3 | 4.9591 | 142.46 | 5.8638 | 352.07 | 19.03% |
| 4 | 4.4603 | 86.51 | 6.0198 | 411.51 | 18.49% |

**最終結果 (Best Epoch 2)**:
- **Val PPL: 334.31**
- **Val Acc: 18.88%**
- 処理時間: 79.6分

---

## 1000サンプル実験

### データ

| 項目 | 値 |
|------|-----|
| サンプル数 | 1000 (train) + 50 (val) |
| 訓練トークン | 502,498 |
| 検証トークン | 25,125 |

### Phase 1結果

| 指標 | 値 |
|------|-----|
| 収束イテレーション | 7/10 (Early Stop) |
| 収束トークン | 496,683/502,498 (98.8%) |
| Effective Rank | **76.6%** |
| 処理時間 | 7.8分 |

### Phase 2結果 (途中まで)

| Epoch | Train Loss | Train PPL | Val Loss | Val PPL | Val Acc |
|-------|-----------|-----------|----------|---------|---------|
| 1 | 6.0771 | 435.79 | 5.7499 | 314.15 | 18.57% |
| **2** | **5.3595** | **212.61** | **5.6358** | **280.27** | **19.91%** |
| 3 | 4.9343 | 138.97 | 5.6719 | 290.60 | 20.24% |

**最終結果 (Best Epoch 2時点)**:
- **Val PPL: 280.27**
- **Val Acc: 19.91%**

---

## 比較分析：Embedding凍結 vs Embedding学習

### 500サンプル比較

| 指標 | Embedding学習 | Embedding凍結 | 改善率 |
|------|--------------|--------------|--------|
| Val PPL | 1189.15 | **334.31** | **-71.9%** |
| Val Acc | 11.58% | **18.88%** | **+63.0%** |

### 1000サンプル比較

| 指標 | Embedding学習 | Embedding凍結 | 改善率 |
|------|--------------|--------------|--------|
| Val PPL | 840.46 | **280.27** | **-66.7%** |
| Val Acc | 13.03% | **19.91%** | **+52.8%** |

---

## 学習パラメータ数の比較

| 設定 | 学習パラメータ | 全パラメータ | 割合 |
|------|--------------|-------------|------|
| Embedding学習 | ~49.2M | 52.78M | 93.2% |
| **Embedding凍結** | **7.09M** | 52.78M | **13.4%** |

**削減効果**: 学習パラメータを**85.6%削減**

---

## 性能向上の要因分析

### 1. 過学習抑制効果

```
Embedding学習時の問題:
- 学習パラメータ: 49.2M
- データ量: 251K〜502Kトークン
- パラメータ/データ比: 約100:1 → 過学習しやすい

Embedding凍結時:
- 学習パラメータ: 7.09M
- パラメータ/データ比: 約14:1〜7:1 → より適切
```

### 2. GPT-2事前学習の恩恵

- GPT-2 Embeddingは大規模コーパス（WebText, 40GB）で学習済み
- 語彙の意味的関係が既にエンコードされている
- これを壊さず活用することで、少ないデータでも高性能

### 3. Weight Tyingの相乗効果

```
Weight Tying + Embedding凍結:
- Output Head = Embedding（共有）
- Embedding凍結 → Output Headも凍結
- 入出力の一貫性が完全に保持される
```

---

## 重要な知見

### 1. 小規模データでのベストプラクティス

**推奨**: データ量が限られている場合（<1Mトークン）は**Embedding凍結**を採用

### 2. Chinchilla則との整合性

```
7.09Mパラメータ × 20 = 141.8Mトークン（理想データ量）
実際: 251K〜502Kトークン

→ 凍結によりパラメータを削減し、データ効率を向上
```

### 3. Phase 1への影響なし

- Embedding凍結はPhase 2のみに影響
- Phase 1のEffective Rank（76.3%〜76.6%）は良好
- ContextBlockの学習は正常に機能

---

## 設定推奨

```python
# config.py での推奨設定

# Phase 2: Embedding凍結を有効化
phase2_freeze_embedding = True

# Weight Tying（必須）
use_weight_tying = True

# 小規模データでの設定
phase2_epochs = 10
phase2_patience = 2  # 早期停止
phase2_batch_size = 512
phase2_learning_rate = 0.002
```

---

## 結論

**Embedding凍結は小規模〜中規模データでの標準設定として採用すべき**

| 効果 | 詳細 |
|------|------|
| PPL改善 | **-66%〜-72%** |
| Accuracy改善 | **+53%〜+63%** |
| パラメータ削減 | **-85.6%** |
| 訓練安定性 | 向上（過学習抑制） |

---

## 実験ログ（生データ）

### 500サンプル実験

```
======================================================================
EXPERIMENT: 500 samples, freeze_embedding=True
======================================================================
  Total tokens: 276,329
  Train tokens: 251,208
  Val tokens: 25,121

Phase 1: 3.9min, Train ER=76.3%
Phase 2: Early stopped at epoch 4

Best epoch: 2
Best validation loss: 5.8121
Best validation PPL: 334.31
Best validation accuracy: 18.88%
Total time: 79.6min
```

### 1000サンプル実験

```
======================================================================
EXPERIMENT: 1000 samples, freeze_embedding=True
======================================================================
  Total tokens: 527,623
  Train tokens: 502,498
  Val tokens: 25,125

Phase 1: 7.8min, Train ER=76.6%
Phase 2: (実験進行中)

Epoch 2時点:
Best validation loss: 5.6358
Best validation PPL: 280.27
Best validation accuracy: 19.91%
```

---

Last Updated: 2025-11-27
Experiment Status: 1000サンプル実験は進行中（Epoch 3まで完了）
