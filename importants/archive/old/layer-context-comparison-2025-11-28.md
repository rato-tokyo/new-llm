# レイヤー数 vs コンテキスト次元 比較実験結果

**実験日時**: 2025-11-28 05:39:13
**GPU**: NVIDIA L4 (23.8GB)
**実行時間**: 2.3時間

> **注意**: この実験は等差減少設計導入前の旧コードで実行されました。
> パラメータ計算が現在のアーキテクチャとは異なります。

## 実験設定

| パラメータ | 値 |
|-----------|-----|
| `num_input_tokens` | 2 |
| `phase1_learning_rate` | 0.002 |
| `phase2_learning_rate` | 0.002 |
| `phase2_freeze_embedding` | True |
| `use_weight_tying` | True |
| `val_samples` | 50 |

### 比較対象

| 実験 | num_layers | context_dim | ContextBlock | TokenBlock | Phase2学習対象 |
|------|-----------|-------------|--------------|------------|---------------|
| A | 6 | 768 | 10.63M | 10.63M | 7.68M |
| B | 3 | 1536 | 14.17M | 7.08M | 5.91M |

## 結果サマリー

| 実験 | Samples | Train Tokens | Val PPL | Val Acc | Train ER% | Val ER% | Phase1 | Phase2 | Total |
|------|---------|--------------|---------|---------|-----------|---------|--------|--------|-------|
| A | 500 | 86,925 | **554.58** | 15.99% | 83.0% | 82.3% | 1.7min | 25.6min | 27.3min |
| B | 500 | 86,925 | 586.36 | **16.39%** | 80.8% | 79.6% | 1.3min | 14.2min | 15.5min |
| A | 1000 | 183,620 | 447.22 | 17.54% | 83.0% | 82.2% | 3.5min | 53.3min | 56.8min |
| B | 1000 | 183,620 | **439.79** | **18.07%** | 80.8% | 79.5% | 2.8min | 29.0min | 31.8min |

## スケーリング則分析

### PPL = A × tokens^α (log-log線形回帰)

**実験A (Deep & Narrow: 6層, 768dim)**:
- トークン数: 86,925 → 183,620
- PPL: 554.58 → 447.22

計算:
```
log(PPL) = α × log(tokens) + log(A)

α = (log(447.22) - log(554.58)) / (log(183620) - log(86925))
α = (6.103 - 6.318) / (12.121 - 11.373)
α = -0.215 / 0.748
α = -0.287

A = exp(log(PPL) - α × log(tokens))
A = exp(6.103 - (-0.287) × 12.121)
A = exp(6.103 + 3.479)
A = exp(9.582)
A = 14,504
```

**結果**: `PPL = 14,504 × tokens^(-0.287)`

予測精度:
- tokens=86,925: 実測554.58, 予測554.58 (err=0.0%)
- tokens=183,620: 実測447.22, 予測447.22 (err=0.0%)

---

**実験B (Shallow & Wide: 3層, 1536dim)**:
- トークン数: 86,925 → 183,620
- PPL: 586.36 → 439.79

計算:
```
α = (log(439.79) - log(586.36)) / (log(183620) - log(86925))
α = (6.086 - 6.374) / (12.121 - 11.373)
α = -0.288 / 0.748
α = -0.385

A = exp(6.086 - (-0.385) × 12.121)
A = exp(6.086 + 4.667)
A = exp(10.753)
A = 46,791
```

**結果**: `PPL = 46,791 × tokens^(-0.385)`

予測精度:
- tokens=86,925: 実測586.36, 予測586.36 (err=0.0%)
- tokens=183,620: 実測439.79, 予測439.79 (err=0.0%)

## スケーリング係数比較

| 実験 | α (傾き) | A (切片) | 解釈 |
|------|---------|---------|------|
| A (6層, 768dim) | **-0.287** | 14,504 | データ増加による改善が緩やか |
| B (3層, 1536dim) | **-0.385** | 46,791 | データ増加による改善が急激 |

### 解釈

1. **実験Bの方がスケーリング効率が良い** (α=-0.385 vs α=-0.287)
   - 実験Bはデータ量を2倍にすると、PPLが約30%改善
   - 実験Aはデータ量を2倍にすると、PPLが約22%改善

2. **500サンプルでは実験Aが有利**
   - A: 554.58 vs B: 586.36 (Aが5.4%良い)

3. **1000サンプルでは実験Bが有利**
   - A: 447.22 vs B: 439.79 (Bが1.7%良い)

4. **クロスオーバーポイント**
   ```
   14,504 × tokens^(-0.287) = 46,791 × tokens^(-0.385)
   tokens^(0.098) = 3.226
   tokens = 3.226^(1/0.098) ≈ 141,000
   ```
   約141,000トークン付近で両実験のPPLが逆転

5. **Effective Rank**
   - 実験A: 82-83% (安定)
   - 実験B: 79-81% (やや低いが許容範囲)

## Phase 1 収束状況

| 実験 | Samples | 最終収束率 | イテレーション | 収束パターン |
|------|---------|-----------|---------------|--------------|
| A | 500 | 100% (86,925/86,925) | 7/10 | 早期停止 |
| A | 1000 | 100% (183,620/183,620) | 7/10 | 早期停止 |
| B | 500 | 97.9% (85,109/86,925) | 9/10 | 早期停止 |
| B | 1000 | 97.9% (179,777/183,620) | 9/10 | 早期停止 |

- 実験Aは7イテレーションで100%収束
- 実験Bは9イテレーションで約98%収束（わずかに遅い）

## Phase 2 過学習パターン

全実験でEpoch 2がベストで、Epoch 3-4で過学習が発生:

| 実験 | Best Epoch | Epoch 2 Val PPL | Epoch 4 Val PPL | 過学習率 |
|------|-----------|-----------------|-----------------|---------|
| A-500 | 2 | 554.58 | 1247.38 | +125% |
| A-1000 | 2 | 447.22 | 702.79 | +57% |
| B-500 | 2 | 586.36 | 1647.25 | +181% |
| B-1000 | 2 | 439.79 | 1007.75 | +129% |

- Phase 2の学習率 0.002 は高すぎる可能性
- Early stopping (patience=2) が適切に機能

## 結論

### 現時点での推奨

1. **小規模データ（~100Kトークン）**: 実験A (6層, 768dim) が有利
2. **大規模データ（>150Kトークン）**: 実験B (3層, 1536dim) が有利
3. **スケーリング効率**: 実験Bの方が良い (α=-0.385)

### 改善提案

1. **Phase 2学習率を下げる**: 0.002 → 0.001
   - 過学習の抑制
   - より多くのエポックで学習可能に

2. **より多くのサンプルサイズでテスト**
   - 2000, 5000サンプルでスケーリング則を検証
   - クロスオーバーポイントの確認

3. **等差減少設計での再実験**
   - 現在のアーキテクチャでの比較が必要

## 注記

この実験は旧アーキテクチャ（等差減少設計導入前）で実行されました。
現在のコードでは次元構造が異なるため、パラメータ数も変わります。
