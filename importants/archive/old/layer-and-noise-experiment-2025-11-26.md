# レイヤー数とコンテキストノイズ実験結果 (2025-11-26)

## 実験目的

E案アーキテクチャにおいて、以下の2つの要因がモデル性能に与える影響を比較検証:
1. **レイヤー数**: 3層 vs 6層
2. **コンテキストノイズ**: ノイズなし vs ノイズあり（Phase 1のみ）

## 実験環境

- **GPU**: NVIDIA L4 (23.8 GB)
- **データセット**: UltraChat 500サンプル (64,000トークン)
  - 訓練: 62,720トークン
  - 検証: 1,280トークン
- **アーキテクチャ**: E案 (Separated ContextBlock + TokenBlock)
- **diversity_weight**: 0.5

## 実験条件

| 実験名 | レイヤー数 | ノイズ | Early Stop Patience | 備考 |
|--------|-----------|--------|---------------------|------|
| 3layer_old | 3 | なし | 3 | 旧設定 |
| 6layer | 6 | なし | 2 | 新設定 |
| 3layer_noise | 3 | あり (0.1) | 2 | ノイズ追加 |

---

## Phase 1 結果比較

### Effective Rank（コンテキストの多様性）

| 実験 | Train ER | Val ER | Train/Val Gap |
|------|----------|--------|---------------|
| 3layer_old | 80.3% (616.92/768) | 69.5% (533.69/768) | -10.8% |
| 6layer | 75.9% (582.66/768) | 61.4% (471.63/768) | -14.5% |
| **3layer_noise** | **78.8% (604.85/768)** | **67.9% (521.48/768)** | **-10.9%** |

### Phase 1 訓練時間

| 実験 | 時間 | イテレーション |
|------|------|----------------|
| 3layer_old | 46.5s | 30回 |
| 6layer | 79.4s | 10回 |
| 3layer_noise | 41.0s | 10回 |

### CVFP収束

| 実験 | 最終CVFP Loss | 最終収束率 |
|------|---------------|-----------|
| 3layer_old | 0.000069 | 100.0% |
| 6layer | 0.000512 | 100.0% |
| 3layer_noise | 0.012928 | ~100% |

---

## Phase 2 結果比較

### 最終性能（Best Validation）

| 実験 | Best Val PPL | Best Val Acc | Best Epoch | Final Val Acc |
|------|-------------|--------------|------------|---------------|
| 3layer_old | 774.75 | 16.18% | 1 | 17.36% |
| 6layer | 995.97 | 13.84% | 1 | 16.03% |
| **3layer_noise** | **828.13** | **18.76%** | **6** | **20.80%** |

### Epoch-by-Epoch 詳細

#### 3layer_old (Early Stop: Epoch 4)
```
Epoch |  Train PPL |    Val PPL |  Val Acc
    1 |    1404.88 |     774.75 |   16.18% ⭐
    2 |     103.42 |    1075.45 |   17.04%
    3 |      27.23 |    1829.88 |   16.65%
    4 |      17.29 |    1748.19 |   17.36%
```
**問題**: Epoch 1が最良で、その後即座に過学習。改善が継続しない。

#### 6layer (Early Stop: Epoch 3)
```
Epoch |  Train PPL |    Val PPL |  Val Acc
    1 |    2086.18 |     995.97 |   13.84% ⭐
    2 |     233.51 |    1025.22 |   15.72%
    3 |      82.82 |    1280.91 |   16.03%
```
**問題**: 初期性能が低く、すぐに過学習。レイヤー増加がマイナスに作用。

#### 3layer_noise (Early Stop: Epoch 8) ⭐ 最良
```
Epoch |  Train PPL |    Val PPL |  Val Acc
    1 |    6511.59 |    2728.76 |    8.13%
    2 |     640.71 |    1478.84 |   11.26%
    3 |     254.45 |    1055.36 |   14.93%
    4 |     110.10 |     919.46 |   16.97%
    5 |      52.20 |     856.90 |   17.75%
    6 |      26.95 |     828.13 |   18.76% ⭐
    7 |      15.78 |     840.57 |   19.78%
    8 |      10.22 |     876.79 |   20.80%
```
**成功**: 6エポックまで継続的にVal PPLが改善。安定した学習曲線。

---

## 重要な発見: 3層ノイズありvsノイズなし比較

### 過学習抑制効果

| 指標 | ノイズなし | ノイズあり | 効果 |
|------|-----------|-----------|------|
| 継続改善エポック数 | 1 | **6** | **+5エポック** |
| Best Val PPLのエポック | 1 | **6** | 後半まで改善 |
| Train/Val PPL乖離 (Epoch 4) | 17.29 vs 1748.19 (101x) | 110.10 vs 919.46 (8.3x) | **12x改善** |
| Final Val Acc | 17.36% | **20.80%** | **+3.44%** |

### 学習曲線の質

**ノイズなし**:
- Epoch 1で最良Val PPL達成後、即座に悪化
- Train PPLは急速に低下するが、Val PPLは発散
- 典型的な**過学習パターン**

**ノイズあり**:
- Epoch 1-6まで継続的にVal PPLが改善
- Train PPLとVal PPLの乖離が小さい
- **健全な学習曲線**

### なぜノイズが効果的か

1. **正則化効果**: コンテキストにノイズを加えることで、ContextBlockが特定の入力パターンに過度に依存することを防ぐ
2. **汎化性能向上**: Phase 1でより汎用的な固定点を学習し、Phase 2で新しいデータに対応しやすくなる
3. **Train/Val Gap縮小**: 訓練データへの過適合を抑制

---

## 6層 vs 3層 比較

### 結果

| 指標 | 3層 | 6層 | 差分 |
|------|-----|-----|------|
| Val ER | 69.5% | 61.4% | **-8.1%** |
| Best Val PPL | 774.75 | 995.97 | **+28.5%悪化** |
| Best Val Acc | 16.18% | 13.84% | **-2.34%** |
| Phase 1時間 | 46.5s | 79.4s | +70.8% |
| パラメータ数 | 84.3M | 91.4M | +8.4% |

### 考察

6層の性能が3層より低い理由:
1. **Effective Rank低下**: 6層では多様性が失われる（-8.1%）
2. **過学習加速**: より多くのパラメータがより早く過学習
3. **Phase 1収束不足**: 10イテレーションでは6層の完全な収束に不十分か
4. **データ量不足**: 64,000トークンは6層モデルには少なすぎる可能性

---

## 総合ランキング

### Phase 2 性能（Best Val PPL低い順）

| 順位 | 実験 | Best Val PPL | Best Val Acc |
|------|------|-------------|--------------|
| 1 | 3layer_old | 774.75 | 16.18% |
| 2 | **3layer_noise** | 828.13 | **18.76%** |
| 3 | 6layer | 995.97 | 13.84% |

### 実用性（学習の安定性）

| 順位 | 実験 | 継続改善エポック | Final Val Acc | 推奨度 |
|------|------|----------------|---------------|-------|
| **1** | **3layer_noise** | **6** | **20.80%** | ⭐⭐⭐ |
| 2 | 3layer_old | 1 | 17.36% | ⭐⭐ |
| 3 | 6layer | 1 | 16.03% | ⭐ |

---

## 結論

### 1. コンテキストノイズは効果的 ✅

**3層ノイズあり**は、Best Val PPLではノイズなしにわずかに劣るが、以下の点で優れている:
- **学習が安定**: 6エポックまで継続的に改善
- **最終精度が高い**: Final Val Acc 20.80% vs 17.36%（+3.44%）
- **過学習抑制**: Train/Val乖離が大幅に縮小

### 2. レイヤー増加は現時点で非推奨 ❌

6層モデルは3層より明確に性能が低い:
- Effective Rank低下
- Val PPL悪化
- 訓練時間増加

データ量を増やす、またはPhase 1イテレーション数を増やすことで改善の余地あり。

### 3. 推奨設定

```python
# 推奨設定
num_layers = 3
phase1_context_noise = 0.1  # コンテキストノイズ追加
early_stopping_patience = 3  # より多くのエポックを許容
```

---

## 今後の検証項目

1. **ノイズ強度の最適化**: 0.05, 0.1, 0.2 で比較
2. **データ量増加時の6層性能**: 500サンプル → 2000サンプルで再検証
3. **Phase 1イテレーション増加**: 6層で30イテレーション実行
4. **Phase 2のノイズ**: TokenBlockにもノイズを追加して効果検証

---

**Last Updated**: 2025-11-26
**Status**: Experiment Completed
**Recommendation**: 3層 + コンテキストノイズ(0.1) を標準設定として採用
