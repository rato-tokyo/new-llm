remote: Enumerating objects: 11, done.
remote: Counting objects: 100% (11/11), done.
remote: Compressing objects: 100% (2/2), done.
remote: Total 6 (delta 4), reused 6 (delta 4), pack-reused 0 (from 0)
Unpacking objects: 100% (6/6), 723 bytes | 361.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   a7e452b..6b985f1  main       -> origin/main
Updating a7e452b..6b985f1
Fast-forward
 src/trainers/phase1/memory.py | 10 ++++++----
 1 file changed, 6 insertions(+), 4 deletions(-)
======================================================================
EXPERIMENT
======================================================================
Configurations: ['C2T2']
Samples: 2000
Context dim: 1000
Output: importants/logs/20251202_084229_c2t2
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)

============================================================
Running C2T2: Context 2L, Token 2L
============================================================
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
Data: 2,403,563 train, 22,723 val tokens
2025-12-02 08:42:31.134566: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-02 08:42:31.152242: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764664951.173620   15403 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764664951.180440   15403 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764664951.197102   15403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764664951.197137   15403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764664951.197140   15403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764664951.197143   15403 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-02 08:42:31.202420: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using G案 architecture: ContextBlock(2L) + TokenBlock(2L)
  num_input_tokens: 1
  Context injection: Layer1=prev, LayerN=current
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
Parameters: 44,861,168 total
  ContextBlock: 3,542,000
  TokenBlock: 2,720,256

[Phase 1] OACD: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=19.9260 [25.4s]
  Iter 3: conv=0% loss=12.5708 [25.2s]
  Iter 4: conv=0% loss=6.0815 [20.6s]
  Iter 5: conv=0% loss=4.5842 [20.6s]
  Iter 6: conv=0% loss=4.4405 [20.7s]
  Iter 7: conv=0% loss=3.8215 [20.7s]
  Iter 8: conv=0% loss=2.7464 [20.7s]
  Iter 9: conv=0% loss=2.2816 [20.8s]
  Iter 10: conv=1% loss=2.2580 [20.8s]
  Iter 11: conv=2% loss=2.0040 [20.7s]
  Iter 12: conv=5% loss=1.6524 [20.8s]
  Iter 13: conv=8% loss=1.4453 [20.9s]
  Iter 14: conv=15% loss=1.2890 [20.8s]
  Iter 15: conv=26% loss=1.1630 [20.9s]
  Iter 16: conv=39% loss=1.0652 [21.1s]
  Iter 17: conv=50% loss=0.9546 [20.8s]
  Iter 18: conv=59% loss=0.8588 [20.8s]
  Iter 19: conv=67% loss=0.7832 [20.7s]
  Iter 20: conv=74% loss=0.7151 [20.7s]
  Iter 21: conv=80% loss=0.6630 [20.8s]
  Iter 22: conv=85% loss=0.6110 [20.7s]
  Iter 23: conv=89% loss=0.5426 [20.8s]
  Iter 24: conv=91% loss=0.4888 [20.9s]
  → Early stop: conv 91% >= 90%
  Done: 91% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [11.5s]
Phase 1: 534.7s, 24 iter, conv=91%, ER=75.3%/73.4%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 2,721,792/44,861,168 parameters

[Phase 2] 2,403,563 train / 22,723 val tokens, 20 epochs
  Epoch 1: train_ppl=433.3 val_ppl=218.2 acc=20.6% [57.5s] ★
  Epoch 2: train_ppl=185.3 val_ppl=169.4 acc=22.5% [56.8s] ★
  Epoch 3: train_ppl=140.1 val_ppl=152.3 acc=23.3% [57.3s] ★
  Epoch 4: train_ppl=116.7 val_ppl=144.1 acc=23.8% [57.0s] ★
  Epoch 5: train_ppl=101.9 val_ppl=140.2 acc=24.1% [57.2s] ★
  Epoch 6: train_ppl=91.4 val_ppl=138.6 acc=24.2% [57.0s] ★
  Epoch 7: train_ppl=83.6 val_ppl=137.9 acc=24.4% [57.2s] ★
  Epoch 8: train_ppl=77.5 val_ppl=138.2 acc=24.4% [57.0s]
  → Early stop at epoch 8
  Best: epoch 7, ppl=137.9, acc=24.4%
Phase 2: 457.0s, Best epoch 7
Result: PPL=137.9, Acc=24.4%
Total time: 991.6s

======================================================================
SUMMARY
======================================================================

Config     Context  Token    Params       Val PPL    Acc      ER%      Time    
--------------------------------------------------------------------------------
C2T2       2        2        44,861,168  137.9      24.4     73.4     991.6   

Results saved to: importants/logs/20251202_084229_c2t2/results.txt

======================================================================
DONE
======================================================================
