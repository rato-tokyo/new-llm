remote: Enumerating objects: 21, done.
remote: Counting objects: 100% (21/21), done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 13 (delta 11), reused 10 (delta 8), pack-reused 0 (from 0)
Unpacking objects: 100% (13/13), 3.36 KiB | 689.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   a7db678..52ee457  main       -> origin/main
Updating a7db678..52ee457
Fast-forward
 CLAUDE.md                                | 35 +++++++++++++++---
 scripts/experiment_context_dim_search.py | 15 +++-----
 src/utils/__init__.py                    |  2 ++
 src/utils/cache.py                       | 61 ++++++++++++++++++++++++++++++++
 4 files changed, 98 insertions(+), 15 deletions(-)
======================================================================
CONTEXT DIM SEARCH EXPERIMENT
======================================================================
Samples: 2000
Dim step: 100
Max dim: 500
Output: importants/logs/20251202_154911_context_dim_search
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)
Loading data...
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
Data: 2,403,563 train, 22,723 val tokens

======================================================================
[DIM=100] Starting experiment...
======================================================================

Creating SimpleLLM (context_dim=100)...
2025-12-02 15:49:13.858222: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-02 15:49:13.873714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764690553.894972    6788 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764690553.901411    6788 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764690553.917816    6788 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764690553.917853    6788 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764690553.917856    6788 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764690553.917858    6788 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-02 15:49:13.922734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding

[Phase 1] Training ContextBlock (context_dim=100)...

[Phase 1] Context: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=2.2938 [5.3s]
  Iter 3: conv=0% loss=2.8659 [4.3s]
  Iter 4: conv=0% loss=2.3467 [4.3s]
  Iter 5: conv=0% loss=1.6051 [4.2s]
  Iter 6: conv=0% loss=1.2889 [4.4s]
  Iter 7: conv=0% loss=1.3741 [4.2s]
  Iter 8: conv=1% loss=1.5569 [4.3s]
  Iter 9: conv=2% loss=1.6828 [4.2s]
  Iter 10: conv=3% loss=1.7553 [4.3s]
  Iter 11: conv=7% loss=1.7872 [4.2s]
  Iter 12: conv=17% loss=1.7518 [4.2s]
  Iter 13: conv=30% loss=1.6449 [4.2s]
  Iter 14: conv=38% loss=1.5089 [4.2s]
  Iter 15: conv=46% loss=1.3977 [4.2s]
  Iter 16: conv=58% loss=1.3281 [4.3s]
  Iter 17: conv=72% loss=1.2781 [4.2s]
  Iter 18: conv=84% loss=1.2241 [4.2s]
  Iter 19: conv=91% loss=1.1527 [4.2s]
  → Early stop: conv 91% >= 90%
  Done: 91% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [2.8s]
Phase 1: 87.9s, 19 iter, conv=91%

[Phase 2 Prep] Collecting context cache...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
      2,403,562/2,403,562 tokens processed...
      22,722/22,722 tokens processed...
    Collecting token embeddings (chunked)...
Cache collection: 475.7s
Effective Rank: Val=75.9% (75.9/100)

[Phase 2] Training TokenBlock...
✓ ContextBlock frozen
✓ Embedding frozen
✓ Training TokenBlock only: 668,928/39,354,940 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 40 epochs
    Epoch 1: train_ppl=7346.1 val_ppl=321.4 acc=18.1% [53.2s] *
    Epoch 2: train_ppl=288.7 val_ppl=219.9 acc=19.6% [54.5s] *
    Epoch 3: train_ppl=217.4 val_ppl=190.2 acc=20.4% [54.5s] *
    Epoch 4: train_ppl=188.7 val_ppl=176.8 acc=20.9% [54.5s] *
    Epoch 5: train_ppl=172.7 val_ppl=168.8 acc=21.1% [54.5s] *
    Epoch 6: train_ppl=162.4 val_ppl=163.6 acc=21.3% [54.6s] *
    Epoch 7: train_ppl=155.0 val_ppl=159.9 acc=21.5% [54.5s] *
    Epoch 8: train_ppl=149.4 val_ppl=157.2 acc=21.7% [54.5s] *
    Epoch 9: train_ppl=145.0 val_ppl=155.1 acc=21.8% [54.5s] *
    Epoch 10: train_ppl=141.4 val_ppl=153.4 acc=21.9% [54.5s] *
    Epoch 11: train_ppl=138.4 val_ppl=152.0 acc=22.0% [54.5s] *
    Epoch 12: train_ppl=135.9 val_ppl=150.9 acc=22.0% [54.6s] *
    Epoch 13: train_ppl=133.8 val_ppl=150.0 acc=22.1% [54.5s] *
    Epoch 14: train_ppl=131.9 val_ppl=149.2 acc=22.1% [54.5s] *
    Epoch 15: train_ppl=130.3 val_ppl=148.5 acc=22.2% [54.5s] *
    Epoch 16: train_ppl=128.8 val_ppl=147.9 acc=22.3% [54.5s] *
    Epoch 17: train_ppl=127.6 val_ppl=147.3 acc=22.3% [54.6s] *
    Epoch 18: train_ppl=126.4 val_ppl=146.8 acc=22.4% [54.6s] *
    Epoch 19: train_ppl=125.4 val_ppl=146.4 acc=22.4% [54.5s] *
    Epoch 20: train_ppl=124.4 val_ppl=146.0 acc=22.4% [54.5s]
    → Early stop at epoch 20
    Best: epoch 19, ppl=146.4, acc=22.4%

[Result] dim=100: PPL=146.4, Acc=22.4%, ER=75.9%, Time=1653.0s
  ★ New best! dim=100, PPL=146.4

======================================================================
[DIM=200] Starting experiment...
======================================================================

Creating SimpleLLM (context_dim=200)...
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding

[Phase 1] Training ContextBlock (context_dim=200)...

[Phase 1] Context: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=4.1616 [6.8s]
  Iter 3: conv=0% loss=4.8892 [6.3s]
  Iter 4: conv=0% loss=3.4552 [6.2s]
  Iter 5: conv=0% loss=2.1761 [6.2s]
  Iter 6: conv=0% loss=2.1362 [6.2s]
  Iter 7: conv=0% loss=2.6399 [6.3s]
  Iter 8: conv=1% loss=3.0135 [6.3s]
  Iter 9: conv=1% loss=3.1787 [6.2s]
  Iter 10: conv=1% loss=3.2592 [6.3s]
  Iter 11: conv=2% loss=3.3030 [6.2s]
  Iter 12: conv=3% loss=3.2692 [6.3s]
  Iter 13: conv=6% loss=3.1555 [6.3s]
  Iter 14: conv=11% loss=2.9916 [6.3s]
  Iter 15: conv=20% loss=2.7849 [6.4s]
  Iter 16: conv=32% loss=2.5482 [6.5s]
  Iter 17: conv=45% loss=2.3028 [6.4s]
  Iter 18: conv=57% loss=2.0577 [6.3s]
  Iter 19: conv=67% loss=1.8174 [6.3s]
  Iter 20: conv=74% loss=1.5935 [6.3s]
  Iter 21: conv=80% loss=1.4020 [6.3s]
  Iter 22: conv=86% loss=1.2481 [6.3s]
  Iter 23: conv=90% loss=1.1223 [6.3s]
  → Early stop: conv 90% >= 90%
  Done: 90% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [3.6s]
Phase 1: 154.1s, 23 iter, conv=90%

[Phase 2 Prep] Collecting context cache...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
      2,403,562/2,403,562 tokens processed...
      22,722/22,722 tokens processed...
    Collecting token embeddings (chunked)...
Cache collection: 477.8s
Effective Rank: Val=70.9% (141.7/200)

[Phase 2] Training TokenBlock...
✓ ContextBlock frozen
✓ Embedding frozen
✓ Training TokenBlock only: 745,728/39,538,840 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 40 epochs
    Epoch 1: train_ppl=5672.5 val_ppl=294.9 acc=18.9% [53.7s] *
    Epoch 2: train_ppl=267.3 val_ppl=202.5 acc=20.4% [54.7s] *
    Epoch 3: train_ppl=201.4 val_ppl=175.1 acc=21.2% [54.5s] *
    Epoch 4: train_ppl=174.3 val_ppl=162.0 acc=21.6% [54.6s] *
    Epoch 5: train_ppl=159.1 val_ppl=154.2 acc=22.0% [54.6s] *
    Epoch 6: train_ppl=149.1 val_ppl=149.1 acc=22.3% [54.6s] *
    Epoch 7: train_ppl=141.9 val_ppl=145.5 acc=22.5% [54.7s] *
    Epoch 8: train_ppl=136.4 val_ppl=142.9 acc=22.6% [54.7s] *
    Epoch 9: train_ppl=132.1 val_ppl=140.9 acc=22.8% [54.8s] *
    Epoch 10: train_ppl=128.7 val_ppl=139.3 acc=22.9% [54.7s] *
    Epoch 11: train_ppl=125.8 val_ppl=138.1 acc=22.9% [54.6s] *
    Epoch 12: train_ppl=123.4 val_ppl=137.1 acc=23.0% [54.8s] *
    Epoch 13: train_ppl=121.3 val_ppl=136.3 acc=23.0% [54.6s] *
    Epoch 14: train_ppl=119.5 val_ppl=135.6 acc=23.1% [54.7s] *
    Epoch 15: train_ppl=118.0 val_ppl=135.0 acc=23.2% [54.8s] *
    Epoch 16: train_ppl=116.6 val_ppl=134.4 acc=23.3% [54.7s] *
    Epoch 17: train_ppl=115.3 val_ppl=134.0 acc=23.3% [54.7s] *
    Epoch 18: train_ppl=114.2 val_ppl=133.5 acc=23.3% [54.8s] *
    Epoch 19: train_ppl=113.2 val_ppl=133.2 acc=23.3% [54.8s]
    → Early stop at epoch 19
    Best: epoch 18, ppl=133.5, acc=23.3%

[Result] dim=200: PPL=133.5, Acc=23.3%, ER=70.9%, Time=1670.1s
  ★ New best! dim=200, PPL=133.5

======================================================================
[DIM=300] Starting experiment...
======================================================================

Creating SimpleLLM (context_dim=300)...
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding

[Phase 1] Training ContextBlock (context_dim=300)...

[Phase 1] Context: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=7.1131 [7.6s]
  Iter 3: conv=0% loss=8.2129 [7.0s]
  Iter 4: conv=0% loss=5.8594 [7.0s]
  Iter 5: conv=0% loss=3.7728 [7.0s]
  Iter 6: conv=0% loss=3.3784 [7.0s]
  Iter 7: conv=0% loss=3.6976 [7.0s]
  Iter 8: conv=0% loss=3.9448 [7.0s]
  Iter 9: conv=0% loss=4.0526 [7.0s]
  Iter 10: conv=0% loss=4.1488 [7.0s]
  Iter 11: conv=1% loss=4.2809 [7.0s]
  Iter 12: conv=2% loss=4.3474 [7.0s]
  Iter 13: conv=4% loss=4.2104 [7.0s]
  Iter 14: conv=7% loss=3.8388 [7.0s]
  Iter 15: conv=10% loss=3.3509 [7.0s]
  Iter 16: conv=15% loss=2.9185 [7.0s]
  Iter 17: conv=22% loss=2.6330 [7.0s]
  Iter 18: conv=35% loss=2.4439 [7.1s]
  Iter 19: conv=52% loss=2.2449 [7.0s]
  Iter 20: conv=64% loss=1.9998 [7.0s]
  Iter 21: conv=71% loss=1.7537 [7.0s]
  Iter 22: conv=77% loss=1.5672 [7.0s]
  Iter 23: conv=84% loss=1.4556 [7.0s]
  Iter 24: conv=90% loss=1.3799 [7.0s]
  → Early stop: conv 90% >= 90%
  Done: 90% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [4.4s]
Phase 1: 180.4s, 24 iter, conv=90%

[Phase 2 Prep] Collecting context cache...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
      2,403,562/2,403,562 tokens processed...
      22,722/22,722 tokens processed...
    Collecting token embeddings (chunked)...
Cache collection: 472.9s
Effective Rank: Val=66.5% (199.5/300)

[Phase 2] Training TokenBlock...
✓ ContextBlock frozen
✓ Embedding frozen
✓ Training TokenBlock only: 822,528/39,742,740 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 40 epochs
    Epoch 1: train_ppl=4812.0 val_ppl=290.0 acc=19.0% [54.1s] *
    Epoch 2: train_ppl=260.1 val_ppl=198.2 acc=20.8% [55.1s] *
    Epoch 3: train_ppl=196.8 val_ppl=171.5 acc=21.7% [54.7s] *
    Epoch 4: train_ppl=169.9 val_ppl=158.8 acc=22.1% [55.1s] *
    Epoch 5: train_ppl=154.9 val_ppl=151.1 acc=22.6% [55.0s] *
    Epoch 6: train_ppl=144.8 val_ppl=146.0 acc=22.7% [55.1s] *
    Epoch 7: train_ppl=137.6 val_ppl=142.4 acc=23.0% [55.1s] *
    Epoch 8: train_ppl=132.1 val_ppl=139.8 acc=23.0% [55.2s] *
    Epoch 9: train_ppl=127.8 val_ppl=137.9 acc=23.1% [55.2s] *
    Epoch 10: train_ppl=124.3 val_ppl=136.4 acc=23.1% [55.2s] *
    Epoch 11: train_ppl=121.4 val_ppl=135.2 acc=23.2% [55.1s] *
    Epoch 12: train_ppl=119.0 val_ppl=134.2 acc=23.3% [55.3s] *
    Epoch 13: train_ppl=116.9 val_ppl=133.3 acc=23.4% [55.1s] *
    Epoch 14: train_ppl=115.1 val_ppl=132.5 acc=23.5% [55.1s] *
    Epoch 15: train_ppl=113.5 val_ppl=131.9 acc=23.6% [55.3s] *
    Epoch 16: train_ppl=112.1 val_ppl=131.3 acc=23.6% [55.2s] *
    Epoch 17: train_ppl=110.8 val_ppl=130.8 acc=23.6% [55.2s] *
    Epoch 18: train_ppl=109.7 val_ppl=130.4 acc=23.6% [55.2s] *
    Epoch 19: train_ppl=108.7 val_ppl=130.0 acc=23.7% [55.3s]
    → Early stop at epoch 19
    Best: epoch 18, ppl=130.4, acc=23.6%

[Result] dim=300: PPL=130.4, Acc=23.6%, ER=66.5%, Time=1699.8s
  ★ New best! dim=300, PPL=130.4

======================================================================
[DIM=400] Starting experiment...
======================================================================

Creating SimpleLLM (context_dim=400)...
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding

[Phase 1] Training ContextBlock (context_dim=400)...

[Phase 1] Context: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=9.0400 [10.0s]
  Iter 3: conv=0% loss=9.8229 [9.1s]
  Iter 4: conv=0% loss=6.4559 [8.9s]
  Iter 5: conv=0% loss=4.3959 [8.9s]
  Iter 6: conv=0% loss=4.1577 [8.9s]
  Iter 7: conv=0% loss=4.5442 [8.9s]
  Iter 8: conv=0% loss=5.0003 [8.9s]
  Iter 9: conv=0% loss=5.2358 [8.9s]
  Iter 10: conv=0% loss=5.2747 [9.0s]
  Iter 11: conv=0% loss=5.2838 [8.9s]
  Iter 12: conv=1% loss=5.2872 [8.9s]
  Iter 13: conv=2% loss=5.1744 [8.9s]
  Iter 14: conv=3% loss=4.8701 [8.9s]
  Iter 15: conv=5% loss=4.4313 [8.9s]
  Iter 16: conv=7% loss=3.9844 [8.9s]
  Iter 17: conv=10% loss=3.5771 [8.9s]
  Iter 18: conv=15% loss=3.1933 [8.9s]
  Iter 19: conv=22% loss=2.8167 [8.9s]
  Iter 20: conv=30% loss=2.4604 [8.9s]
  Iter 21: conv=41% loss=2.1485 [8.9s]
  Iter 22: conv=52% loss=1.8949 [9.0s]
  Iter 23: conv=64% loss=1.6927 [8.9s]
  Iter 24: conv=73% loss=1.5278 [8.9s]
  Iter 25: conv=80% loss=1.3969 [8.9s]
  Iter 26: conv=85% loss=1.3023 [8.9s]
  Iter 27: conv=88% loss=1.2419 [9.0s]
  Iter 28: conv=91% loss=1.2073 [8.9s]
  → Early stop: conv 91% >= 90%
  Done: 91% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [5.2s]
Phase 1: 265.3s, 28 iter, conv=91%

[Phase 2 Prep] Collecting context cache...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
      2,403,562/2,403,562 tokens processed...
      22,722/22,722 tokens processed...
    Collecting token embeddings (chunked)...
Cache collection: 503.1s
Effective Rank: Val=61.4% (245.8/400)

[Phase 2] Training TokenBlock...
✓ ContextBlock frozen
✓ Embedding frozen
✓ Training TokenBlock only: 899,328/39,966,640 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 40 epochs
    Epoch 1: train_ppl=4413.9 val_ppl=295.9 acc=18.9% [54.3s] *
    Epoch 2: train_ppl=262.0 val_ppl=199.5 acc=20.8% [55.3s] *
    Epoch 3: train_ppl=196.5 val_ppl=172.2 acc=21.7% [55.0s] *
    Epoch 4: train_ppl=169.9 val_ppl=159.4 acc=22.3% [55.3s] *
    Epoch 5: train_ppl=154.6 val_ppl=151.7 acc=22.7% [55.3s] *
    Epoch 6: train_ppl=144.5 val_ppl=146.7 acc=22.9% [55.3s] *
    Epoch 7: train_ppl=137.2 val_ppl=143.2 acc=23.0% [55.3s] *
    Epoch 8: train_ppl=131.7 val_ppl=140.5 acc=23.1% [55.3s] *
    Epoch 9: train_ppl=127.3 val_ppl=138.5 acc=23.2% [55.2s] *
    Epoch 10: train_ppl=123.7 val_ppl=136.9 acc=23.4% [55.4s] *
    Epoch 11: train_ppl=120.7 val_ppl=135.6 acc=23.5% [55.3s] *
    Epoch 12: train_ppl=118.2 val_ppl=134.5 acc=23.6% [55.4s] *
    Epoch 13: train_ppl=116.0 val_ppl=133.6 acc=23.7% [55.3s] *
    Epoch 14: train_ppl=114.2 val_ppl=132.7 acc=23.8% [55.3s] *
    Epoch 15: train_ppl=112.5 val_ppl=132.0 acc=23.8% [55.3s] *
    Epoch 16: train_ppl=111.0 val_ppl=131.4 acc=23.9% [55.4s] *
    Epoch 17: train_ppl=109.7 val_ppl=130.9 acc=23.9% [55.2s] *
    Epoch 18: train_ppl=108.5 val_ppl=130.4 acc=23.9% [55.5s] *
    Epoch 19: train_ppl=107.5 val_ppl=130.0 acc=24.0% [55.4s] *
    Epoch 20: train_ppl=106.5 val_ppl=129.6 acc=24.0% [55.5s]
    → Early stop at epoch 20
    Best: epoch 19, ppl=130.0, acc=24.0%

[Result] dim=400: PPL=130.0, Acc=24.0%, ER=61.4%, Time=1874.0s
  ★ New best! dim=400, PPL=130.0

======================================================================
[DIM=500] Starting experiment...
======================================================================

Creating SimpleLLM (context_dim=500)...
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding

[Phase 1] Training ContextBlock (context_dim=500)...

[Phase 1] Context: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.3053 [10.7s]
  Iter 3: conv=0% loss=12.4939 [10.3s]
  Iter 4: conv=0% loss=7.7170 [10.2s]
  Iter 5: conv=0% loss=4.9527 [10.2s]
  Iter 6: conv=0% loss=4.7930 [10.2s]
  Iter 7: conv=0% loss=5.0476 [10.2s]
  Iter 8: conv=0% loss=5.1228 [10.3s]
  Iter 9: conv=0% loss=5.1718 [10.2s]
  Iter 10: conv=0% loss=5.2915 [10.2s]
  Iter 11: conv=0% loss=5.4398 [10.2s]
  Iter 12: conv=1% loss=5.5000 [10.1s]
  Iter 13: conv=1% loss=5.4279 [10.1s]
  Iter 14: conv=1% loss=5.1864 [10.1s]
  Iter 15: conv=3% loss=4.8167 [10.1s]
  Iter 16: conv=4% loss=4.3994 [10.1s]
  Iter 17: conv=7% loss=3.9602 [10.2s]
  Iter 18: conv=10% loss=3.4928 [10.2s]
  Iter 19: conv=16% loss=3.0412 [10.2s]
  Iter 20: conv=24% loss=2.6910 [10.2s]
  Iter 21: conv=35% loss=2.4656 [10.2s]
  Iter 22: conv=47% loss=2.3055 [10.2s]
  Iter 23: conv=57% loss=2.1479 [10.3s]
  Iter 24: conv=65% loss=1.9721 [10.3s]
  Iter 25: conv=71% loss=1.8022 [10.2s]
  Iter 26: conv=75% loss=1.6689 [10.1s]
  Iter 27: conv=78% loss=1.5861 [10.1s]
  Iter 28: conv=81% loss=1.5621 [10.2s]
  Iter 29: conv=83% loss=1.5965 [10.2s]
  Iter 30: conv=86% loss=1.6638 [10.2s]
  Iter 31: conv=90% loss=1.7119 [10.1s]
  Iter 32: conv=93% loss=1.6852 [10.2s]
  → Early stop: conv 93% >= 90%
  Done: 93% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [6.0s]
Phase 1: 344.9s, 32 iter, conv=93%

[Phase 2 Prep] Collecting context cache...
      100,000/2,403,562 tokens processed...
      200,000/2,403,562 tokens processed...
      300,000/2,403,562 tokens processed...
      400,000/2,403,562 tokens processed...
      500,000/2,403,562 tokens processed...
      600,000/2,403,562 tokens processed...
      700,000/2,403,562 tokens processed...
      800,000/2,403,562 tokens processed...
      900,000/2,403,562 tokens processed...
      1,000,000/2,403,562 tokens processed...
      1,100,000/2,403,562 tokens processed...
      1,200,000/2,403,562 tokens processed...
      1,300,000/2,403,562 tokens processed...
      1,400,000/2,403,562 tokens processed...
      1,500,000/2,403,562 tokens processed...
      1,600,000/2,403,562 tokens processed...
      1,700,000/2,403,562 tokens processed...
      1,800,000/2,403,562 tokens processed...
      1,900,000/2,403,562 tokens processed...
      2,000,000/2,403,562 tokens processed...
      2,100,000/2,403,562 tokens processed...
      2,200,000/2,403,562 tokens processed...
      2,300,000/2,403,562 tokens processed...
      2,400,000/2,403,562 tokens processed...
      2,403,562/2,403,562 tokens processed...
      22,722/22,722 tokens processed...
    Collecting token embeddings (chunked)...
Cache collection: 509.1s
Effective Rank: Val=57.7% (288.5/500)

[Phase 2] Training TokenBlock...
✓ ContextBlock frozen
✓ Embedding frozen
✓ Training TokenBlock only: 976,128/40,210,540 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 40 epochs
    Epoch 1: train_ppl=3969.6 val_ppl=296.8 acc=18.9% [55.7s] *
    Epoch 2: train_ppl=267.8 val_ppl=201.8 acc=20.7% [54.9s] *
    Epoch 3: train_ppl=200.6 val_ppl=175.4 acc=21.6% [55.5s] *
    Epoch 4: train_ppl=173.9 val_ppl=162.9 acc=22.1% [55.6s] *
    Epoch 5: train_ppl=158.7 val_ppl=155.6 acc=22.5% [55.4s] *
    Epoch 6: train_ppl=148.6 val_ppl=150.7 acc=22.7% [55.6s] *
    Epoch 7: train_ppl=141.3 val_ppl=147.3 acc=22.8% [55.5s] *
    Epoch 8: train_ppl=135.7 val_ppl=144.7 acc=22.9% [55.5s] *
    Epoch 9: train_ppl=131.2 val_ppl=142.6 acc=23.1% [55.5s] *
    Epoch 10: train_ppl=127.5 val_ppl=141.0 acc=23.1% [55.8s] *
    Epoch 11: train_ppl=124.4 val_ppl=139.6 acc=23.2% [55.5s] *
    Epoch 12: train_ppl=121.8 val_ppl=138.5 acc=23.3% [55.6s] *
    Epoch 13: train_ppl=119.6 val_ppl=137.5 acc=23.5% [55.5s] *
    Epoch 14: train_ppl=117.6 val_ppl=136.7 acc=23.6% [55.7s] *
