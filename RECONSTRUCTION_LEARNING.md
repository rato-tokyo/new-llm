# Context Reconstruction Learning - New-LLM v3

## Core Concept

**æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã®æ­£è§£ = ã€Œå‰ã®æ–‡è„ˆ + ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã€ã‚’åœ§ç¸®ã—ãŸã‚‚ã®**

ã“ã‚Œã¯ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼çš„ãªå­¦ç¿’ã§ã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä¸è¦ã§å®Ÿè£…å¯èƒ½ã€‚

## Architecture

### Example: "èµ¤ã„ãƒªãƒ³ã‚´"

**t=1**:
- **å…¥åŠ›**:
  - æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«: `[0]` (256æ¬¡å…ƒã®ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«)
  - ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿: `embed("èµ¤ã„")` (256æ¬¡å…ƒ)
- **å‡ºåŠ›ã®æ­£è§£**:
  - ãƒˆãƒ¼ã‚¯ãƒ³: `"ãƒªãƒ³ã‚´"` (æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³)
  - æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«: `compress([0, embed("èµ¤ã„")])` (512æ¬¡å…ƒâ†’256æ¬¡å…ƒã«åœ§ç¸®)

### Loss Functions

1. **Token Loss**: `CrossEntropy(predicted_token, next_token)`
2. **Reconstruction Loss**: `MSE(decoded_context, [prev_context, current_token])`
3. **Total Loss**: `token_loss + Î» * reconstruction_loss` (Î»=1.0)

## Model Components

### 1. ContextVectorLLM (æ—¢å­˜)
- Token embedding (256æ¬¡å…ƒ)
- Context vector (256æ¬¡å…ƒ)
- FNN layers
- Token output head
- Context update head (with gating)

### 2. Context Decoder (NEW)
```python
context_decoder = nn.Sequential(
    nn.Linear(256, 512),  # 256 (context) â†’ 512 (context + embed)
    nn.ReLU(),
    nn.Linear(512, 512),
)
```

## Training Process

### Forward Pass
```
for t in range(seq_len):
    # 1. å¾©å…ƒã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
    target = concat([prev_context, token_embed[t]])  # 512æ¬¡å…ƒ

    # 2. FNN forward
    fnn_input = concat([token_embed[t], prev_context])
    hidden = FNN(fnn_input)

    # 3. Token prediction
    token_logits = token_head(hidden)

    # 4. Context update
    new_context = update_context(hidden, prev_context)

    # 5. Reconstruction (for loss)
    reconstructed = context_decoder(new_context)
    reconstruction_loss += MSE(reconstructed, target)
```

### Loss Computation
```python
token_loss = CrossEntropy(predicted_tokens, true_tokens)
reconstruction_loss = MSE(reconstructed, targets)
total_loss = token_loss + reconstruction_loss
```

## Benefits

1. **No External Teacher**: æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä¸è¦
2. **Interpretable**: æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ãŒä½•ã‚’åœ§ç¸®ã—ã¦ã„ã‚‹ã‹æ˜ç¢º
3. **Flexible**: context_dimã®å¤§å°ã§ã€é•·æ–‡é‡è¦– vs ç›´è¿‘é‡è¦–ã‚’èª¿æ•´å¯èƒ½
4. **Simple**: å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«

## Implementation (PyTorch Only)

### Dependencies
- `torch` - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- `tokenizers` (HuggingFace) - BPE tokenizer
- `datasets` (HuggingFace) - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
- `tqdm` - ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼

### Training Script Structure
```python
# 1. Data loading
texts = load_wikitext()
tokenizer = train_bpe_tokenizer(texts)
dataset = tokenize(texts, tokenizer)

# 2. Model
model = ContextVectorLLM(config)

# 3. Training loop
for epoch in range(epochs):
    for batch in dataloader:
        # Forward
        logits, context_trajectory = model(input_ids)

        # Token loss
        token_loss = cross_entropy(logits[:-1], labels[1:])

        # Reconstruction loss
        reconstructed = model.context_decoder(context_trajectory)
        targets = create_targets(context_trajectory, token_embeds)
        recon_loss = mse_loss(reconstructed, targets)

        # Combined loss
        loss = token_loss + recon_loss
        loss.backward()
        optimizer.step()
```

## Expected Results

### Success Criteria
- Token loss: é †èª¿ã«æ¸›å°‘
- Reconstruction loss: é †èª¿ã«æ¸›å°‘
- Perplexity: < 100 (ç›®æ¨™)
- Context vectorãŒæ„å‘³ã®ã‚ã‚‹æƒ…å ±ã‚’ä¿æŒ

### Monitoring
- Epochæ¯ã®loss, perplexity, accuracy
- Token loss vs Reconstruction loss ã®ãƒãƒ©ãƒ³ã‚¹

## Parameter Tuning

### Context Loss Weight (Î»)
- Î»=1.0: Token prediction ã¨ reconstruction ã‚’åŒç­‰é‡è¦– (æ¨å¥¨)
- Î»>1.0: Reconstruction ã‚’é‡è¦–
- Î»<1.0: Token prediction ã‚’é‡è¦–

### Context Dimension
- 256: ãƒãƒ©ãƒ³ã‚¹å‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
- 512: é•·æ–‡é‡è¦–
- 128: ç›´è¿‘é‡è¦–

## Next Steps

1. âœ… Context decoder ã‚’å®Ÿè£… (å®Œäº†)
2. âœ… Forward passã§ reconstruction targets ã‚’ä¿å­˜ (å®Œäº†)
3. ğŸ”„ PyTorchã®ã¿ã®è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ (é€²è¡Œä¸­)
4. â³ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ (1 layer, 2 epochs)
5. â³ è©•ä¾¡ãƒ»æ”¹å–„
