remote: Enumerating objects: 16, done.
remote: Counting objects: 100% (16/16), done.
remote: Compressing objects: 100% (6/6), done.
remote: Total 11 (delta 8), reused 8 (delta 5), pack-reused 0 (from 0)
Unpacking objects: 100% (11/11), 2.29 KiB | 783.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
   3a3692b..af761e9  main       -> origin/main
Updating 3a3692b..af761e9
Fast-forward
 colab_output.txt => colab_output_3layer.txt |   0
 config.py                                   |   6 +-
 src/trainers/phase2.py                      | 133 ++++++++++++++++------------
 3 files changed, 77 insertions(+), 62 deletions(-)
 rename colab_output.txt => colab_output_3layer.txt (100%)

======================================================================
New-LLM Training for Google Colab
======================================================================

âœ… Random seed fixed: 42 (å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼)
ğŸ–¥ï¸  Device: cuda
   GPU: NVIDIA L4
   Memory: 23.8 GB

ğŸ“‹ Configuration:
   Architecture: residual_standard
   Layers: 6
   Context dim: 768
   Diversity weight: 0.5
   Phase 2 epochs: 10
   Early stopping patience: 3
   Context-Fixed Learning: context_out = C*[i] (complete fixing)

ğŸ“¥ Downloading GPT-2 tokenizer...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 138kB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.38MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 6.01MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.01MB/s]
config.json: 100% 665/665 [00:00<00:00, 5.14MB/s]
âœ“ Tokenizer saved

ğŸ“¦ Creating model...
2025-11-26 06:24:28.352077: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-26 06:24:28.368387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764138268.387190   64071 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764138268.392719   64071 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764138268.409277   64071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764138268.409304   64071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764138268.409307   64071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764138268.409310   64071 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-26 06:24:28.414173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
âœ“ Model created: 130,027,345 parameters

ğŸ“Š Loading data from UltraChat...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 243kB/s]
config.json: 100% 665/665 [00:00<00:00, 6.35MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.72MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 5.99MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 8.56MB/s]
   Downloading UltraChat dataset...
Generating train_sft split: 100% 207865/207865 [00:03<00:00, 54333.42 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 59000.45 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 66424.68 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 72396.60 examples/s]
   Cached to: ./cache/ultrachat_500samples_128len.pt
   Total tokens: 64,000
   Train: 62,720 tokens
   Val:   1,280 tokens (fixed size)
   âœ“ Validation auto-generated from training data

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP)
======================================================================


======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP) - Train
======================================================================
Iteration 1/30: é †ä¼æ’­ã®ã¿ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ï¼‰ [136.34s]
Iteration 2/30: åæŸ=100.0% | Total=-0.053928 | CVFP=0.000021 | Div=-0.107877 | Time=9.62s
Iteration 3/30: åæŸ=0.0% | Total=0.406375 | CVFP=0.916799 | Div=-0.104049 | Time=0.85s
Iteration 4/30: åæŸ=0.0% | Total=0.621511 | CVFP=1.350726 | Div=-0.107704 | Time=0.82s
Iteration 5/30: åæŸ=0.0% | Total=0.654029 | CVFP=1.416812 | Div=-0.108754 | Time=0.82s
Iteration 6/30: åæŸ=0.0% | Total=0.584952 | CVFP=1.277719 | Div=-0.107814 | Time=0.82s
Iteration 7/30: åæŸ=0.0% | Total=0.444561 | CVFP=0.994730 | Div=-0.105609 | Time=0.82s
Iteration 8/30: åæŸ=0.0% | Total=0.268968 | CVFP=0.640722 | Div=-0.102786 | Time=0.82s
Iteration 9/30: åæŸ=0.1% | Total=0.120171 | CVFP=0.340407 | Div=-0.100066 | Time=0.82s
Iteration 10/30: åæŸ=9.6% | Total=0.042912 | CVFP=0.184818 | Div=-0.098995 | Time=0.82s
Iteration 11/30: åæŸ=21.8% | Total=0.042634 | CVFP=0.183398 | Div=-0.098131 | Time=0.82s
Iteration 12/30: åæŸ=23.2% | Total=0.027278 | CVFP=0.151906 | Div=-0.097350 | Time=0.82s
Iteration 13/30: åæŸ=49.4% | Total=-0.015394 | CVFP=0.065987 | Div=-0.096776 | Time=0.82s
Iteration 14/30: åæŸ=79.1% | Total=-0.036963 | CVFP=0.022632 | Div=-0.096558 | Time=0.82s
Iteration 15/30: åæŸ=94.8% | Total=-0.043273 | CVFP=0.010142 | Div=-0.096688 | Time=0.82s
Iteration 16/30: åæŸ=99.7% | Total=-0.046241 | CVFP=0.004514 | Div=-0.096995 | Time=0.83s
Iteration 17/30: åæŸ=100.0% | Total=-0.047698 | CVFP=0.001823 | Div=-0.097220 | Time=0.83s
Iteration 18/30: åæŸ=100.0% | Total=-0.048377 | CVFP=0.000820 | Div=-0.097574 | Time=0.82s
Iteration 19/30: åæŸ=100.0% | Total=-0.048736 | CVFP=0.000408 | Div=-0.097881 | Time=0.82s
Iteration 20/30: åæŸ=100.0% | Total=-0.048957 | CVFP=0.000320 | Div=-0.098233 | Time=0.83s
Iteration 21/30: åæŸ=100.0% | Total=-0.049155 | CVFP=0.000255 | Div=-0.098566 | Time=0.82s
Iteration 22/30: åæŸ=100.0% | Total=-0.049337 | CVFP=0.000245 | Div=-0.098919 | Time=0.82s
Iteration 23/30: åæŸ=100.0% | Total=-0.049523 | CVFP=0.000206 | Div=-0.099252 | Time=0.83s
Iteration 24/30: åæŸ=100.0% | Total=-0.049699 | CVFP=0.000204 | Div=-0.099601 | Time=0.83s
Iteration 25/30: åæŸ=100.0% | Total=-0.049878 | CVFP=0.000173 | Div=-0.099929 | Time=0.82s
Iteration 26/30: åæŸ=100.0% | Total=-0.050048 | CVFP=0.000173 | Div=-0.100269 | Time=0.82s
Iteration 27/30: åæŸ=100.0% | Total=-0.050220 | CVFP=0.000151 | Div=-0.100590 | Time=0.83s
Iteration 28/30: åæŸ=100.0% | Total=-0.050383 | CVFP=0.000151 | Div=-0.100917 | Time=0.83s
Iteration 29/30: åæŸ=100.0% | Total=-0.050547 | CVFP=0.000135 | Div=-0.101229 | Time=0.82s
Iteration 30/30: åæŸ=100.0% | Total=-0.050704 | CVFP=0.000136 | Div=-0.101544 | Time=0.83s

Phase 1 å®Œäº†: 62720/62720 ãƒˆãƒ¼ã‚¯ãƒ³ãŒåæŸ


======================================================================
Evaluating on validation data...
======================================================================


â±ï¸  Phase 1 completed in 173.0s
ğŸ’¾ Checkpoint saved: ./checkpoints/model_latest.pt

======================================================================
FIXED-POINT ANALYSIS
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Train
======================================================================

(Sampling 5000/62720 tokens for pairwise analysis)

1. Global Attractor Detection:
  Avg L2 Distance: 33.918251 (Range: [0.000000, 48.719772])
  Avg Cosine Sim:  0.124976 (Range: [-0.606013, 1.000001])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.165588 (Range: [27.066053, 27.242073])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.1656, Median: 27.1424, Std: 0.0446
  Pairwise Dist - Mean: 33.9183
  Sparsity: 0.55% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 286.61 / 768 (37.3%)
  Top 5 Singular Values: [1868.48779296875, 954.0681762695312, 942.379638671875, 739.8680419921875, 499.1927185058594]
  âœ… Good diversity
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Val
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 34.054569 (Range: [0.000000, 48.686253])
  Avg Cosine Sim:  0.118154 (Range: [-0.602470, 1.000000])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.175909 (Range: [27.079741, 27.251976])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.1759, Median: 27.1522, Std: 0.0458
  Pairwise Dist - Mean: 34.0546
  Sparsity: 0.55% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 230.75 / 768 (30.0%)
  Top 5 Singular Values: [668.8953857421875, 349.7662048339844, 332.8782958984375, 267.456787109375, 179.1278533935547]
  âœ… Good diversity
======================================================================


======================================================================
æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯ (Identity Mapping Check)
======================================================================
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦çµ±è¨ˆ (100ã‚µãƒ³ãƒ—ãƒ«):
  å¹³å‡: -0.0018
  æœ€å¤§: 0.1013
  æœ€å°: -0.0858
  é–¾å€¤(0.95)è¶…é: 0/100 (0.0%)

âœ… æ­£å¸¸: æ’ç­‰å†™åƒã§ã¯ã‚ã‚Šã¾ã›ã‚“
    ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚
======================================================================


======================================================================
PHASE 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å­¦ç¿’
======================================================================

âœ“ Full model fine-tuning: 91,429,969/130,027,345 parameters trainable
âœ“ Context-Fixed Learning: context_out replaced with C*[i] (complete fixing)

======================================================================
PHASE 2: Next-Token Prediction Training
         (Context-Fixed Learning)
======================================================================

Training tokens: 62,720
Validation tokens: 1,280
Epochs: 10
Batch size: 2048
Learning rate: 0.002
CVFP layers frozen: False
Early stopping patience: 3
âœ“ Stage 1: Initialize fixed contexts C* from training data
âœ“ Stage 2: Train with context_out = C*[i] (complete fixing)
âœ“ Prediction from concatenated C*[i] + token_out
âœ“ Gradients flow through token_out only

Stage 1: Initializing fixed contexts C*...
âœ“ Training target contexts initialized: torch.Size([62720, 768])
âœ“ Validation target contexts initialized: torch.Size([1280, 768])

Stage 2: Training with fixed contexts...
Epoch 1/10:
  Train Loss: 7.8736 | Train PPL: 2627.08
  Val Loss: 7.4861 | Val PPL: 1783.04 | Val Acc: 10.48%
  âœ“ New best validation loss: 7.4861

Epoch 2/10:
  Train Loss: 6.2556 | Train PPL: 520.92
  Val Loss: 8.5370 | Val PPL: 5100.08 | Val Acc: 12.67%
  âš ï¸ No improvement (1/3)

Epoch 3/10:
  Train Loss: 6.0578 | Train PPL: 427.44
  Val Loss: 8.2450 | Val PPL: 3808.56 | Val Acc: 13.76%
  âš ï¸ No improvement (2/3)

Epoch 4/10:
  Train Loss: 5.8580 | Train PPL: 350.04
  Val Loss: 7.9927 | Val PPL: 2959.42 | Val Acc: 13.45%
  âš ï¸ No improvement (3/3)

â›” Early stopping triggered at epoch 4
   Val loss hasn't improved for 3 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 1
Best validation loss: 7.4861
Best validation PPL: 1783.04
Best validation accuracy: 10.48%
Early stopped at epoch: 4


â±ï¸  Phase 2 completed in 128.3s
ğŸ’¾ Final checkpoint saved: ./checkpoints/model_latest.pt


======================================================================
                    NEW-LLM TRAINING RESULTS                         
======================================================================

[PHASE 1: Context Learning (CVFP)]
  Effective Rank (Train): 37.3% (286.61/768)
  Effective Rank (Val):   30.0% (230.75/768)
  Time: 173.0s
  Status: âœ… PASSED

[PHASE 2: Token Prediction]
  Best Val PPL:    1783.04 (Epoch 1)
  Best Val Acc:    10.48%
  Final Val PPL:   2959.42
  Final Val Acc:   13.45%
  Epochs Run:      4/10
  Time: 128.3s
  Status: âš ï¸  EARLY STOPPED at epoch 4

----------------------------------------------------------------------
  TOTAL TIME: 327.1s
======================================================================

ğŸ“‰ Epoch-by-Epoch Progress:
--------------------------------------------------
 Epoch |  Train PPL |    Val PPL |  Val Acc
--------------------------------------------------
     1 |    2627.08 |    1783.04 |   10.48% â­
     2 |     520.92 |    5100.08 |   12.67%
     3 |     427.44 |    3808.56 |   13.76%
     4 |     350.04 |    2959.42 |   13.45%
--------------------------------------------------

âœ… Training complete!
