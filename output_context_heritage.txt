remote: Enumerating objects: 46, done.
remote: Counting objects: 100% (46/46), done.
remote: Compressing objects: 100% (12/12), done.
remote: Total 27 (delta 17), reused 25 (delta 15), pack-reused 0 (from 0)
Unpacking objects: 100% (27/27), 13.84 KiB | 1.73 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   3f942c0..6f1cb20  main       -> origin/main
Updating 3f942c0..6f1cb20
Fast-forward
 CLAUDE.md                                          |  24 +-
 README.md                                          | 278 +++++++----------
 config/experiment.py                               |  14 +-
 importants/CASCADE_CONTEXT_ARCHITECTURE.md         | 224 ++++++++++++++
 importants/GUIDE_colab_setup.md                    | 329 +++++----------------
 importants/{ => old}/EXPERIMENT_RESULTS_SUMMARY.md |   0
 importants/{ => old}/EXPERIMENT_SUMMARY_2025-12.md |   0
 importants/{ => old}/SUMMARY_algorithms.md         |   0
 importants/{ => old}/SUMMARY_model_design.md       |   0
 importants/{ => old}/SUMMARY_scaling_laws.md       |   0
 scripts/experiment_cascade_context.py              | 270 ++++++++++++++---
 src/models/layers.py                               |   8 +-
 src/trainers/phase1/base.py                        |  12 +-
 src/utils/__init__.py                              |   5 +
 src/utils/memory.py                                |  41 ++-
 15 files changed, 679 insertions(+), 526 deletions(-)
 create mode 100644 importants/CASCADE_CONTEXT_ARCHITECTURE.md
 rename importants/{ => old}/EXPERIMENT_RESULTS_SUMMARY.md (100%)
 rename importants/{ => old}/EXPERIMENT_SUMMARY_2025-12.md (100%)
 rename importants/{ => old}/SUMMARY_algorithms.md (100%)
 rename importants/{ => old}/SUMMARY_model_design.md (100%)
 rename importants/{ => old}/SUMMARY_scaling_laws.md (100%)
======================================================================
CASCADE CONTEXT EXPERIMENT
======================================================================
Samples: 2000
Context dim per block: 500
Combined context dim: 1000
Output: importants/logs/20251202_104903_cascade_context
======================================================================
Device: cuda (NVIDIA L4, 22.2GB)
Loading data...
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
Data: 2,403,563 train, 22,723 val tokens

Creating CascadeContextLLM (cd=500x2=1000)...
2025-12-02 10:49:05.779558: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-02 10:49:05.794939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764672545.815854   47568 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764672545.822284   47568 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764672545.838635   47568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764672545.838669   47568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764672545.838672   47568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764672545.838675   47568 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-02 10:49:05.843580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
✓ Weight Tying: token_output shares weights with token_embedding
Parameters: 41,230,040 total
  ContextBlock A: 635,500
  ContextBlock B: 635,500
  TokenBlock: 1,360,128

[Phase 1A] Training ContextBlock A on full data...

[Phase 1] ContextA: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=11.2666 [12.1s]
  Iter 3: conv=0% loss=13.2642 [11.5s]
  Iter 4: conv=0% loss=10.7598 [9.4s]
  Iter 5: conv=0% loss=7.2918 [9.4s]
  Iter 6: conv=0% loss=4.6390 [9.3s]
  Iter 7: conv=0% loss=3.3870 [9.5s]
  Iter 8: conv=0% loss=3.0625 [9.6s]
  Iter 9: conv=0% loss=3.0001 [9.5s]
  Iter 10: conv=0% loss=2.9394 [9.5s]
  Iter 11: conv=1% loss=2.8780 [9.6s]
  Iter 12: conv=1% loss=2.8119 [9.5s]
  Iter 13: conv=2% loss=2.7116 [9.6s]
  Iter 14: conv=3% loss=2.5479 [9.6s]
  Iter 15: conv=4% loss=2.3241 [9.4s]
  Iter 16: conv=7% loss=2.0890 [9.3s]
  Iter 17: conv=11% loss=1.8964 [9.5s]
  Iter 18: conv=17% loss=1.7516 [9.6s]
  Iter 19: conv=24% loss=1.6315 [9.5s]
  Iter 20: conv=33% loss=1.5246 [9.6s]
  Iter 21: conv=43% loss=1.4299 [9.5s]
  Iter 22: conv=53% loss=1.3410 [9.5s]
  Iter 23: conv=62% loss=1.2527 [9.6s]
  Iter 24: conv=69% loss=1.1693 [9.5s]
  Iter 25: conv=75% loss=1.0994 [9.5s]
  Iter 26: conv=80% loss=1.0442 [9.7s]
  Iter 27: conv=84% loss=1.0022 [9.6s]
  Iter 28: conv=87% loss=0.9743 [9.5s]
  Iter 29: conv=90% loss=0.9620 [9.6s]
  Iter 30: conv=92% loss=0.9573 [9.5s]
  → Early stop: conv 92% >= 90%
  Done: 92% converged
  Collecting cache (parallel)...
    Preparing combined tokens (2,403,562 tokens)...
    Combined tokens ready [0.0s]
  Cache collected (parallel) [5.9s]
Phase 1A: 307.9s, 30 iter, conv=92%
✓ ContextBlock A frozen

[Phase 1B] ContextB: 2,403,562 tokens, 60 iterations
  Input: context_a[i-1] (Cache-Direct method)
  Iter 1: random init
  Iter 2: conv=0% loss=5.8390 [8.9s]
  Iter 3: conv=15% loss=3.2017 [8.6s]
  Iter 4: conv=28% loss=1.5750 [8.8s]
  Iter 5: conv=48% loss=0.7380 [8.8s]
  Iter 6: conv=74% loss=0.4003 [8.7s]
  Iter 7: conv=98% loss=0.3094 [8.7s]
  → Early stop: conv 98% >= 90%
  Done: 98% converged
  Collecting context_b cache...
  Cache collected [7.3s]
Phase 1B: 70.9s, 7 iter, conv=98%
✓ ContextBlock B frozen

[Val Cache] Collecting validation cache...
    Collecting val cache (22,722 tokens)...
    Val cache collected [10.1s]
Val cache collection: 10.1s
  Train cache: torch.Size([2403562, 1000])
  Val cache: torch.Size([22722, 1000])
Effective Rank: Train=77.2%, Val=75.5%

[Phase 2] Training TokenBlock with concatenated context (cd=1000)...
✓ Both ContextBlocks frozen
✓ Embedding frozen
✓ Training TokenBlock only: 1,360,128/41,230,040 parameters

[Phase 2] 2,403,562 train / 22,722 val tokens, 20 epochs
  Epoch 1: train_ppl=335.4 val_ppl=171.6 acc=21.9% [54.3s] ★
  Epoch 2: train_ppl=163.9 val_ppl=145.6 acc=23.2% [56.5s] ★
  Epoch 3: train_ppl=136.2 val_ppl=136.4 acc=23.6% [56.0s] ★
  Epoch 4: train_ppl=122.6 val_ppl=131.8 acc=24.0% [56.4s] ★
  Epoch 5: train_ppl=114.1 val_ppl=129.1 acc=24.1% [56.1s] ★
  Epoch 6: train_ppl=108.3 val_ppl=127.4 acc=24.3% [56.2s] ★
  Epoch 7: train_ppl=103.9 val_ppl=126.3 acc=24.4% [56.2s] ★
  Epoch 8: train_ppl=100.5 val_ppl=125.6 acc=24.5% [56.1s] ★
  Epoch 9: train_ppl=97.8 val_ppl=125.1 acc=24.6% [56.2s] ★
  Epoch 10: train_ppl=95.5 val_ppl=124.7 acc=24.7% [56.2s] ★
  → Early stop at epoch 10 (PPL improvement 0.36 < 0.4)
  Best: epoch 10, ppl=124.7, acc=24.7%

Phase 2: 560.2s, Best epoch 10
Result: PPL=124.7, Acc=24.7%
Total time: 949.2s

======================================================================
SUMMARY - Cascade Context Experiment
======================================================================
Architecture: CascadeContextLLM (A→B cascade, 1L each)
  ContextBlock A: cd=500, trained on full data
  ContextBlock B: cd=500, input=context_a (fixed)
  TokenBlock: cd=1000 (concatenated)
Parameters: 41,230,040
Phase 1A: 307.9s, conv=92%
Phase 1B: 70.9s, conv=98%
Cache collection: 10.1s
Phase 2: 560.2s, epoch 10
Effective Rank: 75.5% (of 1000)
Val PPL: 124.7
Val Acc: 24.7%
Total time: 949.2s
======================================================================

Results saved to: importants/logs/20251202_104903_cascade_context/results.txt

======================================================================
DONE
======================================================================
