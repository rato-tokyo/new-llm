
======================================================================
New-LLM Training for Google Colab (Eæ¡ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
======================================================================

âœ… Random seed fixed: 42 (å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼)
ğŸ–¥ï¸  Device: cuda
   GPU: NVIDIA L4
   Memory: 23.8 GB

ğŸ“‹ Configuration:
   Architecture: Eæ¡ˆ (Separated ContextBlock + TokenBlock)
   Num layers: 3
   Context dim: 768
   Embed dim: 768
   Diversity weight: 0.5
   Phase 2 epochs: 10
   Early stopping patience: 2

ğŸ“¥ Downloading GPT-2 tokenizer...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 173kB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.50MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 67.4MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.20MB/s]
config.json: 100% 665/665 [00:00<00:00, 5.44MB/s]
âœ“ Tokenizer saved

ğŸ“¦ Creating model (Eæ¡ˆ architecture)...
2025-11-26 09:00:50.289013: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-26 09:00:50.305234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764147650.324534   19456 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764147650.330182   19456 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764147650.346900   19456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764147650.346935   19456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764147650.346938   19456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764147650.346941   19456 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-26 09:00:50.351861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using Eæ¡ˆ architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
âœ“ Model created: 84,338,257 parameters

ğŸ“Š Loading data from UltraChat...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 227kB/s]
config.json: 100% 665/665 [00:00<00:00, 6.12MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.48MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 2.10MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.22MB/s]
   Downloading UltraChat dataset...
Generating train_sft split: 100% 207865/207865 [00:03<00:00, 52472.80 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 55244.20 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 71703.99 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 73842.65 examples/s]
   Cached to: ./cache/ultrachat_500samples_128len.pt
   Total tokens: 64,000
   Train: 62,720 tokens
   Val:   1,280 tokens (fixed size)
   âœ“ Validation auto-generated from training data

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP) - ContextBlock
======================================================================


======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (ContextBlock) - Train
======================================================================
Training ContextBlock only (3545856 parameters)
Iteration 1/10: é †ä¼æ’­ã®ã¿ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ï¼‰ [35.65s]
Iteration 2/10: åæŸ=100.0% | Total=-0.046957 | CVFP=0.004465 | Div=-0.098380 | Time=2.66s
Iteration 3/10: åæŸ=0.0% | Total=0.109860 | CVFP=0.326532 | Div=-0.106812 | Time=0.18s
Iteration 4/10: åæŸ=0.0% | Total=0.079819 | CVFP=0.264677 | Div=-0.105040 | Time=0.18s
Iteration 5/10: åæŸ=0.0% | Total=0.027213 | CVFP=0.157745 | Div=-0.103318 | Time=0.18s
Iteration 6/10: åæŸ=0.1% | Total=-0.013170 | CVFP=0.076099 | Div=-0.102438 | Time=0.19s
Iteration 7/10: åæŸ=15.5% | Total=-0.031168 | CVFP=0.040068 | Div=-0.102404 | Time=0.18s
Iteration 8/10: åæŸ=83.6% | Total=-0.039395 | CVFP=0.023922 | Div=-0.102712 | Time=0.18s
Iteration 9/10: åæŸ=99.4% | Total=-0.043376 | CVFP=0.016298 | Div=-0.103050 | Time=0.18s
Iteration 10/10: åæŸ=100.0% | Total=-0.045220 | CVFP=0.012928 | Div=-0.103367 | Time=0.18s

Phase 1 å®Œäº†: 62711/62720 ãƒˆãƒ¼ã‚¯ãƒ³ãŒåæŸ


======================================================================
Evaluating on validation data...
======================================================================


â±ï¸  Phase 1 completed in 41.0s
ğŸ’¾ Checkpoint saved: ./checkpoints/model_latest.pt

======================================================================
FIXED-POINT ANALYSIS
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Train
======================================================================

(Sampling 5000/62720 tokens for pairwise analysis)

1. Global Attractor Detection:
  Avg L2 Distance: 36.468719 (Range: [2.024056, 43.553577])
  Avg Cosine Sim:  0.129499 (Range: [-0.231555, 0.997346])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.757442 (Range: [27.716579, 27.799892])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.7574, Median: 27.7575, Std: 0.0098
  Pairwise Dist - Mean: 36.4687
  Sparsity: 0.64% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 604.85 / 768 (78.8%)
  Top 5 Singular Values: [1118.06494140625, 777.1162109375, 573.2555541992188, 449.01220703125, 405.5528259277344]
  âœ… Good diversity
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Val
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 36.510880 (Range: [0.000000, 43.048920])
  Avg Cosine Sim:  0.127435 (Range: [-0.202412, 1.000000])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.766205 (Range: [27.735970, 27.802240])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.7662, Median: 27.7662, Std: 0.0102
  Pairwise Dist - Mean: 36.5109
  Sparsity: 0.64% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 521.48 / 768 (67.9%)
  Top 5 Singular Values: [400.3413391113281, 278.2320251464844, 210.82284545898438, 152.4429473876953, 140.30772399902344]
  âœ… Good diversity
======================================================================


======================================================================
æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯ (Identity Mapping Check)
======================================================================
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦çµ±è¨ˆ (100ã‚µãƒ³ãƒ—ãƒ«):
  å¹³å‡: 0.0632
  æœ€å¤§: 0.1544
  æœ€å°: -0.0298
  é–¾å€¤(0.95)è¶…é: 0/100 (0.0%)

âœ… æ­£å¸¸: æ’ç­‰å†™åƒã§ã¯ã‚ã‚Šã¾ã›ã‚“
    ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚
======================================================================


======================================================================
PHASE 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å­¦ç¿’ (TokenBlock)
======================================================================

âœ“ ContextBlock frozen
âœ“ token_output layer unfrozen
âœ“ Training TokenBlock + token_output: 42,193,489/84,338,257 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (Eæ¡ˆ - ãƒ¬ã‚¤ãƒ¤ãƒ¼å¯¾å¿œç‰ˆ)
======================================================================

Training tokens: 62,720
Validation tokens: 1,280
Epochs: 10
Batch size: 8192
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: Eæ¡ˆ (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1ã§å­¦ç¿’æ¸ˆã¿)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - Eæ¡ˆ: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§

Epoch 1/10 [295.3s]:
  Train Loss: 8.7813 | Train PPL: 6511.59
  Val Loss: 7.9116 | Val PPL: 2728.76 | Val Acc: 8.13%
  âœ“ New best validation loss: 7.9116

Epoch 2/10 [295.7s]:
  Train Loss: 6.4626 | Train PPL: 640.71
  Val Loss: 7.2990 | Val PPL: 1478.84 | Val Acc: 11.26%
  âœ“ New best validation loss: 7.2990

Epoch 3/10 [295.7s]:
  Train Loss: 5.5391 | Train PPL: 254.45
  Val Loss: 6.9616 | Val PPL: 1055.36 | Val Acc: 14.93%
  âœ“ New best validation loss: 6.9616

Epoch 4/10 [295.4s]:
  Train Loss: 4.7014 | Train PPL: 110.10
  Val Loss: 6.8238 | Val PPL: 919.46 | Val Acc: 16.97%
  âœ“ New best validation loss: 6.8238

Epoch 5/10 [295.7s]:
  Train Loss: 3.9551 | Train PPL: 52.20
  Val Loss: 6.7533 | Val PPL: 856.90 | Val Acc: 17.75%
  âœ“ New best validation loss: 6.7533

Epoch 6/10 [295.7s]:
  Train Loss: 3.2940 | Train PPL: 26.95
  Val Loss: 6.7192 | Val PPL: 828.13 | Val Acc: 18.76%
  âœ“ New best validation loss: 6.7192

Epoch 7/10 [296.5s]:
  Train Loss: 2.7587 | Train PPL: 15.78
  Val Loss: 6.7341 | Val PPL: 840.57 | Val Acc: 19.78%
  âš ï¸ No improvement (1/2)

Epoch 8/10 [296.0s]:
  Train Loss: 2.3247 | Train PPL: 10.22
  Val Loss: 6.7763 | Val PPL: 876.79 | Val Acc: 20.80%
  âš ï¸ No improvement (2/2)

â›” Early stopping triggered at epoch 8
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 6
Best validation loss: 6.7192
Best validation PPL: 828.13
Best validation accuracy: 18.76%
Early stopped at epoch: 8


â±ï¸  Phase 2 completed in 2366.1s
ğŸ’¾ Final checkpoint saved: ./checkpoints/model_latest.pt


======================================================================
                    NEW-LLM TRAINING RESULTS                         
======================================================================

[PHASE 1: Context Learning (CVFP) - ContextBlock]
  Effective Rank (Train): 78.8% (604.85/768)
  Effective Rank (Val):   67.9% (521.48/768)
  Time: 41.0s
  Status: âœ… PASSED

[PHASE 2: Token Prediction - TokenBlock]
  Best Val PPL:    828.13 (Epoch 6)
  Best Val Acc:    18.76%
  Final Val PPL:   876.79
  Final Val Acc:   20.80%
  Epochs Run:      8/10
  Time: 2366.1s
  Status: âš ï¸  EARLY STOPPED at epoch 8

----------------------------------------------------------------------
  TOTAL TIME: 2438.8s
======================================================================

ğŸ“‰ Epoch-by-Epoch Progress:
--------------------------------------------------
 Epoch |  Train PPL |    Val PPL |  Val Acc
--------------------------------------------------
     1 |    6511.59 |    2728.76 |    8.13%
     2 |     640.71 |    1478.84 |   11.26%
     3 |     254.45 |    1055.36 |   14.93%
     4 |     110.10 |     919.46 |   16.97%
     5 |      52.20 |     856.90 |   17.75%
     6 |      26.95 |     828.13 |   18.76% â­
     7 |      15.78 |     840.57 |   19.78%
     8 |      10.22 |     876.79 |   20.80%
--------------------------------------------------

âœ… Training complete!
