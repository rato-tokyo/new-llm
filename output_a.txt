From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
Already up to date.
================================================================================
CONTEXT MODE EXPERIMENT (A案: Final Context Only)
================================================================================
Device: cuda
GPU: NVIDIA L4
Memory: 22.2GB

Samples: 2000
Context dim: 500
Output: importants/logs/20251201_171404_context_mode

Configurations to test:
  - L2_A: layer=2, A案 (final context only)
================================================================================

[L2_A] layer=2, A案 (final context only)
Loading training data...
  Loading from cache: ./cache/ultrachat_2000samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (22723 > 1024). Running this sequence through the model will result in indexing errors
  Train: 2403563 tokens (2000 samples)
  Val:   22723 tokens
    Data: 2,403,563 train, 22,723 val tokens
2025-12-01 17:14:06.053191: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-01 17:14:06.068598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764609246.089211    5813 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764609246.095654    5813 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764609246.111949    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764609246.111975    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764609246.111978    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764609246.111981    5813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-12-01 17:14:06.116809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using A案 (final context only) architecture: ContextBlock(2 layers) + TokenBlock(2 layers)
  num_input_tokens: 1
  token継ぎ足し方式: 全レイヤーでtoken入力
✓ Weight Tying: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Model: 41,822,168 params (layers=2, final_context_only=True)

[Phase 1] OACD: 2,403,563 tokens, 60 iterations
  Iter 1: random init
  Iter 2: conv=0% loss=10.9875 [13.8s]
  Iter 3: conv=0% loss=7.7593 [13.3s]
  Iter 4: conv=0% loss=3.4557 [11.2s]
  Iter 5: conv=0% loss=1.9233 [11.2s]
  Iter 6: conv=0% loss=1.8303 [11.2s]
  Iter 7: conv=1% loss=1.7446 [11.1s]
  Iter 8: conv=3% loss=1.4748 [11.2s]
  Iter 9: conv=9% loss=1.2781 [11.2s]
  Iter 10: conv=26% loss=1.2127 [11.1s]
  Iter 11: conv=58% loss=1.1600 [11.2s]
  Iter 12: conv=80% loss=1.0139 [11.2s]
  Iter 13: conv=89% loss=0.8139 [11.1s]
  Iter 14: conv=93% loss=0.6642 [11.2s]
  → Early stop: conv 93% >= 90%
  Done: 93% converged
  Collecting cache (parallel)...
  Cache collected (parallel) [8.3s]
    Phase 1: 176.8s, 14 iter, conv=93%, ER=81.5%/79.7%
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 1,953,792/41,822,168 parameters

[Phase 2] 2,403,563 train / 22,723 val tokens, 20 epochs
  Epoch 1: train_ppl=438.9 val_ppl=220.2 acc=20.3% [56.8s] ★
  Epoch 2: train_ppl=210.1 val_ppl=178.2 acc=22.0% [57.1s] ★
  Epoch 3: train_ppl=168.8 val_ppl=162.0 acc=22.7% [57.9s] ★
  Epoch 4: train_ppl=146.9 val_ppl=153.3 acc=23.1% [57.5s] ★
  Epoch 5: train_ppl=132.4 val_ppl=147.8 acc=23.6% [57.7s] ★
  Epoch 6: train_ppl=122.0 val_ppl=143.9 acc=23.9% [57.5s] ★
  Epoch 7: train_ppl=114.0 val_ppl=141.3 acc=24.0% [57.5s] ★
  Epoch 8: train_ppl=107.5 val_ppl=139.7 acc=24.1% [57.7s] ★
  Epoch 9: train_ppl=102.2 val_ppl=138.6 acc=24.2% [57.6s] ★
  Epoch 10: train_ppl=97.8 val_ppl=137.8 acc=24.3% [57.6s] ★
  Epoch 11: train_ppl=94.0 val_ppl=137.3 acc=24.4% [57.6s] ★
  Epoch 12: train_ppl=90.6 val_ppl=137.0 acc=24.6% [57.5s] ★
  Epoch 13: train_ppl=87.7 val_ppl=136.9 acc=24.6% [57.6s] ★
  Epoch 14: train_ppl=85.2 val_ppl=137.0 acc=24.6% [57.7s]
  → Early stop at epoch 14
  Best: epoch 13, ppl=136.9, acc=24.6%
    Phase 2: 805.3s, PPL=136.9, Acc=24.6%

================================================================================
SUMMARY
================================================================================

Config     Layers   Mode         Params       Val PPL    Acc      ER%      Time      
------------------------------------------------------------------------------------------
L2_A       2        A案           41,822,168  136.9      24.6    % 79.7     982.1     s

------------------------------------------------------------------------------------------

Best PPL:  L2_A (PPL=136.9)
Best Acc:  L2_A (Acc=24.6%)

Total time: 1006.1s (16.8 min)

Results saved to: importants/logs/20251201_171404_context_mode/results.txt

================================================================================
DONE
================================================================================
