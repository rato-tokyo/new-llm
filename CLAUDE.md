# New-LLM Project Guidelines

---

## ğŸ¯ Infini-Pythia Architecture (Memory-Only)

**Pythia-70Mãƒ™ãƒ¼ã‚¹ã«1å±¤ç›®Infini-Attentionï¼ˆMemory-Onlyï¼‰ã‚’å°å…¥ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Infini-Pythia:
Token Embedding (512-dim)
       â†“
Layer 0: InfiniAttentionLayer (NoPE, Memory Only)
  â””â”€ Memory Attention (linear attention)
       â†“
Layer 1-5: PythiaLayer (RoPE)
  â”œâ”€ Multi-Head Attention
  â””â”€ MLP
       â†“
Output Head (512 â†’ vocab)
```

### Infini-Attention (Memory-Only)

```
ãƒ¡ãƒ¢ãƒªæ›´æ–° (Delta Rule):
  M_s = M_{s-1} + Ïƒ(K)^T @ (V - retrieved_V)

ãƒ¡ãƒ¢ãƒªå–å¾—:
  A_mem = Ïƒ(Q) @ M / (Ïƒ(Q) @ z)

Ïƒ(x) = ELU(x) + 1
```

### Multi-Memory Bank

```python
# è¤‡æ•°ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ã§æƒ…å ±æ··åˆã‚’ä½æ¸›
model = InfiniPythiaModel(
    num_memory_banks=2,      # 2ã¤ã®ãƒãƒ³ã‚¯
    segments_per_bank=4,     # 4ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§æ¬¡ãƒãƒ³ã‚¯ã«åˆ‡æ›¿
)
```

### ALiBi (Attention with Linear Biases)

ç·šå½¢åŒ–è¿‘ä¼¼ã§ALiBiã‚’åœ§ç¸®ãƒ¡ãƒ¢ãƒªã«çµ„ã¿è¾¼ã‚€:

```
ãƒ¡ãƒ¢ãƒªæ›´æ–° (ALiBié‡ã¿ä»˜ã):
  M_Ï† = Î£_i w_i * Ï†(K_i) * V_i^T
  z_Ï† = Î£_i w_i * Ï†(K_i)

  w_i = exp(-slope * segment_distance)  # é ã„ã»ã©å°ã•ã„é‡ã¿

ãƒ¡ãƒ¢ãƒªå–å¾—:
  Output = Ï†(Q) @ M_Ï† / (Ï†(Q) @ z_Ï†)
```

```python
# ALiBiä»˜ããƒ¢ãƒ‡ãƒ«
model = InfiniPythiaModel(
    use_alibi=True,      # ALiBiæœ‰åŠ¹åŒ–
    alibi_scale=1.0,     # ã‚¹ãƒ­ãƒ¼ãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆå¤§ãã„ã»ã©æ¸›è¡°ãŒå¼·ã„ï¼‰
)
```

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# æ¨™æº–å®Ÿé¨“ï¼ˆä¸¡ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒï¼‰
python3 scripts/experiment_infini.py --samples 5000 --epochs 30

# Infiniã®ã¿
python3 scripts/experiment_infini.py --skip-baseline

# Baselineã®ã¿
python3 scripts/experiment_infini.py --skip-infini

# Multi-Memory Bank
python3 scripts/experiment_infini.py --num-memory-banks 2 --segments-per-bank 4

# ALiBiä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
python3 scripts/experiment_infini.py --alibi --skip-baseline

# ALiBi (å¼·ã„æ¸›è¡°)
python3 scripts/experiment_infini.py --alibi --alibi-scale 2.0 --skip-baseline

# Long Contextè¨“ç·´ãƒ»è©•ä¾¡
python3 scripts/experiment_infini.py --long-context-train --long-context
```

### Multi-Memory Infini-Attention (Attention-based Selection)

è¤‡æ•°ã®ç‹¬ç«‹ã—ãŸãƒ¡ãƒ¢ãƒªã‚’Attention-basedæ–¹å¼ã§å‹•çš„ã«é¸æŠãƒ»æ··åˆã€‚

```
Multi-Memory Infini-Pythia:
Token Embedding (512-dim)
       â†“
Layer 0: MultiMemoryInfiniAttentionLayer
  â”œâ”€ Memory 0, 1, 2, ... (ç‹¬ç«‹ã—ãŸãƒ¡ãƒ¢ãƒª)
  â”œâ”€ é–¢é€£åº¦: phi(Q) @ z_i
  â””â”€ Softmaxé‡ã¿ä»˜ã‘æ··åˆ
       â†“
Layer 1-5: PythiaLayer (RoPE)
       â†“
Output Head (512 â†’ vocab)
```

**ç‰¹å¾´**:
- å„ãƒ¡ãƒ¢ãƒªã¯ç‹¬ç«‹ã—ã¦æ›´æ–°ï¼ˆãƒ©ã‚¦ãƒ³ãƒ‰ãƒ­ãƒ“ãƒ³ï¼‰
- ã‚¯ã‚¨ãƒªã¨ãƒ¡ãƒ¢ãƒªã®zï¼ˆæ­£è¦åŒ–é …ï¼‰ã¨ã®å†…ç©ã§é–¢é€£åº¦è¨ˆç®—
- Softmaxé‡ã¿ä»˜ã‘ã§å…¨ãƒ¡ãƒ¢ãƒªã‚’æ··åˆ
- è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—ï¼ˆå­¦ç¿’ãŒå®‰å®šï¼‰

```bash
# Multi-Memoryå®Ÿé¨“ï¼ˆ4ãƒ¡ãƒ¢ãƒªï¼‰
python3 scripts/experiment_multi_memory.py --num-memories 4

# 8ãƒ¡ãƒ¢ãƒªã§å®Ÿé¨“
python3 scripts/experiment_multi_memory.py --num-memories 8 --samples 10000

# ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¹ã‚­ãƒƒãƒ—
python3 scripts/experiment_multi_memory.py --skip-baseline --num-memories 4
```

```python
from src.models.multi_memory_pythia import MultiMemoryInfiniPythiaModel

model = MultiMemoryInfiniPythiaModel(
    vocab_size=50304,
    hidden_size=512,
    num_layers=6,
    num_heads=8,
    num_memories=4,  # ç‹¬ç«‹ã—ãŸãƒ¡ãƒ¢ãƒªæ•°
)
```

---

## ğŸ“Š Reversal Curse è©•ä¾¡

### æ¦‚è¦

Reversal Curseã¯ã€ŒA is Bã€ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã€ŒB is Aã€ã‚‚æ¨è«–ã§ãã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹æŒ‡æ¨™ã€‚

### æŒ‡æ¨™

| æŒ‡æ¨™ | å®šç¾© | è§£é‡ˆ |
|------|------|------|
| Forward PPL | é †æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãŸã‚ä½ã„ |
| Backward PPL | é€†æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ãŸã‚é«˜ã„ |
| Reversal Ratio | Forward / Backward | 1.0ã«è¿‘ã„ã»ã©è‰¯ã„ |
| Reversal Gap | Backward - Forward | 0ã«è¿‘ã„ã»ã©è‰¯ã„ |

### å®Ÿè£…

```python
from src.utils.evaluation import evaluate_reversal_curse
from src.data.reversal_pairs import get_reversal_pairs

tokenizer = get_tokenizer(config.tokenizer_name)
reversal_pairs = get_reversal_pairs()
reversal_result = evaluate_reversal_curse(model, tokenizer, reversal_pairs, device)
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

### Reversal Curseè©•ä¾¡å¿…é ˆ

**ã™ã¹ã¦ã®å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã€Reversal Curseè©•ä¾¡ã‚’å¿…ãšå®Ÿè¡Œã™ã‚‹ã“ã¨ã€‚**

---

## âš ï¸ éå»ã®ãƒã‚°ã¨æ•™è¨“

### 1. Infini-Attention ãƒ¡ãƒ¢ãƒªå‹¾é…ãƒã‚°

```python
# âŒ ãƒã‚°: ãƒ¡ãƒ¢ãƒªæ›´æ–°ã§ã‚°ãƒ©ãƒ•ãŒæ®‹ã‚Šã€äºŒé‡backwardã‚¨ãƒ©ãƒ¼
self.memory = self.memory + memory_update

# âœ… ä¿®æ­£: detach()ã§ã‚°ãƒ©ãƒ•ã‚’åˆ‡æ–­
self.memory = (self.memory + memory_update).detach()
```

### 2. PPLç•°å¸¸å€¤ã®è¨ºæ–­åŸºæº–

| PPL | çŠ¶æ…‹ | å¯¾å‡¦ |
|-----|------|------|
| < 5 | **ç•°å¸¸** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯/å› æœãƒã‚¹ã‚¯ãƒã‚° | ã‚³ãƒ¼ãƒ‰ç‚¹æ¤œå¿…é ˆ |
| 5-30 | **ç–‘ã‚ã—ã„** - éå­¦ç¿’ã®å¯èƒ½æ€§ | ãƒ‡ãƒ¼ã‚¿é‡ãƒ»åˆ†å‰²ã‚’ç¢ºèª |
| 30-100 | æ­£å¸¸ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ | - |
| 100-500 | æ­£å¸¸ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼‰ | - |
| > 1000 | å­¦ç¿’ä¸è¶³ | epochå¢—åŠ /lrèª¿æ•´ |

### 3. Long Contextè©•ä¾¡ã§untrained modelã‚’ä½¿ç”¨

```python
# âŒ ãƒã‚°: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦è©•ä¾¡
pythia_model = PythiaModel(...)  # æœªè¨“ç·´ã®ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
result = evaluate_long_documents(pythia_model, ...)

# âœ… ä¿®æ­£: è¨“ç·´æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
pythia_model = PythiaModel(...)
pythia_model.load_state_dict(results["pythia"]["model_state_dict"])
result = evaluate_long_documents(pythia_model, ...)
```

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pythia.py                   # PythiaConfig
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiment_infini.py        # Infini-Attentionå®Ÿé¨“
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ reversal_pairs.py       # Reversal Curseè©•ä¾¡ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pythia.py               # PythiaModel (RoPE)
â”‚   â”‚   â”œâ”€â”€ infini_attention.py     # InfiniAttention, InfiniAttentionLayer
â”‚   â”‚   â”œâ”€â”€ infini_pythia.py        # InfiniPythiaModel (1å±¤Infini + RoPE)
â”‚   â”‚   â”œâ”€â”€ multi_memory_attention.py  # MultiMemoryInfiniAttention
â”‚   â”‚   â””â”€â”€ multi_memory_pythia.py  # MultiMemoryInfiniPythiaModel
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ training.py             # å…±é€šå­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚       â”œâ”€â”€ evaluation.py           # è©•ä¾¡é–¢æ•°
â”‚       â”œâ”€â”€ device.py               # ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”‚       â”œâ”€â”€ data_pythia.py          # Pileãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
â”‚       â””â”€â”€ seed.py                 # ã‚·ãƒ¼ãƒ‰è¨­å®š
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ experiments/                # å®Ÿé¨“çµæœ
â””â”€â”€ CLAUDE.md
```

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-06 | **Multi-Memory Attentionè¿½åŠ **: Attention-basedé¸æŠã§è¤‡æ•°ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„æ··åˆ |
| 2025-12-06 | **ALiBiä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ **: ç·šå½¢åŒ–è¿‘ä¼¼ã§ALiBiã‚’ãƒ¡ãƒ¢ãƒªã«çµ„ã¿è¾¼ã¿ |
| 2025-12-05 | **Memory-Onlyã«é›†ä¸­**: Local Attentionå‰Šé™¤ã€ã‚³ãƒ¼ãƒ‰ç°¡ç´ åŒ– |
| 2025-12-05 | **Multi-Memory Bankè¿½åŠ **: è¤‡æ•°ãƒãƒ³ã‚¯ã§æƒ…å ±æ··åˆä½æ¸› |
| 2025-12-05 | **Long Contextè©•ä¾¡ãƒã‚°ä¿®æ­£**: è¨“ç·´æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ |
| 2025-12-05 | **Infini-Pythiaå®Ÿè£…**: 1å±¤ç›®Infini + RoPE |

---

Last Updated: 2025-12-06
