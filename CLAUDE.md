# New-LLM Project Guidelines

## âš¡ PARALLEL PROCESSING ADOPTED - ä¸¦åˆ—å‡¦ç†ç‰ˆæ¡ç”¨ (2025-11-25)

**ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’æ¨™æº–å®Ÿè£…ã¨ã—ã¦å®Œå…¨æ¡ç”¨ã—ã¾ã—ãŸã€‚**

### æ€§èƒ½æŒ‡æ¨™

**ä¸¦åˆ—ç‰ˆå®Ÿè£…** ([src/trainers/phase1.py](src/trainers/phase1.py)):
- **Effective Rank**: 55.9% (429/768) - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
- **å‡¦ç†æ™‚é–“**: ~11ç§’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ265ç§’ã®**23xé«˜é€ŸåŒ–**ï¼‰
- **dist_reg_weight**: 0.9ï¼ˆå¤šæ§˜æ€§90%, CVFP 10%ï¼‰
- **max_iterations**: 10
- **åæŸç‡**: 27.2%ï¼ˆå¤šæ§˜æ€§å„ªå…ˆã®ãŸã‚CVFPåæŸç‡ã¯ä½ã‚ã€ã“ã‚Œã¯æ­£å¸¸ï¼‰

### è¨­è¨ˆè©³ç´°

**Iteration 0**: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ï¼ˆå›ºå®šç‚¹ç›®æ¨™ç¢ºç«‹ï¼‰
**Iteration 1+**: ä¸¦åˆ—å‡¦ç†ï¼ˆå‰å›contextã‚’ä½¿ç”¨ï¼‰

**ä¸¦åˆ—åŒ–ã®ç‰¹å¾´**:
- Token i ã«ã¯ previous_contexts[i-1] ã‚’ä½¿ç”¨ï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³åˆ†ã®ãšã‚Œï¼‰
- æƒ…å ±é…å»¶ãŒã‚ã‚‹ãŒã€dist_reg_weight=0.9ã«ã‚ˆã‚Šå¤šæ§˜æ€§ã‚’è£œå„Ÿ
- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–

**æ—§ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆã¨ã®æ¯”è¼ƒ**:
- ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ: 66.6% ER, 265ç§’
- ä¸¦åˆ—ç‰ˆ: 55.9% ER, 11ç§’
- ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: -10.7% ER vs 23xé«˜é€ŸåŒ–

---

## ğŸ“Š MANDATORY: æ•°å€¤å ±å‘Šãƒ«ãƒ¼ãƒ« - å…·ä½“çš„ãªæ•°å€¤ã§ã®å ±å‘Šç¾©å‹™

### çµ¶å¯¾éµå®ˆ: ã™ã¹ã¦ã®å®Ÿé¨“çµæœã¯å…·ä½“çš„ãªæ•°å€¤ã§å ±å‘Šã™ã‚‹

**ç¦æ­¢äº‹é …**:
- âŒ "GOOD", "EXCELLENT", "MODERATE" ãªã©ã®æŠ½è±¡çš„è¡¨ç¾ã§ã®å ±å‘Š
- âŒ "æ”¹å–„ã—ãŸ", "è‰¯å¥½", "é©åˆ‡" ãªã©ã®å®šæ€§çš„è©•ä¾¡ã®ã¿ã®å ±å‘Š
- âŒ æ•°å€¤ã‚’ä¼´ã‚ãªã„åˆ¤å®šçµæœã®å ±å‘Š

**å¿…é ˆå ±å‘Šé …ç›®**:
- âœ… **åæŸç‡ï¼ˆè¨“ç·´ãƒ»æ¤œè¨¼ä¸¡æ–¹ï¼‰**: **å…·ä½“çš„ãªãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã¨åæŸãƒˆãƒ¼ã‚¯ãƒ³æ•°** (ä¾‹: è¨“ç·´ 0.0% (0/6400), æ¤œè¨¼ 0.0% (0/1280))
- âœ… Effective Rank: **å®Ÿæ•°å€¤/ç·æ¬¡å…ƒæ•°ã¨ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸** (ä¾‹: 627.29/768 = 81.7%)
- âœ… CVFPãƒ­ã‚¹: **å®Ÿæ•°å€¤** (ä¾‹: 0.001873)
- âœ… åæŸå·®åˆ†: **å®Ÿæ•°å€¤** (ä¾‹: final_diff = 0.000745)
- âœ… ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: **å®Ÿæ•°** (ä¾‹: 10/10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†)

**âš ï¸ åæŸç‡ã®å ±å‘Šã¯çµ¶å¯¾ã«çœç•¥ã—ã¦ã¯ã„ã‘ãªã„**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã®åæŸç‡ã‚’å¿…ãšå ±å‘Š
- åæŸç‡ = åæŸã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•° / ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°
- 0.0%ã¯ã€ŒåæŸå¤±æ•—ã€ã§ã¯ãªãã€Œå…¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œèµ°ã€ã‚’æ„å‘³ã™ã‚‹

**å ±å‘Šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹**:
```
è¨“ç·´çµæœ:
- åæŸç‡: 0.0% (0/6400ãƒˆãƒ¼ã‚¯ãƒ³) - 10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œèµ°
- Effective Rank: 689.26/768 (89.7%)
- CVFPãƒ­ã‚¹: 0.001732

æ¤œè¨¼çµæœ:
- åæŸç‡: 0.0% (0/1280ãƒˆãƒ¼ã‚¯ãƒ³) - 10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œèµ°
- Effective Rank: 627.29/768 (81.7%)

CVFPåæŸãƒã‚§ãƒƒã‚¯:
- final_diff = 0.000745 (é–¾å€¤ < 0.001ã‚¯ãƒªã‚¢)
```

---

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL DESIGN - CVFP FIXED-POINT LEARNING (2025-11-26 ä¿®æ­£) ğŸš¨ğŸš¨ğŸš¨

### CVFPç†è«–: å›ºå®šç‚¹å­¦ç¿’ã®æ­£ã—ã„å®Ÿè£…

**å›ºå®šç‚¹å­¦ç¿’ã®å®šç¾©**: `f(x) = x` ã¨ãªã‚‹ç‚¹ã«åæŸã•ã›ã‚‹

ã“ã‚Œã¯ã€ŒåŒã˜å…¥åŠ›ã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã—ãŸã¨ãã€å‡ºåŠ›ãŒå¤‰åŒ–ã—ãªããªã‚‹ã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

**æ­£ã—ã„å®Ÿè£…ï¼ˆCVFPç†è«–ã«åŸºã¥ãï¼‰**:
```
Iteration 0: contexts_0 ã‚’å‡ºåŠ› â†’ previous_contexts = contexts_0ï¼ˆå­¦ç¿’ãªã—ï¼‰
Iteration 1: contexts_1 ã‚’å‡ºåŠ› â†’ CVFPæå¤± = MSE(contexts_1, previous_contexts)
             previous_contexts = contexts_1 ã«æ›´æ–°
Iteration 2: contexts_2 ã‚’å‡ºåŠ› â†’ CVFPæå¤± = MSE(contexts_2, previous_contexts)
             previous_contexts = contexts_2 ã«æ›´æ–°
...
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- âœ… CVFPæå¤±ã¯**å‰å›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆprevious_contextsï¼‰**ã¨æ¯”è¼ƒ
- âœ… previous_contextsã¯**æ¯ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›´æ–°**ã—ã¦ã‚ˆã„
- âœ… åæŸåˆ¤å®šã‚‚å‰å›ã¨ã®å·®ã§è¡Œã†
- âŒ ~~Iteration 0ã‚’å›ºå®šç›®æ¨™ã¨ã—ã¦ä¿å­˜~~ â† ã“ã‚Œã¯é–“é•ã„

**ãªãœå‰å›ã¨ã®æ¯”è¼ƒãŒæ­£ã—ã„ã‹**:
1. å›ºå®šç‚¹ = å¤‰åŒ–ãŒãªããªã‚‹ç‚¹
2. `MSE(current, previous) â†’ 0` ã¯ã€Œå‡ºåŠ›ãŒå®‰å®šã—ãŸã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹
3. ã“ã‚ŒãŒå›ºå®šç‚¹ `f(x) = x` ã®å®šç¾©ã«åˆè‡´ã™ã‚‹

**æ­£ã—ã„ã‚³ãƒ¼ãƒ‰ï¼ˆphase1.pyï¼‰**:
```python
# CVFPæå¤±: å‰å›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨æ¯”è¼ƒï¼ˆå›ºå®šç‚¹ã¸ã®åæŸï¼‰
cvfp_loss = compute_cvfp_loss(contexts, previous_contexts)

# æ›´æ–°
previous_contexts = contexts.detach()
```

---

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL BUG FIX - CONTEXT CARRYOVER (2025-11-24) ğŸš¨ğŸš¨ğŸš¨

### è‡´å‘½çš„ãƒã‚°ä¿®æ­£: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼•ãç¶™ãï¼ˆçµ¶å¯¾ã«å¿˜ã‚Œã¦ã¯ã„ã‘ãªã„ï¼‰

**è‡´å‘½çš„ãªå•é¡Œ**:
- è¨“ç·´ãƒ»æ¤œè¨¼ã®ä¸¡æ–¹ã§å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¼ãƒ­ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ãŸ
- **ã“ã‚Œã¯CVFPå­¦ç¿’ã®æ ¹æœ¬ã‚’ç ´å£Šã™ã‚‹è‡´å‘½çš„ãƒã‚°**
- å›ºå®šç‚¹å­¦ç¿’ãŒå…¨ãæ©Ÿèƒ½ã—ã¦ã„ãªã‹ã£ãŸ

**ä¿®æ­£å†…å®¹**:
```python
# âŒâŒâŒ çµ¶å¯¾ã«ã‚„ã£ã¦ã¯ã„ã‘ãªã„é–“é•ã£ãŸå®Ÿè£…ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰
# æ¯ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ = CVFPå­¦ç¿’ã®ç ´å£Š
context = torch.zeros(1, self.model.context_dim, device=device)  # è‡´å‘½çš„ãƒã‚°

# âœ…âœ…âœ… å¿…é ˆã®æ­£ã—ã„å®Ÿè£…ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰
# ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…ãšå¼•ãç¶™ã
if self.previous_contexts is None:
    # åˆå›ã®ã¿ã‚¼ãƒ­åˆæœŸåŒ–
    context = torch.zeros(1, self.model.context_dim, device=device)
else:
    # å‰ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€çµ‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…ãšå¼•ãç¶™ãï¼ˆCVFPå­¦ç¿’ã®æ ¸å¿ƒï¼‰
    context = self.previous_contexts[-1].unsqueeze(0).detach()
```

**ãªãœã“ã‚ŒãŒè‡´å‘½çš„ã‹**:
1. **CVFP = Context Vector Fixed-Point**: å›ºå®šç‚¹ã¸ã®åæŸãŒç›®çš„
2. **å›ºå®šç‚¹å­¦ç¿’**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é‡ã­ã¦åŒã˜ç‚¹ã«åæŸã™ã‚‹ã“ã¨ãŒç›®æ¨™
3. **å¼•ãç¶™ãŒãªã„ = å­¦ç¿’ã—ã¦ã„ãªã„**: æ¯å›ãƒªã‚»ãƒƒãƒˆã§ã¯å›ºå®šç‚¹ã«åˆ°é”ä¸å¯èƒ½
4. **æ¤œè¨¼ãƒ­ã‚¹ã‚¼ãƒ­ã®è¬**: ãƒã‚°ã®ã›ã„ã§è¦‹ã‹ã‘ä¸Šè‰¯ã„çµæœã«è¦‹ãˆã¦ã„ãŸ

**äºŒåº¦ã¨åŒã˜é–“é•ã„ã‚’ã—ãªã„ãŸã‚ã«**:
- âš ï¸ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼•ãç¶™ãã¯**CVFPå­¦ç¿’ã®ç”Ÿå‘½ç·š**
- âš ï¸ `previous_contexts`ã®æœ€çµ‚å€¤ã‚’æ¬¡ã®åˆæœŸå€¤ã«ã™ã‚‹ã“ã¨ã¯**çµ¶å¯¾å¿…é ˆ**
- âš ï¸ ã“ã®ä¿®æ­£ãªã—ã§ã¯ã€ã™ã¹ã¦ã®å®Ÿé¨“çµæœãŒç„¡æ„å‘³ã«ãªã‚‹

---

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL DESIGN - PHASE 2 CONTEXT-FIXED LEARNING (2025-11-26) ğŸš¨ğŸš¨ğŸš¨

### Phase 2: Context-Fixed Token Predictionï¼ˆå®Œå…¨å›ºå®šæ–¹å¼ï¼‰

**Phase 2ã¯2æ®µéšå‡¦ç†ã§å®Ÿè¡Œã•ã‚Œã‚‹**:

#### Stage 1: åˆæœŸåŒ–ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ãªã—ï¼‰
- Phase 2é–‹å§‹æ™‚ã«1å›ã ã‘å®Ÿè¡Œ
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã—ã€å›ºå®šæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«C*ã‚’ç”Ÿæˆ
- **C*ã¯ä»¥é™çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„**

```python
# Stage 1: å›ºå®šæ–‡è„ˆC*ã®ç”Ÿæˆ
with torch.no_grad():
    context = torch.zeros(...)
    C_star = []  # å›ºå®šæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«
    for token_id in token_ids:
        token_embed = get_embedding(token_id)
        context, token_out = cvfp_block(context, token_embed)
        C_star.append(context)  # C*[i]ã¨ã—ã¦ä¿å­˜
```

#### Stage 2: å­¦ç¿’ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã‚ã‚Šï¼‰
- å…¥åŠ›: `[C*[i-1], token_embed[i]]` - å›ºå®šæ–‡è„ˆã‚’ä½¿ç”¨
- å‡ºåŠ›: `[context_out, token_out]` - CVFPãƒ–ãƒ­ãƒƒã‚¯ã®å‡ºåŠ›
- **context_outã¯C*[i]ã§å®Œå…¨ã«ç½®æ›**ï¼ˆMSEåˆ¶ç´„ã§ã¯ãªãå€¤ãã®ã‚‚ã®ï¼‰
- äºˆæ¸¬: `logits = Linear(concat(C*[i], token_out))`

```python
# Stage 2: Context-Fixed Learning
for i, token_id in enumerate(input_ids):
    # å…¥åŠ›: å›ºå®šæ–‡è„ˆC*[i-1]ï¼ˆi=0ã®å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ï¼‰
    input_context = C_star[i-1] if i > 0 else torch.zeros(...)

    # CVFPãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†
    context_out, token_out = cvfp_block(input_context, token_embed)

    # CRITICAL: context_outã¯ä½¿ã‚ãšã€C*[i]ã§å®Œå…¨ç½®æ›
    fixed_context = C_star[i].detach()

    # äºˆæ¸¬: å›ºå®šæ–‡è„ˆ + token_out
    combined = torch.cat([fixed_context, token_out], dim=-1)
    logits = token_output(combined)

    # æå¤±ã¯äºˆæ¸¬æå¤±ã®ã¿ï¼ˆcontext_stability_lossã¯ä¸è¦ï¼‰
    loss = CrossEntropy(logits, target)
```

### è¨˜å·å®šç¾©

| è¨˜å· | æ„å‘³ |
|------|------|
| `C*[i]` | **å›ºå®šç›®æ¨™æ–‡è„ˆ** - Stage 1ã§è¨ˆç®—ã—ãŸã€ãƒˆãƒ¼ã‚¯ãƒ³iã‚’å‡¦ç†ã—ãŸå¾Œã®æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¸å¤‰ï¼‰ |
| `context_out` | **å­¦ç¿’æ™‚ã®å‡ºåŠ›æ–‡è„ˆ** - Stage 2ã§CVFPãƒ–ãƒ­ãƒƒã‚¯ãŒå‡ºåŠ›ã™ã‚‹æ–‡è„ˆï¼ˆ**ä½¿ç”¨ã—ãªã„**ï¼‰ |
| `token_out` | **å­¦ç¿’æ™‚ã®å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³** - Stage 2ã§CVFPãƒ–ãƒ­ãƒƒã‚¯ãŒå‡ºåŠ›ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³è¡¨ç¾ï¼ˆäºˆæ¸¬ã«ä½¿ç”¨ï¼‰ |

### å‹¾é…ãƒ•ãƒ­ãƒ¼

```
å…¥åŠ›: [C*[i-1], token_embed[i]]
         â†“
    CVFPãƒ–ãƒ­ãƒƒã‚¯
         â†“
å‡ºåŠ›: [context_out, token_out]
         â†“
    context_outã¯ç ´æ£„ã€C*[i]ã‚’ä½¿ç”¨
         â†“
    combined = [C*[i], token_out]
         â†“
    logits = token_output(combined)
         â†“
    loss = CrossEntropy(logits, target)
```

**å‹¾é…ã®æµã‚Œ**:
- âœ… `token_out` â†’ CVFPãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæ›´æ–°ã•ã‚Œã‚‹ï¼‰
- âœ… `token_output`å±¤ï¼ˆæ›´æ–°ã•ã‚Œã‚‹ï¼‰
- âŒ `context_out` â†’ CVFPãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå‹¾é…ãªã— - æœªä½¿ç”¨ã®ãŸã‚ï¼‰
- âŒ `C*[i]` â†’ CVFPãƒ–ãƒ­ãƒƒã‚¯ï¼ˆå‹¾é…ãªã— - detachã®ãŸã‚ï¼‰

### é‡è¦ãªè¨­è¨ˆå¤‰æ›´ï¼ˆ2025-11-26ï¼‰

- âŒ **æ—§è¨­è¨ˆï¼ˆv1.0ï¼‰**: MSEåˆ¶ç´„ã«ã‚ˆã‚‹ã€Œç·©ã„ã€å›ºå®š
  ```python
  context_stability_loss = MSE(context_out, C_star[i])  # ç·©ã„åˆ¶ç´„
  ```
- âœ… **æ–°è¨­è¨ˆï¼ˆv2.0ï¼‰**: context_outã‚’C*[i]ã§å®Œå…¨ç½®æ›ï¼ˆå®Œå…¨å›ºå®šï¼‰
  ```python
  fixed_context = C_star[i].detach()  # å®Œå…¨å›ºå®š
  combined = torch.cat([fixed_context, token_out], dim=-1)
  ```

### ãªãœå®Œå…¨å›ºå®šãŒå¿…è¦ã‹

1. **Phase 1ã®ä¿è­·**: Phase 1ã§å­¦ç¿’ã—ãŸæ–‡è„ˆè¡¨ç¾ã‚’ç¢ºå®Ÿã«ä¿è­·
2. **æ˜ç¢ºãªå½¹å‰²åˆ†é›¢**: contextéƒ¨åˆ†ã®å­¦ç¿’ã‚’åˆ¶é™ã—ã€token_outéƒ¨åˆ†ã«é›†ä¸­
3. **å®‰å®šã—ãŸå­¦ç¿’**: æ–‡è„ˆãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«é›†ä¸­ã§ãã‚‹

---

## âš¡ 55.9% Effective Rank - Parallel Version Baseline (2025-11-25)

### ä¸¦åˆ—å‡¦ç†ç‰ˆã®æ€§èƒ½ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

**ä¸¦åˆ—å‡¦ç†ç‰ˆã¯æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§55.9% Effective Rankã‚’é”æˆï¼ˆ23xé«˜é€ŸåŒ–ï¼‰**

**å®Ÿæ¸¬å€¤**:
- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: 55.9% Effective Rank (429/768)
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ~60% Effective Rank
- **å‡¦ç†æ™‚é–“**: ~11ç§’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ265ç§’ã®23xé«˜é€ŸåŒ–ï¼‰
- **åæŸç‡**: 27.2%ï¼ˆå¤šæ§˜æ€§å„ªå…ˆã®ãŸã‚CVFPåæŸç‡ã¯ä½ã‚ã€ã“ã‚Œã¯æ­£å¸¸ï¼‰

**æ—§ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆï¼ˆå‚è€ƒï¼‰**:
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 66.6% Effective Rank
- å‡¦ç†æ™‚é–“: 265ç§’
- åæŸç‡: 30.0%

**ä¸¦åˆ—ç‰ˆæ¡ç”¨ã®ç†ç”±**:
- âœ… 23xé«˜é€ŸåŒ–ã«ã‚ˆã‚Šå®Ÿç”¨æ€§ãŒå¤§å¹…å‘ä¸Š
- âœ… 55.9% ERã¯å®Ÿç”¨çš„ãªå¤šæ§˜æ€§ã‚’ç¶­æŒ
- âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: -10.7% ER vs åœ§å€’çš„é«˜é€ŸåŒ–

---

## Core Implementation - Parallel Processing

### 1. Diversity Loss: Global Mean-Based Tracking

**âœ… ä¸¦åˆ—ç‰ˆå®Ÿè£… ([src/trainers/phase1.py](src/trainers/phase1.py))**:

```python
def compute_diversity_loss(contexts):
    """
    å¤šæ§˜æ€§æå¤±: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡ã‹ã‚‰ã®åå·®ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰

    Args:
        contexts: ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ [num_tokens, context_dim]

    Returns:
        diversity_loss: å¤šæ§˜æ€§æå¤±ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰
    """
    context_mean = contexts.mean(dim=0)  # [context_dim]
    deviation = contexts - context_mean  # [num_tokens, context_dim]
    diversity_loss = -torch.norm(deviation, p=2) / len(contexts)
    return diversity_loss
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¹³å‡ã‹ã‚‰ã®åå·®ã‚’è¨ˆç®—
- è² ã®æå¤±ã«ã‚ˆã‚Šã€å¹³å‡ã‹ã‚‰ã®åå·®ã‚’æœ€å¤§åŒ–ï¼ˆå¤šæ§˜æ€§ä¿ƒé€²ï¼‰
- ãƒãƒƒãƒå‡¦ç†ã«æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…

### 2. ãƒ‡ãƒ¼ã‚¿ä»•æ§˜ - çµ¶å¯¾å›ºå®š

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿**:
- ã‚½ãƒ¼ã‚¹: UltraChat (HuggingFaceH4/ultrachat_200k)
- ã‚µãƒ³ãƒ—ãƒ«æ•°: 50
- ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 6400
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥: `./cache/ultrachat_50samples_128len.pt`

**æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿** (çµ¶å¯¾ä»•æ§˜):
- ã‚½ãƒ¼ã‚¹: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œ20%ã‹ã‚‰ç”Ÿæˆ
- ãƒˆãƒ¼ã‚¯ãƒ³æ•°: 1280
- ãƒ•ã‚¡ã‚¤ãƒ«: `./data/example_val.txt`
- **å¿…é ˆæ¡ä»¶**: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã“ã¨
- ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ: `scripts/create_val_from_train.py`

### 3. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ«ãƒ¼ãƒ« - å³æ ¼

**ç¦æ­¢äº‹é …**:
- âŒ `val_data_source = "auto_split"` ã¯å³ç¦ï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰
- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
- âŒ æ‰‹å‹•ã§ä½œæˆã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãªæ¤œè¨¼ãƒ†ã‚­ã‚¹ãƒˆ

**å¿…é ˆæ‰‹é †**:
```bash
# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
python3 scripts/create_val_from_train.py

# config.pyã®è¨­å®šï¼ˆçµ¶å¯¾å›ºå®šï¼‰
val_data_source = "text_file"
val_text_file = "./data/example_val.txt"
```

### 4. é”æˆçµæœ - ä¸¦åˆ—ç‰ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (2025-11-25)

**å®Ÿæ¸¬å€¤ (ä¸¦åˆ—ç‰ˆ, dist_reg_weight=0.9)**:
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ~60% Effective Rank - 6400ãƒˆãƒ¼ã‚¯ãƒ³
- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: **55.9% Effective Rank (429/768)** - 1280ãƒˆãƒ¼ã‚¯ãƒ³
- **å‡¦ç†æ™‚é–“**: ~11ç§’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ265ç§’ã®23xé«˜é€ŸåŒ–ï¼‰
- **åæŸç‡**: è¨“ç·´ 27.2%ï¼ˆå¤šæ§˜æ€§å„ªå…ˆã®ãŸã‚ä½ã‚ã€ã“ã‚Œã¯æ­£å¸¸ï¼‰

**ä¸¦åˆ—ç‰ˆæ€§èƒ½**:
- `dist_reg_weight = 0.9` ã«ã‚ˆã‚Šã€ä¸¦åˆ—ç‰ˆã®æƒ…å ±é…å»¶ã‚’å¤šæ§˜æ€§å¼·åŒ–ã§è£œå„Ÿ
- 23xé«˜é€ŸåŒ–ã«ã‚ˆã‚Šå®Ÿç”¨æ€§ãŒå¤§å¹…å‘ä¸Š
- ã“ã®æ•°å€¤ã‚’ä¸¦åˆ—ç‰ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã™ã‚‹

### 5. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åæŸæ€§ãƒã‚§ãƒƒã‚¯ - Validation Convergence Check

**æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åæŸç‡ã¯è¨“ç·´æ™‚ã«è¨ˆç®—ã•ã‚Œãªã„**ï¼ˆ1å›ã®é †ä¼æ’­ã®ã¿ï¼‰ã€‚
ä»£ã‚ã‚Šã«ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®åæŸæ€§ã‚’ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç¢ºèªï¼š

```bash
python3 check_val_convergence.py --num_trials 10
```

**å‹•ä½œ**:
1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
2. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°å›é †ä¼æ’­ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10å›ï¼‰
3. å„è©¦è¡Œã§CVFPæå¤±ï¼ˆå‰å›ã¨ã®å·®åˆ†MSEï¼‰ã‚’è¨ˆç®—
4. æå¤±ã®æ¨ç§»ã‚’è¡¨ç¤ºã—ã€æ¸›å°‘å‚¾å‘ã‚’è‡ªå‹•åˆ¤å®š

**å‡ºåŠ›ä¾‹**:
```
Trial  1/10: CVFP Loss = N/A (baseline, no previous context)
Trial  2/10: CVFP Loss = 0.245123
Trial  3/10: CVFP Loss = 0.183456
Trial  4/10: CVFP Loss = 0.142789
...
Trial 10/10: CVFP Loss = 0.098234

Statistics:
  - Initial Loss (Trial 2): 0.245123
  - Final Loss (Trial 10): 0.098234
  - Reduction: -59.93%
  - Slope (linear fit): -0.018234

Verdict:
  âœ… CONVERGING: Loss is decreasing - model is converging on validation data
```

**åˆ¤å®šåŸºæº–**:
- âœ… CONVERGING: æå¤±ãŒæ˜ç¢ºã«æ¸›å°‘ï¼ˆslope < -0.001ï¼‰
- âœ… CONVERGED: æå¤±ãŒå®‰å®šï¼ˆ|slope| < 0.001 ã‹ã¤ std < 0.01ï¼‰
- âŒ DIVERGING: æå¤±ãŒå¢—åŠ ï¼ˆslope > 0.001ï¼‰
- âš ï¸ UNSTABLE: æå¤±ãŒä¸å®‰å®šï¼ˆä¸Šè¨˜ä»¥å¤–ï¼‰

**ä½¿ç”¨å ´é¢**:
- è¨“ç·´å®Œäº†å¾Œã«æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®åæŸæ€§ã‚’ç¢ºèª
- ãƒ¢ãƒ‡ãƒ«ã®å›ºå®šç‚¹å­¦ç¿’ãŒæ±åŒ–ã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼
- ç•°ãªã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ¯”è¼ƒ

---

## Architecture Configuration - Parallel Version

```python
# Model Architecture
num_layers = 6                  # 6-layer CVFP blocks
context_dim = 768               # GPT-2 aligned
embed_dim = 768                 # GPT-2 pretrained
hidden_dim = 1536               # 2 Ã— embed_dim
layernorm_mix = 1.0             # Full LayerNorm (CRITICAL)

# Diversity Regularization (ä¸¦åˆ—ç‰ˆæœ€é©åŒ–)
dist_reg_weight = 0.9           # 90% diversity, 10% CVFP (parallel optimized)
                                # ä¸¦åˆ—ç‰ˆã®æƒ…å ±é…å»¶ã‚’å¤šæ§˜æ€§å¼·åŒ–ã§è£œå„Ÿ

# Training
phase1_learning_rate = 0.002    # Fast convergence
phase1_max_iterations = 10      # ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
```

---

## Training Pipeline - Standard Workflow

### Phase 1: CVFP Learning

```bash
# Standard test (uses fixed train/val data)
python3 test.py
```

**å®Ÿè¡Œå†…å®¹**:
1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (6400ãƒˆãƒ¼ã‚¯ãƒ³ from cache)
2. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (1280ãƒˆãƒ¼ã‚¯ãƒ³ from text file)
3. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ (Phase1Trainer)
4. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
5. **3ã¤ã®å¿…é ˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ** (è©³ç´°ã¯ä¸‹è¨˜)

---

## 3 Critical Checks - ABSOLUTELY REQUIRED (çµ¶å¯¾å¿…è¦ãª3ã¤ã®ãƒã‚§ãƒƒã‚¯)

**ã“ã‚Œã‚‰ã®ãƒã‚§ãƒƒã‚¯ã‚’çœãã¨å•é¡ŒãŒå¤šç™ºã—ã¾ã™ã€‚test.pyã§å¿…ãšå®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚**

### Check 1: Effective Rank (å¤šæ§˜æ€§ç¢ºèª)

**ç›®çš„**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãŒå¤šæ§˜ãªæ¬¡å…ƒã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ç¢ºèª

**å®Ÿè£…**: `analyze_fixed_points(contexts)` in `src/evaluation/metrics.py`

**åˆæ ¼åŸºæº–**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 88-89% Effective Rank
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 81-82% Effective Rank

**å¤±æ•—ä¾‹**:
- âŒ Effective Rank < 30%: æ¬¡å…ƒãŒåã£ã¦ã„ã‚‹ï¼ˆå¤šæ§˜æ€§ãªã—ï¼‰
- âŒ Global attractor: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åæŸ

### Check 2: Identity Mapping Check (æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯)

**ç›®çš„**: ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã§ãã¦ã„ã‚‹ã‹ã€å˜ãªã‚‹æ’ç­‰å†™åƒã§ãªã„ã‹ç¢ºèª

**å®Ÿè£…**: `check_identity_mapping(model, token_embeds, contexts, device)` in `src/evaluation/metrics.py`

**åˆæ ¼åŸºæº–**:
- âœ… Zero context ã¨ã®å·®åˆ† > 0.1
- âœ… Token embedding ã¨ã®é¡ä¼¼åº¦ < 0.95

**å¤±æ•—ä¾‹**:
- âŒ å­¦ç¿’å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã¨åŒã˜ â†’ å­¦ç¿’ãªã—
- âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã¨åŒä¸€ â†’ æ’ç­‰å†™åƒ

### Check 3: CVFP Convergence Check (å›ºå®šç‚¹åæŸãƒã‚§ãƒƒã‚¯)

**ç›®çš„**: å›ºå®šç‚¹å­¦ç¿’ãŒã§ãã¦ã„ã‚‹ã‹ã€åå¾©å®Ÿè¡Œã§å®‰å®šã—ãŸçµæœã«ãªã‚‹ã‹ç¢ºèª

**å®Ÿè£…**: `check_cvfp_convergence(trainer, token_ids, device)` in `src/evaluation/metrics.py`

**åˆæ ¼åŸºæº–**:
- âœ… Final diff < 1e-3 (GOODä»¥ä¸Š)
- âœ… ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®å¤‰åŒ–ãŒæ¸›å°‘å‚¾å‘

**å¤±æ•—ä¾‹**:
- âŒ Final diff > 1e-2: å›ºå®šç‚¹ã«åæŸã—ã¦ã„ãªã„
- âŒ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§å¤‰åŒ–ãŒå¢—åŠ  â†’ ç™ºæ•£ã—ã¦ã„ã‚‹

---

## Reproducibility - å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼

**ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š (å¿…é ˆ)**:

```python
def set_seed(seed=42):
    """å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆå™¨ã®ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
```

**ãªãœå¿…è¦ã‹**:
- åŒã˜ã‚³ãƒ¼ãƒ‰ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã§**å®Œå…¨ã«åŒã˜çµæœ**ã‚’ä¿è¨¼
- å®Ÿè£…ãŒç¶­æŒã•ã‚Œã¦ã„ã‚‹ã‹ã®ç¢ºèªã«ä¸å¯æ¬ 
- ãƒ‡ãƒãƒƒã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å®¹æ˜“ã«

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ Effective Rank: **å®Œå…¨ã«åŒã˜å€¤** (å°æ•°ç‚¹ä»¥ä¸‹ã¾ã§ä¸€è‡´)
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ Effective Rank: **å®Œå…¨ã«åŒã˜å€¤** (å°æ•°ç‚¹ä»¥ä¸‹ã¾ã§ä¸€è‡´)
- 3ã¤ã®ãƒã‚§ãƒƒã‚¯çµæœ: æ¯å›åŒã˜

---

## File Structure - Final Organization

**Main Scripts**:
- `test.py` - æ¨™æº–ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ6400è¨“ç·´ + 1280æ¤œè¨¼ï¼‰
- `train.py` - ãƒ•ãƒ«è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `config.py` - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

**Data Generation**:
- `scripts/create_val_from_train.py` - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰

**Core Implementation**:
- `src/training/phase1_trainer.py` - Phase 1è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆDimension Usage Statisticsï¼‰
- `src/models/new_llm_residual.py` - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- `src/data/loader.py` - ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆauto_splitç¦æ­¢ãƒ­ã‚¸ãƒƒã‚¯ï¼‰

---

## Validation Data Policy - CRITICAL

### å¿…é ˆä»•æ§˜

**æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®éƒ¨åˆ†é›†åˆã§ãªã‘ã‚Œã°ãªã‚‰ãªã„**:
- å…¨ã¦ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨
- ãƒ©ãƒ³ãƒ€ãƒ ãªåˆ†å‰²ã¯ç¦æ­¢ï¼ˆ`auto_split` ä½¿ç”¨ã§ã‚¨ãƒ©ãƒ¼ï¼‰
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ç”Ÿæˆï¼ˆ`create_val_from_train.py`ï¼‰

### ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ­ã‚¸ãƒƒã‚¯

`loader.py` ã§å®Ÿè£…æ¸ˆã¿:
```python
if config.val_data_source == "auto_split":
    raise ValueError(
        "âŒ CRITICAL ERROR: auto_split is STRICTLY FORBIDDEN!"
        "Use val_data_source='text_file' with data/example_val.txt"
    )
```

---

## Code Quality Standards

### Principles

1. **No Hardcoding**: All hyperparameters in config.py
2. **Single Responsibility**: Each module has one clear purpose
3. **Immutable Data**: Training/validation data are fixed
4. **Error Prevention**: Auto-split is forbidden with error

### Anti-Patterns to Avoid

- âŒ Changing train/val data without regeneration
- âŒ Using auto_split for validation
- âŒ Modifying diversity loss implementation
- âŒ Changing architecture without full retraining

---

## Performance Benchmarks

**CPU Performance (Apple Silicon/Intel)**:
- Training speed: 250-330 tok/s
- 6400 tokens: ~25 seconds per iteration
- Validation: ~4 seconds (1280 tokens)

**Expected Results (commit 9ee3281 baseline)**:
- Training Effective Rank: 74.0% (568.31/768)
- Validation Effective Rank: 66.6% (511.56/768)
- Training Convergence: 30.0% (1923/6400 tokens)
- Validation Convergence: 100.0% (1280/1280 tokens)
- Full 10 iterations complete

---

## No Hardcoding Policy - Reinforced

**å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯config.pyã§å®šç¾©**:
```python
# âœ… Good
learning_rate = config.phase1_learning_rate
num_samples = config.num_samples

# âŒ Bad
learning_rate = 0.002  # Hardcoded!
num_samples = 50       # Hardcoded!
```

---

## ğŸ› CRITICAL BUG FIX HISTORY - November 24, 2025

### Bug #1: F.normalize() in CVFP Loss Calculation (src/training/phase1_trainer.py)

**Problem**:
- Location: [phase1_trainer.py:265-267](src/training/phase1_trainer.py#L265-L267)
- CVFP loss used `F.normalize()` on both `new_context` and `previous_context`
- This only enforces **cosine similarity** (direction), not **value equality**
- Fixed points require `f(x) = x` (exact values), not just same direction

**Symptoms**:
- 0% convergence rate despite 10 iterations
- MSE ~32-33 (vs threshold 0.1 = 300x larger)
- CVFP loss increasing instead of decreasing

**Root Cause**:
```python
# âŒ WRONG: Normalization prevents value convergence
cvfp_loss = F.mse_loss(
    F.normalize(new_context, p=2, dim=1),      # Only matches direction
    F.normalize(previous_context, p=2, dim=1)  # Norms can still diverge
)
```

**Fix**:
```python
# âœ… CORRECT: Raw MSE for exact value matching
cvfp_loss = F.mse_loss(new_context, previous_token_context)
```

**Affected File**: [src/training/phase1_trainer.py:267](src/training/phase1_trainer.py#L267)

---

### Bug #2: Missing context.detach() Between Tokens (src/training/phase1_trainer.py)

**Problem**:
- Location: [phase1_trainer.py:226-240](src/training/phase1_trainer.py#L226-L240)
- Context passed between tokens without `detach()`
- Gradient graph reused across token sequence
- RuntimeError: "Trying to backward through the graph a second time"

**Root Cause**:
```python
# âŒ WRONG: Gradient graph carries over
context = self._train_one_token(
    token_embed.unsqueeze(0),
    context,  # No detach - gradient accumulates across tokens
    token_idx=t
)
current_contexts[t] = context.squeeze(0)  # No detach for convergence check
```

**Fix**:
```python
# âœ… CORRECT: Detach between tokens
context = self._train_one_token(
    token_embed.unsqueeze(0),
    context.detach(),  # Break gradient flow between tokens
    token_idx=t
)
current_contexts[t] = context.squeeze(0).detach()  # Detach for convergence tracking
```

**Affected Lines**:
- [phase1_trainer.py:228](src/training/phase1_trainer.py#L228) - Training token processing
- [phase1_trainer.py:240](src/training/phase1_trainer.py#L240) - Convergence tracking

---

### Verification Results (After Fixes)

**With dist_reg_weight=0.01** (99% CVFP, 1% Diversity):
- âœ… Convergence mechanism works: 96.0% training, 100.0% validation
- âœ… CVFP loss decreases: 1.02 â†’ 0.021 â†’ 0.025
- âŒ Effective Rank collapsed: 6.9% training, 1.1% validation (vs 66.6% baseline)
- **Conclusion**: Bug fixed, but diversity weight too low

**With dist_reg_weight=0.5** (50% CVFP, 50% Diversity) - Baseline (commit 9ee3281):
- âœ… Training Effective Rank: 74.0% (568.31/768)
- âœ… Validation Effective Rank: 66.6% (511.56/768)
- âœ… Training Convergence: 30.0% (1923/6400)
- âœ… Validation Convergence: 100.0% (1280/1280)

---

## ğŸ“ NEW-LLM Detailed Architecture Specification

### Core Components

**1. CVFPLayer (Context Vector Fixed-Point Layer)**
- Location: [src/models/new_llm_residual.py:15-102](src/models/new_llm_residual.py#L15-L102)
- Input: `context [batch, context_dim]`, `token_embed [batch, embed_dim]`
- Output: `new_context [batch, context_dim]`, `new_token [batch, embed_dim]`
- Architecture:
  - FNN: `[context + token] â†’ [hidden_dim]` with ReLU
  - Split: `hidden_dim â†’ delta_context + delta_token`
  - Residual: `new_context = context + delta_context`
  - LayerNorm: Optional mixing with `layernorm_mix` parameter

**2. CVFPBlock (Multiple Layers)**
- Location: [src/models/new_llm_residual.py:105-150](src/models/new_llm_residual.py#L105-L150)
- Sequential execution of `num_layers` CVFPLayer instances
- Passes context and token through all layers

**3. NewLLMResidual (Main Model)**
- Location: [src/models/new_llm_residual.py:153-314](src/models/new_llm_residual.py#L153-L314)
- Token Embedding: GPT-2 pretrained (768-dim, frozen)
- CVFP Blocks: 6 blocks (configurable via `layer_structure`)
- Output Head: Linear layer `context_dim â†’ vocab_size`

**4. Phase1Trainer (CVFP Fixed-Point Learning)**
- Location: [src/training/phase1_trainer.py](src/training/phase1_trainer.py)
- Training loop: Iterative refinement until convergence
- Loss function:
  - CVFP Loss: `MSE(context_t, context_{t-1})` - **NO normalization**
  - Diversity Loss: EMA-based per-dimension variance tracking
  - Total: `(1-w) * cvfp_loss + w * diversity_loss`
- Convergence: MSE < threshold (0.1) for 95% of tokens
- Early stopping: When 95% converged (training only)

### Key Design Decisions

**Dimension Constraints**:
- `hidden_dim = context_dim + embed_dim` (MANDATORY)
- Default: `context_dim=768, embed_dim=768, hidden_dim=1536`
- Reason: FNN output must split into delta_context + delta_token

**Context Carryover** (CRITICAL):
- Between iterations: `context = previous_contexts[-1]` (NOT zero reset)
- Between tokens: `context = context.detach()` (gradient isolation)
- Reason: Fixed-point learning requires continuity

**Gradient Management**:
- Token embeddings: Frozen (GPT-2 pretrained)
- Context params: Trained (all CVFP layers)
- Between tokens: Detached (prevent cross-token gradients)
- Reason: Stable training with efficient gradient flow

**Diversity Regularization**:
- Method: Per-dimension variance tracking with EMA
- Implementation: Negative L2 norm of deviation from mean
- Memory: O(context_dim) - 6KB for 768-dim
- Reason: Encourage usage of all dimensions

---

## Context Size Monitoring Policy

**Claude Codeã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†**:
- 100,000ãƒˆãƒ¼ã‚¯ãƒ³è¶…éæ™‚: åˆå›å ±å‘Š
- ä»¥é™10,000ãƒˆãƒ¼ã‚¯ãƒ³åˆ»ã¿ã§ç¶™ç¶šå ±å‘Š
- 190,000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Š: æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚’å¼·ãæ¨å¥¨

---

Last Updated: 2025-11-24 (Bug Fixes + Architecture Documentation)
