# New-LLM Project Guidelines

## ğŸ¯ Context-KV Attention Architecture (2025-12-03)

**KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹Context-KV Attentionæ–¹å¼ã‚’æ¡ç”¨ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
Context-KV Attention:
  - 100ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«Contextã‚’åœ§ç¸®
  - åœ§ç¸®ã•ã‚ŒãŸContextã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä½¿ç”¨
  - ~99% KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›

Position 350 ã®å ´åˆ:
  KV Cache = [context_0-99, context_100-199, context_200-299, context_300-350]
           = 4 context vectors ã®ã¿
```

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# Colabï¼ˆGPUï¼‰: 200ã‚µãƒ³ãƒ—ãƒ«
python3 scripts/experiment_context_kv.py -s 200 --chunk-size 100

# ã‚«ã‚¹ã‚¿ãƒ contextæ¬¡å…ƒ
python3 scripts/experiment_context_kv.py -s 200 -c 256 128 --chunk-size 50
```

---

## ğŸ¯ OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (Phase 1)

**Phase 1ã§ã¯OACD (Origin-Anchored Centroid Dispersion) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¡ç”¨ã€‚**

```python
def oacd_loss(contexts, centroid_weight=0.1):
    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–
    dispersion_loss = -||X - mean(X)|| / n

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = ||mean(X)||Â²

    return dispersion_loss + centroid_weight * centroid_loss
```

---

## ğŸš¨ğŸš¨ Phase 1å­¦ç¿’ã§ã¯é †æ¬¡å‡¦ç†ç¦æ­¢ (CRITICAL) ğŸš¨ğŸš¨

**Phase 1å­¦ç¿’ã§ã¯ã€é †æ¬¡å‡¦ç†ã¯å³ç¦ã€‚å¿…ãšshifted_prev_contextæ–¹å¼ã§ä¸¦åˆ—å‡¦ç†ã™ã‚‹ã“ã¨ã€‚**

```python
# âŒ ç¦æ­¢: Phase 1å­¦ç¿’ã§é †æ¬¡å‡¦ç†ï¼ˆéå¸¸ã«é…ã„ï¼‰
for i in range(num_tokens):
    new_context = model.forward_context(prev_context, token_embed)
    prev_context = new_context

# âœ… æ¨å¥¨: shifted_prev_contextæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
for iteration in range(max_iterations):
    shifted_prev_context = torch.cat([zero_init, previous_contexts[:-1]], dim=0)
    new_contexts = model.forward_context(shifted_prev_context, input_embeds)
    previous_contexts = new_contexts
```

---

## ğŸš¨ CPU/GPUãƒ†ãƒ³ã‚½ãƒ«ç®¡ç†

**å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§OOMã‚’é˜²ããŸã‚ã€ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚’å¾¹åº•ã€‚**

```python
# âŒ ä¿®æ­£å‰
batch_contexts = previous_contexts[start_idx:end_idx].detach()

# âœ… ä¿®æ­£å¾Œ
batch_contexts = previous_contexts[start_idx:end_idx].detach().to(self.device)
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒã®Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports

# å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
python3 -m ruff check scripts/experiment_context_kv.py
python3 -m mypy scripts/experiment_context_kv.py --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Core Components

**1. ContextKVAttentionLLM**
- è¤‡æ•°ã®ContextBlockï¼ˆå„1å±¤å›ºå®šï¼‰
- Context-KV Attention Layer
- Token Embedding: GPT-2 pretrained (768-dim, frozen)
- Weight Tying: token_output shares weights with token_embedding

**2. ContextBlock**
- 1å±¤å›ºå®šã€Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze
- OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¤šæ§˜æ€§å­¦ç¿’

**3. Context-KV Attention**
- Contextã‚’K,Vã«å¤‰æ›
- ãƒãƒ£ãƒ³ã‚¯å˜ä½ã®contextã§Attention

### Phase 1: å¤šæ§˜æ€§å­¦ç¿’ï¼ˆOACDï¼‰

- **å­¦ç¿’å¯¾è±¡**: ContextBlockã®ã¿
- **æå¤±**: OACDï¼ˆå¤šæ§˜æ€§æå¤±ï¼‰

### Phase 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

- **ContextBlock**: frozenï¼ˆé‡ã¿å›ºå®šï¼‰
- **Context-KV Attention + FFN**: å­¦ç¿’
- **æå¤±**: CrossEntropyï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰

---

## Code Quality Standards

### Principles

1. **No Hardcoding**: All hyperparameters in config.py
2. **Single Responsibility**: Each module has one clear purpose
3. **Type Hints Required**: é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å‹æ³¨é‡ˆã‚’å¿…é ˆ

---

## File Structure

**Main Scripts**:
- `scripts/experiment_context_kv.py` - Context-KV Attentionå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**Core Implementation**:
- `src/models/context_kv.py` - ContextKVAttentionLLM
- `src/models/blocks.py` - ContextBlockï¼ˆ1å±¤å›ºå®šï¼‰
- `src/models/layers.py` - ContextLayer
- `src/trainers/phase1/memory.py` - Phase 1è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯
- `src/losses/diversity.py` - OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

---

Last Updated: 2025-12-03 (Context-KV Attentionæ–¹å¼ã«å®Œå…¨ç§»è¡Œ)
