# New-LLM Project Guidelines

---

## ğŸ¯ ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æŸ”è»Ÿãªè¨­è¨ˆã€‚**

### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

```
å¾“æ¥: 4ã¤ã®å›ºå®šãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
  PythiaModel, InfiniPythiaModel, MultiMemoryPythiaModel, HierarchicalMemoryPythiaModel

æ–°è¨­è¨ˆ: 1ã¤ã®æ±ç”¨ãƒ¢ãƒ‡ãƒ« + 4ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—
  TransformerLM + [PythiaLayer, InfiniLayer, MultiMemoryLayer, HierarchicalLayer]
```

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
TransformerLM:
  Token Embedding (512-dim)
         â†“
  Layer 0, 1, ..., N-1 (ä»»æ„ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—)
         â†“
  Final LayerNorm
         â†“
  LM Head (512 â†’ vocab)
```

### ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | èª¬æ˜ |
|----------|------|
| `PythiaLayer` | æ¨™æº–Pythia (RoPE + Softmax Attention) |
| `InfiniLayer` | Infini-Attention (Memory + Linear Attention, NoPE) |
| `MultiMemoryLayer` | è¤‡æ•°ç‹¬ç«‹ãƒ¡ãƒ¢ãƒª + Attention-basedé¸æŠ |
| `HierarchicalLayer` | éšå±¤çš„ãƒ¡ãƒ¢ãƒª + å­¦ç¿’å¯èƒ½ãªå±•é–‹ã‚²ãƒ¼ãƒˆ |

---

## ğŸ­ ãƒ¢ãƒ‡ãƒ«ä½œæˆ

### create_model() ãƒ•ã‚¡ã‚¯ãƒˆãƒª

```python
from src.models import create_model

# åŸºæœ¬çš„ãªä½¿ã„æ–¹
model = create_model("pythia")       # æ¨™æº–Pythiaï¼ˆ6å±¤ï¼‰
model = create_model("infini")       # 1å±¤Infini + 5å±¤Pythia
model = create_model("multi_memory") # 1å±¤Multi-Memory + 5å±¤Pythia
model = create_model("hierarchical") # 1å±¤Hierarchical + 5å±¤Pythia

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ã
model = create_model("multi_memory", num_memories=8)
model = create_model("hierarchical", num_memories=4, use_delta_rule=False)
model = create_model("infini", num_memory_banks=2, segments_per_bank=4)
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹æˆ

```python
from src.models import TransformerLM
from src.models.layers import InfiniLayer, PythiaLayer, MultiMemoryLayer

# 2å±¤Infini + 4å±¤Pythia
layers = [
    InfiniLayer(hidden_size=512, num_heads=8, intermediate_size=2048),
    InfiniLayer(hidden_size=512, num_heads=8, intermediate_size=2048),
    *[PythiaLayer(hidden_size=512, num_heads=8, intermediate_size=2048) for _ in range(4)]
]
model = TransformerLM(layers=layers)

# å…¨å±¤Infini
layers = [InfiniLayer(512, 8, 2048) for _ in range(6)]
model = TransformerLM(layers=layers)

# æ··åˆæ§‹æˆ
layers = [
    MultiMemoryLayer(512, 8, 2048, num_memories=4),
    InfiniLayer(512, 8, 2048),
    *[PythiaLayer(512, 8, 2048) for _ in range(4)]
]
model = TransformerLM(layers=layers)
```

### åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | å¯¾è±¡ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------------|------|------------|------|
| `use_delta_rule` | å…¨memoryç³» | `True` | Delta Ruleä½¿ç”¨ |
| `num_memories` | multi_memory, hierarchical | `4` | ãƒ¡ãƒ¢ãƒªæ•° |
| `num_memory_banks` | infini | `1` | ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯æ•° |
| `segments_per_bank` | infini | `4` | ãƒãƒ³ã‚¯ã‚ãŸã‚Šã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•° |

---

## ğŸ’¾ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ä¿å­˜ãƒ»è»¢é€

```python
import torch
from src.models import create_model

# ===== PC A =====
model = create_model("infini")
model.reset_memory()

# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªã‚’è“„ç©
for batch in data_loader:
    _ = model(batch, update_memory=True)

# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’ä¿å­˜
state = model.get_memory_state()
torch.save(state, "memory.pt")

# ===== PC B =====
state = torch.load("memory.pt")
model = create_model("infini")
model.set_memory_state(state)

# ãƒ¡ãƒ¢ãƒªãŒå¼•ãç¶™ãŒã‚ŒãŸçŠ¶æ…‹ã§æ¨è«–
output = model(input_ids)
```

### ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º |
|--------|--------|
| Infini (1 bank) | ~135 KB |
| Multi-Memory (4) | ~540 KB |
| Hierarchical (4) | ~540 KB |

---

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
src/models/
â”œâ”€â”€ __init__.py          # create_model() ãƒ•ã‚¡ã‚¯ãƒˆãƒª + exports
â”œâ”€â”€ layers/              # ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”‚   â”œâ”€â”€ __init__.py      # exports
â”‚   â”œâ”€â”€ base.py          # BaseLayer åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ pythia.py        # PythiaLayer (RoPE + Softmax)
â”‚   â”œâ”€â”€ infini.py        # InfiniLayer (Memory + Linear)
â”‚   â”œâ”€â”€ multi_memory.py  # MultiMemoryLayer
â”‚   â””â”€â”€ hierarchical.py  # HierarchicalLayer
â”œâ”€â”€ model.py             # TransformerLMï¼ˆæ±ç”¨ãƒ¢ãƒ‡ãƒ«ï¼‰
â”œâ”€â”€ continuous.py        # ContinuousLMï¼ˆé›¢æ•£åŒ–ã‚¹ã‚­ãƒƒãƒ—ä»®èª¬æ¤œè¨¼ï¼‰
â”œâ”€â”€ base_components.py   # PythiaMLP, init_weights
â”œâ”€â”€ memory_utils.py      # elu_plus_one, causal_linear_attention
â””â”€â”€ position_encoding.py # RoPE
```

---

## ğŸ§ª çµ±ä¸€å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
python3 scripts/experiment.py --models pythia infini multi_memory hierarchical

# Infiniã®ã¿
python3 scripts/experiment.py --models infini

# è¨­å®šã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
python3 scripts/experiment.py --models infini --samples 10000 --epochs 50 --lr 5e-5
```

---

## ğŸ“Š Reversal Curse è©•ä¾¡

| æŒ‡æ¨™ | å®šç¾© | è§£é‡ˆ |
|------|------|------|
| Forward PPL | é †æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãŸã‚ä½ã„ |
| Backward PPL | é€†æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ãŸã‚é«˜ã„ |
| Reversal Gap | Backward - Forward | 0ã«è¿‘ã„ã»ã©è‰¯ã„ |

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

### è¨“ç·´-è©•ä¾¡ä¸€è²«æ€§ï¼ˆTraining-Evaluation Consistencyï¼‰

**è¨“ç·´æ™‚ã¨è©•ä¾¡æ™‚ã®æ¡ä»¶ã¯å¿…ãšæƒãˆã‚‹ã€‚**

```python
# âŒ æ‚ªã„ä¾‹: é›¢æ•£çš„ãªthresholdï¼ˆè¨“ç·´æ™‚ã«ãƒã‚¤ã‚¢ã‚¹ç™ºç”Ÿï¼‰
# è¨“ç·´: threshold=0.5ã§ã‚¹ã‚­ãƒƒãƒ—ã€å‡ºåŠ›ä½ç½®ã®ã¿loss
# â†’ é«˜ç¢ºä¿¡åº¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã§lossè¨ˆç®— â†’ ç•°å¸¸ã«ä½ã„PPL
def train():
    output_mask = gate_prob > 0.5  # é›¢æ•£çš„åˆ¤å®š
    loss = (losses * output_mask).sum() / output_mask.sum()

# âœ… è‰¯ã„ä¾‹: é€£ç¶šçš„é‡ã¿ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­¦ç¿’ã«å¯„ä¸ï¼‰
# gate_probã‚’é‡ã¿ã¨ã—ã¦ä½¿ç”¨ â†’ å‹¾é…ãŒå¸¸ã«æµã‚Œã‚‹
def train():
    weighted_loss = (losses * gate_prob).sum() / gate_prob.sum()

def evaluate():
    weighted_ppl = exp(sum(gate_prob * log_loss) / sum(gate_prob))
```

**åŸå‰‡**:
1. é€£ç¶šçš„ãªé‡ã¿ã‚’ä½¿ç”¨ï¼ˆé›¢æ•£çš„ãªthresholdã¯å­¦ç¿’ãƒã‚¤ã‚¢ã‚¹ã‚’ç”Ÿã‚€ï¼‰
2. å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­¦ç¿’ã«å¯„ä¸ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
3. ç”Ÿæˆæ™‚ã®ã¿thresholdã‚’ä½¿ç”¨ï¼ˆè¨“ç·´ãƒ»è©•ä¾¡ã«ã¯ä½¿ã‚ãªã„ï¼‰

---

## ğŸ”„ Lagged Cache Trainingï¼ˆLCTï¼‰æ–¹å¼ - å‰Šé™¤ç¦æ­¢

**âš ï¸ ã“ã®æ–¹å¼ã¯æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ ¸å¿ƒçš„æ‰‹æ³•ã§ã™ã€‚çµ¶å¯¾ã«å‰Šé™¤ãƒ»å¤‰æ›´ã—ãªã„ã“ã¨ã€‚**

### æ¦‚è¦

ã€Œå‰ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å‡ºåŠ›ã‚’ã€ä»Šå›ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã€è¨“ç·´æ–¹å¼ã€‚

å†å¸°çš„ä¾å­˜é–¢ä¿‚ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ï¼ˆRNNçš„æ§‹é€ ï¼‰ã‚’ä¸¦åˆ—å‡¦ç†å¯èƒ½ã«ã™ã‚‹ç”»æœŸçš„ãªæ‰‹æ³•ã€‚

### å•é¡Œ

å†å¸°çš„ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¯æœ¬æ¥ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«:
```
é€šå¸¸ã®å†å¸°è¨“ç·´ï¼ˆé…ã„ï¼‰:
  for t in range(seq_len):
      h_t = model(h_{t-1}, x_t)  # 1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤å‡¦ç†
      # â†’ O(seq_len) å›ã®forward pass
```

### è§£æ±ºç­–: Lagged Cache Training

```python
# 1. åˆæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—ï¼ˆæœ€åˆã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿ï¼‰
hidden_cache = model.init_hidden_cache(input_ids)  # [batch, seq_len, hidden]

# 2. è¨“ç·´ãƒ«ãƒ¼ãƒ—
for epoch in epochs:
    for batch_idx, batch in enumerate(train_loader):
        # å‰ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨
        prev_cache = hidden_caches[batch_idx]

        # ä¸¦åˆ—å‡¦ç†ã§å…¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä¸€åº¦ã«è¨ˆç®—
        # ä½ç½®t ã®å…¥åŠ› = prev_cache[t-1]ï¼ˆå‰ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®ä½ç½®t-1ã®å‡ºåŠ›ï¼‰
        loss, new_hidden = model.forward_with_cache(input_ids, prev_cache)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°ï¼ˆæ¬¡ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
        hidden_caches[batch_idx] = new_hidden.detach()

        loss.backward()
        optimizer.step()
```

### ãªãœå‹•ä½œã™ã‚‹ã®ã‹

1. **è¿‘ä¼¼ã®ä»®å®š**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã®å¤‰åŒ–ã¯å¾®å°
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°é‡ãŒå°ã•ã‘ã‚Œã°ã€h_t ã®å¤‰åŒ–ã‚‚å°ã•ã„
   - prev_cache â‰ˆ çœŸã®h_{t-1}ï¼ˆ1ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é…ã‚Œï¼‰

2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ›´æ–°**: å­¦ç¿’ãŒé€²ã‚€ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚è¿½å¾“
   - å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§new_hiddenã‚’ä¿å­˜
   - æ¬¡ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ãã‚Œã‚’å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨
   - â†’ å¾ã€…ã«çœŸã®å†å¸°å‹•ä½œã«åæŸ

3. **è¨ˆç®—åŠ¹ç‡**:
   - ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«: O(seq_len) forward passes per batch
   - LCT: O(1) forward pass per batch
   - é€Ÿåº¦å‘ä¸Š: ~20å€ï¼ˆseq_len=128ã®å ´åˆï¼‰

### é©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«

- Continuous LMï¼ˆh_{t-1} â†’ x_tï¼‰
- Context-Pythiaï¼ˆcontext_{t-1} â†’ context_tï¼‰
- DProjï¼ˆprev_proj â†’ new_projï¼‰
- ä»»æ„ã®RNNçš„å†å¸°æ§‹é€ 

### å®Ÿè£…ä¾‹ï¼ˆContinuousLMï¼‰

```python
# src/models/continuous.py
def forward_with_cache(self, input_ids, prev_hidden_cache):
    # ä½ç½®0: æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿
    first_embed = self.embed_in(input_ids[:, :1])

    # ä½ç½®1ä»¥é™: prev_hidden_cache[t-1] ã‚’å¤‰æ›ã—ã¦ä½¿ç”¨
    rest_input = self.hidden_proj(prev_hidden_cache[:, :-1, :])
    hidden_input = torch.cat([first_embed, rest_input], dim=1)

    # ä¸¦åˆ—ã§Transformerå‡¦ç†
    h = self._forward_layers(hidden_input)
    return logits, h  # hã‚’æ¬¡ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦è¿”ã™
```

### æ€§èƒ½æ¯”è¼ƒï¼ˆå®Ÿæ¸¬å€¤ï¼‰

| æ–¹å¼ | æ™‚é–“/epoch | é€Ÿåº¦æ¯” |
|------|-----------|--------|
| ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ« | 450s | 1x |
| LCT | 20s | 22x |

---

## âš ï¸ éå»ã®ãƒã‚°ã¨æ•™è¨“

### 1. Infini-Attention ãƒ¡ãƒ¢ãƒªå‹¾é…ãƒã‚°

```python
# âŒ ãƒã‚°: ãƒ¡ãƒ¢ãƒªæ›´æ–°ã§ã‚°ãƒ©ãƒ•ãŒæ®‹ã‚Šã€äºŒé‡backwardã‚¨ãƒ©ãƒ¼
self.memory = self.memory + memory_update

# âœ… ä¿®æ­£: detach()ã§ã‚°ãƒ©ãƒ•ã‚’åˆ‡æ–­
self.memory = (self.memory + memory_update).detach()
```

### 2. PPLç•°å¸¸å€¤ã®è¨ºæ–­åŸºæº–

| PPL | çŠ¶æ…‹ | å¯¾å‡¦ |
|-----|------|------|
| < 5 | **ç•°å¸¸** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯/å› æœãƒã‚¹ã‚¯ãƒã‚° | ã‚³ãƒ¼ãƒ‰ç‚¹æ¤œå¿…é ˆ |
| 5-30 | **ç–‘ã‚ã—ã„** - éå­¦ç¿’ã®å¯èƒ½æ€§ | ãƒ‡ãƒ¼ã‚¿é‡ãƒ»åˆ†å‰²ã‚’ç¢ºèª |
| 30-100 | æ­£å¸¸ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ | - |
| 100-500 | æ­£å¸¸ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼‰ | - |
| > 1000 | å­¦ç¿’ä¸è¶³ | epochå¢—åŠ /lrèª¿æ•´ |

### 3. Linear Attentionã®head_dimæ¬¡å…ƒæ•°å•é¡Œï¼ˆé‡è¦ï¼‰

**head_dimãŒå°ã•ã„ã¨Linear AttentionãŒæ©Ÿèƒ½ã—ãªã„ã€‚**

```python
# âŒ å•é¡Œ: head_dim=64ï¼ˆhidden_size=512 / num_heads=8ï¼‰
# â†’ 64æ¬¡å…ƒç©ºé–“ã§ã¯ç•°ãªã‚‹ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ãŒç›´äº¤ã—ã‚„ã™ã„
# â†’ Ïƒ(Q) @ Ïƒ(K)^T â‰ˆ 0 â†’ ãƒ¡ãƒ¢ãƒªã‹ã‚‰ä½•ã‚‚å–ã‚Šå‡ºã›ãªã„

# âœ… è§£æ±º: ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆmemory_head_dim=hidden_size=512ï¼‰
# InfiniLayerã¯è‡ªå‹•çš„ã«ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’ä½¿ç”¨
```

**æ•™è¨“**:
1. Linear Attentionã«ã¯head_dim >= 256ãŒå¿…è¦
2. å¯èƒ½ãªã‚‰ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆhead_dim=hidden_sizeï¼‰ã‚’ä½¿ç”¨
3. alphaãŒå°ã•ã„ã¾ã¾ â†’ head_dimã‚’ç–‘ã†

### 4. PPLè©•ä¾¡æ–¹æ³•ã«ã‚ˆã‚‹PPLç•°å¸¸å€¤

**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²è©•ä¾¡ã§ç•°å¸¸ã«é«˜ã„PPLï¼ˆ10,000+ï¼‰ãŒå‡ºã‚‹åŸå› ã€‚**

```python
# âŒ å•é¡Œ: å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒç‹¬ç«‹ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ï¼‰
for start in range(0, seq_len, segment_length):
    segment = tokens[start:end]
    # â†’ ã€Œæ–‡æ›¸ã®é€”ä¸­ã€ã‚’ã€Œæ–‡æ›¸ã®å…ˆé ­ã€ã¨ã—ã¦æ‰±ã†ãŸã‚é«˜PPL

# âœ… æ­£ã—ã„è©•ä¾¡: Sliding Windowæ–¹å¼
stride = 512
for start in range(0, seq_len - 1, stride):
    input_ids = tokens[start:start+2048]
    labels = input_ids.clone()
    labels[0, :stride] = -100  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã¯lossè¨ˆç®—ã‹ã‚‰é™¤å¤–
```

| è©•ä¾¡æ–¹æ³• | PPL |
|----------|-----|
| Sliding window (stride=512) | **40.96** âœ“ |
| Segment-based | 14,204 âŒ |

### 5. çŸ­ã„æ–‡ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œï¼ˆReversal Curseè©•ä¾¡ï¼‰

**çŸ­ã„æ–‡ã‚’EOSã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã¨ã€è¨“ç·´ã¨è©•ä¾¡ã®åˆ†å¸ƒãŒä¹–é›¢ã™ã‚‹ã€‚**

```python
# âŒ å•é¡Œ: çŸ­ã„æ–‡ã‚’seq_lengthã¾ã§EOSã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
sentence = "Paris is the capital of France"  # 6ãƒˆãƒ¼ã‚¯ãƒ³
tokens = tokenize(sentence) + [EOS] * 122     # 128ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
# â†’ ãƒ¢ãƒ‡ãƒ«ã¯ä¸»ã«EOSâ†’EOSã‚’å­¦ç¿’ï¼ˆ94%ãŒEOSï¼‰
# â†’ è©•ä¾¡æ™‚ï¼ˆ6ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰ã§PPLãŒç•°å¸¸ã«é«˜ããªã‚‹

# âœ… æ­£ã—ã„æ–¹æ³•: è¤‡æ•°ã®çŸ­ã„æ–‡ã‚’é€£çµ
all_sentences = "Paris is the capital of France EOS Tokyo is the capital of Japan EOS ..."
# â†’ æ–‡ã®å†…å®¹ãŒé€£ç¶šã—ã¦ç¾ã‚Œã‚‹ï¼ˆEOSã¯12%ç¨‹åº¦ã€åŒºåˆ‡ã‚Šã¨ã—ã¦ã®ã¿ï¼‰
# â†’ è¨“ç·´ã¨è©•ä¾¡ã®åˆ†å¸ƒãŒä¸€è‡´
```

**ç—‡çŠ¶**: Forward PPL > Backward PPLï¼ˆé€šå¸¸ã¯é€†ï¼‰
**åŸå› **: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®94%ãŒEOSãƒˆãƒ¼ã‚¯ãƒ³ã§ã€å®Ÿéš›ã®æ–‡å†…å®¹ã‚’ã»ã¨ã‚“ã©å­¦ç¿’ã—ã¦ã„ãªã„
**å¯¾ç­–**: ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã›ãšã€æ–‡ã‚’é€£çµã—ã¦ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ã‚µãƒ³ãƒ—ãƒ«ä½œæˆ

---

## ğŸ”§ Pretrained LLMã¸ã®Infini-Attentionå°å…¥ï¼ˆå¤±æ•—ï¼‰

**âš ï¸ Layerç½®ãæ›ãˆæ–¹å¼ã¯å…¨ã¦å¤±æ•—ã€‚**

| æ–¹å¼ | çµæœ |
|------|------|
| Layer 0ç½®ãæ›ãˆ | âŒ RoPEæå¤±ã€PPLå¤§å¹…åŠ£åŒ– |
| è’¸ç•™+Fine-tune | âŒ PPL 44â†’1237ï¼ˆ28å€åŠ£åŒ–ï¼‰ |
| Parallel Adapter | âŒ alphaãŒå­¦ç¿’ã•ã‚Œãªã„ |

**æ¨å¥¨**: ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼ˆæœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰

è©³ç´°ã¯ `docs/experiments/2025-12-06_distill_finetune_failure.md` ã‚’å‚ç…§ã€‚

---

## ğŸ”§ Continuous LM

**ä»®èª¬: ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã«ã‚ˆã‚‹é›¢æ•£åŒ–ã§æƒ…å ±ãŒå¤±ã‚ã‚Œã¦ã„ã‚‹**

### èƒŒæ™¯

é€šå¸¸ã®LMã§ã¯ã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬æ™‚ã«é›¢æ•£åŒ–ãŒç™ºç”Ÿã™ã‚‹ï¼š

```
é€šå¸¸LM (Discrete):
  h_t â†’ LM Head â†’ token â†’ Embedding â†’ x_{t+1}
        â†‘                    â†‘
        é›¢æ•£åŒ–              å†åŸ‹ã‚è¾¼ã¿
        (æƒ…å ±æå¤±)
```

ã“ã®é›¢æ•£åŒ–ã‚¹ãƒ†ãƒƒãƒ—ã§æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ã®ã§ã¯ï¼Ÿã¨ã„ã†ä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹ã€‚

### Continuous LMã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

```
Continuous LM:
  h_t â†’ proj â†’ x_{t+1}   (é›¢æ•£åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€æƒ…å ±ä¿æŒ)
```

å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†æ™‚ã®æœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã‚’ã€ç›´æ¥æ¬¡ã®å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚

### ãƒ¢ãƒ¼ãƒ‰ä¸€è¦§

| ãƒ¢ãƒ¼ãƒ‰ | å…¥åŠ›æ–¹å¼ | extra_pass | use_h1 | èª¬æ˜ |
|--------|----------|------------|--------|------|
| discrete | tokenåŸ‹ã‚è¾¼ã¿ | - | - | é€šå¸¸ã®LMï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ |
| continuous | h_{t-1}ã‚’ç›´æ¥ä½¿ç”¨ | False | - | é›¢æ•£åŒ–ã‚¹ã‚­ãƒƒãƒ— |
| continuous_extra | h_{t-1}ã‚’ç›´æ¥ä½¿ç”¨ | True | False | 1å›è¿½åŠ å‡¦ç†ã€h2ã®ã¿ä½¿ç”¨ |
| continuous_combined | h_{t-1}ã‚’ç›´æ¥ä½¿ç”¨ | True | True | 1å›è¿½åŠ å‡¦ç†ã€h1+h2ã‚’ä½¿ç”¨ |

### å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
discrete (é€šå¸¸LM):
  token_A â†’ embed â†’ layers â†’ h1 â†’ LM Head â†’ "B"äºˆæ¸¬

continuous:
  h_{t-1} â†’ proj â†’ layers â†’ h1 â†’ LM Head â†’ "B"äºˆæ¸¬

continuous_extra (extra_pass=True, use_h1=False):
  h_{t-1} â†’ proj â†’ layers â†’ h1 â†’ proj â†’ layers â†’ h2 â†’ LM Head â†’ "B"äºˆæ¸¬

continuous_combined (extra_pass=True, use_h1=True):
  h_{t-1} â†’ proj â†’ layers â†’ h1 â†’ proj â†’ layers â†’ h2 â†’ combine(h1,h2) â†’ LM Head â†’ "B"äºˆæ¸¬
```

### ä½¿ç”¨æ–¹æ³•

```python
from src.models import create_model

# ãƒ¢ãƒ‡ãƒ«ä½œæˆ
model = create_model("continuous")

# Discreteï¼ˆé€šå¸¸LMã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
loss, stats = model.compute_loss(input_ids, labels, mode="discrete")

# Continuousï¼ˆé›¢æ•£åŒ–ã‚¹ã‚­ãƒƒãƒ—ï¼‰
loss, stats = model.compute_loss(input_ids, labels, mode="continuous")

# Continuous + è¿½åŠ å‡¦ç†ï¼ˆh2ã®ã¿ï¼‰
loss, stats = model.compute_loss(input_ids, labels, mode="continuous", extra_pass=True)

# Continuous + è¿½åŠ å‡¦ç†ï¼ˆh1+h2ï¼‰
loss, stats = model.compute_loss(input_ids, labels, mode="continuous", extra_pass=True, use_h1=True)
```

### å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# å…¨ãƒ¢ãƒ¼ãƒ‰æ¯”è¼ƒ
python3 scripts/experiment_continuous.py --models discrete continuous continuous_extra continuous_combined

# NoPEï¼ˆPosition Encodingãªã—ï¼‰ã§å®Ÿé¨“
python3 scripts/experiment_continuous.py --nope
```

---

## ğŸ”§ 2-Pass Processing ã«ã‚ˆã‚‹ç™ºè¦‹ï¼ˆå®Ÿé¨“è¨˜éŒ²ï¼‰

**æ„å›³ã›ãšç™ºè¦‹: 2å›å‡¦ç†ãŒReversal Curseã‚’æ”¹å–„**

### æ¦‚è¦

SelectiveOutputLMå®Ÿè£…ä¸­ã«ã€Transformerã‚’2å›é€šã™æ–¹å¼ãŒReversal Curseã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚

```
1å›å‡¦ç†: token â†’ embed â†’ layers â†’ h1 â†’ output
2å›å‡¦ç†: token â†’ embed â†’ layers â†’ h1 â†’ proj â†’ layers â†’ h2 â†’ output
```

### çµæœ

| Model | Val PPL | Forward PPL | Gap |
|-------|---------|-------------|-----|
| 1å›å‡¦ç† | **484.6** | 12868.4 | -1799.1 |
| 2å›å‡¦ç† | 516.8 | **9576.1** | **+1114.1** |

- Val PPLã¯æ‚ªåŒ–ã™ã‚‹ãŒã€Reversal Curseã®GapãŒå¤§å¹…æ”¹å–„
- ãªãœ2å›å‡¦ç†ã§è¨˜æ†¶ãŒå®šç€ã™ã‚‹ã®ã‹ã¯æœªè§£æ˜

è©³ç´°ã¯ `docs/experiments/2025-12-07_two_pass_discovery.md` ã‚’å‚ç…§ã€‚

**æ³¨æ„**: ã“ã®ç™ºè¦‹ã¯ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§ã®ãŸã‚ã‚³ãƒ¼ãƒ‰å‰Šé™¤æ¸ˆã¿ã€‚è¨˜éŒ²ã®ã¿æ®‹ã™ã€‚

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-07 | **LCTæ–¹å¼è¿½åŠ **: Lagged Cache Training - å†å¸°çš„ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—è¨“ç·´å¯èƒ½ã«ã™ã‚‹æ‰‹æ³•ã€‚22å€é«˜é€ŸåŒ– |
| 2025-12-07 | **Continuous LMå®Ÿè£…**: é›¢æ•£åŒ–ã‚¹ã‚­ãƒƒãƒ—ä»®èª¬ã®æ¤œè¨¼ã€‚extra_pass/use_h1ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¿½åŠ  |
| 2025-12-07 | **2-Passç™ºè¦‹ã‚’è¨˜éŒ²**: Transformerã‚’2å›é€šã™ã¨Reversal CurseãŒæ”¹å–„ï¼ˆã‚³ãƒ¼ãƒ‰ã¯å‰Šé™¤ã€è¨˜éŒ²ã®ã¿ï¼‰ |
| 2025-12-07 | **è¨“ç·´-è©•ä¾¡ä¸€è²«æ€§ãƒãƒªã‚·ãƒ¼è¿½åŠ **: è¨“ç·´æ™‚ã¨è©•ä¾¡æ™‚ã®æ¡ä»¶ã‚’æƒãˆã‚‹ã“ã¨ã‚’å¿…é ˆåŒ– |
| 2025-12-06 | **SelectiveOutputLMè¿½åŠ **: å­¦ç¿’å¯èƒ½ã‚²ãƒ¼ãƒˆã«ã‚ˆã‚‹é¸æŠçš„å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆå¾Œã«å¤±æ•—ã¨åˆ¤æ˜ï¼‰ |
| 2025-12-06 | **ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ç§»è¡Œ**: TransformerLM + 4ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—ã€ã‚³ãƒ¼ãƒ‰31%å‰Šæ¸› |
| 2025-12-06 | **Layerç½®ãæ›ãˆæ–¹å¼ã‚’å‰Šé™¤**: è’¸ç•™+Fine-tuneç­‰ã™ã¹ã¦å¤±æ•—ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ã«é›†ä¸­ |
| 2025-12-06 | **ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ãƒ¡ãƒ¢ãƒªå°å…¥**: memory_head_dim=512ã§Linear Attentionã®è¡¨ç¾åŠ›ã‚’æœ€å¤§åŒ– |
| 2025-12-06 | **PPLè©•ä¾¡æ–¹æ³•ã®æ•™è¨“è¿½åŠ **: Sliding windowæ–¹å¼ãŒæ­£ã—ã„ |
| 2025-12-06 | **ãƒ¡ãƒ¢ãƒªè»¢é€APIè¿½åŠ **: get_memory_state/set_memory_stateã§åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’åˆ¥PCã«è»¢é€å¯èƒ½ |
| 2025-12-06 | **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªè¿½åŠ **: create_model()ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ä½œæˆ |
| 2025-12-06 | **Hierarchical Memoryè¿½åŠ **: å­¦ç¿’å¯èƒ½ãªå±•é–‹åˆ¤æ–­ã€Coarse-to-Fineæ¤œç´¢ |
| 2025-12-06 | **Multi-Memory Attentionè¿½åŠ **: Attention-basedé¸æŠã§è¤‡æ•°ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„æ··åˆ |
| 2025-12-05 | **Infini-Pythiaå®Ÿè£…**: 1å±¤ç›®Infini + RoPE |

---

Last Updated: 2025-12-07
