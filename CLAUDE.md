# New-LLM Project Guidelines

---

## ğŸ¯ Infini-Pythia Architecture (Memory-Only)

**Pythia-70Mãƒ™ãƒ¼ã‚¹ã«1å±¤ç›®Infini-Attentionï¼ˆMemory-Onlyï¼‰ã‚’å°å…¥ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Infini-Pythia:
Token Embedding (512-dim)
       â†“
Layer 0: InfiniAttentionLayer (NoPE, Memory Only)
  â””â”€ Memory Attention (linear attention)
       â†“
Layer 1-5: PythiaLayer (RoPE)
  â”œâ”€ Multi-Head Attention
  â””â”€ MLP
       â†“
Output Head (512 â†’ vocab)
```

### Infini-Attention (Memory-Only)

```
ãƒ¡ãƒ¢ãƒªæ›´æ–° (Delta Rule):
  M_s = M_{s-1} + Ïƒ(K)^T @ (V - retrieved_V)

ãƒ¡ãƒ¢ãƒªå–å¾—:
  A_mem = Ïƒ(Q) @ M / (Ïƒ(Q) @ z)

Ïƒ(x) = ELU(x) + 1
```

### Multi-Memory Bank

```python
# è¤‡æ•°ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ã§æƒ…å ±æ··åˆã‚’ä½æ¸›
model = InfiniPythiaModel(
    num_memory_banks=2,      # 2ã¤ã®ãƒãƒ³ã‚¯
    segments_per_bank=4,     # 4ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§æ¬¡ãƒãƒ³ã‚¯ã«åˆ‡æ›¿
)
```

### ALiBi (Attention with Linear Biases)

ç·šå½¢åŒ–è¿‘ä¼¼ã§ALiBiã‚’åœ§ç¸®ãƒ¡ãƒ¢ãƒªã«çµ„ã¿è¾¼ã‚€:

```
ãƒ¡ãƒ¢ãƒªæ›´æ–° (ALiBié‡ã¿ä»˜ã):
  M_Ï† = Î£_i w_i * Ï†(K_i) * V_i^T
  z_Ï† = Î£_i w_i * Ï†(K_i)

  w_i = exp(-slope * segment_distance)  # é ã„ã»ã©å°ã•ã„é‡ã¿

ãƒ¡ãƒ¢ãƒªå–å¾—:
  Output = Ï†(Q) @ M_Ï† / (Ï†(Q) @ z_Ï†)
```

```python
# ALiBiä»˜ããƒ¢ãƒ‡ãƒ«
model = InfiniPythiaModel(
    use_alibi=True,      # ALiBiæœ‰åŠ¹åŒ–
    alibi_scale=1.0,     # ã‚¹ãƒ­ãƒ¼ãƒ—ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆå¤§ãã„ã»ã©æ¸›è¡°ãŒå¼·ã„ï¼‰
)
```

---

## ğŸ­ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒª

`create_model()`ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ä½œæˆã€‚

```python
from src.models import create_model

# åŸºæœ¬çš„ãªä½¿ã„æ–¹
model = create_model("pythia")       # æ¨™æº–Pythia
model = create_model("infini")       # Infini-Pythia
model = create_model("multi_memory") # Multi-Memory
model = create_model("hierarchical") # Hierarchical

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ã
model = create_model("infini", use_alibi=True, alibi_scale=1.0)
model = create_model("multi_memory", num_memories=8)
model = create_model("hierarchical", num_memories=4, use_delta_rule=False)

# ã‚«ã‚¹ã‚¿ãƒ config
from config.pythia import PythiaConfig
config = PythiaConfig()
model = create_model("infini", config, use_alibi=True)
```

### åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | å¯¾è±¡ãƒ¢ãƒ‡ãƒ« | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------------|------------|------------|------|
| `use_delta_rule` | å…¨memoryç³» | `True` | Delta Ruleä½¿ç”¨ |
| `num_memories` | multi_memory, hierarchical | `4` | ãƒ¡ãƒ¢ãƒªæ•° |
| `num_memory_banks` | infini | `1` | ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯æ•° |
| `segments_per_bank` | infini | `4` | ãƒãƒ³ã‚¯ã‚ãŸã‚Šã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•° |
| `use_alibi` | infini | `False` | ALiBiæœ‰åŠ¹åŒ– |
| `alibi_scale` | infini | `1.0` | ALiBiã‚¹ãƒ­ãƒ¼ãƒ—ã‚¹ã‚±ãƒ¼ãƒ« |

---

## ğŸ’¾ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ä¿å­˜ãƒ»è»¢é€

åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’åˆ¥PCã«è»¢é€å¯èƒ½ã€‚

```python
import torch
from src.models import create_model

# ===== PC A =====
model = create_model("infini")
model.reset_memory()

# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªã‚’è“„ç©
for batch in data_loader:
    _ = model(batch, update_memory=True)

# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’ä¿å­˜
state = model.get_memory_state()
torch.save(state, "memory.pt")

# ===== PC B =====
# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
state = torch.load("memory.pt")
model = create_model("infini")
model.set_memory_state(state)

# ãƒ¡ãƒ¢ãƒªãŒå¼•ãç¶™ãŒã‚ŒãŸçŠ¶æ…‹ã§æ¨è«–
output = model(input_ids)
```

### ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ã‚­ãƒ¼

| ãƒ¢ãƒ‡ãƒ« | ã‚­ãƒ¼ |
|--------|------|
| `InfiniPythiaModel` | `memories`, `memory_norms`, `current_bank`, `segment_counter` |
| `MultiMemoryInfiniPythiaModel` | `memories`, `memory_norms`, `current_memory_idx` |
| `HierarchicalMemoryPythiaModel` | `fine_memories`, `fine_memory_norms`, `current_memory_idx` |

### ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º |
|--------|--------|
| Infini (1 bank) | ~135 KB |
| Multi-Memory (4) | ~540 KB |
| Hierarchical (4) | ~540 KB |

---

## ğŸ§ª çµ±ä¸€å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å…¨ãƒ¢ãƒ‡ãƒ«ã‚’çµ±ä¸€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿé¨“å¯èƒ½ã€‚

```bash
# å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
python3 scripts/experiment.py --models pythia infini multi_memory hierarchical

# Infiniã®ã¿
python3 scripts/experiment.py --models infini

# Multi-Memoryã¨Hierarchicalæ¯”è¼ƒ
python3 scripts/experiment.py --models multi_memory hierarchical --num-memories 4

# ALiBiä»˜ãInfini
python3 scripts/experiment.py --models infini --alibi

# è¨­å®šã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
python3 scripts/experiment.py --models infini --samples 10000 --epochs 50 --lr 5e-5

# 8ãƒ¡ãƒ¢ãƒªã§å®Ÿé¨“
python3 scripts/experiment.py --models hierarchical --num-memories 8
```

### ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—

| ã‚¿ã‚¤ãƒ— | èª¬æ˜ |
|--------|------|
| `pythia` | æ¨™æº–Pythia (RoPE) |
| `infini` | Infini-Pythia (1å±¤ç›®Infini + RoPE) |
| `multi_memory` | Multi-Memory (è¤‡æ•°ç‹¬ç«‹ãƒ¡ãƒ¢ãƒª) |
| `hierarchical` | Hierarchical (éšå±¤çš„ãƒ¡ãƒ¢ãƒª) |

### ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®ä½¿ç”¨

```python
from src.utils.experiment_runner import (
    ExperimentConfig,
    ModelType,
    run_experiment,
)

# è¨­å®š
config = ExperimentConfig(
    num_samples=5000,
    seq_length=256,
    num_epochs=30,
    num_memories=4,
)

# å®Ÿé¨“å®Ÿè¡Œ
results = run_experiment(
    model_types=[ModelType.INFINI, ModelType.HIERARCHICAL],
    exp_config=config,
)
```

---

## ğŸ“Š Reversal Curse è©•ä¾¡

### æ¦‚è¦

Reversal Curseã¯ã€ŒA is Bã€ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã€ŒB is Aã€ã‚‚æ¨è«–ã§ãã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹æŒ‡æ¨™ã€‚

### æŒ‡æ¨™

| æŒ‡æ¨™ | å®šç¾© | è§£é‡ˆ |
|------|------|------|
| Forward PPL | é †æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãŸã‚ä½ã„ |
| Backward PPL | é€†æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ãŸã‚é«˜ã„ |
| Reversal Ratio | Forward / Backward | 1.0ã«è¿‘ã„ã»ã©è‰¯ã„ |
| Reversal Gap | Backward - Forward | 0ã«è¿‘ã„ã»ã©è‰¯ã„ |

### å®Ÿè£…

```python
from src.utils.evaluation import evaluate_reversal_curse
from src.data.reversal_pairs import get_reversal_pairs

tokenizer = get_tokenizer(config.tokenizer_name)
reversal_pairs = get_reversal_pairs()
reversal_result = evaluate_reversal_curse(model, tokenizer, reversal_pairs, device)
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

### Reversal Curseè©•ä¾¡å¿…é ˆ

**ã™ã¹ã¦ã®å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã€Reversal Curseè©•ä¾¡ã‚’å¿…ãšå®Ÿè¡Œã™ã‚‹ã“ã¨ã€‚**

---

## âš ï¸ éå»ã®ãƒã‚°ã¨æ•™è¨“

### 1. Infini-Attention ãƒ¡ãƒ¢ãƒªå‹¾é…ãƒã‚°

```python
# âŒ ãƒã‚°: ãƒ¡ãƒ¢ãƒªæ›´æ–°ã§ã‚°ãƒ©ãƒ•ãŒæ®‹ã‚Šã€äºŒé‡backwardã‚¨ãƒ©ãƒ¼
self.memory = self.memory + memory_update

# âœ… ä¿®æ­£: detach()ã§ã‚°ãƒ©ãƒ•ã‚’åˆ‡æ–­
self.memory = (self.memory + memory_update).detach()
```

### 2. PPLç•°å¸¸å€¤ã®è¨ºæ–­åŸºæº–

| PPL | çŠ¶æ…‹ | å¯¾å‡¦ |
|-----|------|------|
| < 5 | **ç•°å¸¸** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯/å› æœãƒã‚¹ã‚¯ãƒã‚° | ã‚³ãƒ¼ãƒ‰ç‚¹æ¤œå¿…é ˆ |
| 5-30 | **ç–‘ã‚ã—ã„** - éå­¦ç¿’ã®å¯èƒ½æ€§ | ãƒ‡ãƒ¼ã‚¿é‡ãƒ»åˆ†å‰²ã‚’ç¢ºèª |
| 30-100 | æ­£å¸¸ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ | - |
| 100-500 | æ­£å¸¸ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼‰ | - |
| > 1000 | å­¦ç¿’ä¸è¶³ | epochå¢—åŠ /lrèª¿æ•´ |

### 3. Long Contextè©•ä¾¡ã§untrained modelã‚’ä½¿ç”¨

```python
# âŒ ãƒã‚°: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦è©•ä¾¡
pythia_model = PythiaModel(...)  # æœªè¨“ç·´ã®ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
result = evaluate_long_documents(pythia_model, ...)

# âœ… ä¿®æ­£: è¨“ç·´æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
pythia_model = PythiaModel(...)
pythia_model.load_state_dict(results["pythia"]["model_state_dict"])
result = evaluate_long_documents(pythia_model, ...)
```

### 4. PPLè©•ä¾¡æ–¹æ³•ã«ã‚ˆã‚‹PPLç•°å¸¸å€¤ï¼ˆé‡è¦ï¼‰

**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²è©•ä¾¡ã§ç•°å¸¸ã«é«˜ã„PPLï¼ˆ10,000+ï¼‰ãŒå‡ºã‚‹åŸå› ã¨å¯¾ç­–ã€‚**

#### å•é¡Œã®ç™ºè¦‹çµŒç·¯

Pythia-70mã®å…¬å¼WikiText-2 PPLã¯ç´„32-35ã ãŒã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²è©•ä¾¡ã§ã¯14,000+ã«ãªã£ãŸã€‚

#### åŸå› : ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã§ã®äºˆæ¸¬

```python
# âŒ å•é¡Œã®ã‚ã‚‹è©•ä¾¡æ–¹æ³•ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã€éé‡è¤‡ï¼‰
for start in range(0, seq_len, segment_length):
    segment = tokens[start:end]
    input_ids = segment[:-1].unsqueeze(0)  # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒç‹¬ç«‹
    labels = segment[1:].unsqueeze(0)
    outputs = model(input_ids, labels=labels)
    # â†’ å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹éš›ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„
    # â†’ ã€Œæ–‡æ›¸ã®é€”ä¸­ã€ã‚’ã€Œæ–‡æ›¸ã®å…ˆé ­ã€ã¨ã—ã¦æ‰±ã†ãŸã‚é«˜PPL
```

#### æ­£ã—ã„è©•ä¾¡æ–¹æ³•: Sliding Window

```python
# âœ… æ­£ã—ã„è©•ä¾¡æ–¹æ³•ï¼ˆSliding Windowï¼‰
stride = 512
context_length = 2048

for start in range(0, seq_len - 1, stride):
    end = min(start + context_length, seq_len)
    input_ids = tokens[start:end].unsqueeze(0)

    # æœ€åˆã®strideå€‹ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆlossè¨ˆç®—ã—ãªã„ï¼‰
    labels = input_ids.clone()
    labels[0, :stride] = -100  # -100ã¯lossè¨ˆç®—ã‹ã‚‰é™¤å¤–

    outputs = model(input_ids, labels=labels)
```

#### PPLæ¯”è¼ƒï¼ˆWikiText-2ã§ã®å®Ÿæ¸¬å€¤ï¼‰

| è©•ä¾¡æ–¹æ³• | PPL | è§£é‡ˆ |
|----------|-----|------|
| Sliding window (stride=512) | **40.96** | âœ“ æ­£å¸¸ï¼ˆå…¬å¼å€¤ã«è¿‘ã„ï¼‰ |
| Simple non-overlapping (2048) | 15,885 | âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—å•é¡Œ |
| Segment-based (256 tokens) | 14,204 | âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—å•é¡Œ |

#### æ•™è¨“

1. **PPLè©•ä¾¡ã¯å¿…ãšSliding windowæ–¹å¼ã‚’ä½¿ç”¨**
2. **ç•°å¸¸ã«é«˜ã„PPLï¼ˆ1000+ï¼‰ãŒå‡ºãŸã‚‰è©•ä¾¡æ–¹æ³•ã‚’ç–‘ã†**
3. **è¨“ç·´æ™‚ã¨è©•ä¾¡æ™‚ã§ç•°ãªã‚‹æ–¹æ³•ã‚’ä½¿ã†ã¨æ¯”è¼ƒãŒä¸æ­£ç¢ºã«ãªã‚‹**
4. **Pythia-70mã®WikiText-2 PPLã¯ç´„32-35ãŒæ­£å¸¸**

---

## ğŸ”§ Pretrained LLMã¸ã®Infini-Attentionå°å…¥

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¯”è¼ƒ

| æ–¹å¼ | èª¬æ˜ | çµæœ |
|------|------|------|
| **Case C: Layer 0ç½®ãæ›ãˆ** | Layer 0ã‚’Infini Layerã«å®Œå…¨ç½®ãæ›ãˆ | âŒ RoPEæå¤±ã€å¾Œç¶šãƒ¬ã‚¤ãƒ¤ãƒ¼ä¸é©åˆ |
| **Parallel Adapterï¼ˆæ¨å¥¨ï¼‰** | Layer 0ã«ä¸¦åˆ—ã§Infini Adapterã‚’è¿½åŠ  | âœ“ æ—¢å­˜æ€§èƒ½ç¶­æŒã—ãªãŒã‚‰ãƒ¡ãƒ¢ãƒªè¿½åŠ  |

### Parallel Adapter ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Input Embedding
      â†“
â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
â”‚  Original â”‚  Infini Adapter
â”‚  Layer 0  â”‚  (Memory)
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”‚
      â†“            â†“
    Output + Î± Ã— Infini_Output
      â†“
Layer 1-5 (unchanged)
```

### ä½¿ç”¨æ–¹æ³•

```python
from src.models.infini_adapter import create_pythia_with_parallel_infini

# ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆFull Fine-tune å¿…é ˆï¼‰
model = create_pythia_with_parallel_infini(
    model_name="EleutherAI/pythia-70m",
    use_delta_rule=True,
    use_alibi=False,
    initial_alpha=0.0,  # 0ã‹ã‚‰å­¦ç¿’é–‹å§‹
    freeze_base_model=False,  # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è¨“ç·´ï¼ˆå¿…é ˆï¼‰
)

# è¨“ç·´å¾Œ
print(f"Learned alpha: {model.get_alpha()}")  # å­¦ç¿’ã•ã‚ŒãŸalphaå€¤

# ãƒ¡ãƒ¢ãƒªæ“ä½œ
model.reset_memory()
state = model.get_memory_state()
model.set_memory_state(state)
```

### è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# WikiText-2 Full Fine-tuningï¼ˆæ¨å¥¨ï¼‰
python3 scripts/train_parallel_adapter_wikitext.py --method sliding --epochs 30

# WikiText-2ã§ã®è©•ä¾¡
python3 scripts/evaluate_wikitext.py --parallel-adapter parallel_adapter_wikitext_sliding_full.pt
```

---

## ğŸ“ 2æ®µéšè¨“ç·´: è’¸ç•™ + Fine-tuning

**alphaãŒå°ã•ããªã‚‹å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€‚**

### å•é¡Œ: Full Fine-tuningã§alphaãŒå°ã•ã„

```
Parallel Adapter (Full Fine-tune):
  Epoch 1: alpha=0.0002  # ã»ã¼ã‚¼ãƒ­
  â†’ ãƒ¢ãƒ‡ãƒ«ãŒInfiniå‡ºåŠ›ã‚’ã»ã¨ã‚“ã©ä½¿ã‚ãªã„
```

Pretrained Pythiaã¯æ—¢ã«ã€Œå‹•ãã€ãŸã‚ã€Infiniã‚’ä½¿ã‚ãªã„æ–¹å‘ã«æœ€é©åŒ–ã•ã‚Œã‚‹ã€‚

### è§£æ±ºç­–: 2æ®µéšè¨“ç·´

```
Stage 1: Knowledge Distillation
  - Infini LayerãŒã‚ªãƒªã‚¸ãƒŠãƒ«Layer 0ã®å‡ºåŠ›ã‚’æ¨¡å€£
  - MSE Lossã§è¨“ç·´
  - å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨ã®æ•´åˆæ€§ã‚’ç¢ºä¿

Stage 2: Full Fine-tuning with Layer-wise LR
  - å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¨“ç·´
  - Layer 0ï¼ˆInfiniï¼‰ã«é«˜ã„å­¦ç¿’ç‡ã‚’è¨­å®š
  - LM lossã§æœ€é©åŒ–
```

### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºæœ¬çš„ãªä½¿ã„æ–¹
python3 scripts/train_infini_distill_finetune.py --distill-epochs 10 --finetune-epochs 20

# Layer 0ã®å­¦ç¿’ç‡ã‚’2å€ã«
python3 scripts/train_infini_distill_finetune.py --layer0-lr-scale 2.0

# ä»–ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å­¦ç¿’ç‡ã‚’0.5å€ã«ï¼ˆLayer 0å„ªå…ˆï¼‰
python3 scripts/train_infini_distill_finetune.py --layer0-lr-scale 2.0 --other-lr-scale 0.5

# è’¸ç•™ã®ã¿
python3 scripts/train_infini_distill_finetune.py --distill-epochs 10 --finetune-epochs 0

# ALiBiä»˜ã
python3 scripts/train_infini_distill_finetune.py --alibi
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------------|------------|------|
| `--distill-epochs` | 10 | è’¸ç•™ã‚¨ãƒãƒƒã‚¯æ•° |
| `--finetune-epochs` | 20 | Fine-tuningã‚¨ãƒãƒƒã‚¯æ•° |
| `--distill-lr` | 1e-4 | è’¸ç•™å­¦ç¿’ç‡ |
| `--finetune-lr` | 1e-5 | Fine-tuningåŸºæœ¬å­¦ç¿’ç‡ |
| `--layer0-lr-scale` | 1.0 | Layer 0ã®å­¦ç¿’ç‡å€ç‡ |
| `--other-lr-scale` | 1.0 | ä»–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å­¦ç¿’ç‡å€ç‡ |

### ä»•çµ„ã¿

```
Stage 1 (Distillation):
  ã‚ªãƒªã‚¸ãƒŠãƒ«Layer 0 â†’ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå‡ºåŠ›
  Infini Layer     â†’ äºˆæ¸¬å‡ºåŠ›
  Loss = MSE(äºˆæ¸¬, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ)

Stage 2 (Fine-tuning):
  ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—:
    - layer0_infini:  lr = base_lr Ã— layer0_lr_scale
    - embeddings:     lr = base_lr Ã— other_lr_scale
    - layers_1_to_n:  lr = base_lr Ã— other_lr_scale
    - final:          lr = base_lr Ã— other_lr_scale
```

---

### é‡è¦ãªæ•™è¨“: éƒ¨åˆ†çš„å¾®èª¿æ•´ã¯æ©Ÿèƒ½ã—ãªã„

**Adapter ã®ã¿ã®è¨“ç·´ï¼ˆéƒ¨åˆ†çš„å¾®èª¿æ•´ï¼‰ã¯æ©Ÿèƒ½ã—ãªã„ã€‚å¿…ãšå…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã® Full Fine-tune ãŒå¿…è¦ã€‚**

#### å•é¡Œ

```python
# âŒ éƒ¨åˆ†çš„å¾®èª¿æ•´ï¼ˆAdapter ã®ã¿è¨“ç·´ï¼‰
model = create_pythia_with_parallel_infini(
    freeze_base_model=True,  # ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å‡çµ
)
# â†’ å¾Œç¶šãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒæ–°ã—ã„å‡ºåŠ›åˆ†å¸ƒã«é©å¿œã§ããšã€PPL ãŒå¤§å¹…ã«åŠ£åŒ–
# â†’ å®Ÿé¨“çµæœ: Baseline 40.96 â†’ Adapter only 391.74ï¼ˆ350+ åŠ£åŒ–ï¼‰
```

#### åŸå› 

```
Adapter ã®ã¿è¨“ç·´:
Layer 0 å‡ºåŠ› + Î± Ã— Infiniå‡ºåŠ› â†’ Layer 1-5ï¼ˆå›ºå®šã€é©å¿œä¸å¯ï¼‰
                                    â†“
                              å‡ºåŠ›åˆ†å¸ƒã®ä¸æ•´åˆ â†’ é«˜PPL
```

Pretrained Pythia ã® Layer 1-5 ã¯ã€å…ƒã® Layer 0 å‡ºåŠ›åˆ†å¸ƒã‚’å‰æã«è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã€‚
Infini Adapter ãŒå‡ºåŠ›ã‚’å¤‰æ›´ã™ã‚‹ã¨ã€å¾Œç¶šãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå¯¾å¿œã§ããªã„ã€‚

#### æ­£ã—ã„æ–¹æ³•

```python
# âœ… Full Fine-tuneï¼ˆå…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è¨“ç·´ï¼‰
model = create_pythia_with_parallel_infini(
    freeze_base_model=False,  # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼è¨“ç·´å¯èƒ½
)
# â†’ Layer 1-5 ãŒæ–°ã—ã„å‡ºåŠ›åˆ†å¸ƒã«é©å¿œå¯èƒ½
```

#### æ•™è¨“ã¾ã¨ã‚

1. **Pretrained LLM ã¸ã® Adapter è¿½åŠ ã¯ Full Fine-tune ãŒå¿…é ˆ**
2. **éƒ¨åˆ†çš„å¾®èª¿æ•´ã§ã¯å¾Œç¶šãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é©å¿œãŒä¸å¯èƒ½**
3. **ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ã¨ Pretrained é©å¿œã¯æ ¹æœ¬çš„ã«ç•°ãªã‚‹**

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pythia.py                   # PythiaConfig
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiment.py               # çµ±ä¸€å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ reversal_pairs.py       # Reversal Curseè©•ä¾¡ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py             # create_model() ãƒ•ã‚¡ã‚¯ãƒˆãƒª
â”‚   â”‚   â”œâ”€â”€ pythia.py               # PythiaModel (RoPE)
â”‚   â”‚   â”œâ”€â”€ infini_attention.py     # InfiniAttention, InfiniAttentionLayer
â”‚   â”‚   â”œâ”€â”€ infini_pythia.py        # InfiniPythiaModel (1å±¤Infini + RoPE)
â”‚   â”‚   â”œâ”€â”€ multi_memory_attention.py  # MultiMemoryInfiniAttention
â”‚   â”‚   â”œâ”€â”€ multi_memory_pythia.py  # MultiMemoryInfiniPythiaModel
â”‚   â”‚   â”œâ”€â”€ hierarchical_memory.py  # HierarchicalMemoryAttention
â”‚   â”‚   â””â”€â”€ hierarchical_pythia.py  # HierarchicalMemoryPythiaModel
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ experiment_runner.py    # çµ±ä¸€å®Ÿé¨“ãƒ©ãƒ³ãƒŠãƒ¼
â”‚       â”œâ”€â”€ training.py             # å…±é€šå­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚       â”œâ”€â”€ evaluation.py           # è©•ä¾¡é–¢æ•°
â”‚       â”œâ”€â”€ device.py               # ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”‚       â”œâ”€â”€ data_pythia.py          # Pileãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
â”‚       â””â”€â”€ seed.py                 # ã‚·ãƒ¼ãƒ‰è¨­å®š
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ experiments/                # å®Ÿé¨“çµæœ
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-06 | **éƒ¨åˆ†çš„å¾®èª¿æ•´ã¯æ©Ÿèƒ½ã—ãªã„**: Adapter ã®ã¿è¨“ç·´ã¯ NGã€Full Fine-tune å¿…é ˆ |
| 2025-12-06 | **PPLè©•ä¾¡æ–¹æ³•ã®æ•™è¨“è¿½åŠ **: Sliding windowæ–¹å¼ãŒæ­£ã—ã„ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã¯é«˜PPLã«ãªã‚‹ |
| 2025-12-06 | **Parallel Adapterå®Ÿè£…**: Pretrained LLMã«Infini-Attentionã‚’ä¸¦åˆ—æŒ¿å…¥ã™ã‚‹æ–¹å¼ |
| 2025-12-06 | **WikiText-2è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¿½åŠ **: æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®æ­£ç¢ºãªPPLè©•ä¾¡ |
| 2025-12-06 | **ãƒ¡ãƒ¢ãƒªè»¢é€APIè¿½åŠ **: get_memory_state/set_memory_stateã§åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’åˆ¥PCã«è»¢é€å¯èƒ½ |
| 2025-12-06 | **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªè¿½åŠ **: create_model()ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ä½œæˆ |
| 2025-12-06 | **å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ±ä¸€**: experiment.pyã«çµ±åˆã€experiment_runner.pyè¿½åŠ  |
| 2025-12-06 | **Hierarchical Memoryè¿½åŠ **: å­¦ç¿’å¯èƒ½ãªå±•é–‹åˆ¤æ–­ã€Coarse-to-Fineæ¤œç´¢ |
| 2025-12-06 | **Multi-Memory Attentionè¿½åŠ **: Attention-basedé¸æŠã§è¤‡æ•°ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„æ··åˆ |
| 2025-12-06 | **ALiBiä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¿½åŠ **: ç·šå½¢åŒ–è¿‘ä¼¼ã§ALiBiã‚’ãƒ¡ãƒ¢ãƒªã«çµ„ã¿è¾¼ã¿ |
| 2025-12-05 | **Memory-Onlyã«é›†ä¸­**: Local Attentionå‰Šé™¤ã€ã‚³ãƒ¼ãƒ‰ç°¡ç´ åŒ– |
| 2025-12-05 | **Multi-Memory Bankè¿½åŠ **: è¤‡æ•°ãƒãƒ³ã‚¯ã§æƒ…å ±æ··åˆä½æ¸› |
| 2025-12-05 | **Long Contextè©•ä¾¡ãƒã‚°ä¿®æ­£**: è¨“ç·´æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ |
| 2025-12-05 | **Infini-Pythiaå®Ÿè£…**: 1å±¤ç›®Infini + RoPE |

---

Last Updated: 2025-12-06
