# New-LLM Project Guidelines

## ğŸ¯ Project Goal: Context-Pythia (2025-12-03)

**Pythia-70Mã®å…¨Layerã‚’Context-based Attentionã«ç½®ãæ›ãˆã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã‚’50%å‰Šæ¸›ã™ã‚‹ã€‚**

### Target Architecture

```
Context-Pythia:
  Token Embedding (512-dim)
       â†“
  ContextBlock: 512 â†’ 256 (åœ§ç¸®)
       â†“
  Layer 0-5: å…¨ã¦ context (256-dim) ã‚’å…¥åŠ›
       â†“
  Output Head (vocab_size)
```

### Key Decisions

| é …ç›® | æ±ºå®š |
|------|------|
| **ç½®ãæ›ãˆå±¤** | å…¨6Layer |
| **Contextæ¬¡å…ƒ** | 256ï¼ˆ50%åœ§ç¸®ï¼‰ |
| **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿** | Pileï¼ˆPythiaã¨åŒã˜ï¼‰ã€é–‹ç™ºæ™‚ã¯é™å®šã‚µãƒ³ãƒ—ãƒ« |
| **å­¦ç¿’æ–¹æ³•** | Phase 1ï¼ˆOACDï¼‰â†’ Phase 2ï¼ˆå…¨ä½“å­¦ç¿’ï¼‰ |
| **è©•ä¾¡æŒ‡æ¨™** | PPL + LAMBADA |
| **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç›®æ¨™** | 50% |

---

## ğŸ† Baseline: Pythia Scaling Suite

**æˆ‘ã€…ã®ãƒ©ã‚¤ãƒãƒ«ã€‚Pythiaãƒ¢ãƒ‡ãƒ«ã‚¹ã‚¤ãƒ¼ãƒˆã®æ€§èƒ½ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç›®æ¨™ã€‚**

### Pythia Model Suite

| Model | Params | Layers | Hidden | Heads | Training Data |
|-------|--------|--------|--------|-------|---------------|
| Pythia-70M | 70M | 6 | 512 | 8 | Pile (~300B tokens) |
| Pythia-160M | 160M | 12 | 768 | 12 | Pile (~300B tokens) |
| Pythia-410M | 410M | 24 | 1024 | 16 | Pile (~300B tokens) |
| Pythia-1B | 1B | 16 | 2048 | 8 | Pile (~300B tokens) |
| Pythia-1.4B | 1.4B | 24 | 2048 | 16 | Pile (~300B tokens) |

### Pythia-70M Specifications

- **Architecture**: GPT-NeoX (Transformer decoder)
- **Layers**: 6
- **Hidden Size**: 512
- **Attention Heads**: 8
- **Intermediate Size**: 2048
- **Position Encoding**: Rotary (RoPE, 25%)
- **Vocab Size**: 50,304
- **Training**: ~300B tokens on the Pile
- **Parallel Attention**: Yes (attention + MLP in parallel)

### Evaluation Benchmarks (from Pythia paper)

- **LAMBADA**: é•·è·é›¢ä¾å­˜æ€§ï¼ˆæœ€çµ‚å˜èªäºˆæ¸¬ï¼‰
- **WikiText**: Perplexity
- **HellaSwag**: å¸¸è­˜æ¨è«–
- **PIQA**: ç‰©ç†çš„ç›´æ„Ÿ
- **ARC**: æ¨è«–

å‚è€ƒ: [Pythia Paper](https://arxiv.org/abs/2304.01373), [GitHub](https://github.com/EleutherAI/pythia)

---

## ğŸ“ Context-Pythia Architecture

### æ–°æ–¹å¼: Contextæ¬¡å…ƒåœ§ç¸®

```
é€šå¸¸Pythia:
  KV Cache = hidden_size (512) Ã— seq_len Ã— num_layers (6)

Context-Pythia:
  KV Cache = context_dim (256) Ã— seq_len Ã— num_layers (6)

å‰Šæ¸›ç‡ = 1 - (256/512) = 50%
```

### Components

**1. ContextBlock**
- å…¥åŠ›: prev_context (256) + token_embed (512)
- å‡ºåŠ›: context (256)
- Phase 1ã§OACDå­¦ç¿’ã€Phase 2ã§freeze

**2. ContextPythiaLayer**
- å…¥åŠ›: context (256-dim)
- query_key_value: Linear(256 â†’ 1536)
- å‡ºåŠ›: hidden_states (512-dim)
- 6 Layersã€å…¨ã¦åŒã˜æ§‹é€ 

**3. Output Head**
- Linear(512 â†’ vocab_size)

---

## ğŸ¯ OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (Phase 1)

**ContextBlockã®å¤šæ§˜æ€§å­¦ç¿’ã«ä½¿ç”¨ã€‚**

```python
def oacd_loss(contexts, centroid_weight=0.1):
    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–
    dispersion_loss = -||X - mean(X)|| / n

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = ||mean(X)||Â²

    return dispersion_loss + centroid_weight * centroid_loss
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒ

### Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports
```

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ï¼ˆé™å®šãƒ‡ãƒ¼ã‚¿ï¼‰
python3 scripts/experiment_pythia_comparison.py --dev

# ãƒ•ãƒ«ãƒ¢ãƒ¼ãƒ‰
python3 scripts/experiment_pythia_comparison.py --samples 10000
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

```python
# âŒ ç¦æ­¢
parser.add_argument('--samples', type=int, default=200)

# âœ… æ¨å¥¨
config = PythiaConfig()
parser.add_argument('--samples', type=int, default=config.dev_num_samples)
```

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pythia.py              # PythiaConfig, ContextPythiaConfig
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiment_pythia_comparison.py  # æ¯”è¼ƒå®Ÿé¨“
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pythia.py          # PythiaModel (baseline)
â”‚   â”‚   â””â”€â”€ context_pythia.py  # ContextPythiaModel (ours)
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ diversity.py       # OACD algorithm
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## Evaluation Metrics

### Primary

| Metric | Purpose |
|--------|---------|
| **PPL (Perplexity)** | è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°å“è³ª |
| **LAMBADA Accuracy** | é•·è·é›¢ä¾å­˜æ€§ï¼ˆæœ€çµ‚å˜èªäºˆæ¸¬ï¼‰ |
| **KV Cache Memory** | å®Ÿéš›ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |

### Comparison Plan

```
Baseline: PythiaModel (our reproduction)
Ours:     ContextPythiaModel (50% KV reduction)

Evaluate on:
- WikiText-2 PPL
- LAMBADA accuracy
- torch.cuda.max_memory_allocated()
```

---

## Related Work

- **DeepSeek MLA**: Low-rank KV compression (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨)
- **æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: Context-based dimension reduction (å…¨Layer)

---

Last Updated: 2025-12-03 (å…¨Layerç½®ãæ›ãˆæ–¹å¼ã«ç§»è¡Œ)
