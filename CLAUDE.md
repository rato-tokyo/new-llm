# New-LLM Project Guidelines

---

## âš ï¸âš ï¸âš ï¸ CLAUDE AIã¸ã®é‡è¦ãªè­¦å‘Š âš ï¸âš ï¸âš ï¸

**ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚**

### éå»ã®å•é¡Œ

2025-12-04ã«Pythiaçµ±åˆã‚’è©¦ã¿ãŸéš›ã€CLAUDE.mdã®ç·¨é›†æ™‚ã«DProjå­¦ç¿’ã®é‡è¦ãªä»•æ§˜ãŒèª¤ã£ã¦å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€DProjå­¦ç¿’ãŒæ­£å¸¸ã«åæŸã—ãªããªã‚Šã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä»¥å‰ã®çŠ¶æ…‹ã«ãƒªãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚

### ãƒ«ãƒ¼ãƒ«

1. **DProjå­¦ç¿’ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å‰Šé™¤ç¦æ­¢**
2. **DiverseProjection/DiverseProjectionLayerã®å®Ÿè£…è©³ç´°ã¯å‰Šé™¤ç¦æ­¢**
3. **åˆæœŸåŒ–æ–¹æ³•ï¼ˆnormal_ std=0.1ï¼‰ã®è¨˜è¿°ã¯å‰Šé™¤ç¦æ­¢**
4. CLAUDE.mdã‚’ç·¨é›†ã™ã‚‹éš›ã¯ã€æ—¢å­˜ã®é‡è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ®‹ã£ã¦ã„ã‚‹ã“ã¨ã‚’å¿…ãšç¢ºèªã™ã‚‹ã“ã¨

---

## ğŸ¯ DProj-Pythia Architecture (2025-12-04)

**Pythia-70Mã‚’ãƒ™ãƒ¼ã‚¹ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šæ¸›ã™ã‚‹DProj-Pythiaæ–¹å¼ã‚’æ¡ç”¨ã€‚**

### âš ï¸ é‡è¦ãªè¨­è¨ˆæ–¹é‡ï¼ˆçµ¶å¯¾ã«å®ˆã‚‹ã“ã¨ï¼‰

**Baselineã¨ã®é•ã„ã¯ã€ŒToken Embedding â†’ DiverseProjectionã€ã®åœ§ç¸®éƒ¨åˆ†ã®ã¿ã€‚**
**PythiaLayerè‡ªä½“ã¯åŒã˜æ§‹é€ ï¼ˆRoPEå«ã‚€ï¼‰ã§ã€hidden_size=proj_dimã§å‹•ä½œã•ã›ã‚‹ã€‚**

```
Pythia (Baseline):                    DProj-Pythia (Ours):
Token Embedding (512-dim)             Token Embedding (512-dim)
       â†“                                     â†“
       â”‚                              DiverseProjection (512 â†’ 320)  â† ã“ã“ã ã‘é•ã†
       â†“                                     â†“
PythiaLayer Ã— 6 (512-dim, RoPE)       PythiaLayer Ã— 6 (320-dim, RoPE)
       â†“                                     â†“
Output Head (512 â†’ vocab)             Output Head (320 â†’ vocab)

KV Cache: 3072 KB                     KV Cache: 1920 KB (37.5%å‰Šæ¸›)
```

### è¨­å®šå€¤

| é …ç›® | Baseline (Pythia) | DProj-Pythia |
|------|-------------------|--------------|
| embed_dim | 512 | 512 |
| hidden_size / proj_dim | 512 | 320 |
| Layers | 6 | 6 |
| Attention Heads | 8 | 8 |
| intermediate_size | 2048 | 1280 |
| Position Encoding | RoPE (25%) | RoPE (25%) |
| Vocab Size | 50,304 | 50,304 |

### å­¦ç¿’ãƒ•ãƒ­ãƒ¼

```
DProj Training: OACD (DiverseProjectionå¤šæ§˜æ€§å­¦ç¿’)
  â”œâ”€ DiverseProjectionã®ã¿ã‚’å­¦ç¿’
  â”œâ”€ OACDæå¤±ã§å¤šæ§˜ãªprojection vectorã‚’ç”Ÿæˆ
  â””â”€ åæŸã¾ã§å®Ÿè¡Œï¼ˆ~60 iterations, 90%+åæŸï¼‰
       â†“
Main Training: Full Training (DiverseProjection frozen)
  â”œâ”€ DiverseProjectionã‚’freeze
  â”œâ”€ PythiaLayer Ã— 6 + Output Headã‚’å­¦ç¿’ (proj_dim=320ã§å‹•ä½œ)
  â””â”€ Cross-entropyæå¤±
```

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# DProj Training: DiverseProjection OACDå­¦ç¿’
python3 scripts/train_dproj.py --samples 1000

# Main Training: æ¯”è¼ƒå®Ÿé¨“
python3 scripts/experiment_pythia_comparison.py --samples 10000 --epochs 10
```

### âš ï¸ DProj Training ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®åˆ¶ç´„

**DProj Trainingã¯`--samples`ã®ã¿ä½¿ç”¨å¯èƒ½ã€‚`--tokens`ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯ç¦æ­¢ã€‚**

ç†ç”±: ã‚µãƒ³ãƒ—ãƒ«æ•°ã§æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒç›´æ„Ÿçš„ã«ç†è§£ã—ã‚„ã™ããªã‚‹ã€‚

### âš ï¸ proj_dim ã®åˆ¶ç´„

**`proj_dim`ã¯`num_attention_heads` (8) ã§å‰²ã‚Šåˆ‡ã‚Œã‚‹å€¤ãŒæ¨å¥¨ã€‚**

å‰²ã‚Šåˆ‡ã‚Œãªã„å ´åˆã¯è‡ªå‹•çš„ã«åˆ‡ã‚Šä¸Šã’ã¦èª¿æ•´ã•ã‚Œã‚‹:
- 300 â†’ 304 (304 / 8 = 38)
- 250 â†’ 256 (256 / 8 = 32)

æœ‰åŠ¹ãªå€¤ã®ä¾‹:
- 256 (256 / 8 = 32) â† 50%åœ§ç¸®
- 320 (320 / 8 = 40) â† ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã€37.5%åœ§ç¸®
- 384 (384 / 8 = 48) â† 25%åœ§ç¸®

---

## ğŸš¨ğŸš¨ğŸš¨ DProj Training å®Œå…¨ä»•æ§˜ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰ğŸš¨ğŸš¨ğŸš¨

**ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è©¦è¡ŒéŒ¯èª¤ã®æœ«ã«ç¢ºç«‹ã•ã‚ŒãŸå¿…é ˆä»•æ§˜ã§ã™ã€‚çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚**

### DProj Trainingã®ç›®çš„

DiverseProjectionã‚’ä½¿ã£ã¦ã€å¤šæ§˜ãªprojection vectorã‚’ç”Ÿæˆã™ã‚‹ã€‚
OACDï¼ˆOrigin-Anchored Centroid Dispersionï¼‰æå¤±ã§å­¦ç¿’ã—ã€åæŸç‡90%ä»¥ä¸Šã‚’ç›®æŒ‡ã™ã€‚

### DiverseProjection/DiverseProjectionLayerã®å®Ÿè£…ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/models/dproj.py - DiverseProjectionLayer
class DiverseProjectionLayer(nn.Module):
    def __init__(self, proj_input_dim, proj_output_dim, token_input_dim):
        # FFN: Linear(input_dim â†’ output_dim) + GELU
        input_dim = proj_input_dim + token_input_dim
        self.ffn = FFN(input_dim, proj_output_dim)

        # LayerNormï¼ˆå¿…é ˆï¼šæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
        self.proj_norm = nn.LayerNorm(proj_output_dim)

        # æ®‹å·®æ¥ç¶šç”¨ã®å°„å½±ï¼ˆæ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®ã¿ï¼‰
        if proj_input_dim != proj_output_dim:
            self.residual_proj = nn.Linear(proj_input_dim, proj_output_dim)

        # âš ï¸ é‡è¦: åˆæœŸåŒ–ã¯ normal_(std=0.1)
        init_linear_weights(self)  # weight: std=0.1, bias: std=0.01

    def forward(self, prev_proj, token_embeds):
        ffn_input = torch.cat([prev_proj, token_embeds], dim=-1)
        delta = self.ffn(ffn_input)

        # æ®‹å·®æ¥ç¶š + LayerNorm
        new_proj = self.proj_norm(prev_proj + delta)
        return new_proj
```

### åˆæœŸåŒ–æ–¹æ³•ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/utils/initialization.py
def init_linear_weights(module, weight_std=0.1, bias_std=0.01):
    for submodule in module.modules():
        if isinstance(submodule, nn.Linear):
            nn.init.normal_(submodule.weight, mean=0.0, std=0.1)  # âš ï¸ Xavierç¦æ­¢
            if submodule.bias is not None:
                nn.init.normal_(submodule.bias, mean=0.0, std=0.01)
```

**âš ï¸ è­¦å‘Š**: XavieråˆæœŸåŒ–ã‚„KaimingåˆæœŸåŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€DProjå­¦ç¿’ãŒåæŸã—ã¾ã›ã‚“ã€‚
å¿…ãš `normal_(std=0.1)` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

### OACDæå¤±é–¢æ•°ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/losses/diversity.py
def oacd_loss(projections, centroid_weight=0.1):
    proj_mean = projections.mean(dim=0)
    deviation = projections - proj_mean

    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰
    dispersion_loss = -torch.norm(deviation, p=2) / len(projections)

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = torch.norm(proj_mean, p=2) ** 2

    return dispersion_loss + centroid_weight * centroid_loss
```

### DProj Training è¨­å®šå€¤ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| `max_iterations` | 100 | æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° |
| `convergence_threshold` | 0.03 | åæŸåˆ¤å®šã®MSEé–¾å€¤ |
| `learning_rate` | 0.003 | å­¦ç¿’ç‡ |
| `batch_size` | 5000 | ãƒãƒƒãƒã‚µã‚¤ã‚º |
| `gradient_clip` | 2.0 | å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤ |
| `proj_noise` | 0.05 | ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºï¼ˆåæŸå„ªå…ˆï¼‰ |
| `early_stopping_threshold` | 0.95 | åæŸç‡95%ã§æ—©æœŸåœæ­¢ |

### embed_normï¼ˆåŸ‹ã‚è¾¼ã¿æ­£è¦åŒ–ï¼‰ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# âš ï¸ é‡è¦: åŸ‹ã‚è¾¼ã¿å¾Œã®æ­£è¦åŒ–ãŒå¿…é ˆï¼ˆDProjå­¦ç¿’åæŸã«å¿…è¦ï¼‰
self.embed_norm = nn.LayerNorm(hidden_size)

# ä½¿ç”¨æ™‚
token_embeds = model.embed_in(token_ids)
token_embeds = model.embed_norm(token_embeds)  # âš ï¸ å¿…é ˆ
```

**âš ï¸ è­¦å‘Š**: embed_normãŒãªã„ã¨DProjå­¦ç¿’ãŒåæŸã—ã¾ã›ã‚“ã€‚

### shifted_prev_projæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# âŒ ç¦æ­¢: é †æ¬¡å‡¦ç†ï¼ˆéå¸¸ã«é…ã„ï¼‰
for i in range(num_tokens):
    new_proj = model.dproj(prev_proj, token_embed)
    prev_proj = new_proj

# âœ… å¿…é ˆ: shifted_prev_projæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
for iteration in range(max_iterations):
    # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰é–‹å§‹
    init_proj = torch.zeros(1, proj_dim)
    shifted_prev_proj = torch.cat([init_proj, previous_projs[:-1]], dim=0)

    # ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†
    new_projs = model.dproj(shifted_prev_proj, token_embeds)
    previous_projs = new_projs
```

### å‹¾é…ç´¯ç©ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# ãƒãƒƒãƒã”ã¨ã«å‹¾é…ã‚’è¨ˆç®—ãƒ»ç´¯ç©
optimizer.zero_grad()
for batch in batches:
    loss = oacd_loss(batch_output)
    scaled_loss = loss / num_batches  # ãƒãƒƒãƒæ•°ã§å‰²ã‚‹
    scaled_loss.backward()  # å‹¾é…ç´¯ç©

# æœ€å¾Œã«ã¾ã¨ã‚ã¦æ›´æ–°
torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
optimizer.step()
```

### CPU/GPUãƒ¡ãƒ¢ãƒªåˆ†é›¢ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# token_embedsã¨previous_projsã¯CPUã«ä¿æŒ
token_embeds = token_embeds_gpu.cpu()
previous_projs = projs.cpu()

# ãƒãƒƒãƒã”ã¨ã«GPUã«è»¢é€ã—ã¦å‡¦ç†
for start_idx in range(0, num_tokens, batch_size):
    batch_projs = previous_projs[start_idx:end_idx].to(device)
    batch_embeds = token_embeds[start_idx:end_idx].to(device)

    # å‡¦ç†å¾Œã¯å³åº§ã«CPUã«æˆ»ã™
    all_projs_cpu.append(batch_output.detach().cpu())
```

### åæŸç‡è¨ˆç®—ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
def compute_convergence_rate(current, previous, threshold=0.03):
    token_losses = ((current - previous) ** 2).mean(dim=1)
    converged_count = (token_losses < threshold).sum()
    return converged_count / len(current)
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒ

### Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢ï¼ˆå³ç¦ï¼‰

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Core Components

**1. DProjPythiaModel (Ours)**
- Token Embedding: vocab â†’ embed_dim (512)
- embed_norm: LayerNormï¼ˆDProjå­¦ç¿’åæŸã«å¿…é ˆï¼‰
- DiverseProjection: embed_dim (512) â†’ proj_dim (320)
- PythiaLayer Ã— 6: hidden_size=proj_dim (320)ã€RoPEå«ã‚€
- Output Head: proj_dim (320) â†’ vocab_size

**2. DiverseProjection (DProj)**
- 1å±¤å›ºå®šã€DProj Trainingã§å­¦ç¿’ã€Main Trainingã§freeze
- OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¤šæ§˜æ€§å­¦ç¿’
- åˆæœŸåŒ–: normal_(std=0.1)

**3. PythiaModel (Baseline)**
- Token Embedding: vocab â†’ hidden_size (512)
- PythiaLayer Ã— 6: hidden_size (512)
- Output Head: hidden_size (512) â†’ vocab_size

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ dproj_pythia.pt           # DProj checkpoint
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dproj.py                  # DProj Trainingè¨­å®š
â”‚   â””â”€â”€ pythia.py                 # PythiaConfig, DProjPythiaConfig
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_dproj.py            # DProj Training: DiverseProjection OACDå­¦ç¿’
â”‚   â”œâ”€â”€ experiment_pythia_comparison.py  # Pythia vs DProj-Pythiaæ¯”è¼ƒ
â”‚   â””â”€â”€ experiment_ka_comparison.py      # KA-Attentionå®Ÿé¨“
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pythia.py             # PythiaModel (baseline)
â”‚   â”‚   â”œâ”€â”€ dproj_pythia.py       # DProjPythiaModel (ours)
â”‚   â”‚   â”œâ”€â”€ dproj.py              # DiverseProjection, DiverseProjectionLayer
â”‚   â”‚   â””â”€â”€ ka_attention.py       # KA-Attentionå®Ÿé¨“
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ diversity.py          # OACD algorithm
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_pythia.py        # Pileãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚       â”œâ”€â”€ training.py           # å…±é€šå­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚       â””â”€â”€ initialization.py     # é‡ã¿åˆæœŸåŒ–
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-04 | **Rename**: Phase 1 â†’ DProj Training, ContextBlock â†’ DiverseProjection |
| 2025-12-04 | **KA-Attention**: V ã‚’ A ã«ç½®ãæ›ãˆã‚‹å®Ÿé¨“å®Ÿè£… |
| 2025-12-04 | **é‡è¦**: PythiaLayerã‚’proj_dim (320)ã§å‹•ä½œã•ã›ã‚‹è¨­è¨ˆã«å¤‰æ›´ |
| 2025-12-04 | Main Trainingæ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¿½åŠ ã€DProj Trainingãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |
| 2025-12-04 | embed_normè¿½åŠ ï¼ˆDProjå­¦ç¿’åæŸã«å¿…é ˆï¼‰ |
| 2025-12-04 | Pythia-70Mçµ±åˆï¼ˆDProj-Pythiaæ–¹å¼ã«å®Œå…¨ç§»è¡Œï¼‰ |
| 2025-12-04 | DProjå­¦ç¿’ä»•æ§˜ã®è©³ç´°ã‚’è¿½è¨˜ï¼ˆPythiaçµ±åˆå¤±æ•—ã‹ã‚‰ã®å¾©æ—§å¾Œï¼‰ |
| 2025-12-03 | Context-KV Attentionæ–¹å¼ï¼ˆæ—§æ–¹å¼ã€å‰Šé™¤æ¸ˆã¿ï¼‰ |

---

Last Updated: 2025-12-04
