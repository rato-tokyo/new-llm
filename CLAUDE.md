# Claude Code Development Guidelines for New-LLM Project

## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

**New-LLM**: æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«å›ºå®šç‚¹ç‰¹æ€§ï¼ˆCVFP Propertyï¼‰ã‚’ç”¨ã„ãŸæ–°ã—ã„è¨€èªãƒ¢ãƒ‡ãƒ«

- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: UltraChatï¼ˆå¯¾è©±ãƒ‡ãƒ¼ã‚¿ï¼‰ã®ã¿
- **ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ **: ãƒ­ãƒ¼ã‚«ãƒ«GPUï¼ˆé€æ¬¡å‡¦ç†ã®ãŸã‚Colabä¸é©ï¼‰
- **äºŒæ®µéšå­¦ç¿’**:
  - Phase 1: æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã®å›ºæœ‰ç‚¹å­¦ç¿’
  - Phase 2: å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³å­¦ç¿’

---

## ğŸ¯ Context Vector Fixed-Point Property (CVFP Property) - å‰Šé™¤ä¸èƒ½ãƒ«ãƒ¼ãƒ«

**New-LLMã®æ ¹æœ¬åŸç†ï¼šæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ä¸å‹•ç‚¹ç‰¹æ€§**

### åŸºæœ¬ä»®èª¬

**ååˆ†å¤§ãã„ n ã«å¯¾ã—ã¦ã€nå›ç¹°ã‚Šè¿”ã—ãŸæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã¨ n+1å›ç¹°ã‚Šè¿”ã—ãŸæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã¯ã»ã¨ã‚“ã©åŒã˜ã«ãªã‚‹**

- **æ­£å¼åç§°**: Context Vector Fixed-Point Property (CVFP Property)
- **ç•¥ç§°**: CVFPç‰¹æ€§
- **æ—¥æœ¬èª**: æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ä¸å‹•ç‚¹ç‰¹æ€§

### Phase 1 è¨“ç·´ã®åŸç†

**å„iteration ã§å‰å›ã®å‡ºåŠ›ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã—ã€æ–‡è„ˆã‚’å¼•ãç¶™ã„ã§å‡¦ç†**:

```python
# Iteration 1: ã‚¼ãƒ­ã‹ã‚‰é–‹å§‹ã€Forward pass onlyï¼ˆå­¦ç¿’ãªã—ï¼‰
context = torch.zeros(1, context_dim)
for t, token_embed in enumerate(token_embeds):
    context = model._update_context_one_step(token_embed, context)
    fixed_contexts[t] = context  # ä¿å­˜

# Iteration 2+: å‰å›ã®å‡ºåŠ›ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å­¦ç¿’
context = torch.zeros(1, context_dim)  # æ¯å›ã‚¼ãƒ­ã‹ã‚‰é–‹å§‹
for t, token_embed in enumerate(token_embeds):
    # å„ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«å­¦ç¿’
    optimizer.zero_grad()
    context_new = model._update_context_one_step(token_embed, context)
    loss = mse_loss(context_new, fixed_contexts[t])  # å‰å›ã®åŒã˜ä½ç½®ã¨æ¯”è¼ƒ
    loss.backward()
    optimizer.step()

    context = context_new.detach()  # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¸å¼•ãç¶™ãï¼ˆå‹¾é…ã¯åˆ‡ã‚‹ï¼‰
    context.requires_grad = True
    fixed_contexts[t] = context_new.detach()  # æ¬¡ã®iterationã®ãŸã‚ã«æ›´æ–°
```

**CVFPç‰¹æ€§ã®å®Ÿç¾æ–¹æ³•**:
- âœ… **æ–‡è„ˆã®å¼•ãç¶™ã**: å„ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†æ™‚ã€å‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æ–‡è„ˆã‚’å¼•ãç¶™ã
- âœ… **å„ãƒˆãƒ¼ã‚¯ãƒ³ã§å­¦ç¿’**: `zero_grad()` â†’ `backward()` â†’ `step()`ã®ã‚µã‚¤ã‚¯ãƒ«
- âœ… **å‹¾é…ã¯åˆ‡ã‚‹**: `context.detach()`ã§å‹¾é…ã‚’åˆ‡ã‚‹ãŒã€å€¤ã¯æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¸
- âœ… **å›ºå®šç‚¹ã«åæŸ**: å‰å›iterationã®åŒã˜ä½ç½®ã®æ–‡è„ˆã¨ä¸€è‡´ã™ã‚‹ã‚ˆã†å­¦ç¿’

**ã“ã®ç‰¹æ€§ã¯New-LLMã®å­˜åœ¨æ„ç¾©ã§ã‚ã‚Šã€çµ¶å¯¾ã«å‰Šé™¤ãƒ»å¤‰æ›´ã—ã¦ã¯ãªã‚‰ãªã„**

---

## âš™ï¸ Distribution Regularization - CRITICAL

**æ¬¡å…ƒå´©å£Šã‚’é˜²ããŸã‚ã®æ­£å‰‡åŒ–æ‰‹æ³•**

### å•é¡Œã®çµŒç·¯

16æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ã§æ¬¡å…ƒå´©å£ŠãŒç™ºç”Ÿï¼š
- Train Effective Rank: 4.55/16 (28%)
- Val Effective Rank: 1.01/16 (6%) - Global Attractor

æ§˜ã€…ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦è¡Œã—ãŸãŒã€**Distribution Regularization**ãŒå”¯ä¸€ã®æˆåŠŸ

### å®Ÿè£…

```python
# å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†ã—ã¦ã‹ã‚‰å­¦ç¿’
all_contexts = []
for t, token_embed in enumerate(token_embeds):
    context = model._update_context_one_step(token_embed.unsqueeze(0), context)
    all_contexts.append(context)
    fixed_contexts[t] = context.detach().squeeze(0)
    context = context.detach()
    context.requires_grad = True

# æå¤±è¨ˆç®—
if iteration > 0:
    optimizer.zero_grad()

    # Stack all contexts: [num_tokens, context_dim]
    all_contexts_tensor = torch.cat(all_contexts, dim=0)

    # CVFP loss: å›ºå®šç‚¹ã¸ã®åæŸ
    cvfp_loss = mse_loss(all_contexts_tensor, fixed_contexts)

    # Distribution regularization loss
    # å„æ¬¡å…ƒï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ã§ã®ï¼‰ãŒæ­£è¦åˆ†å¸ƒN(0,1)ã«è¿‘ã¥ã
    dim_mean = all_contexts_tensor.mean(dim=0)  # [context_dim]
    dim_var = all_contexts_tensor.var(dim=0)    # [context_dim]

    mean_penalty = (dim_mean ** 2).mean()
    var_penalty = ((dim_var - 1.0) ** 2).mean()
    dist_loss = mean_penalty + var_penalty

    # åˆè¨ˆ
    dist_weight = 0.2  # 20% distribution, 80% CVFP
    total_loss = (1 - dist_weight) * cvfp_loss + dist_weight * dist_loss

    total_loss.backward()
    optimizer.step()
```

### è¨­å®šæ–¹æ³•

```python
# config.py
use_distribution_reg = True     # åˆ†å¸ƒæ­£å‰‡åŒ–ã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰
dist_reg_weight = 0.2           # åˆ†å¸ƒæ­£å‰‡åŒ–ã®é‡ã¿
                                # 0.2: 80% CVFP, 20% åˆ†å¸ƒæ­£å‰‡åŒ–ï¼ˆæ¨å¥¨ï¼‰
```

### å®Ÿé¨“çµæœï¼ˆ16æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ã€k=0.2ï¼‰

| æŒ‡æ¨™ | DDR baseline | **Distribution Reg** | æ”¹å–„ç‡ |
|------|-------------|---------------------|--------|
| **Train ER** | 4.55/16 (28%) | **8.33/16 (52%)** | **1.8å€** |
| **Val ER** | 1.01/16 (6%) | **7.54/16 (47%)** | **7.5å€** |
| **Val L2è·é›¢** | 0.007 | **4.199** | **600å€** |
| **Val Cosine** | 0.99997 | **0.37698** | Global Attractorè§£æ¶ˆ |

### é‡è¦ãªç™ºè¦‹

**1. å³åº§ã«åæŸ**
```
Iteration 2: Loss=0.103548 (CVFP=0.000000, Dist=0.517741), Converged=100.0%
```
- CVFP Loss = 0 â†’ ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå›ºå®šç‚¹ã«åæŸ
- åˆ†å¸ƒæ­£å‰‡åŒ–ã®ã¿ãŒãƒšãƒŠãƒ«ãƒ†ã‚£

**2. ãƒãƒ«ãƒ ã®çµ±ä¸€**
```
Train: Avg Norm: 3.999980 (Range: [3.999978, 3.999982])
Val:   Avg Norm: 4.002080 (Range: [3.997518, 4.006338])
```
- åˆ†æ•£=1ã¸ã®æ­£å‰‡åŒ– â†’ âˆš16 = 4.0ã®ãƒãƒ«ãƒ ã«åæŸ

---

## ğŸš« Phase 1æœªè§£æ±ºæ™‚ã®Phase 2å®Ÿè¡Œç¦æ­¢ãƒãƒªã‚·ãƒ¼ - CRITICAL

**âš ï¸ Phase 1ã§æ¬¡å…ƒå´©å£ŠãŒè§£æ±ºã—ãªã„é™ã‚Šã€Phase 2ã¯å®Ÿè¡Œã—ãªã„**

### åŸºæœ¬åŸå‰‡

**Phase 1ï¼ˆå›ºæœ‰ç‚¹å­¦ç¿’ï¼‰ã§ä»¥ä¸‹ã®æ¡ä»¶ã‚’æº€ãŸã•ãªã„é™ã‚Šã€Phase 2ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰ã‚’å®Ÿè¡Œã—ã¦ã¯ãªã‚‰ãªã„**:

1. **Train Effective Rank**: æœ€ä½ã§ã‚‚ 50/256 (20%) ä»¥ä¸Š
2. **Val Effective Rank**: æœ€ä½ã§ã‚‚ 20/256 (8%) ä»¥ä¸Š

### ç†ç”±

**Phase 1ãŒå¤±æ•—ã—ã¦ã„ã‚‹çŠ¶æ…‹ã§Phase 2ã‚’å®Ÿè¡Œã—ã¦ã‚‚ç„¡æ„å‘³**:
- Val Effective Rank 1.08/256 = ã»ã¼1æ¬¡å…ƒã«å´©å£Š
- ã“ã®çŠ¶æ…‹ã§50ã‚¨ãƒãƒƒã‚¯è¨“ç·´ã—ã¦ã‚‚ã€è¡¨ç¾åŠ›ãŒãªã„
- è¨ˆç®—æ™‚é–“ã®ç„¡é§„ï¼ˆæ•°æ™‚é–“ã€œæ•°æ—¥ï¼‰

### å®Ÿè£…ãƒ«ãƒ¼ãƒ«

```python
# Phase 1çµ‚äº†å¾Œã€Val Effective Rankã‚’ãƒã‚§ãƒƒã‚¯
MIN_TRAIN_RANK = 50.0  # Minimum 50/256 (20%)
MIN_VAL_RANK = 20.0    # Minimum 20/256 (8%)

if train_effective_rank < MIN_TRAIN_RANK or val_effective_rank < MIN_VAL_RANK:
    print_flush("\nâš ï¸  PHASE 1 FAILED - DIMENSION COLLAPSE DETECTED")
    print_flush(f"   Phase 2 skipped. Fix dimension collapse first.")
    return

# Effective RankãŒååˆ†ãªå ´åˆã®ã¿Phase 2å®Ÿè¡Œ
print_flush(f"\nâœ… Phase 1 successful: Val Effective Rank = {val_effective_rank:.2f}/256")
print_flush(f"   Proceeding to Phase 2...")
```

---

## ğŸ¯ Phase 1ã¨Phase 2ã®Train/ValåŒºåˆ¥ - CRITICAL

**Phase 1ã¨Phase 2ã§ã€Train/Valã®æ‰±ã„ãŒç•°ãªã‚‹**

### Phase 1: å›ºå®šç‚¹ã®è¨ˆç®—

**ç›®çš„**: æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ç”ŸæˆNNã‚’å­¦ç¿’ã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã«å¯¾ã™ã‚‹å›ºå®šç‚¹ã‚’è¨ˆç®—ã™ã‚‹

**Train/ValåŒºåˆ¥**:
- âœ… **Train**: æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆlayersï¼ˆcontext generation layersï¼‰ã‚’å­¦ç¿’
- âœ… **Val**: å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã§å›ºå®šç‚¹ã‚’è¨ˆç®—ï¼ˆè©•ä¾¡ã®ã¿ã€å­¦ç¿’ãªã—ï¼‰

**ç†æƒ³çš„ãªå‹•ä½œ**:
- Trainã§å­¦ç¿’ã—ãŸæ–‡è„ˆç”ŸæˆNNãŒã€æœªçŸ¥ã®Val dataã«å¯¾ã—ã¦ã‚‚å®‰å®šã—ãŸå›ºå®šç‚¹ã‚’è¨ˆç®—ã§ãã‚‹
- Valã®åæŸç‡ãŒTrainã¨åŒç­‰ãªã‚‰ã€Phase 1ã®å­¦ç¿’ãŒæˆåŠŸ

### Phase 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

**ç›®çš„**: å›ºå®šç‚¹æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹token_output layerã‚’å­¦ç¿’

**Train/ValåŒºåˆ¥**:
- âœ… **Train**: token_output layerã‚’å­¦ç¿’
- âœ… **Val**: å­¦ç¿’ãªã—ã€è©•ä¾¡ã®ã¿

---

## ğŸ§ª ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªå®Ÿè¡Œä¾‹

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆ256æ¬¡å…ƒã€4å±¤ï¼‰
python3 tests/phase2_experiments/test_residual.py

# 16æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«ï¼ˆæ¬¡å…ƒå´©å£Šãƒ†ã‚¹ãƒˆï¼‰
python3 tests/phase2_experiments/test_residual.py \
    --context-dim 16 \
    --embed-dim 16 \
    --hidden-dim 32 \
    --num-samples 10

# 3å±¤ãƒ¢ãƒ‡ãƒ«
python3 tests/phase2_experiments/test_residual.py \
    --num-layers 3

# Distribution Regularizationã®é‡ã¿ã‚’å¤‰æ›´
python3 tests/phase2_experiments/test_residual.py \
    --context-dim 16 \
    --embed-dim 16 \
    --hidden-dim 32 \
    --dist-reg-weight 0.5  # 50% distribution, 50% CVFP

# Phase 1ã®ã¿å®Ÿè¡Œ
python3 tests/phase2_experiments/test_residual.py \
    --context-dim 16 \
    --embed-dim 16 \
    --hidden-dim 32 \
    --skip-phase2
```

### ã™ã¹ã¦ã®å¼•æ•°

```
ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£:
  --context-dim INT       æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰
  --embed-dim INT         ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 256ï¼‰
  --hidden-dim INT        ä¸­é–“å±¤æ¬¡å…ƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 512ï¼‰
  --num-layers INT        å˜å±¤ãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 4ï¼‰
                          4ãªã‚‰[1,1,1,1], 3ãªã‚‰[1,1,1]ã‚’ç”Ÿæˆ

Phase 1è¨­å®š:
  --phase1-max-iter INT        æœ€å¤§åå¾©å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
  --phase1-lr-warmup FLOAT     Warmup LRï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.002ï¼‰
  --phase1-lr-medium FLOAT     Medium LRï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.0005ï¼‰
  --phase1-lr-finetune FLOAT   Finetune LRï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.0001ï¼‰

Distribution Regularization:
  --dist-reg-weight FLOAT  æ­£å‰‡åŒ–ã®é‡ã¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.2ï¼‰
  --no-dist-reg            åˆ†å¸ƒæ­£å‰‡åŒ–ã‚’ç„¡åŠ¹åŒ–

Phase 2è¨­å®š:
  --phase2-lr FLOAT         å­¦ç¿’ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.0001ï¼‰
  --phase2-epochs INT       ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
  --phase2-batch-size INT   ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 32ï¼‰

ãƒ‡ãƒ¼ã‚¿è¨­å®š:
  --num-samples INT         è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰
  --train-val-split FLOAT   Train/Valåˆ†å‰²æ¯”ç‡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.8ï¼‰

ãã®ä»–:
  --device STR           ãƒ‡ãƒã‚¤ã‚¹ï¼ˆcpu/cudaã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cpuï¼‰
  --skip-phase2          Phase 2ã‚’ã‚¹ã‚­ãƒƒãƒ—
  --freeze-context       Phase 2ã§æ–‡è„ˆã‚’å›ºå®š
```

---

## ğŸ“Š å®Ÿé¨“çµæœã®å®Œå…¨ç¢ºèªãƒãƒªã‚·ãƒ¼ - CRITICAL

**âš ï¸ å®Ÿé¨“çµæœã‚’å ±å‘Šã™ã‚‹éš›ã¯ã€å¿…ãšå…¨ã¦ã®æƒ…å ±ã‚’ç¢ºèªã™ã‚‹ã“ã¨**

### å¿…é ˆç¢ºèªé …ç›®

å®Ÿé¨“çµæœã‚’åˆ†æãƒ»å ±å‘Šã™ã‚‹éš›ã¯ã€**ä»¥ä¸‹ã®å…¨é …ç›®ã‚’å¿…ãšç¢ºèª**ï¼š

1. **åæŸéç¨‹**
   - å…¨iterationã®åæŸç‡ã¨Loss
   - Early stoppingã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°

2. **å›ºæœ‰ç‚¹åˆ†æï¼ˆFIXED-POINT ANALYSISï¼‰**
   - âœ… Global Attractor Detectionï¼ˆL2è·é›¢ã€Cosineé¡ä¼¼åº¦ï¼‰
   - âœ… Zero Solution Detectionï¼ˆå¹³å‡ãƒãƒ«ãƒ ï¼‰
   - âœ… Distribution Statisticsï¼ˆãƒãƒ«ãƒ çµ±è¨ˆã€Pairwiseè·é›¢ï¼‰
   - âœ… **Information Contentï¼ˆEffective Rankã€ç‰¹ç•°å€¤ï¼‰** â† **çµ¶å¯¾ã«è¦‹è½ã¨ã™ãª**

3. **Train/Valä¸¡æ–¹**
   - Trainã®çµæœã ã‘ã§ãªãã€**Valã®çµæœã‚‚å¿…ãšç¢ºèª**
   - Train/Valã®å·®åˆ†ã‚’åˆ†æ

### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

å®Ÿé¨“çµæœå ±å‘Šå‰ã«å¿…ãšç¢ºèªï¼š

- [ ] åæŸéç¨‹ã®å…¨iterationã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ
- [ ] å›ºæœ‰ç‚¹åˆ†æã®4ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ã¦ç¢ºèªã—ãŸã‹ï¼Ÿ
- [ ] **Effective Rankã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ**
- [ ] **ç‰¹ç•°å€¤ï¼ˆTop 5 Singular Valuesï¼‰ã‚’ç¢ºèªã—ãŸã‹ï¼Ÿ**
- [ ] Train/Valä¸¡æ–¹ã®çµæœã‚’æ¯”è¼ƒã—ãŸã‹ï¼Ÿ
- [ ] ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼è­¦å‘Šï¼ˆâš ï¸ DEGENERATEï¼‰ã‚’è¦‹è½ã¨ã—ã¦ã„ãªã„ã‹ï¼Ÿ

**ã“ã®åŸå‰‡ã‚’å®ˆã‚‰ãªã„ã¨ã€é‡å¤§ãªå•é¡Œã‚’è¦‹è½ã¨ã™**

---

## ğŸ”§ ã‚³ãƒ¼ãƒ‰å“è³ªãƒãƒªã‚·ãƒ¼

### 1. ã‚³ãƒ¼ãƒ‰é‡è¤‡ã®å¾¹åº•æ’é™¤ - DRYåŸå‰‡

**âœ… æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ - å…±é€šé–¢æ•°ã§ä¸€å…ƒåŒ–**:

```python
def _compute_batch_metrics(model, input_ids, device, context_loss_weight):
    """å…±é€šã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆtrainã¨valä¸¡æ–¹ã§ä½¿ç”¨ï¼‰"""
    token_loss = F.cross_entropy(...)
    recon_loss = F.mse_loss(...)
    return {'loss': loss, 'token_loss': token_loss, 'recon_loss': recon_loss}

def train_epoch(...):
    metrics = _compute_batch_metrics(...)  # å…±é€šé–¢æ•°ã‚’ä½¿ç”¨
    optimizer.zero_grad()
    metrics['loss'].backward()
    optimizer.step()

def evaluate(...):
    metrics = _compute_batch_metrics(...)  # åŒã˜é–¢æ•°ã‚’ä½¿ç”¨
```

### 2. ãƒ•ã‚¡ã‚¤ãƒ«å‘½åè¦å‰‡ - å®Œå…¨å›ºå®šæ–¹é‡

- âœ… **å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«å**: åŒã˜ç¨®é¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¸¸ã«åŒã˜åå‰ã‚’ä½¿ã†
- âŒ **ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¥å°¾è¾ç¦æ­¢**: `_v1`, `_v2`, `_old`, `_new`, `_fixed` ãªã©ã¯ä½¿ã‚ãªã„
- âŒ **æ—¥ä»˜æ¥å°¾è¾ç¦æ­¢**: `_20250117`, `_latest` ãªã©ã¯ä½¿ã‚ãªã„
- âœ… **ä¸Šæ›¸ã**: æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯å¸¸ã«åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«åã§ä¸Šæ›¸ã

### 3. Code Cleanup Policy

**å¤ã„ã‚³ãƒ¼ãƒ‰ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã§ã™**

- âœ— ä½¿ã‚ã‚Œãªããªã£ãŸãƒ¡ã‚½ãƒƒãƒ‰ãƒ»é–¢æ•°
- âœ— ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
- âœ— ãƒ‡ãƒãƒƒã‚°ç”¨ã®ä¸€æ™‚ã‚³ãƒ¼ãƒ‰
- âœ— ä¸è¦ã«ãªã£ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ»å¼•æ•°

---

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

```
new-llm/
â”œâ”€â”€ config.py                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                  # ãƒ¢ãƒ‡ãƒ«å®Ÿè£…
â”‚   â””â”€â”€ utils/                   # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ phase1_experiments/      # Phase 1å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â””â”€â”€ phase2_experiments/      # Phase 2å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚       â””â”€â”€ test_residual.py     # ãƒ¡ã‚¤ãƒ³å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ experiments/             # å®Ÿé¨“çµæœãƒ¬ãƒãƒ¼ãƒˆ
â””â”€â”€ cache/                       # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰
    â”œâ”€â”€ tokenizer/               # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    â””â”€â”€ manual_val_tokens.pt     # æ‰‹å‹•Validationãƒ‡ãƒ¼ã‚¿
```

---

## ğŸš¨ é‡è¦ãªæ³¨æ„äº‹é …

### ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œæ™‚ã®ãƒ­ã‚°å‡ºåŠ›ãƒãƒªã‚·ãƒ¼

**âš ï¸ teeã‚³ãƒãƒ³ãƒ‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ç¦æ­¢**

```bash
# âŒ çµ¶å¯¾ç¦æ­¢ - teeã¯çµ¶å¯¾ã«ä½¿ã‚ãªã„
python3 -u script.py 2>&1 | tee /tmp/log.txt &

# âœ… æ­£ã—ã„ - ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã®ã¿ä½¿ç”¨
python3 -u script.py > /tmp/log.txt 2>&1 &
```

**ãªãœç¦æ­¢ã‹**:
- `tee`ã¯ãƒ‘ã‚¤ãƒ—çµŒç”±ã®ãŸã‚ã€**å‡ºåŠ›ãŒå®Œå…¨ã«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã•ã‚Œã‚‹**
- ãƒ—ãƒ­ã‚»ã‚¹ãŒæ•°æ™‚é–“å®Ÿè¡Œã•ã‚Œã¦ã‚‚ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ›´æ–°ã•ã‚Œãªã„

---

## ã¾ã¨ã‚

**é‰„å‰‡**:
- âœ… CVFPç‰¹æ€§ã¯çµ¶å¯¾ã«å‰Šé™¤ãƒ»å¤‰æ›´ã—ãªã„
- âœ… Distribution Regularizationã‚’ä½¿ç”¨ï¼ˆæ¬¡å…ƒå´©å£Šå¯¾ç­–ï¼‰
- âœ… Phase 1ãŒæˆåŠŸã—ãªã„é™ã‚ŠPhase 2ã¯å®Ÿè¡Œã—ãªã„
- âœ… å®Ÿé¨“çµæœã¯å…¨é …ç›®ï¼ˆç‰¹ã«Effective Rankï¼‰ã‚’å¿…ãšç¢ºèª
- âœ… ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŸ”è»Ÿã«è¨­å®šå¤‰æ›´å¯èƒ½
