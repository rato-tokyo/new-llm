# New-LLM Project Guidelines

## ğŸ¯ Gæ¡ˆï¼ˆprev_and_current_contextï¼‰æ¡ç”¨æ±ºå®š (2025-12-02)

**Context Modeã¯Gæ¡ˆã«ä¸€æœ¬åŒ–ã€‚Eæ¡ˆ/Aæ¡ˆ/Fæ¡ˆã¯å‰Šé™¤äºˆå®šã€‚**

### æ±ºå®šã®èƒŒæ™¯

4ã¤ã®Context Modeã‚’æ¯”è¼ƒå®Ÿé¨“ã—ãŸçµæœï¼š

| Mode | Val PPL | Val Acc | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | æ‹¡å¼µæ€§ |
|------|---------|---------|-----------|--------|
| Eæ¡ˆ (layerwise) | **128.1** | **24.9%** | âŒ ä½ã„ | âŒ ä½ã„ |
| Gæ¡ˆ (prev_and_current) | 132.2 | 24.4% | âœ… é«˜ã„ | âœ… é«˜ã„ |
| Aæ¡ˆ (final_only) | 136.9 | 24.6% | âœ… é«˜ã„ | â–³ |
| Fæ¡ˆ (first_layer_only) | 137.9 | 24.4% | âœ… é«˜ã„ | âŒ |

### Eæ¡ˆãŒç†è«–ä¸Šã¯æœ€è‰¯ã ãŒã€Gæ¡ˆã‚’é¸æŠã™ã‚‹ç†ç”±

**1. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**
```
Eæ¡ˆ: cache = [num_layers, num_tokens, context_dim]  # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°å€
Gæ¡ˆ: cache = [num_tokens, context_dim]              # å›ºå®š
```
å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§Eæ¡ˆã¯ãƒ¡ãƒ¢ãƒªãŒå³ã—ããªã‚‹ã€‚

**2. æ‹¡å¼µæ€§**
```python
# Gæ¡ˆã¯3å±¤ä»¥ä¸Šã«è‡ªç„¶ã«æ‹¡å¼µå¯èƒ½
# Layer 1 â† context[i-2]  (2ã¤å‰)
# Layer 2 â† context[i-1]  (1ã¤å‰)
# Layer 3 â† context[i]    (ç¾åœ¨)
```

**3. ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§**
è¤‡æ•°ã®Context Modeã‚’ç¶­æŒã™ã‚‹ã‚³ã‚¹ãƒˆãŒé«˜ã„ã€‚

### ç²¾åº¦å·®ã¯è¨±å®¹ç¯„å›²

- PPLå·®: 4.1 (+3.2%)
- Accå·®: 0.5%

ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ ã§ã©ã¡ã‚‰ã‚‚æ”¹å–„ã™ã‚‹ãŸã‚ã€ã“ã®å·®ã¯è¨±å®¹å¯èƒ½ã€‚

### Gæ¡ˆã®å‹•ä½œ

```python
# 2å±¤ã®å ´åˆ
TokenBlock Layer 1 â† context_cache[i-1]  # å‰ãƒˆãƒ¼ã‚¯ãƒ³æ™‚ç‚¹
TokenBlock Layer 2 â† context_cache[i]    # ç¾åœ¨ãƒˆãƒ¼ã‚¯ãƒ³æ™‚ç‚¹

# æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³(i=0)ã§ã¯ prev = current
```

### å°†æ¥çš„ãªEæ¡ˆå¾©æ´»æ¡ä»¶

Eæ¡ˆãŒå¿…è¦ã«ãªã£ãŸå ´åˆã®å¯¾å¿œï¼š
1. ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰è¨ˆç®—ï¼ˆPhase 2ã§ContextBlockå†è¨ˆç®—ï¼‰
2. å¤§å®¹é‡GPUä½¿ç”¨

è©³ç´°: `importants/experiment-results-20251202-context-mode-all-comparison.md`

---

## ğŸ¯ OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¡ç”¨ (2025-12-01)

**Phase 1ã§ã¯OACD (Origin-Anchored Centroid Dispersion) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¡ç”¨ã€‚**

### OACDã®ç‰¹å¾´

```python
def oacd_loss(contexts, centroid_weight=0.1):
    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–
    dispersion_loss = -||X - mean(X)|| / n

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = ||mean(X)||Â²

    return dispersion_loss + centroid_weight * centroid_loss
```

**ç‰¹å¾´**:
- é‡å¿ƒã‚’åŸç‚¹ã«å›ºå®šã™ã‚‹ã“ã¨ã§ã€å®‰å®šã—ãŸå¹³è¡¡ç‚¹ã‚’å®Ÿç¾
- ã€Œè‡ªå·±å¹³è¡¡ã€åŠ¹æœã‚’ç¶­æŒï¼ˆç›¸å¯¾çš„ç›®æ¨™ï¼‰
- ã‚·ãƒ³ãƒ—ãƒ«ãªæå¤±é–¢æ•°ã§é«˜ã„Effective Rankï¼ˆ80%+ï¼‰ã‚’é”æˆ

### å®Ÿé¨“çµæœ (context_dim=500)

| ã‚µãƒ³ãƒ—ãƒ« | ãƒˆãƒ¼ã‚¯ãƒ³ | Val PPL | Acc | ER% | Î±å€¤ |
|---------|---------|---------|-----|-----|-----|
| 50 | 62,891 | 573.8 | 17.8% | 81.2% | - |
| 100 | 122,795 | 383.4 | 19.3% | 81.2% | - |
| 200 | 240,132 | **290.1** | **20.2%** | 81.3% | **-0.509** |

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆCPUï¼‰: å‹•ä½œç¢ºèª
python3 scripts/run_experiment.py -s 2

# Colabï¼ˆGPUï¼‰: æœ¬æ ¼å®Ÿé¨“
python3 scripts/run_experiment.py -s 50 100 200

# context_dimæŒ‡å®š
python3 scripts/run_experiment.py -s 50 100 200 -c 500
```

---

## ğŸš¨ num_layers = 2 æ¨å¥¨ (2025-12-02æ›´æ–°)

**Gæ¡ˆæ¡ç”¨ã«ã‚ˆã‚Šã€2å±¤ä»¥ä¸ŠãŒæ¨™æº–æ§‹æˆã€‚**

```python
# config.py
num_layers = 2  # Gæ¡ˆã§ã¯2å±¤ä»¥ä¸ŠãŒå¿…è¦ï¼ˆprev/currentã®å·®åˆ†æ´»ç”¨ï¼‰
```

**ç†ç”±**:
- Gæ¡ˆã¯ã€Œå‰ã®contextã€ã¨ã€Œç¾åœ¨ã®contextã€ã‚’ç•°ãªã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«æ³¨å…¥
- 1å±¤ã§ã¯Gæ¡ˆã®æ„å‘³ãŒãªã„ï¼ˆprev = currentã«ãªã‚‹ï¼‰
- 2å±¤: Layer1=prev, Layer2=current
- 3å±¤: Layer1=prev, Layer2=none, Layer3=currentï¼ˆæ‹¡å¼µå¯èƒ½ï¼‰

---

## ğŸ’» ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿé¨“ã®æ³¨æ„äº‹é … - CPUç’°å¢ƒ (2025-12-01)

**ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼ˆMac/CPUï¼‰ã§ã¯å‡¦ç†ãŒé…ã„ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã€‚**

### æ¨å¥¨è¨­å®š

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿé¨“ï¼ˆCPUï¼‰: 2-5ã‚µãƒ³ãƒ—ãƒ«ã§ååˆ†
python3 scripts/run_experiment.py -s 2

# Colabï¼ˆGPUï¼‰: 100ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šã§æœ¬æ ¼å®Ÿé¨“
python3 scripts/run_experiment.py -s 50 100 200
```

### ãƒ­ãƒ¼ã‚«ãƒ« vs Colab æ¯”è¼ƒ

| ç’°å¢ƒ | æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«æ•° | å‡¦ç†æ™‚é–“ç›®å®‰ |
|------|--------------|-------------|
| **ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆCPUï¼‰** | 2-5 | æ•°åˆ†ã€œåæ•°åˆ† |
| **Colabï¼ˆGPUï¼‰** | 100-500 | æ•°åˆ† |

---

## ğŸš¨ CPU/GPUãƒ†ãƒ³ã‚½ãƒ«ç®¡ç† - é‡è¦æ•™è¨“ (2025-12-01)

**å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ2000ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šï¼‰ã§OOMã‚’é˜²ããŸã‚ã€ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚’å¾¹åº•ã€‚**

### å•é¡Œã®ç—‡çŠ¶

```
RuntimeError: Expected all tensors to be on the same device, but got tensors is on cpu,
different from other tensors on cuda:0
```

### æ ¹æœ¬åŸå› 

OOMå¯¾ç­–ã§ãƒ†ãƒ³ã‚½ãƒ«ã‚’CPUã«ä¿æŒã™ã‚‹è¨­è¨ˆã«å¤‰æ›´ã—ãŸéš›ã€GPUè»¢é€æ¼ã‚ŒãŒç™ºç”Ÿï¼š
1. `previous_contexts`: CPUã«ä¿æŒ â†’ ãƒãƒƒãƒå‡¦ç†æ™‚ã«GPUè»¢é€å¿…è¦
2. `token_embeds`: CPUã«ä¿æŒ â†’ `combine_batch`å¾Œã«GPUè»¢é€å¿…è¦
3. `last_context`: CPUä¸Šã§å–å¾— â†’ GPUè»¢é€å¿…è¦

### ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³

```python
# âŒ ä¿®æ­£å‰: CPUãƒ†ãƒ³ã‚½ãƒ«ã‚’ãã®ã¾ã¾ä½¿ç”¨
batch_contexts = previous_contexts[start_idx:end_idx].detach()
batch_combined = self._build_combined_tokens_batch(token_embeds, ...)

# âœ… ä¿®æ­£å¾Œ: æ˜ç¤ºçš„ã«GPUè»¢é€
batch_contexts = previous_contexts[start_idx:end_idx].detach().to(self.device)
batch_combined = self._build_combined_tokens_batch(token_embeds, ...).to(self.device)
```

### ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼ˆOOMå¯¾ç­–ã‚³ãƒ¼ãƒ‰å¤‰æ›´æ™‚ï¼‰

- [ ] CPUã«ä¿æŒã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç‰¹å®š
- [ ] GPUæ¼”ç®—ã«æ¸¡ã™å‰ã«`.to(self.device)`ã‚’è¿½åŠ 
- [ ] ãƒ«ãƒ¼ãƒ—å†…ã®ã™ã¹ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«è»¢é€ã‚’ç¢ºèª
- [ ] `torch.cat`ã‚„æ¼”ç®—ã®å…¥åŠ›ãƒ‡ãƒã‚¤ã‚¹ã‚’çµ±ä¸€

---

## ğŸš¨ Effective Rankè¨ˆç®—ã®æ•´åˆæ€§ - é‡è¦æ•™è¨“ (2025-12-01)

**Phase 1 Validation Early Stoppingã®Val ERã¨æœ€çµ‚è©•ä¾¡ã®ERãŒå¤§å¹…ã«ä¹–é›¢ã™ã‚‹å•é¡Œã‚’ä¿®æ­£ã€‚**

### å•é¡Œã®ç—‡çŠ¶

- `_quick_validate()` ãŒè¿”ã™Val ER: 3-30%
- æœ€çµ‚è©•ä¾¡ã®Val ER: 64%
- **ç´„2-20å€ã®ä¹–é›¢**

### æ ¹æœ¬åŸå› ï¼ˆ3ã¤ï¼‰

**1. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®é•ã„ï¼ˆæœ€å¤§ã®åŸå› ï¼‰**
- `_quick_validate()`: 500ãƒˆãƒ¼ã‚¯ãƒ³ â†’ **ERãŒä½ãå‡ºã‚‹**
- æœ€çµ‚è©•ä¾¡: 31,024ãƒˆãƒ¼ã‚¯ãƒ³ â†’ ERãŒæ­£ç¢ºã«å‡ºã‚‹
- **ä¿®æ­£**: `phase1_val_sample_size = 10000` ã«å¢—åŠ 

**2. ERè¨ˆç®—æ–¹æ³•ã®é•ã„**
- `_quick_validate()`: å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã‚’ä½¿ç”¨
- `analyze_fixed_points()`: SVDã®ç‰¹ç•°å€¤ã‚’ä½¿ç”¨
- **ä¿®æ­£**: ä¸¡æ–¹ã¨ã‚‚SVDãƒ™ãƒ¼ã‚¹ã«çµ±ä¸€

**3. ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®é•ã„**
- `_quick_validate()`: `collect_all_layers=False`
- `evaluate()`: `collect_all_layers=True`ã€`token_embeds[:-1]`ã§æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã
- **ä¿®æ­£**: `_quick_validate()`ã‚’`evaluate()`ã¨å®Œå…¨ã«åŒã˜å‡¦ç†ã«å¤‰æ›´

---

## âš ï¸ COLABç’°å¢ƒãƒªã‚»ãƒƒãƒˆå¯¾ç­– (2025-11-29)

**Colabã¯é »ç¹ã«ç’°å¢ƒãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¶ˆå¤±ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚**

### è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«

| ãƒ•ã‚¡ã‚¤ãƒ« | ç”¨é€” | è‡ªå‹•ç”Ÿæˆå…ƒ |
|----------|------|----------|
| `./data/example_val.txt` | æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ | `MemoryDataProvider._generate_val_file()` |
| `./cache/ultrachat_*samples_full.pt` | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | `MemoryDataProvider._load_train_data()` |

### Colabã§ã®æ¨å¥¨æ‰‹é †

```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªæ›´æ–°
!cd /content/new-llm && git pull

# 2. å®Ÿé¨“å®Ÿè¡Œï¼ˆæ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
!cd /content/new-llm && python3 scripts/run_experiment.py -s 50 100 200
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒã®Lint/Type Check (2025-11-29)

**pyenvç’°å¢ƒã§ã¯ruffã‚„mypyã‚’ç›´æ¥å®Ÿè¡Œã§ããªã„ãŸã‚ã€`python3 -m` ã§å®Ÿè¡Œã™ã‚‹ã€‚**

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports

# ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
python3 -m ruff check src/trainers/phase1/memory.py
python3 -m mypy src/trainers/phase1/memory.py --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢ (2025-11-29)

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ç¦æ­¢äº‹é …

1. **ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°ã§ã®åˆ†å²ç¦æ­¢**
   ```python
   # âŒ ç¦æ­¢: å¤ã„ãƒ‘ã‚¹ã‚’æ®‹ã™
   def func(cache=None):
       if cache is None:
           cache = build_cache()  # å¤ã„ãƒ‘ã‚¹

   # âœ… æ­£è§£: å¿…é ˆå¼•æ•°ã«ã™ã‚‹
   def func(cache):
       pass  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å‘¼ã³å‡ºã—å…ƒã§å¿…ãšæº–å‚™
   ```

2. **å¤ã„ãƒ¡ã‚½ãƒƒãƒ‰ã®æ®‹å­˜ç¦æ­¢**
   - æ–°ã—ã„è¨­è¨ˆã«ç½®ãæ›ãˆãŸã‚‰ã€å¤ã„ãƒ¡ã‚½ãƒƒãƒ‰ã¯å³åº§ã«å‰Šé™¤
   - ã€Œå¿µã®ãŸã‚ã€ã§æ®‹ã™ã¨ã€èª¤ã£ã¦å¤ã„ãƒ‘ã‚¹ãŒå®Ÿè¡Œã•ã‚Œã‚‹

---

## ğŸš€ PHASE 2 CACHE REUSE - Phase 1ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨ (2025-11-29)

**Phase 1ã§è¨ˆç®—ã—ãŸå…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›ã‚’Phase 2ã§å†åˆ©ç”¨ã—ã€627ç§’ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†æ§‹ç¯‰ã‚’çœç•¥ã€‚**

### æ–°æ–¹å¼: Phase 1ã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¸¡ã™

```python
# Phase 1: return_all_layers=True ã§å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›ã‚‚å–å¾—
train_contexts, train_context_cache, train_token_embeds = phase1_trainer.train(
    ..., return_all_layers=True
)

# Phase 2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å—ã‘å–ã‚Šã€å†æ§‹ç¯‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
phase2_trainer.train_full(
    ...,
    train_context_cache=train_context_cache,
    train_token_embeds=train_token_embeds,
)
```

---

## ğŸ§Š EMBEDDING FREEZE ADOPTED - Embeddingå‡çµæ¡ç”¨ (2025-11-27)

**Phase 2ã§Embeddingå‡çµã‚’æ¨™æº–æ¡ç”¨ã€‚**

### å®Ÿé¨“çµæœ

| æŒ‡æ¨™ | Embeddingå­¦ç¿’ | Embeddingå‡çµ | æ”¹å–„ç‡ |
|------|--------------|--------------|--------|
| Val PPL (500samples) | 1189.15 | **334.31** | **-71.9%** |
| Val Acc (500samples) | 11.58% | **18.88%** | **+63.0%** |

### è¨­å®š

```python
# config.py
phase2_freeze_embedding = True  # æ¨å¥¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
use_weight_tying = True         # æ¨å¥¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
```

---

## ğŸ”— WEIGHT TYING ADOPTED - é‡ã¿å…±æœ‰æ¡ç”¨ (2025-11-27)

**Weight Tyingã‚’æ¨™æº–æ¡ç”¨ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ç´„38Må‰Šæ¸›ã€‚**

| é …ç›® | Without Weight Tying | With Weight Tying |
|------|---------------------|-------------------|
| å…¨ä½“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 91.43M | **52.78M** (-42%) |
| Output Head | 38.65M | **0** (å…±æœ‰) |

---

## ğŸ“Š MANDATORY: æ•°å€¤å ±å‘Šãƒ«ãƒ¼ãƒ«

### çµ¶å¯¾éµå®ˆ: ã™ã¹ã¦ã®å®Ÿé¨“çµæœã¯å…·ä½“çš„ãªæ•°å€¤ã§å ±å‘Šã™ã‚‹

**ç¦æ­¢äº‹é …**:
- âŒ "GOOD", "EXCELLENT" ãªã©ã®æŠ½è±¡çš„è¡¨ç¾ã§ã®å ±å‘Š
- âŒ æ•°å€¤ã‚’ä¼´ã‚ãªã„åˆ¤å®šçµæœã®å ±å‘Š

**å¿…é ˆå ±å‘Šé …ç›®**:
- âœ… **åæŸç‡**: å…·ä½“çš„ãªãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ (ä¾‹: 1.9%)
- âœ… Effective Rank: **å®Ÿæ•°å€¤/ç·æ¬¡å…ƒæ•°ã¨ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸** (ä¾‹: 406/500 = 81.2%)
- âœ… Val PPL: **å®Ÿæ•°å€¤** (ä¾‹: 290.1)
- âœ… Val Acc: **å®Ÿæ•°å€¤** (ä¾‹: 20.2%)
- âœ… Î±å€¤: **å®Ÿæ•°å€¤** (ä¾‹: -0.509)

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Core Components

**1. ContextLayer / TokenLayer**
- ContextLayer: æ–‡è„ˆå‡¦ç†å°‚ç”¨ï¼ˆtokenç¶™ãè¶³ã—æ–¹å¼ï¼‰
- TokenLayer: ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†å°‚ç”¨

**2. ContextBlock / TokenBlock**
- ContextBlock: Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze
- TokenBlock: Phase 2ã§å­¦ç¿’

**3. LLM (Main Model)**
- Token Embedding: GPT-2 pretrained (768-dim, frozen in Phase 2)
- Weight Tying: token_output shares weights with token_embedding

### Phase 1: å¤šæ§˜æ€§å­¦ç¿’ï¼ˆOACDï¼‰

- **å­¦ç¿’å¯¾è±¡**: ContextBlockã®ã¿
- **TokenBlock**: æœªä½¿ç”¨
- **æå¤±**: å¤šæ§˜æ€§æå¤±ã®ã¿ï¼ˆOACDï¼‰

### Phase 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

- **ContextBlock**: frozenï¼ˆé‡ã¿å›ºå®šï¼‰
- **TokenBlock**: å­¦ç¿’
- **æå¤±**: CrossEntropyï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰ã®ã¿

---

## Code Quality Standards

### Principles

1. **No Hardcoding**: All hyperparameters in config.py
2. **Single Responsibility**: Each module has one clear purpose
3. **Error Prevention**: Strict validation

### Anti-Patterns to Avoid

- âŒ Changing architecture without full retraining
- âŒ Using deprecated features
- âŒ Leaving backward compatibility code

---

## File Structure

**Main Scripts**:
- `scripts/run_experiment.py` - æ¨™æº–å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆPhase 1 + Phase 2ï¼‰
- `config.py` - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

**Core Implementation**:
- `src/trainers/phase1/memory.py` - Phase 1è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯
- `src/trainers/phase2.py` - Phase 2è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯
- `src/models/llm.py` - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- `src/losses/diversity.py` - OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

---

Last Updated: 2025-12-02 (Gæ¡ˆæ¡ç”¨æ±ºå®šã€num_layers=2æ¨å¥¨)
