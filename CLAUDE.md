# New-LLM Project Guidelines

---

## âš ï¸âš ï¸âš ï¸ CLAUDE AIã¸ã®é‡è¦ãªè­¦å‘Š âš ï¸âš ï¸âš ï¸

**ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚**

### éå»ã®å•é¡Œ

2025-12-04ã«Pythiaçµ±åˆã‚’è©¦ã¿ãŸéš›ã€CLAUDE.mdã®ç·¨é›†æ™‚ã«Phase 1ã®é‡è¦ãªä»•æ§˜ãŒèª¤ã£ã¦å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€Phase 1ã®å­¦ç¿’ãŒæ­£å¸¸ã«åæŸã—ãªããªã‚Šã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä»¥å‰ã®çŠ¶æ…‹ã«ãƒªãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚

### ãƒ«ãƒ¼ãƒ«

1. **Phase 1ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å‰Šé™¤ç¦æ­¢**
2. **ContextBlock/ContextLayerã®å®Ÿè£…è©³ç´°ã¯å‰Šé™¤ç¦æ­¢**
3. **åˆæœŸåŒ–æ–¹æ³•ï¼ˆnormal_ std=0.1ï¼‰ã®è¨˜è¿°ã¯å‰Šé™¤ç¦æ­¢**
4. CLAUDE.mdã‚’ç·¨é›†ã™ã‚‹éš›ã¯ã€æ—¢å­˜ã®é‡è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ®‹ã£ã¦ã„ã‚‹ã“ã¨ã‚’å¿…ãšç¢ºèªã™ã‚‹ã“ã¨

---

## ğŸ¯ Context-KV Attention Architecture (2025-12-03)

**KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹Context-KV Attentionæ–¹å¼ã‚’æ¡ç”¨ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
Context-KV Attention:
  - 100ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«Contextã‚’åœ§ç¸®
  - åœ§ç¸®ã•ã‚ŒãŸContextã‚’KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä½¿ç”¨
  - ~99% KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›

Position 350 ã®å ´åˆ:
  KV Cache = [context_0-99, context_100-199, context_200-299, context_300-350]
           = 4 context vectors ã®ã¿
```

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# Colabï¼ˆGPUï¼‰: 200ã‚µãƒ³ãƒ—ãƒ«
python3 scripts/experiment_context_kv.py -s 200 --chunk-size 100

# ã‚«ã‚¹ã‚¿ãƒ contextæ¬¡å…ƒ
python3 scripts/experiment_context_kv.py -s 200 -c 256 128 --chunk-size 50
```

---

## ğŸš¨ğŸš¨ğŸš¨ Phase 1 å®Œå…¨ä»•æ§˜ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰ğŸš¨ğŸš¨ğŸš¨

**ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è©¦è¡ŒéŒ¯èª¤ã®æœ«ã«ç¢ºç«‹ã•ã‚ŒãŸå¿…é ˆä»•æ§˜ã§ã™ã€‚çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚**

### Phase 1ã®ç›®çš„

ContextBlockã‚’ä½¿ã£ã¦ã€å¤šæ§˜ãªcontext vectorã‚’ç”Ÿæˆã™ã‚‹ã€‚
OACDï¼ˆOrigin-Anchored Centroid Dispersionï¼‰æå¤±ã§å­¦ç¿’ã—ã€åæŸç‡90%ä»¥ä¸Šã‚’ç›®æŒ‡ã™ã€‚

### ContextBlock/ContextLayerã®å®Ÿè£…ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/models/layers.py - ContextLayer
class ContextLayer(nn.Module):
    def __init__(self, context_input_dim, context_output_dim, token_input_dim):
        # FFN: Linear(input_dim â†’ output_dim) + GELU
        input_dim = context_input_dim + token_input_dim
        self.fnn = FFN(input_dim, context_output_dim)

        # LayerNormï¼ˆå¿…é ˆï¼šæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
        self.context_norm = nn.LayerNorm(context_output_dim)

        # æ®‹å·®æ¥ç¶šç”¨ã®å°„å½±ï¼ˆæ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®ã¿ï¼‰
        if context_input_dim != context_output_dim:
            self.residual_proj = nn.Linear(context_input_dim, context_output_dim)

        # âš ï¸ é‡è¦: åˆæœŸåŒ–ã¯ normal_(std=0.1)
        init_linear_weights(self)  # weight: std=0.1, bias: std=0.01

    def forward(self, context, token_embeds):
        fnn_input = torch.cat([context, token_embeds], dim=-1)
        delta_context = self.fnn(fnn_input)

        # æ®‹å·®æ¥ç¶š + LayerNorm
        new_context = self.context_norm(context + delta_context)
        return new_context
```

### åˆæœŸåŒ–æ–¹æ³•ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/utils/initialization.py
def init_linear_weights(module, weight_std=0.1, bias_std=0.01):
    for submodule in module.modules():
        if isinstance(submodule, nn.Linear):
            nn.init.normal_(submodule.weight, mean=0.0, std=0.1)  # âš ï¸ Xavierç¦æ­¢
            if submodule.bias is not None:
                nn.init.normal_(submodule.bias, mean=0.0, std=0.01)
```

**âš ï¸ è­¦å‘Š**: XavieråˆæœŸåŒ–ã‚„KaimingåˆæœŸåŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Phase 1ãŒåæŸã—ã¾ã›ã‚“ã€‚
å¿…ãš `normal_(std=0.1)` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

### OACDæå¤±é–¢æ•°ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/losses/diversity.py
def oacd_loss(contexts, centroid_weight=0.1):
    context_mean = contexts.mean(dim=0)
    deviation = contexts - context_mean

    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰
    dispersion_loss = -torch.norm(deviation, p=2) / len(contexts)

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = torch.norm(context_mean, p=2) ** 2

    return dispersion_loss + centroid_weight * centroid_loss
```

### Phase 1 è¨­å®šå€¤ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| `max_iterations` | 60 | æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° |
| `convergence_threshold` | 0.03 | åæŸåˆ¤å®šã®MSEé–¾å€¤ |
| `learning_rate` | 0.002 | å­¦ç¿’ç‡ |
| `batch_size` | 5000 | ãƒãƒƒãƒã‚µã‚¤ã‚º |
| `gradient_clip` | 2.0 | å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤ |
| `context_noise` | 0.1 | ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºï¼ˆæ±åŒ–æ€§èƒ½å‘ä¸Šï¼‰ |
| `early_stopping_threshold` | 0.9 | åæŸç‡90%ã§æ—©æœŸåœæ­¢ |

### shifted_prev_contextæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# âŒ ç¦æ­¢: é †æ¬¡å‡¦ç†ï¼ˆéå¸¸ã«é…ã„ï¼‰
for i in range(num_tokens):
    new_context = model.forward_context(prev_context, token_embed)
    prev_context = new_context

# âœ… å¿…é ˆ: shifted_prev_contextæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
for iteration in range(max_iterations):
    # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰é–‹å§‹
    init_ctx = torch.zeros(1, context_dim)
    shifted_prev_context = torch.cat([init_ctx, previous_contexts[:-1]], dim=0)

    # ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†
    new_contexts = model.context_block(shifted_prev_context, token_embeds)
    previous_contexts = new_contexts
```

### å‹¾é…ç´¯ç©ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# ãƒãƒƒãƒã”ã¨ã«å‹¾é…ã‚’è¨ˆç®—ãƒ»ç´¯ç©
optimizer.zero_grad()
for batch in batches:
    loss = oacd_loss(batch_output)
    scaled_loss = loss / num_batches  # ãƒãƒƒãƒæ•°ã§å‰²ã‚‹
    scaled_loss.backward()  # å‹¾é…ç´¯ç©

# æœ€å¾Œã«ã¾ã¨ã‚ã¦æ›´æ–°
torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
optimizer.step()
```

### CPU/GPUãƒ¡ãƒ¢ãƒªåˆ†é›¢ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# token_embedsã¨previous_contextsã¯CPUã«ä¿æŒ
token_embeds = token_embeds_gpu.cpu()
previous_contexts = contexts.cpu()

# ãƒãƒƒãƒã”ã¨ã«GPUã«è»¢é€ã—ã¦å‡¦ç†
for start_idx in range(0, num_tokens, batch_size):
    batch_contexts = previous_contexts[start_idx:end_idx].to(device)
    batch_embeds = token_embeds[start_idx:end_idx].to(device)

    # å‡¦ç†å¾Œã¯å³åº§ã«CPUã«æˆ»ã™
    all_contexts_cpu.append(batch_output.detach().cpu())
```

### åæŸç‡è¨ˆç®—ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
def compute_convergence_rate(current, previous, threshold=0.03):
    token_losses = ((current - previous) ** 2).mean(dim=1)
    converged_count = (token_losses < threshold).sum()
    return converged_count / len(current)
```

---

## ğŸ¯ OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (Phase 1)

**Phase 1ã§ã¯OACD (Origin-Anchored Centroid Dispersion) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¡ç”¨ã€‚**

```python
def oacd_loss(contexts, centroid_weight=0.1):
    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–
    dispersion_loss = -||X - mean(X)|| / n

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = ||mean(X)||Â²

    return dispersion_loss + centroid_weight * centroid_loss
```

---

## ğŸš¨ CPU/GPUãƒ†ãƒ³ã‚½ãƒ«ç®¡ç†

**å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§OOMã‚’é˜²ããŸã‚ã€ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚’å¾¹åº•ã€‚**

```python
# âŒ ä¿®æ­£å‰
batch_contexts = previous_contexts[start_idx:end_idx].detach()

# âœ… ä¿®æ­£å¾Œ
batch_contexts = previous_contexts[start_idx:end_idx].detach().to(self.device)
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒã®Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports

# å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
python3 -m ruff check scripts/experiment_context_kv.py
python3 -m mypy scripts/experiment_context_kv.py --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Core Components

**1. ContextKVAttentionLLM**
- è¤‡æ•°ã®ContextBlockï¼ˆå„1å±¤å›ºå®šï¼‰
- Context-KV Attention Layer
- Token Embedding: GPT-2 pretrained (768-dim, frozen)
- Weight Tying: token_output shares weights with token_embedding

**2. ContextBlock**
- 1å±¤å›ºå®šã€Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze
- OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¤šæ§˜æ€§å­¦ç¿’

**3. Context-KV Attention**
- Contextã‚’K,Vã«å¤‰æ›
- ãƒãƒ£ãƒ³ã‚¯å˜ä½ã®contextã§Attention

### Phase 1: å¤šæ§˜æ€§å­¦ç¿’ï¼ˆOACDï¼‰

- **å­¦ç¿’å¯¾è±¡**: ContextBlockã®ã¿
- **æå¤±**: OACDï¼ˆå¤šæ§˜æ€§æå¤±ï¼‰

### Phase 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬

- **ContextBlock**: frozenï¼ˆé‡ã¿å›ºå®šï¼‰
- **Context-KV Attention + FFN**: å­¦ç¿’
- **æå¤±**: CrossEntropyï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰

---

## Code Quality Standards

### Principles

1. **No Hardcoding**: All hyperparameters in config.py
2. **Single Responsibility**: Each module has one clear purpose
3. **Type Hints Required**: é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å‹æ³¨é‡ˆã‚’å¿…é ˆ

---

## File Structure

**Main Scripts**:
- `scripts/experiment_context_kv.py` - Context-KV Attentionå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**Core Implementation**:
- `src/models/context_kv.py` - ContextKVAttentionLLM
- `src/models/blocks.py` - ContextBlockï¼ˆ1å±¤å›ºå®šï¼‰
- `src/models/layers.py` - ContextLayer
- `src/trainers/phase1/memory.py` - Phase 1è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯
- `src/losses/diversity.py` - OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-04 | Phase 1ä»•æ§˜ã®è©³ç´°ã‚’è¿½è¨˜ï¼ˆPythiaçµ±åˆå¤±æ•—ã‹ã‚‰ã®å¾©æ—§å¾Œï¼‰ |
| 2025-12-03 | Context-KV Attentionæ–¹å¼ã«å®Œå…¨ç§»è¡Œ |

---

Last Updated: 2025-12-04
