# New-LLM Project Guidelines

---

## ğŸ¯ Infini-Pythia Architecture (Memory-Only)

**Pythia-70Mãƒ™ãƒ¼ã‚¹ã«1å±¤ç›®Infini-Attentionï¼ˆMemory-Onlyï¼‰ã‚’å°å…¥ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Infini-Pythia:
Token Embedding (512-dim)
       â†“
Layer 0: InfiniAttentionLayer (NoPE, Memory Only)
  â””â”€ Memory Attention (linear attention)
       â†“
Layer 1-5: PythiaLayer (RoPE)
  â”œâ”€ Multi-Head Attention
  â””â”€ MLP
       â†“
Output Head (512 â†’ vocab)
```

### Infini-Attention (Memory-Only)

```
ãƒ¡ãƒ¢ãƒªæ›´æ–° (Delta Rule):
  M_s = M_{s-1} + Ïƒ(K)^T @ (V - retrieved_V)

ãƒ¡ãƒ¢ãƒªå–å¾—:
  A_mem = Ïƒ(Q) @ M / (Ïƒ(Q) @ z)

Ïƒ(x) = ELU(x) + 1
```

### Multi-Memory Bank

```python
# è¤‡æ•°ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ã§æƒ…å ±æ··åˆã‚’ä½æ¸›
model = InfiniPythiaModel(
    num_memory_banks=2,      # 2ã¤ã®ãƒãƒ³ã‚¯
    segments_per_bank=4,     # 4ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§æ¬¡ãƒãƒ³ã‚¯ã«åˆ‡æ›¿
)
```

### ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ãƒ¡ãƒ¢ãƒªï¼ˆé‡è¦ï¼‰

Linear Attentionã¯head_dimãŒå°ã•ã„ã¨æ©Ÿèƒ½ã—ãªã„ã€‚
ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆmemory_head_dim=hidden_size=512ï¼‰ã§æœ€å¤§ã®è¡¨ç¾åŠ›ã‚’ç¢ºä¿ã€‚

```python
# ãƒ¡ãƒ¢ãƒªè¡Œåˆ—: [memory_head_dim, memory_head_dim] = [512, 512]
# Q, K, V: Linear(hidden_size â†’ memory_head_dim)
# Output: Linear(memory_head_dim â†’ hidden_size)
```

---

## ğŸ­ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒª

`create_model()`ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ä½œæˆã€‚

```python
from src.models import create_model

# åŸºæœ¬çš„ãªä½¿ã„æ–¹
model = create_model("pythia")       # æ¨™æº–Pythia
model = create_model("infini")       # Infini-Pythia
model = create_model("multi_memory") # Multi-Memory
model = create_model("hierarchical") # Hierarchical

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ã
model = create_model("multi_memory", num_memories=8)
model = create_model("hierarchical", num_memories=4, use_delta_rule=False)

# ã‚«ã‚¹ã‚¿ãƒ config
from config.pythia import PythiaConfig
config = PythiaConfig()
model = create_model("infini", config)
```

### åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | å¯¾è±¡ãƒ¢ãƒ‡ãƒ« | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------------|------------|------------|------|
| `use_delta_rule` | å…¨memoryç³» | `True` | Delta Ruleä½¿ç”¨ |
| `num_memories` | multi_memory, hierarchical | `4` | ãƒ¡ãƒ¢ãƒªæ•° |
| `num_memory_banks` | infini | `1` | ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯æ•° |
| `segments_per_bank` | infini | `4` | ãƒãƒ³ã‚¯ã‚ãŸã‚Šã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•° |

---

## ğŸ’¾ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ä¿å­˜ãƒ»è»¢é€

åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’åˆ¥PCã«è»¢é€å¯èƒ½ã€‚

```python
import torch
from src.models import create_model

# ===== PC A =====
model = create_model("infini")
model.reset_memory()

# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªã‚’è“„ç©
for batch in data_loader:
    _ = model(batch, update_memory=True)

# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’ä¿å­˜
state = model.get_memory_state()
torch.save(state, "memory.pt")

# ===== PC B =====
# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿
state = torch.load("memory.pt")
model = create_model("infini")
model.set_memory_state(state)

# ãƒ¡ãƒ¢ãƒªãŒå¼•ãç¶™ãŒã‚ŒãŸçŠ¶æ…‹ã§æ¨è«–
output = model(input_ids)
```

### ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ã‚­ãƒ¼

| ãƒ¢ãƒ‡ãƒ« | ã‚­ãƒ¼ |
|--------|------|
| `InfiniPythiaModel` | `memories`, `memory_norms`, `current_bank`, `segment_counter` |
| `MultiMemoryInfiniPythiaModel` | `memories`, `memory_norms`, `current_memory_idx` |
| `HierarchicalMemoryPythiaModel` | `fine_memories`, `fine_memory_norms`, `current_memory_idx` |

### ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º |
|--------|--------|
| Infini (1 bank) | ~135 KB |
| Multi-Memory (4) | ~540 KB |
| Hierarchical (4) | ~540 KB |

---

## ğŸ§ª çµ±ä¸€å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å…¨ãƒ¢ãƒ‡ãƒ«ã‚’çµ±ä¸€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å®Ÿé¨“å¯èƒ½ã€‚

```bash
# å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
python3 scripts/experiment.py --models pythia infini multi_memory hierarchical

# Infiniã®ã¿
python3 scripts/experiment.py --models infini

# Multi-Memoryã¨Hierarchicalæ¯”è¼ƒ
python3 scripts/experiment.py --models multi_memory hierarchical --num-memories 4

# è¨­å®šã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
python3 scripts/experiment.py --models infini --samples 10000 --epochs 50 --lr 5e-5

# 8ãƒ¡ãƒ¢ãƒªã§å®Ÿé¨“
python3 scripts/experiment.py --models hierarchical --num-memories 8
```

### ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—

| ã‚¿ã‚¤ãƒ— | èª¬æ˜ |
|--------|------|
| `pythia` | æ¨™æº–Pythia (RoPE) |
| `infini` | Infini-Pythia (1å±¤ç›®Infini + RoPE) |
| `multi_memory` | Multi-Memory (è¤‡æ•°ç‹¬ç«‹ãƒ¡ãƒ¢ãƒª) |
| `hierarchical` | Hierarchical (éšå±¤çš„ãƒ¡ãƒ¢ãƒª) |

### ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰ã®ä½¿ç”¨

```python
from src.utils.experiment_runner import (
    ExperimentConfig,
    ModelType,
    run_experiment,
)

# è¨­å®š
config = ExperimentConfig(
    num_samples=5000,
    seq_length=256,
    num_epochs=30,
    num_memories=4,
)

# å®Ÿé¨“å®Ÿè¡Œ
results = run_experiment(
    model_types=[ModelType.INFINI, ModelType.HIERARCHICAL],
    exp_config=config,
)
```

---

## ğŸ“Š Reversal Curse è©•ä¾¡

### æ¦‚è¦

Reversal Curseã¯ã€ŒA is Bã€ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã€ŒB is Aã€ã‚‚æ¨è«–ã§ãã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹æŒ‡æ¨™ã€‚

### æŒ‡æ¨™

| æŒ‡æ¨™ | å®šç¾© | è§£é‡ˆ |
|------|------|------|
| Forward PPL | é †æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãŸã‚ä½ã„ |
| Backward PPL | é€†æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ãŸã‚é«˜ã„ |
| Reversal Ratio | Forward / Backward | 1.0ã«è¿‘ã„ã»ã©è‰¯ã„ |
| Reversal Gap | Backward - Forward | 0ã«è¿‘ã„ã»ã©è‰¯ã„ |

### å®Ÿè£…

```python
from src.utils.evaluation import evaluate_reversal_curse
from src.data.reversal_pairs import get_reversal_pairs

tokenizer = get_tokenizer(config.tokenizer_name)
reversal_pairs = get_reversal_pairs()
reversal_result = evaluate_reversal_curse(model, tokenizer, reversal_pairs, device)
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

### Reversal Curseè©•ä¾¡å¿…é ˆ

**ã™ã¹ã¦ã®å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã€Reversal Curseè©•ä¾¡ã‚’å¿…ãšå®Ÿè¡Œã™ã‚‹ã“ã¨ã€‚**

---

## âš ï¸ éå»ã®ãƒã‚°ã¨æ•™è¨“

### 1. Infini-Attention ãƒ¡ãƒ¢ãƒªå‹¾é…ãƒã‚°

```python
# âŒ ãƒã‚°: ãƒ¡ãƒ¢ãƒªæ›´æ–°ã§ã‚°ãƒ©ãƒ•ãŒæ®‹ã‚Šã€äºŒé‡backwardã‚¨ãƒ©ãƒ¼
self.memory = self.memory + memory_update

# âœ… ä¿®æ­£: detach()ã§ã‚°ãƒ©ãƒ•ã‚’åˆ‡æ–­
self.memory = (self.memory + memory_update).detach()
```

### 2. PPLç•°å¸¸å€¤ã®è¨ºæ–­åŸºæº–

| PPL | çŠ¶æ…‹ | å¯¾å‡¦ |
|-----|------|------|
| < 5 | **ç•°å¸¸** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯/å› æœãƒã‚¹ã‚¯ãƒã‚° | ã‚³ãƒ¼ãƒ‰ç‚¹æ¤œå¿…é ˆ |
| 5-30 | **ç–‘ã‚ã—ã„** - éå­¦ç¿’ã®å¯èƒ½æ€§ | ãƒ‡ãƒ¼ã‚¿é‡ãƒ»åˆ†å‰²ã‚’ç¢ºèª |
| 30-100 | æ­£å¸¸ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ | - |
| 100-500 | æ­£å¸¸ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼‰ | - |
| > 1000 | å­¦ç¿’ä¸è¶³ | epochå¢—åŠ /lrèª¿æ•´ |

### 3. Long Contextè©•ä¾¡ã§untrained modelã‚’ä½¿ç”¨

```python
# âŒ ãƒã‚°: æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¦è©•ä¾¡
pythia_model = PythiaModel(...)  # æœªè¨“ç·´ã®ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
result = evaluate_long_documents(pythia_model, ...)

# âœ… ä¿®æ­£: è¨“ç·´æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰
pythia_model = PythiaModel(...)
pythia_model.load_state_dict(results["pythia"]["model_state_dict"])
result = evaluate_long_documents(pythia_model, ...)
```

### 5. Linear Attentionã®head_dimæ¬¡å…ƒæ•°å•é¡Œï¼ˆé‡è¦ï¼‰

**head_dimãŒå°ã•ã„ã¨Linear AttentionãŒæ©Ÿèƒ½ã—ãªã„ã€‚**

#### å•é¡Œ

```python
# âŒ å•é¡Œ: head_dim=64ï¼ˆhidden_size=512 / num_heads=8ï¼‰
head_dim = hidden_size // num_heads  # = 64

# ãƒ¡ãƒ¢ãƒªè¡Œåˆ—: [num_heads, head_dim, head_dim] = [8, 64, 64]
# â†’ 64æ¬¡å…ƒç©ºé–“ã§ã¯ç•°ãªã‚‹ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ãŒç›´äº¤ã—ã‚„ã™ã„
# â†’ Ïƒ(Q) @ Ïƒ(K)^T â‰ˆ 0 â†’ ãƒ¡ãƒ¢ãƒªã‹ã‚‰ä½•ã‚‚å–ã‚Šå‡ºã›ãªã„
```

#### åŸå› : ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ã®ç›´äº¤æ€§

Linear Attentionã§ã¯ `Ïƒ(K)` ã‚’ä½¿ã£ã¦ãƒ¡ãƒ¢ãƒªã«æ›¸ãè¾¼ã¿ã€`Ïƒ(Q)` ã§å–ã‚Šå‡ºã™ï¼š

```
ãƒ¡ãƒ¢ãƒªæ›´æ–°: M = M + Ïƒ(K)^T @ V
ãƒ¡ãƒ¢ãƒªå–å¾—: output = Ïƒ(Q) @ M / (Ïƒ(Q) @ z)
```

**64æ¬¡å…ƒç©ºé–“ã ã¨**ï¼š
- ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®Kãƒ™ã‚¯ãƒˆãƒ«ãŒé«˜ç¢ºç‡ã§ç›´äº¤
- `Ïƒ(Q) @ Ïƒ(K)^T` ãŒã»ã¼ã‚¼ãƒ­
- ãƒ¡ãƒ¢ãƒªã‹ã‚‰æœ‰ç”¨ãªæƒ…å ±ãŒå–ã‚Šå‡ºã›ãªã„
- ãƒ¢ãƒ‡ãƒ«ãŒãƒ¡ãƒ¢ãƒªã‚’ç„¡è¦–ã™ã‚‹æ–¹å‘ã«å­¦ç¿’ â†’ alphaãŒå°ã•ã„ã¾ã¾

#### è§£æ±ºç­–: ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆmemory_head_dim=hidden_sizeï¼‰

```python
# âœ… ä¿®æ­£: memory_head_dim=hidden_sizeï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼‰
memory_head_dim = hidden_size  # = 512

# ãƒ¡ãƒ¢ãƒªè¡Œåˆ—: [memory_head_dim, memory_head_dim] = [512, 512]
# â†’ 512æ¬¡å…ƒç©ºé–“ã§ååˆ†ãªè¡¨ç¾åŠ›ã‚’ç¢ºä¿
# â†’ ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«é–“ã®é¡ä¼¼åº¦ãŒè¨ˆç®—å¯èƒ½
```

#### æ•™è¨“

1. **Linear Attentionã«ã¯head_dim >= 256ãŒå¿…è¦**
2. **å¯èƒ½ãªã‚‰ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆhead_dim=hidden_sizeï¼‰ã‚’ä½¿ç”¨**
3. **alphaãŒå°ã•ã„ã¾ã¾ â†’ head_dimã‚’ç–‘ã†**
4. **ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã¯è¡¨ç¾åŠ›ã‚’çŠ ç‰²ã«ã™ã‚‹**

### 4. PPLè©•ä¾¡æ–¹æ³•ã«ã‚ˆã‚‹PPLç•°å¸¸å€¤ï¼ˆé‡è¦ï¼‰

**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²è©•ä¾¡ã§ç•°å¸¸ã«é«˜ã„PPLï¼ˆ10,000+ï¼‰ãŒå‡ºã‚‹åŸå› ã¨å¯¾ç­–ã€‚**

#### å•é¡Œã®ç™ºè¦‹çµŒç·¯

Pythia-70mã®å…¬å¼WikiText-2 PPLã¯ç´„32-35ã ãŒã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²è©•ä¾¡ã§ã¯14,000+ã«ãªã£ãŸã€‚

#### åŸå› : ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ã§ã®äºˆæ¸¬

```python
# âŒ å•é¡Œã®ã‚ã‚‹è©•ä¾¡æ–¹æ³•ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã€éé‡è¤‡ï¼‰
for start in range(0, seq_len, segment_length):
    segment = tokens[start:end]
    input_ids = segment[:-1].unsqueeze(0)  # å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒç‹¬ç«‹
    labels = segment[1:].unsqueeze(0)
    outputs = model(input_ids, labels=labels)
    # â†’ å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹éš›ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãªã„
    # â†’ ã€Œæ–‡æ›¸ã®é€”ä¸­ã€ã‚’ã€Œæ–‡æ›¸ã®å…ˆé ­ã€ã¨ã—ã¦æ‰±ã†ãŸã‚é«˜PPL
```

#### æ­£ã—ã„è©•ä¾¡æ–¹æ³•: Sliding Window

```python
# âœ… æ­£ã—ã„è©•ä¾¡æ–¹æ³•ï¼ˆSliding Windowï¼‰
stride = 512
context_length = 2048

for start in range(0, seq_len - 1, stride):
    end = min(start + context_length, seq_len)
    input_ids = tokens[start:end].unsqueeze(0)

    # æœ€åˆã®strideå€‹ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆlossè¨ˆç®—ã—ãªã„ï¼‰
    labels = input_ids.clone()
    labels[0, :stride] = -100  # -100ã¯lossè¨ˆç®—ã‹ã‚‰é™¤å¤–

    outputs = model(input_ids, labels=labels)
```

#### PPLæ¯”è¼ƒï¼ˆWikiText-2ã§ã®å®Ÿæ¸¬å€¤ï¼‰

| è©•ä¾¡æ–¹æ³• | PPL | è§£é‡ˆ |
|----------|-----|------|
| Sliding window (stride=512) | **40.96** | âœ“ æ­£å¸¸ï¼ˆå…¬å¼å€¤ã«è¿‘ã„ï¼‰ |
| Simple non-overlapping (2048) | 15,885 | âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—å•é¡Œ |
| Segment-based (256 tokens) | 14,204 | âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—å•é¡Œ |

#### æ•™è¨“

1. **PPLè©•ä¾¡ã¯å¿…ãšSliding windowæ–¹å¼ã‚’ä½¿ç”¨**
2. **ç•°å¸¸ã«é«˜ã„PPLï¼ˆ1000+ï¼‰ãŒå‡ºãŸã‚‰è©•ä¾¡æ–¹æ³•ã‚’ç–‘ã†**
3. **è¨“ç·´æ™‚ã¨è©•ä¾¡æ™‚ã§ç•°ãªã‚‹æ–¹æ³•ã‚’ä½¿ã†ã¨æ¯”è¼ƒãŒä¸æ­£ç¢ºã«ãªã‚‹**
4. **Pythia-70mã®WikiText-2 PPLã¯ç´„32-35ãŒæ­£å¸¸**

---

## ğŸ”§ Pretrained LLMã¸ã®Infini-Attentionå°å…¥ï¼ˆå¤±æ•—ï¼‰

**âš ï¸ Layerç½®ãæ›ãˆæ–¹å¼ã¯å…¨ã¦å¤±æ•—ã€‚è©³ç´°ã¯ `docs/experiments/2025-12-06_distill_finetune_failure.md` ã‚’å‚ç…§ã€‚**

### è©¦è¡Œã—ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨çµæœ

| æ–¹å¼ | èª¬æ˜ | çµæœ |
|------|------|------|
| Layer 0ç½®ãæ›ãˆ | Layer 0ã‚’Infini Layerã«å®Œå…¨ç½®ãæ›ãˆ | âŒ RoPEæå¤±ã€PPLå¤§å¹…åŠ£åŒ– |
| 2æ®µéšè¨“ç·´ï¼ˆè’¸ç•™+Fine-tuneï¼‰ | è’¸ç•™å¾Œã«Full Fine-tune | âŒ PPL 44â†’1237ï¼ˆ28å€åŠ£åŒ–ï¼‰ |
| Parallel Adapter | ä¸¦åˆ—ã§Infiniè¿½åŠ  | âŒ alphaãŒå­¦ç¿’ã•ã‚Œãªã„ |

### å¤±æ•—ã®æ ¹æœ¬åŸå› 

1. **RoPEæƒ…å ±ã®å–ªå¤±**: Linear Attention (NoPE)ã¯ä½ç½®æƒ…å ±ã‚’æŒãŸãªã„
2. **å¾Œç¶šãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã®ä¸æ•´åˆ**: Layer 1-5ã¯RoPEä»˜ãå…¥åŠ›ã‚’å‰æã«è¨“ç·´ã•ã‚Œã¦ã„ã‚‹
3. **è’¸ç•™ã®é™ç•Œ**: å‡ºåŠ›ã‚’æ¨¡å€£ã—ã¦ã‚‚å†…éƒ¨è¡¨ç¾ã¯è¤‡è£½ã§ããªã„

### æ—¢å­˜ç ”ç©¶ã®çŸ¥è¦‹

- **Google Infini-Attention**: **å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼**ã‚’ç½®æ› + 30K stepsç¶™ç¶šäº‹å‰è¨“ç·´ã§æˆåŠŸ
- **Hugging Faceå®Ÿé¨“**: Llama 3 8Bã§ã®å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ç½®æ›ã¯å¤±æ•—
- **çµè«–**: å˜ä¸€ãƒ¬ã‚¤ãƒ¤ãƒ¼ç½®æ›ã¯æŠ€è¡“çš„ã«å›°é›£ã€‚å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ç½®æ›ã‹ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ãŒå¿…è¦

### æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

| ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ | èª¬æ˜ |
|------------|------|
| **ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´** | æœ€åˆã‹ã‚‰Infiniã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§è¨“ç·´ï¼ˆæœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®InfiniPythiaï¼‰ |
| **RoPE Scaling** | YaRNç­‰ã®æ—¢å­˜æ‰‹æ³•ã§é•·æ–‡å¯¾å¿œ |
| **å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ç½®æ›** | å¤§è¦æ¨¡ç¶™ç¶šäº‹å‰è¨“ç·´ãŒå¿…è¦ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆå¤§ï¼‰ |

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pythia.py                   # PythiaConfig
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiment.py               # çµ±ä¸€å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ reversal_pairs.py       # Reversal Curseè©•ä¾¡ãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py             # create_model() ãƒ•ã‚¡ã‚¯ãƒˆãƒª
â”‚   â”‚   â”œâ”€â”€ pythia.py               # PythiaModel (RoPE)
â”‚   â”‚   â”œâ”€â”€ infini_attention.py     # InfiniAttention, InfiniAttentionLayer
â”‚   â”‚   â”œâ”€â”€ infini_pythia.py        # InfiniPythiaModel (1å±¤Infini + RoPE)
â”‚   â”‚   â”œâ”€â”€ multi_memory_attention.py  # MultiMemoryInfiniAttention
â”‚   â”‚   â”œâ”€â”€ multi_memory_pythia.py  # MultiMemoryInfiniPythiaModel
â”‚   â”‚   â”œâ”€â”€ hierarchical_memory.py  # HierarchicalMemoryAttention
â”‚   â”‚   â””â”€â”€ hierarchical_pythia.py  # HierarchicalMemoryPythiaModel
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ experiment_runner.py    # çµ±ä¸€å®Ÿé¨“ãƒ©ãƒ³ãƒŠãƒ¼
â”‚       â”œâ”€â”€ training.py             # å…±é€šå­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚       â”œâ”€â”€ evaluation.py           # è©•ä¾¡é–¢æ•°
â”‚       â”œâ”€â”€ device.py               # ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”‚       â”œâ”€â”€ data_pythia.py          # Pileãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
â”‚       â””â”€â”€ seed.py                 # ã‚·ãƒ¼ãƒ‰è¨­å®š
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ experiments/                # å®Ÿé¨“çµæœ
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-06 | **Layerç½®ãæ›ãˆæ–¹å¼ã‚’å‰Šé™¤**: è’¸ç•™+Fine-tuneç­‰ã™ã¹ã¦å¤±æ•—ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ã«é›†ä¸­ |
| 2025-12-06 | **ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ãƒ¡ãƒ¢ãƒªå°å…¥**: memory_head_dim=512ã§Linear Attentionã®è¡¨ç¾åŠ›ã‚’æœ€å¤§åŒ–ã€ALiBiç‰ˆå‰Šé™¤ |
| 2025-12-06 | **PPLè©•ä¾¡æ–¹æ³•ã®æ•™è¨“è¿½åŠ **: Sliding windowæ–¹å¼ãŒæ­£ã—ã„ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²ã¯é«˜PPLã«ãªã‚‹ |
| 2025-12-06 | **WikiText-2è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¿½åŠ **: æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®æ­£ç¢ºãªPPLè©•ä¾¡ |
| 2025-12-06 | **ãƒ¡ãƒ¢ãƒªè»¢é€APIè¿½åŠ **: get_memory_state/set_memory_stateã§åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’åˆ¥PCã«è»¢é€å¯èƒ½ |
| 2025-12-06 | **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªè¿½åŠ **: create_model()ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ä½œæˆ |
| 2025-12-06 | **å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ±ä¸€**: experiment.pyã«çµ±åˆã€experiment_runner.pyè¿½åŠ  |
| 2025-12-06 | **Hierarchical Memoryè¿½åŠ **: å­¦ç¿’å¯èƒ½ãªå±•é–‹åˆ¤æ–­ã€Coarse-to-Fineæ¤œç´¢ |
| 2025-12-06 | **Multi-Memory Attentionè¿½åŠ **: Attention-basedé¸æŠã§è¤‡æ•°ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„æ··åˆ |
| 2025-12-05 | **Memory-Onlyã«é›†ä¸­**: Local Attentionå‰Šé™¤ã€ã‚³ãƒ¼ãƒ‰ç°¡ç´ åŒ– |
| 2025-12-05 | **Multi-Memory Bankè¿½åŠ **: è¤‡æ•°ãƒãƒ³ã‚¯ã§æƒ…å ±æ··åˆä½æ¸› |
| 2025-12-05 | **Long Contextè©•ä¾¡ãƒã‚°ä¿®æ­£**: è¨“ç·´æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ |
| 2025-12-05 | **Infini-Pythiaå®Ÿè£…**: 1å±¤ç›®Infini + RoPE |

---

Last Updated: 2025-12-06
