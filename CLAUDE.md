# New-LLM Project Guidelines

## ğŸ—ï¸ å…±æœ‰è¨­å®šã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨ - ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Œäº† (2025-12-01)

**å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–“ã§è¨­å®šã‚¯ãƒ©ã‚¹ã‚’å…±æœ‰åŒ–ã—ã€ã‚³ãƒ¼ãƒ‰é‡è¤‡ã‚’è§£æ¶ˆã€‚**

### å…±æœ‰è¨­å®šã‚¯ãƒ©ã‚¹

```python
# src/experiments/config.py ã‹ã‚‰ä½¿ç”¨
from src.experiments.config import DataConfig, Phase1Config, Phase2Config

# ãƒ‡ãƒ¼ã‚¿è¨­å®š
data_config = DataConfig.from_base(base_config, num_samples=100)

# Phase 1è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
phase1_config = Phase1Config.from_base(
    base_config, device,
    context_dim=1000,
    dist_reg_weight=0.9,  # å¤šæ§˜æ€§90%, CVFP 10%ï¼ˆæ¨™æº–è¨­å®šï¼‰
)

# Phase 2è¨­å®š
phase2_config = Phase2Config.from_base(
    base_config, device,
    context_dim=1000,
)
```

### å¯¾å¿œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

- `src/experiments/runner.py` - ExperimentRunner
- `scripts/diversity_algorithm_experiment.py` - Phase 1ã®ã¿
- `scripts/diversity_full_experiment.py` - Phase 1 + Phase 2 + Î±åˆ†æ

---

## ğŸš¨ num_layers = 1 å›ºå®šãƒ«ãƒ¼ãƒ« - çµ¶å¯¾éµå®ˆ (2025-12-01)

**`config.py`ã®`num_layers`ã¯1ã«å›ºå®šã€‚å¤‰æ›´ç¦æ­¢ã€‚**

```python
# config.py - çµ¶å¯¾ã«å¤‰æ›´ã—ãªã„
num_layers = 1  # å›ºå®š
```

**ç†ç”±**:
- ç¾åœ¨ã®å®Ÿé¨“ãƒ•ã‚§ãƒ¼ã‚ºã§ã¯1ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ¤œè¨¼ä¸­
- ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°å¤‰æ›´ã¯æ˜ç¤ºçš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã®ã¿

---

## ğŸ’» ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿé¨“ã®æ³¨æ„äº‹é … - CPUç’°å¢ƒ (2025-12-01)

**ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼ˆMac/CPUï¼‰ã§ã¯å‡¦ç†ãŒé…ã„ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã€‚**

### æ¨å¥¨è¨­å®š

```bash
# ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿé¨“ï¼ˆCPUï¼‰: 2-5ã‚µãƒ³ãƒ—ãƒ«ã§ååˆ†
python3 scripts/diversity_algorithm_experiment.py -a MCDL -s 2

# Colabï¼ˆGPUï¼‰: 100ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šã§æœ¬æ ¼å®Ÿé¨“
python3 scripts/diversity_algorithm_experiment.py -a MCDL ODCM -s 100
```

### ãƒ­ãƒ¼ã‚«ãƒ« vs Colab æ¯”è¼ƒ

| ç’°å¢ƒ | æ¨å¥¨ã‚µãƒ³ãƒ—ãƒ«æ•° | å‡¦ç†æ™‚é–“ç›®å®‰ |
|------|--------------|-------------|
| **ãƒ­ãƒ¼ã‚«ãƒ«ï¼ˆCPUï¼‰** | 2-5 | æ•°åˆ†ã€œåæ•°åˆ† |
| **Colabï¼ˆGPUï¼‰** | 100-500 | æ•°åˆ† |

### æ³¨æ„

- ãƒ­ãƒ¼ã‚«ãƒ«ã§ã¯å‹•ä½œç¢ºèªç¨‹åº¦ã«ã¨ã©ã‚ã‚‹
- æœ¬æ ¼çš„ãªå®Ÿé¨“ã¯Colabç’°å¢ƒã§å®Ÿè¡Œ
- ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ã¨Effective RankãŒä¸æ­£ç¢ºã«ãªã‚‹å¯èƒ½æ€§ã‚ã‚Š

---

## ğŸ¯ å¤šæ§˜æ€§æå¤±ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆ2ç¨®é¡æ¡ç”¨ï¼‰(2025-12-01)

**Phase 1ã§ä½¿ç”¨ã—ã¦ã„ã‚‹å¤šæ§˜æ€§ç¢ºä¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚**

### æ¡ç”¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ä¸€è¦§

| åç§° | èª¬æ˜ | è¨ˆç®—ã‚³ã‚¹ãƒˆ |
|------|------|-----------|
| **MCDL** | Mean-Centered Dispersion Lossï¼ˆç¾è¡Œãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰ | O(nÃ—d) æœ€é€Ÿ |
| **ODCM** | Off-Diagonal Covariance Minimizationï¼ˆVICRegé¢¨ã€æ¨å¥¨ï¼‰ | O(nÃ—d + dÂ²) |

### ğŸ”¬ MCDLãŒæœ€è‰¯ã§ã‚ã‚‹ç†ç”± - é‡è¦åˆ†æ (2025-12-01)

**å®Ÿé¨“çµæœ**: é€šå¸¸Layerç‰ˆ + context_dim=500 + dwr=0.9

| ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  | Val PPL | Acc | Î±å€¤ | Val ER |
|------------|---------|-----|-----|--------|
| **MCDL** | **289.1** | **20.3%** | -0.423 | 82.7% |
| ODCM | 308.5 | 19.6% | -0.568 | 86.7% |

**MCDLãŒæœ€è‰¯PPLã‚’é”æˆã—ãŸç†ç”±**:

1. **é©åº¦ãªå¤šæ§˜æ€§**: ER=82.7%ï¼ˆéå‰°ãª95%ã‚ˆã‚Šé©åˆ‡ï¼‰
2. **æ—©æœŸåœæ­¢**: 10 iterã§å®Œäº†ï¼ˆä»–ã¯45-60 iterï¼‰â†’ éå­¦ç¿’å›é¿
3. **è¨€èªæ§‹é€ ä¿æŒ**: çƒçŠ¶åˆ†å¸ƒã‚’å¼·åˆ¶ã—ãªã„ãŸã‚è‡ªç„¶ãªè¡¨ç¾ã‚’ç¶­æŒ
4. **å˜ç´”ãªç›®çš„é–¢æ•°**: å®‰å®šã—ãŸæœ€é©åŒ–ã€æ»‘ã‚‰ã‹ãªå‹¾é…

### ğŸ”¬ MCDLã®ç‰¹ç•°ãªåæŸç‰¹æ€§ - CVFPãªã—ã§ã‚‚åæŸ

**MCDLã ã‘ãŒCVFPç„¡åŠ¹(dwr=1.0)ã§ã‚‚åƒ…ã‹ã«åæŸã™ã‚‹**:

| è¨­å®š | MCDL | ODCM |
|------|------|------|
| dwr=1.0 (200s) | **2%** | 0% |
| dwr=0.9 (200s) | **7%** | 0% |

**ç†ç”±**: MCDLã®ã€Œè‡ªå·±å¹³è¡¡ã€åŠ¹æœ

```python
# MCDL: å¹³å‡ã‹ã‚‰ã®åå·®ã‚’æœ€å¤§åŒ–
L = -||X - mean(X)||

# å‹¾é…ã®æ–¹å‘: å¹³å‡ã‹ã‚‰é›¢ã‚Œã‚
# ã—ã‹ã—å…¨ç‚¹ãŒé›¢ã‚Œã‚‹ã¨å¹³å‡è‡ªä½“ã‚‚ç§»å‹•
# â†’ ã€Œè¿½ã„ã‹ã‘ã£ã“ã€ãŒå¹³è¡¡ç‚¹ã«åæŸ
```

**ä»–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒåæŸã—ãªã„ç†ç”±**:
- ODCM: ã€Œç„¡ç›¸é–¢ã«ã—ã‚ã€â†’ é™ç•Œãªãç™ºæ•£

**æ•™è¨“**:
- **ç›¸å¯¾çš„ç›®æ¨™ï¼ˆå¹³å‡ã‹ã‚‰ã®åå·®ï¼‰ã¯åæŸã—ã‚„ã™ã„**
- **çµ¶å¯¾çš„ç›®æ¨™ï¼ˆç„¡ç›¸é–¢ã€å‡ç­‰åˆ†å¸ƒï¼‰ã¯åæŸã—ã«ãã„**

### å®Ÿè£…å ´æ‰€

`src/losses/diversity.py`:
```python
from src.losses.diversity import (
    DIVERSITY_ALGORITHMS,      # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¾æ›¸
    ALGORITHM_DESCRIPTIONS,    # èª¬æ˜è¾æ›¸
)
```

### æ¯”è¼ƒå®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# Phase 1ã®ã¿ï¼ˆERæ¯”è¼ƒï¼‰
python3 scripts/diversity_algorithm_experiment.py -a MCDL ODCM -s 50 100

# Phase 1 + Phase 2ï¼ˆÎ±å€¤æ¯”è¼ƒï¼‰- CVFPç„¡åŠ¹
python3 scripts/diversity_full_experiment.py

# context_dimæŒ‡å®š
python3 scripts/diversity_algorithm_experiment.py -a MCDL ODCM -s 50 --context-dim 1000
```

### dist_reg_weightã®è¨­å®š

```python
# dist_reg_weight = 0.9 ã§å¤šæ§˜æ€§90%ã€CVFP 10%ï¼ˆæ¨å¥¨ãƒ»æ¨™æº–è¨­å®šï¼‰
# æå¤±è¨ˆç®—: (1-dist_reg_weight)*cvfp + dist_reg_weight*diversity
phase1_config = Phase1Config.from_base(
    base_config, device,
    dist_reg_weight=0.9,  # å¤šæ§˜æ€§90%, CVFP 10%ï¼ˆå®‰å®šã—ãŸè¨“ç·´ï¼‰
)
```

**æ³¨æ„**: `dist_reg_weight=1.0`ï¼ˆCVFPå®Œå…¨ç„¡åŠ¹ï¼‰ã¯è¨“ç·´ãŒä¸å®‰å®šã«ãªã‚Šæ—©æœŸåœæ­¢ã—ã‚„ã™ã„ã€‚0.9æ¨å¥¨ã€‚

---

## ğŸš¨ Effective Rankè¨ˆç®—ã®æ•´åˆæ€§ - é‡è¦æ•™è¨“ (2025-12-01)

**Phase 1 Validation Early Stoppingã®Val ERã¨æœ€çµ‚è©•ä¾¡ã®ERãŒå¤§å¹…ã«ä¹–é›¢ã™ã‚‹å•é¡Œã‚’ä¿®æ­£ã€‚**

### å•é¡Œã®ç—‡çŠ¶

- `_quick_validate()` ãŒè¿”ã™Val ER: 3-30%
- æœ€çµ‚è©•ä¾¡ã®Val ER: 64%
- **ç´„2-20å€ã®ä¹–é›¢**

### æ ¹æœ¬åŸå› ï¼ˆ3ã¤ï¼‰

**1. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®é•ã„ï¼ˆæœ€å¤§ã®åŸå› ï¼‰**
- `_quick_validate()`: 500ãƒˆãƒ¼ã‚¯ãƒ³ â†’ **ERãŒä½ãå‡ºã‚‹**
- æœ€çµ‚è©•ä¾¡: 31,024ãƒˆãƒ¼ã‚¯ãƒ³ â†’ ERãŒæ­£ç¢ºã«å‡ºã‚‹
- **ä¿®æ­£**: `phase1_val_sample_size = 10000` ã«å¢—åŠ 

**2. ERè¨ˆç®—æ–¹æ³•ã®é•ã„**
- `_quick_validate()`: å…±åˆ†æ•£è¡Œåˆ—ã®å›ºæœ‰å€¤åˆ†è§£ã‚’ä½¿ç”¨
- `analyze_fixed_points()`: SVDã®ç‰¹ç•°å€¤ã‚’ä½¿ç”¨
- **ä¿®æ­£**: ä¸¡æ–¹ã¨ã‚‚SVDãƒ™ãƒ¼ã‚¹ã«çµ±ä¸€

**3. ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®é•ã„**
- `_quick_validate()`: `collect_all_layers=False`
- `evaluate()`: `collect_all_layers=True`ã€`token_embeds[:-1]`ã§æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ã
- **ä¿®æ­£**: `_quick_validate()`ã‚’`evaluate()`ã¨å®Œå…¨ã«åŒã˜å‡¦ç†ã«å¤‰æ›´

### ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰

```python
def _quick_validate(self, val_token_ids: torch.Tensor) -> float:
    self.model.eval()
    sample_size = min(len(val_token_ids), self.config.phase1_val_sample_size)
    sample_ids = val_token_ids[:sample_size]

    with torch.no_grad():
        token_embeds = self.model.token_embedding(sample_ids.unsqueeze(0).to(self.device))
        token_embeds = self.model.embed_norm(token_embeds).squeeze(0)
        input_token_embeds = token_embeds[:-1]  # evaluate()ã¨åŒã˜
        contexts, _, _ = self._forward_sequential(input_token_embeds, None, collect_all_layers=True)
        effective_rank = self._compute_effective_rank(contexts)  # SVDãƒ™ãƒ¼ã‚¹

    self.model.train()
    return effective_rank
```

### æ•™è¨“

1. **ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã¯ERè¨ˆç®—ã«å¤§ããå½±éŸ¿**: 500ãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯ä¸ååˆ†ã€10000ä»¥ä¸Šæ¨å¥¨
2. **è¨ˆç®—æ–¹æ³•ã¯å®Œå…¨ã«çµ±ä¸€ã™ã¹ã**: SVD vs å›ºæœ‰å€¤åˆ†è§£ã§çµæœãŒç•°ãªã‚‹
3. **ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚‚å®Œå…¨ã«çµ±ä¸€ã™ã¹ã**: `collect_all_layers`ã‚„`[:-1]`ã®é•ã„ã§ã‚‚çµæœãŒå¤‰ã‚ã‚‹
4. **åŒã˜ã‚·ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã™ã‚‹ã¨ã€ç•°ãªã‚‹è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ä¼¼ãŸçµæœã«ãªã‚Šã‚„ã™ã„**

---

## ğŸ“ˆ Î±å€¤ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åˆ†ææ©Ÿèƒ½ (2025-11-30)

**ãƒ‡ãƒ¼ã‚¿é‡å¢—åŠ ã«ä¼´ã†Î±å€¤ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ï¼‰ã®å¤‰åŒ–ã‚’æ¸¬å®šå¯èƒ½ã«ã€‚**

### ä½¿ç”¨æ–¹æ³•

```bash
# åˆæœŸã‚µãƒ³ãƒ—ãƒ«50, å€ç‡2, ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦4ç‚¹, 2ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
python3 scripts/scaling_experiment.py --alpha-scaling \
  --init-samples 50 --multiplier 2 --window-size 4 --num-windows 2
```

**ç”Ÿæˆã•ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°**: `[50, 100, 200, 400, 800]`ï¼ˆ5ç‚¹ï¼‰

**ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦åˆ†æ**:
- æ¡å–æ¨™æœ¬1: `[50, 100, 200, 400]` â†’ Î±â‚
- æ¡å–æ¨™æœ¬2: `[100, 200, 400, 800]` â†’ Î±â‚‚

### å‡ºåŠ›ä¾‹

```
ALPHA PROGRESSION ANALYSIS (Sliding Window)
Window size: 4 points
================================================================================

1L_768d_1tok:
--------------------------------------------------------------------------------
  Window              Samples                    Tokens          Î±            A       RÂ²
--------------------------------------------------------------------------------
       1               50-400           62,891-487,654   -0.48123    1.07e+05   0.9934
       2              100-800          122,795-976,543   -0.49456    1.12e+05   0.9952

  Î± change: -0.48123 â†’ -0.49456 (Î” = -0.01333, -2.77%)
  Trend: â†“ IMPROVING (Î± becoming more negative = better scaling)
```

### å¼•æ•°ä¸€è¦§

| å¼•æ•° | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------|-----------|------|
| `--alpha-scaling` | - | Î±å€¤ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹åŒ– |
| `--init-samples` | 50 | åˆæœŸã‚µãƒ³ãƒ—ãƒ«æ•° |
| `--multiplier` | 2.0 | ã‚µãƒ³ãƒ—ãƒ«æ•°ã®å€ç‡ |
| `--window-size` | 4 | 1ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚ãŸã‚Šã®ç‚¹æ•° |
| `--num-windows` | 2 | ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ•° |

### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«

- `alpha_progression.json`: å„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®Î±å€¤ã€Aå€¤ã€RÂ²ã€ãƒˆãƒ¼ã‚¯ãƒ³ç¯„å›²

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒã®Lint/Type Check (2025-11-29)

**pyenvç’°å¢ƒã§ã¯ruffã‚„mypyã‚’ç›´æ¥å®Ÿè¡Œã§ããªã„ãŸã‚ã€`python3 -m` ã§å®Ÿè¡Œã™ã‚‹ã€‚**

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports

# ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿
python3 -m ruff check src/trainers/phase1/memory.py src/experiments/runner.py
python3 -m mypy src/trainers/phase1/memory.py src/experiments/runner.py --ignore-missing-imports
```

**æ³¨æ„**: `ruff` ã‚„ `mypy` ã‚’ç›´æ¥å®Ÿè¡Œã™ã‚‹ã¨ `command not found` ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚

---

## âš ï¸ COLABç’°å¢ƒãƒªã‚»ãƒƒãƒˆå¯¾ç­– (2025-11-29)

**Colabã¯é »ç¹ã«ç’°å¢ƒãŒãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¶ˆå¤±ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚**

### è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«

| ãƒ•ã‚¡ã‚¤ãƒ« | ç”¨é€” | è‡ªå‹•ç”Ÿæˆå…ƒ |
|----------|------|----------|
| `./data/example_val.txt` | æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ | `MemoryDataProvider._generate_val_file()` |
| `./cache/ultrachat_*samples_full.pt` | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ | `MemoryDataProvider._load_train_data()` |

### ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã®å ´æ‰€

**ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã¯`src/providers/data/memory.py`ã«é›†ç´„:**

```python
# src/providers/data/memory.py
class MemoryDataProvider:
    def _load_train_data(self, tokenizer):
        """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã‘ã‚Œã°ç”Ÿæˆï¼‰"""
        ...

    def _load_val_data(self, tokenizer):
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãªã‘ã‚Œã°è‡ªå‹•ç”Ÿæˆï¼‰"""
        if not os.path.exists(file_path):
            self._generate_val_file(file_path, tokenizer)
        ...

    def _generate_val_file(self, file_path, tokenizer):
        """æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆï¼ˆUltraChatã®ã‚µãƒ³ãƒ—ãƒ«1000-1020ï¼‰"""
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆ0-999ï¼‰ã¨é‡è¤‡ã—ãªã„ç¯„å›²ã‹ã‚‰ç”Ÿæˆ
        ...
```

### æ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä»•æ§˜

- **å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«**: `./data/example_val.txt`ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã«ä¾å­˜ã—ãªã„ï¼‰
- **ã‚½ãƒ¼ã‚¹**: UltraChatã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹1000-1020ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨é‡è¤‡ã—ãªã„é ˜åŸŸï¼‰
- **è‡ªå‹•ç”Ÿæˆ**: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰æ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°è‡ªå‹•ç”Ÿæˆ

### Colabã§ã®æ¨å¥¨æ‰‹é †

```bash
# 1. ãƒªãƒã‚¸ãƒˆãƒªæ›´æ–°
!cd /content/new-llm && git pull

# 2. å®Ÿé¨“å®Ÿè¡Œï¼ˆæ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹ï¼‰
!cd /content/new-llm && python3 scripts/architecture_comparison_experiment.py
```

---

## ğŸš¨ CRITICAL: å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢ (2025-11-29)

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ç¦æ­¢äº‹é …

1. **ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°ã§ã®åˆ†å²ç¦æ­¢**
   ```python
   # âŒ ç¦æ­¢: å¤ã„ãƒ‘ã‚¹ã‚’æ®‹ã™
   def func(cache=None):
       if cache is None:
           cache = build_cache()  # å¤ã„ãƒ‘ã‚¹

   # âœ… æ­£è§£: å¿…é ˆå¼•æ•°ã«ã™ã‚‹
   def func(cache):
       pass  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯å‘¼ã³å‡ºã—å…ƒã§å¿…ãšæº–å‚™
   ```

2. **å¤ã„ãƒ¡ã‚½ãƒƒãƒ‰ã®æ®‹å­˜ç¦æ­¢**
   - æ–°ã—ã„è¨­è¨ˆã«ç½®ãæ›ãˆãŸã‚‰ã€å¤ã„ãƒ¡ã‚½ãƒƒãƒ‰ã¯å³åº§ã«å‰Šé™¤
   - ã€Œå¿µã®ãŸã‚ã€ã§æ®‹ã™ã¨ã€èª¤ã£ã¦å¤ã„ãƒ‘ã‚¹ãŒå®Ÿè¡Œã•ã‚Œã‚‹

3. **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¦æ­¢**
   ```python
   # âŒ ç¦æ­¢
   def process(mode="old"):
       if mode == "old": ...

   # âœ… æ­£è§£: æ–°ã—ã„æ–¹å¼ã®ã¿
   def process(required_param):
       ...
   ```

### ç†ç”±

- Colabã§å¤ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã€æ„å›³ã›ãšå¤ã„ãƒ‘ã‚¹ãŒå®Ÿè¡Œã•ã‚Œã‚‹
- ãƒ‡ãƒãƒƒã‚°å›°é›£ãªãƒã‚°ã®åŸå› ã¨ãªã‚‹
- ã‚³ãƒ¼ãƒ‰ã®è¤‡é›‘æ€§ãŒå¢—åŠ ã™ã‚‹

### ãƒ«ãƒ¼ãƒ«

- æ–°æ©Ÿèƒ½ã‚’è¿½åŠ ã—ãŸã‚‰ã€å¤ã„æ©Ÿèƒ½ã¯**å®Œå…¨å‰Šé™¤**
- å¼•æ•°ã¯å¯èƒ½ãªé™ã‚Š**å¿…é ˆ**ã«ã™ã‚‹
- ã€Œã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ã¯å°†æ¥ã®ãƒˆãƒ©ãƒ–ãƒ«ã®ç¨®

---

## ğŸš€ PHASE 2 CACHE REUSE - Phase 1ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨ (2025-11-29)

**Phase 1ã§è¨ˆç®—ã—ãŸå…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›ã‚’Phase 2ã§å†åˆ©ç”¨ã—ã€627ç§’ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†æ§‹ç¯‰ã‚’çœç•¥ã€‚**

### å¾“æ¥ã®å•é¡Œç‚¹

```python
# Phase 1ã§å–å¾—æ¸ˆã¿ï¼ˆä½¿ã‚ã‚Œã¦ã„ãªã‹ã£ãŸï¼ï¼‰
train_contexts = phase1_trainer.train(...)  # æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿

# Phase 2ã§å†è¨ˆç®—ï¼ˆ627ç§’ã‹ã‹ã£ã¦ã„ãŸï¼‰
phase2_trainer.train_full(...)  # _build_context_cache ã§å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›ã‚’å†è¨ˆç®—
```

### æ–°æ–¹å¼: Phase 1ã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¸¡ã™

```python
# Phase 1: return_all_layers=True ã§å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›ã‚‚å–å¾—
train_contexts, train_context_cache, train_token_embeds = phase1_trainer.train(
    ..., return_all_layers=True
)
val_contexts, val_context_cache, val_token_embeds = phase1_trainer.evaluate(
    ..., return_all_layers=True
)

# Phase 2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å—ã‘å–ã‚Šã€å†æ§‹ç¯‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
phase2_trainer.train_full(
    ...,
    train_context_cache=train_context_cache,
    train_token_embeds=train_token_embeds,
    val_context_cache=val_context_cache,
    val_token_embeds=val_token_embeds
)
# â†’ "Using pre-built context cache from Phase 1 (skipping cache build)" ã¨è¡¨ç¤º
```

### æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

| å‡¦ç† | å¾“æ¥ | æ–°æ–¹å¼ |
|------|------|--------|
| Phase 2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰ | 627ç§’ | **0ç§’ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰** |
| å…¨ä½“æ™‚é–“ (500 samples) | 24åˆ† | **ç´„14åˆ†ï¼ˆ40%çŸ­ç¸®ï¼‰** |

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

- 500ã‚µãƒ³ãƒ—ãƒ«ï¼ˆ53ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰: ç´„9.3GB

### âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥åé›†ã®ä¸¦åˆ—åŒ– (2025-11-29 æ›´æ–°)

**Phase 1ã§ç¢ºå®šã—ãŸcontextã‚’ä½¿ãˆã°ã€å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ›ã®è¨ˆç®—ã¯ä¸¦åˆ—åŒ–å¯èƒ½ã€‚**

**ç™ºè¦‹**: ä¸¦åˆ—å‡¦ç†ã¨ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã®contextã¯**ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦99.7%**ã§ã»ã¼åŒä¸€ã€‚

```python
# ä¸¦åˆ—ã‚­ãƒ£ãƒƒã‚·ãƒ¥åé›†ï¼ˆæ–°æ–¹å¼ï¼‰: æ•°ç§’ã§å®Œäº†
shifted_contexts = [initial_context] + previous_contexts[:-1]  # 1ã¤ãšã‚‰ã—
all_layer_outputs = model.forward_with_intermediates_batch(shifted_contexts, token_embeds)
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- Phase 2ã§ã¯ token i ã®å‡¦ç†ã« `previous_contexts[i-1]` ã‚’ä½¿ç”¨ï¼ˆ1ã¤ãšã‚‰ã—ï¼‰
- token 0 ã¯åˆæœŸcontextï¼ˆã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’ä½¿ç”¨
- 51ç§’ â†’ æ•°ç§’ã«é«˜é€ŸåŒ–

---

## ğŸ¯ TOKENç¶™ãè¶³ã—æ–¹å¼ï¼ˆæ¨™æº–æ¡ç”¨ï¼‰

**å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§tokenå…¥åŠ›ã™ã‚‹æ–¹å¼ã‚’æ¨™æº–æ¡ç”¨ã€‚**

### å®Ÿé¨“çµæœï¼ˆ500ã‚µãƒ³ãƒ—ãƒ«ï¼‰

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| Val PPL | **324** |
| Val Acc | **19.0%** |
| Effective Rank | 76% |

### çµè«–

- tokenç¶™ãè¶³ã—ã«ã‚ˆã£ã¦æƒ…å ±ãŒè±Šå¯Œã«ãªã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
- **Effective Rankï¼ˆERï¼‰ã¯tokenç¶™ãè¶³ã—ã®å‰¯ç”£ç‰©**

---

## ğŸ”¬ ä¸¦åˆ—vs ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ã®contextå·®ã¯ç„¡è¦–å¯èƒ½ (2025-11-29)

**å®Ÿé¨“çµæœ**: 56,602ãƒˆãƒ¼ã‚¯ãƒ³ã§ã®æ¯”è¼ƒ

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆå¹³å‡ï¼‰ | **0.9969** |
| ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼ˆæœ€å°ï¼‰ | 0.9890 |
| MSE | 0.006156 |

**çµè«–**: Phase 1ã®ä¸¦åˆ—å‡¦ç†ã¯å“è³ªã‚’çŠ ç‰²ã«ã—ã¦ã„ãªã„ã€‚

---

## âš¡ PHASE 2 CACHE MODE - ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼é«˜é€ŸåŒ– (2025-11-28)

**Phase 2ã§ContextBlockã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼ã‚’æ¡ç”¨ã—ã€5ã€œ20å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã€‚**

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼ã®æ¦‚è¦

```python
# Step 1: ContextBlockå‡ºåŠ›ã‚’å…¨ãƒˆãƒ¼ã‚¯ãƒ³åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ1å›ã®ã¿ï¼‰
with torch.no_grad():
    for token in tokens:
        context_outputs = context_block(context, token)
        context_cache.append(context_outputs)
        context = context_outputs[-1]

# Step 2: TokenBlockã‚’ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†
for batch in batches:
    batch_token_out = token_block(batch_contexts, batch_tokens)
    loss.backward()
```

### ãƒãƒƒãƒã‚µã‚¤ã‚ºè‡ªå‹•è¨ˆç®—ï¼ˆGPUãƒ¡ãƒ¢ãƒªãƒ™ãƒ¼ã‚¹ï¼‰ - 2025-11-29 ä¿®æ­£

```python
# config.py
phase2_batch_size = None  # GPUãƒ¡ãƒ¢ãƒªã«åŸºã¥ã„ã¦è‡ªå‹•è¨ˆç®—
phase2_memory_safety_factor = 0.5  # å®‰å…¨ä¿‚æ•°
phase2_min_batch_size = 256
phase2_max_batch_size = 16384
```

### ãƒ¡ãƒ¢ãƒªè¦ä»¶

ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º = `num_tokens Ã— num_layers Ã— context_dim Ã— 4bytes`

ä¾‹: 10ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã€6å±¤ã€768dim = **ç´„1.84GB**

---

## ğŸš¨ GPU OOMå•é¡Œã®æ•™è¨“ - CRITICAL (2025-11-29)

**500ã‚µãƒ³ãƒ—ãƒ«å®Ÿé¨“ã§ç¹°ã‚Šè¿”ã—ç™ºç”Ÿã—ãŸOOMã‚¨ãƒ©ãƒ¼ã‹ã‚‰å¾—ãŸæ•™è¨“**

### å•é¡Œã®æœ¬è³ª

PyTorchã®GPUãƒ¡ãƒ¢ãƒªç®¡ç†ã«ã¯è¤‡æ•°ã®æ¦‚å¿µãŒã‚ã‚‹ï¼š

| ç”¨èª | æ„å‘³ | å–å¾—æ–¹æ³• |
|------|------|----------|
| `total_memory` | GPUç‰©ç†ãƒ¡ãƒ¢ãƒªç·é‡ | `torch.cuda.get_device_properties(0).total_memory` |
| `memory_allocated` | å®Ÿéš›ã«ä½¿ç”¨ä¸­ã®ãƒ†ãƒ³ã‚½ãƒ« | `torch.cuda.memory_allocated()` |
| `memory_reserved` | PyTorchãŒç¢ºä¿æ¸ˆã¿ï¼ˆãƒ—ãƒ¼ãƒ«ï¼‰ | `torch.cuda.memory_reserved()` |

**é‡è¦**: `reserved - allocated` ã¯ã€ŒPyTorchãƒ—ãƒ¼ãƒ«å†…ã®æœªä½¿ç”¨é ˜åŸŸã€ã§ã‚ã‚Šã€æ–°è¦ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ä½¿ãˆã‚‹ã¨ã¯é™ã‚‰ãªã„ã€‚

### OOMãŒç™ºç”Ÿã—ãŸåŸå› 

```
500 samples:
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥: 10.3GB
- ãƒ¢ãƒ‡ãƒ«+å‹¾é…: ç´„11GB
- allocated: 13.8GB
- backwardæ™‚ã«è¿½åŠ ã§1.65GBå¿…è¦ â†’ OOM
```

**èª¤ã£ãŸè¨ˆç®—**ï¼ˆä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ï¼‰:
```python
free_gb = total_gb - reserved_gb  # âŒ è¦‹ã‹ã‘ä¸Š8GBç©ºããŒã‚ã‚‹
batch_size = free_gb Ã— 500 Ã— 0.8  # âŒ 8800ãƒˆãƒ¼ã‚¯ãƒ³
# â†’ backwardæ™‚ã«OOM
```

**æ­£ã—ã„è¨ˆç®—**ï¼ˆä¿®æ­£å¾Œï¼‰:
```python
free_gb = total_gb - allocated_gb  # âœ… å®Ÿéš›ã®ç©ºã
per_token_mb = vocab_size Ã— 4 Ã— 3.5  # âœ… backwardç”¨ã«3.5å€
available_mb = free_gb Ã— 1024 Ã— safety Ã— 0.5  # âœ… ã•ã‚‰ã«50%ãƒãƒ¼ã‚¸ãƒ³
batch_size = available_mb / per_token_mb  # âœ… ~3200ãƒˆãƒ¼ã‚¯ãƒ³
```

### backwardæ™‚ã®ãƒ¡ãƒ¢ãƒªè¦ä»¶

forwardæ™‚ã‚ˆã‚Š**å¤§å¹…ã«å¤šã„**ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ï¼š

1. **logits**: `batch_size Ã— vocab_size Ã— 4bytes` (ç´„192MB @ batch=1000)
2. **gradients**: logitsã¨åŒã‚µã‚¤ã‚º
3. **ä¸­é–“ãƒãƒƒãƒ•ã‚¡**: è¿½åŠ 50%ç¨‹åº¦
4. **åˆè¨ˆ**: `batch_size Ã— vocab_size Ã— 4 Ã— 3.5`

### å¿…é ˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨ˆç®—æ™‚ï¼š
- [ ] `memory_allocated()`ã‚’ä½¿ç”¨ï¼ˆ`memory_reserved()`ã§ã¯ãªã„ï¼‰
- [ ] `torch.cuda.synchronize()` + `empty_cache()` ã‚’å…ˆã«å®Ÿè¡Œ
- [ ] backwardç”¨ã«3.5å€ã®ãƒ¡ãƒ¢ãƒªã‚’è¦‹ç©ã‚‚ã‚‹
- [ ] safety_factor Ã— 0.5 ã®è¿½åŠ ãƒãƒ¼ã‚¸ãƒ³ã‚’é©ç”¨
- [ ] è¨ˆç®—çµæœã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã—ã¦ç¢ºèªå¯èƒ½ã«

### Colabå®Ÿè¡Œæ™‚ã®æ³¨æ„

1. **ãƒ©ãƒ³ã‚¿ã‚¤ãƒ å†èµ·å‹•ãŒå¿…é ˆ**: `git pull`å¾Œã‚‚Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹
2. **ç¢ºèªã‚³ãƒãƒ³ãƒ‰**:
   ```bash
   !cd /content/new-llm && git fetch origin && git reset --hard origin/main
   !grep "3.5" /content/new-llm/src/trainers/phase2.py  # ä¿®æ­£ç¢ºèª
   ```
3. **æœŸå¾…ã•ã‚Œã‚‹ãƒ­ã‚°**:
   ```
   Calculating optimal batch size...
     GPU Memory: total=22.2GB, allocated=13.8GB, reserved=XX.XGB, free=8.4GB
     Per-token memory: 0.67MB, available: 2150.4MB
     Batch size calculation: free=8.4GB Ã— safety=0.5 â†’ 3204 tokens
   âš ï¸ Auto-adjusting batch_size: 8864 â†’ 3204
   ```

---

## âš ï¸ CVFPåæŸã®ç‰¹å¾´ - é‡è¦ãªå‰æçŸ¥è­˜

### Iteration 2ã§ã®æ—©æœŸåæŸã¯ç„¡è¦–ã™ã¹ã

**CVFPã®ç‰¹å¾´**: Iteration 2ã§100%åæŸã¨è¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ãŒã€ã“ã‚Œã¯å‚è€ƒã«ãªã‚‰ãªã„ã€‚

**ç†ç”±**:
- åˆæœŸçŠ¶æ…‹ï¼ˆIteration 1ï¼‰ã‹ã‚‰ã®å¤‰åŒ–ãŒå°ã•ã„ãŸã‚ã€é–¾å€¤ã‚’ã‚¯ãƒªã‚¢ã—ã‚„ã™ã„
- å®Ÿéš›ã®å›ºå®šç‚¹å­¦ç¿’ã«ã¯ã€ã‚ˆã‚Šå¤šãã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦
- `phase1_min_iterations`ã§æœ€ä½ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã‚’ä¿è¨¼ã—ã¦ã„ã‚‹

**æ­£ã—ã„è§£é‡ˆ**:
```
Iteration 2/10: åæŸ=100.0%  â† ã“ã‚Œã¯ç„¡è¦–ã™ã‚‹
Iteration 3/10: åæŸ=0.0%    â† ã“ã“ã‹ã‚‰ãŒæœ¬å½“ã®å­¦ç¿’
...
Iteration 10/10: åæŸ=XX%    â† æœ€çµ‚çµæœã‚’è¦‹ã‚‹
```

**è¨­å®š**:
```python
# config.py
phase1_min_iterations = 5   # æœ€ä½5ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¿è¨¼ï¼ˆæ—©æœŸåœæ­¢é˜²æ­¢ï¼‰
phase1_max_iterations = 40  # æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°
```

---

## ğŸ§Š EMBEDDING FREEZE ADOPTED - Embeddingå‡çµæ¡ç”¨ (2025-11-27)

**Phase 2ã§Embeddingå‡çµã‚’æ¨™æº–æ¡ç”¨ã—ã¾ã—ãŸã€‚**

### å®Ÿé¨“çµæœ

| æŒ‡æ¨™ | Embeddingå­¦ç¿’ | Embeddingå‡çµ | æ”¹å–„ç‡ |
|------|--------------|--------------|--------|
| Val PPL (500samples) | 1189.15 | **334.31** | **-71.9%** |
| Val Acc (500samples) | 11.58% | **18.88%** | **+63.0%** |
| Val PPL (1000samples) | 840.46 | **280.27** | **-66.7%** |
| Val Acc (1000samples) | 13.03% | **19.91%** | **+52.8%** |
| å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 49.2M | **7.09M** | **-85.6%** |

### è¨­å®š

```python
# config.py
phase2_freeze_embedding = True  # æ¨å¥¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
use_weight_tying = True         # æ¨å¥¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
```

### æ¡ç”¨ç†ç”±

1. **éå­¦ç¿’æŠ‘åˆ¶**: å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’85.6%å‰Šæ¸›ã—ã€æ±åŒ–æ€§èƒ½ãŒå¤§å¹…å‘ä¸Š
2. **GPT-2äº‹å‰å­¦ç¿’ã®æ©æµ**: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã§å­¦ç¿’æ¸ˆã¿ã®æ„å‘³è¡¨ç¾ã‚’ä¿æŒ
3. **Weight Tyingã¨ã®ç›¸ä¹—åŠ¹æœ**: å…¥å‡ºåŠ›ã®ä¸€è²«æ€§ãŒå®Œå…¨ã«ä¿æŒ

### è©³ç´°

- è©³ç´°ãªå®Ÿé¨“çµæœ: [importants/embedding-freeze-experiment-2025-11-27.md](importants/embedding-freeze-experiment-2025-11-27.md)

---

## ğŸ”— WEIGHT TYING ADOPTED - é‡ã¿å…±æœ‰æ¡ç”¨ (2025-11-27)

**Weight Tyingã‚’æ¨™æº–æ¡ç”¨ã—ã¾ã—ãŸã€‚**

### æ¦‚è¦

Token Embeddingã¨Output Headã§é‡ã¿ã‚’å…±æœ‰ã™ã‚‹æ‰‹æ³•ï¼ˆGPT-2ã¨åŒã˜ï¼‰ã€‚
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ç´„38Må‰Šæ¸›ã—ã€ãƒ¢ãƒ‡ãƒ«åŠ¹ç‡ã‚’å¤§å¹…ã«å‘ä¸Šã€‚

### åŠ¹æœ

| é …ç›® | Without Weight Tying | With Weight Tying |
|------|---------------------|-------------------|
| å…¨ä½“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | 91.43M | **52.78M** (-42%) |
| Output Head | 38.65M | **0** (å…±æœ‰) |
| Phase 2å­¦ç¿’å¯¾è±¡ | 45.74M | 45.69M |

### è¨­å®š

```python
# config.py
use_weight_tying = True  # æ¨å¥¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
```

### æ¡ç”¨ç†ç”±

1. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡**: å°ã€œä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ100Mä»¥ä¸‹ï¼‰ã§ã¯ç‰¹ã«åŠ¹æœçš„
2. **Chinchillaå‰‡ã¨ã®æ•´åˆ**: UltraChatãƒ‡ãƒ¼ã‚¿é‡ã«å¯¾ã—ã¦ã‚ˆã‚Šé©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«
3. **æ¥­ç•Œæ¨™æº–**: GPT-2, GPT-3, BERT, LLaMA, Mistralãªã©å¤šãã®ãƒ¢ãƒ‡ãƒ«ã§æ¡ç”¨

### æ³¨æ„ç‚¹

- **Embeddingå‡çµæ™‚**: Output Headã‚‚è‡ªå‹•çš„ã«å‡çµã•ã‚Œã‚‹ï¼ˆWeight Tyingï¼‰
- **Embeddingå­¦ç¿’æ™‚**: `unfreeze_token_output()`ã§è‡ªå‹•çš„ã«Embeddingã®å‹¾é…ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã‚‹

---

## âš¡ PARALLEL PROCESSING ADOPTED - ä¸¦åˆ—å‡¦ç†ç‰ˆæ¡ç”¨ (2025-11-25)

**ä¸¦åˆ—å‡¦ç†ç‰ˆã‚’æ¨™æº–å®Ÿè£…ã¨ã—ã¦å®Œå…¨æ¡ç”¨ã—ã¾ã—ãŸã€‚**

### æ€§èƒ½æŒ‡æ¨™

**ä¸¦åˆ—ç‰ˆå®Ÿè£…** ([src/trainers/phase1.py](src/trainers/phase1.py)):
- **Effective Rank**: 55.9% (429/768) - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
- **å‡¦ç†æ™‚é–“**: ~11ç§’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ265ç§’ã®**23xé«˜é€ŸåŒ–**ï¼‰
- **dist_reg_weight**: 0.9ï¼ˆå¤šæ§˜æ€§90%, CVFP 10%ï¼‰
- **max_iterations**: 10
- **åæŸç‡**: 27.2%ï¼ˆå¤šæ§˜æ€§å„ªå…ˆã®ãŸã‚CVFPåæŸç‡ã¯ä½ã‚ã€ã“ã‚Œã¯æ­£å¸¸ï¼‰

### è¨­è¨ˆè©³ç´°

**Iteration 0**: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ï¼ˆå›ºå®šç‚¹ç›®æ¨™ç¢ºç«‹ï¼‰
**Iteration 1+**: ä¸¦åˆ—å‡¦ç†ï¼ˆå‰å›contextã‚’ä½¿ç”¨ï¼‰

**ä¸¦åˆ—åŒ–ã®ç‰¹å¾´**:
- Token i ã«ã¯ previous_contexts[i-1] ã‚’ä½¿ç”¨ï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³åˆ†ã®ãšã‚Œï¼‰
- æƒ…å ±é…å»¶ãŒã‚ã‚‹ãŒã€dist_reg_weight=0.9ã«ã‚ˆã‚Šå¤šæ§˜æ€§ã‚’è£œå„Ÿ
- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹é«˜é€ŸåŒ–

**æ—§ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆã¨ã®æ¯”è¼ƒ**:
- ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ: 66.6% ER, 265ç§’
- ä¸¦åˆ—ç‰ˆ: 55.9% ER, 11ç§’
- ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: -10.7% ER vs 23xé«˜é€ŸåŒ–

---

## ğŸ“Š MANDATORY: æ•°å€¤å ±å‘Šãƒ«ãƒ¼ãƒ« - å…·ä½“çš„ãªæ•°å€¤ã§ã®å ±å‘Šç¾©å‹™

### çµ¶å¯¾éµå®ˆ: ã™ã¹ã¦ã®å®Ÿé¨“çµæœã¯å…·ä½“çš„ãªæ•°å€¤ã§å ±å‘Šã™ã‚‹

**ç¦æ­¢äº‹é …**:
- âŒ "GOOD", "EXCELLENT", "MODERATE" ãªã©ã®æŠ½è±¡çš„è¡¨ç¾ã§ã®å ±å‘Š
- âŒ "æ”¹å–„ã—ãŸ", "è‰¯å¥½", "é©åˆ‡" ãªã©ã®å®šæ€§çš„è©•ä¾¡ã®ã¿ã®å ±å‘Š
- âŒ æ•°å€¤ã‚’ä¼´ã‚ãªã„åˆ¤å®šçµæœã®å ±å‘Š

**å¿…é ˆå ±å‘Šé …ç›®**:
- âœ… **åæŸç‡ï¼ˆè¨“ç·´ãƒ»æ¤œè¨¼ä¸¡æ–¹ï¼‰**: **å…·ä½“çš„ãªãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ã¨åæŸãƒˆãƒ¼ã‚¯ãƒ³æ•°** (ä¾‹: è¨“ç·´ 0.0% (0/6400), æ¤œè¨¼ 0.0% (0/1280))
- âœ… Effective Rank: **å®Ÿæ•°å€¤/ç·æ¬¡å…ƒæ•°ã¨ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸** (ä¾‹: 627.29/768 = 81.7%)
- âœ… CVFPãƒ­ã‚¹: **å®Ÿæ•°å€¤** (ä¾‹: 0.001873)
- âœ… åæŸå·®åˆ†: **å®Ÿæ•°å€¤** (ä¾‹: final_diff = 0.000745)
- âœ… ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: **å®Ÿæ•°** (ä¾‹: 10/10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†)

**âš ï¸ åæŸç‡ã®å ±å‘Šã¯çµ¶å¯¾ã«çœç•¥ã—ã¦ã¯ã„ã‘ãªã„**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä¸¡æ–¹ã®åæŸç‡ã‚’å¿…ãšå ±å‘Š
- åæŸç‡ = åæŸã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•° / ç·ãƒˆãƒ¼ã‚¯ãƒ³æ•°
- 0.0%ã¯ã€ŒåæŸå¤±æ•—ã€ã§ã¯ãªãã€Œå…¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œèµ°ã€ã‚’æ„å‘³ã™ã‚‹

**å ±å‘Šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¾‹**:
```
è¨“ç·´çµæœ:
- åæŸç‡: 0.0% (0/6400ãƒˆãƒ¼ã‚¯ãƒ³) - 10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œèµ°
- Effective Rank: 689.26/768 (89.7%)
- CVFPãƒ­ã‚¹: 0.001732

æ¤œè¨¼çµæœ:
- åæŸç‡: 0.0% (0/1280ãƒˆãƒ¼ã‚¯ãƒ³) - 10ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œèµ°
- Effective Rank: 627.29/768 (81.7%)

CVFPåæŸãƒã‚§ãƒƒã‚¯:
- final_diff = 0.000745 (é–¾å€¤ < 0.001ã‚¯ãƒªã‚¢)
```

---

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL DESIGN - CVFP FIXED-POINT LEARNING (2025-11-26 ä¿®æ­£) ğŸš¨ğŸš¨ğŸš¨

### CVFPç†è«–: å›ºå®šç‚¹å­¦ç¿’ã®æ­£ã—ã„å®Ÿè£…

**å›ºå®šç‚¹å­¦ç¿’ã®å®šç¾©**: `f(x) = x` ã¨ãªã‚‹ç‚¹ã«åæŸã•ã›ã‚‹

ã“ã‚Œã¯ã€ŒåŒã˜å…¥åŠ›ã‚’ç¹°ã‚Šè¿”ã—å‡¦ç†ã—ãŸã¨ãã€å‡ºåŠ›ãŒå¤‰åŒ–ã—ãªããªã‚‹ã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹ã€‚

**æ­£ã—ã„å®Ÿè£…ï¼ˆCVFPç†è«–ã«åŸºã¥ãï¼‰**:
```
Iteration 0: contexts_0 ã‚’å‡ºåŠ› â†’ previous_contexts = contexts_0ï¼ˆå­¦ç¿’ãªã—ï¼‰
Iteration 1: contexts_1 ã‚’å‡ºåŠ› â†’ CVFPæå¤± = MSE(contexts_1, previous_contexts)
             previous_contexts = contexts_1 ã«æ›´æ–°
Iteration 2: contexts_2 ã‚’å‡ºåŠ› â†’ CVFPæå¤± = MSE(contexts_2, previous_contexts)
             previous_contexts = contexts_2 ã«æ›´æ–°
...
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- âœ… CVFPæå¤±ã¯**å‰å›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆprevious_contextsï¼‰**ã¨æ¯”è¼ƒ
- âœ… previous_contextsã¯**æ¯ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›´æ–°**ã—ã¦ã‚ˆã„
- âœ… åæŸåˆ¤å®šã‚‚å‰å›ã¨ã®å·®ã§è¡Œã†
- âŒ ~~Iteration 0ã‚’å›ºå®šç›®æ¨™ã¨ã—ã¦ä¿å­˜~~ â† ã“ã‚Œã¯é–“é•ã„

**ãªãœå‰å›ã¨ã®æ¯”è¼ƒãŒæ­£ã—ã„ã‹**:
1. å›ºå®šç‚¹ = å¤‰åŒ–ãŒãªããªã‚‹ç‚¹
2. `MSE(current, previous) â†’ 0` ã¯ã€Œå‡ºåŠ›ãŒå®‰å®šã—ãŸã€ã“ã¨ã‚’æ„å‘³ã™ã‚‹
3. ã“ã‚ŒãŒå›ºå®šç‚¹ `f(x) = x` ã®å®šç¾©ã«åˆè‡´ã™ã‚‹

**æ­£ã—ã„ã‚³ãƒ¼ãƒ‰ï¼ˆphase1.pyï¼‰**:
```python
# CVFPæå¤±: å‰å›ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨æ¯”è¼ƒï¼ˆå›ºå®šç‚¹ã¸ã®åæŸï¼‰
cvfp_loss = compute_cvfp_loss(contexts, previous_contexts)

# æ›´æ–°
previous_contexts = contexts.detach()
```

---

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL BUG FIX - CONTEXT CARRYOVER (2025-11-24) ğŸš¨ğŸš¨ğŸš¨

### è‡´å‘½çš„ãƒã‚°ä¿®æ­£: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼•ãç¶™ãï¼ˆçµ¶å¯¾ã«å¿˜ã‚Œã¦ã¯ã„ã‘ãªã„ï¼‰

**è‡´å‘½çš„ãªå•é¡Œ**:
- è¨“ç·´ãƒ»æ¤œè¨¼ã®ä¸¡æ–¹ã§å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã”ã¨ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¼ãƒ­ãƒªã‚»ãƒƒãƒˆã•ã‚Œã¦ã„ãŸ
- **ã“ã‚Œã¯CVFPå­¦ç¿’ã®æ ¹æœ¬ã‚’ç ´å£Šã™ã‚‹è‡´å‘½çš„ãƒã‚°**
- å›ºå®šç‚¹å­¦ç¿’ãŒå…¨ãæ©Ÿèƒ½ã—ã¦ã„ãªã‹ã£ãŸ

**ä¿®æ­£å†…å®¹**:
```python
# âŒâŒâŒ çµ¶å¯¾ã«ã‚„ã£ã¦ã¯ã„ã‘ãªã„é–“é•ã£ãŸå®Ÿè£…ï¼ˆå‰Šé™¤æ¸ˆã¿ï¼‰
# æ¯ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ = CVFPå­¦ç¿’ã®ç ´å£Š
context = torch.zeros(1, self.model.context_dim, device=device)  # è‡´å‘½çš„ãƒã‚°

# âœ…âœ…âœ… å¿…é ˆã®æ­£ã—ã„å®Ÿè£…ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰
# ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…ãšå¼•ãç¶™ã
if self.previous_contexts is None:
    # åˆå›ã®ã¿ã‚¼ãƒ­åˆæœŸåŒ–
    context = torch.zeros(1, self.model.context_dim, device=device)
else:
    # å‰ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æœ€çµ‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¿…ãšå¼•ãç¶™ãï¼ˆCVFPå­¦ç¿’ã®æ ¸å¿ƒï¼‰
    context = self.previous_contexts[-1].unsqueeze(0).detach()
```

**ãªãœã“ã‚ŒãŒè‡´å‘½çš„ã‹**:
1. **CVFP = Context Vector Fixed-Point**: å›ºå®šç‚¹ã¸ã®åæŸãŒç›®çš„
2. **å›ºå®šç‚¹å­¦ç¿’**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é‡ã­ã¦åŒã˜ç‚¹ã«åæŸã™ã‚‹ã“ã¨ãŒç›®æ¨™
3. **å¼•ãç¶™ãŒãªã„ = å­¦ç¿’ã—ã¦ã„ãªã„**: æ¯å›ãƒªã‚»ãƒƒãƒˆã§ã¯å›ºå®šç‚¹ã«åˆ°é”ä¸å¯èƒ½
4. **æ¤œè¨¼ãƒ­ã‚¹ã‚¼ãƒ­ã®è¬**: ãƒã‚°ã®ã›ã„ã§è¦‹ã‹ã‘ä¸Šè‰¯ã„çµæœã«è¦‹ãˆã¦ã„ãŸ

**äºŒåº¦ã¨åŒã˜é–“é•ã„ã‚’ã—ãªã„ãŸã‚ã«**:
- âš ï¸ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¼•ãç¶™ãã¯**CVFPå­¦ç¿’ã®ç”Ÿå‘½ç·š**
- âš ï¸ `previous_contexts`ã®æœ€çµ‚å€¤ã‚’æ¬¡ã®åˆæœŸå€¤ã«ã™ã‚‹ã“ã¨ã¯**çµ¶å¯¾å¿…é ˆ**
- âš ï¸ ã“ã®ä¿®æ­£ãªã—ã§ã¯ã€ã™ã¹ã¦ã®å®Ÿé¨“çµæœãŒç„¡æ„å‘³ã«ãªã‚‹

---

## ğŸš¨ğŸš¨ğŸš¨ CRITICAL DESIGN - åˆ†é›¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ Eæ¡ˆ (2025-11-26) ğŸš¨ğŸš¨ğŸš¨

### åˆ†é›¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¦‚è¦ï¼ˆEæ¡ˆ - ãƒ¬ã‚¤ãƒ¤ãƒ¼å¯¾å¿œç‰ˆï¼‰

ContextBlockã¨TokenBlockã‚’**ç‰©ç†çš„ã«åˆ†é›¢**ã—ã€**TokenBlock Layer i ãŒ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§**ã™ã‚‹ã€‚

```
ContextBlock (Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze):
  Layer 1: [context_0, token_embed] â†’ context_1
  Layer 2: [context_1, token_embed] â†’ context_2
  Layer 3: [context_2, token_embed] â†’ context_3 (= C*)

TokenBlock (Phase 2ã§å­¦ç¿’):
  Layer 1: [context_1, token_embed] â†’ token_1
  Layer 2: [context_2, token_1]     â†’ token_2
  Layer 3: [context_3, token_2]     â†’ token_3 (= token_out)
```

**é‡è¦**: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å‚ç…§ã™ã‚‹

### æ¯”è¼ƒè¡¨

| æ¡ˆ | TokenBlockã¸ã®contextå…¥åŠ› | ç‰¹å¾´ |
|----|--------------------------|------|
| **Aæ¡ˆï¼ˆæ—§å®Ÿè£…ï¼‰** | å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§åŒã˜ context_3 (C*) | ã‚·ãƒ³ãƒ—ãƒ« |
| **Dæ¡ˆ** | TokenBlockå†…ã§contextã‚‚æ®‹å·®æ›´æ–° | è¡¨ç¾åŠ›é«˜ã„ãŒã€C*ãŒå¤‰è³ª |
| **Eæ¡ˆï¼ˆæ¡ç”¨ï¼‰** | Layer i ã§ ContextBlock Layer i ã®å‡ºåŠ› | æ®µéšçš„æ–‡è„ˆã€C*ç¶­æŒ |

### Eæ¡ˆã®åˆ©ç‚¹

1. **æ®µéšçš„ãªæ–‡è„ˆæƒ…å ±**: æµ…ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã¯æµ…ã„æ–‡è„ˆã€æ·±ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã¯æ·±ã„æ–‡è„ˆã‚’ä½¿ç”¨
2. **C*ã®ä¿æŒ**: ContextBlockã¯frozenãªã®ã§ã€Phase 1ã§å­¦ç¿’ã—ãŸæ–‡è„ˆè¡¨ç¾ãŒç¶­æŒã•ã‚Œã‚‹
3. **Transformerã¨ã®é¡ä¼¼æ€§**: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ç•°ãªã‚‹æ·±ã•ã®è¡¨ç¾ã‚’å‚ç…§
4. **ç‰©ç†çš„åˆ†é›¢ç¶­æŒ**: ContextBlockã¨TokenBlockã¯åˆ¥ã®é‡ã¿è¡Œåˆ—ã®ã¾ã¾

### Phase 1: ContextBlockå­¦ç¿’ï¼ˆå›ºå®šç‚¹å­¦ç¿’ï¼‰

- **å­¦ç¿’å¯¾è±¡**: ContextBlockã®ã¿
- **TokenBlock**: æœªä½¿ç”¨
- **æå¤±**: CVFPæå¤± + å¤šæ§˜æ€§æå¤±

```python
# Phase 1ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼
for token_id in token_ids:
    token_embed = get_embedding(token_id)
    context = context_block(context, token_embed)  # ContextBlockã®ã¿ä½¿ç”¨
```

### Phase 2: TokenBlockå­¦ç¿’ï¼ˆEæ¡ˆ - ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰

- **ContextBlock**: frozenï¼ˆé‡ã¿å›ºå®šï¼‰
- **TokenBlock**: å­¦ç¿’
- **token_output**: å­¦ç¿’
- **æå¤±**: CrossEntropyï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰ã®ã¿
- **Eæ¡ˆ**: TokenBlock Layer i ã¯ ContextBlock Layer i ã®å‡ºåŠ›ã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚‹

```python
# Phase 2ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼ˆEæ¡ˆï¼‰
for token_id in token_ids:
    token_embed = get_embedding(token_id)

    # Step 1: ContextBlockï¼ˆfrozenï¼‰- å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ã‚’ä¿å­˜
    with torch.no_grad():
        context_outputs = []  # [context_1, context_2, context_3]
        context = context_in
        for layer in context_block.layers:
            context = layer(context, token_embed)
            context_outputs.append(context)

    # Step 2: TokenBlockï¼ˆå­¦ç¿’ï¼‰- å¯¾å¿œã™ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®contextã‚’ä½¿ç”¨
    token = token_embed
    for i, layer in enumerate(token_block.layers):
        token = layer(context_outputs[i], token)  # Layer i ã® context ã‚’ä½¿ç”¨

    # Step 3: äºˆæ¸¬
    logits = token_output(token)
    loss = CrossEntropy(logits, target)
```

### Eæ¡ˆã®å®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰

```python
# ContextBlock
def forward_with_intermediates(self, context, token_embed):
    """å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‡ºåŠ›ã‚’è¿”ã™"""
    outputs = []
    for layer in self.layers:
        context = layer(context, token_embed)
        outputs.append(context)
    return outputs  # [context_1, context_2, ..., context_N]

# TokenBlock
def forward_with_contexts(self, context_list, token):
    """å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå¯¾å¿œã™ã‚‹contextã‚’ä½¿ç”¨"""
    for i, layer in enumerate(self.layers):
        token = layer(context_list[i], token)
    return token
```

### è¨˜å·å®šç¾©ï¼ˆEæ¡ˆï¼‰

| è¨˜å· | æ„å‘³ |
|------|------|
| `context_i` | ContextBlock Layer i ã®å‡ºåŠ› |
| `context_N` | æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼å‡ºåŠ› = C* |
| `token_out` | TokenBlockï¼ˆå­¦ç¿’ï¼‰ã®æœ€çµ‚å‡ºåŠ›ã€äºˆæ¸¬ã«ä½¿ç”¨ |

### å‹¾é…ãƒ•ãƒ­ãƒ¼ï¼ˆEæ¡ˆï¼‰

```
å…¥åŠ›: [context_0, token_embed]
         â†“
    ContextBlock Layer 1ï¼ˆfrozenï¼‰â†’ context_1
         â†“                              â†“
    ContextBlock Layer 2ï¼ˆfrozenï¼‰â†’ context_2  â†’  TokenBlock Layer 1ï¼ˆå­¦ç¿’ï¼‰â†’ token_1
         â†“                              â†“                                        â†“
    ContextBlock Layer 3ï¼ˆfrozenï¼‰â†’ context_3  â†’  TokenBlock Layer 2ï¼ˆå­¦ç¿’ï¼‰â†’ token_2
                                        â†“                                        â†“
                                   TokenBlock Layer 3ï¼ˆå­¦ç¿’ï¼‰â†’ token_3
                                                                  â†“
                                                         token_outputï¼ˆå­¦ç¿’ï¼‰
                                                                  â†“
                                                    logits â†’ CrossEntropy
```

**å‹¾é…ã®æµã‚Œ**:
- âŒ `context_i` â†’ ContextBlockï¼ˆfrozenã€å‹¾é…ãªã—ï¼‰
- âœ… `token_i` â†’ TokenBlockï¼ˆå­¦ç¿’ï¼‰
- âœ… `token_output`å±¤ï¼ˆå­¦ç¿’ï¼‰

### è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

```python
# config.py
num_layers = 1  # ç¾åœ¨ã¯1ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å›ºå®š
```

### åˆ¶ç´„æ¡ä»¶

- ContextBlockã¨TokenBlockã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã¯`num_layers`ã§çµ±ä¸€ã•ã‚Œã‚‹

---

## âš¡ 55.9% Effective Rank - Parallel Version Baseline (2025-11-25)

### ä¸¦åˆ—å‡¦ç†ç‰ˆã®æ€§èƒ½ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

**ä¸¦åˆ—å‡¦ç†ç‰ˆã¯æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§55.9% Effective Rankã‚’é”æˆï¼ˆ23xé«˜é€ŸåŒ–ï¼‰**

**å®Ÿæ¸¬å€¤**:
- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: 55.9% Effective Rank (429/768)
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ~60% Effective Rank
- **å‡¦ç†æ™‚é–“**: ~11ç§’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ265ç§’ã®23xé«˜é€ŸåŒ–ï¼‰
- **åæŸç‡**: 27.2%ï¼ˆå¤šæ§˜æ€§å„ªå…ˆã®ãŸã‚CVFPåæŸç‡ã¯ä½ã‚ã€ã“ã‚Œã¯æ­£å¸¸ï¼‰

**æ—§ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆï¼ˆå‚è€ƒï¼‰**:
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 66.6% Effective Rank
- å‡¦ç†æ™‚é–“: 265ç§’
- åæŸç‡: 30.0%

**ä¸¦åˆ—ç‰ˆæ¡ç”¨ã®ç†ç”±**:
- âœ… 23xé«˜é€ŸåŒ–ã«ã‚ˆã‚Šå®Ÿç”¨æ€§ãŒå¤§å¹…å‘ä¸Š
- âœ… 55.9% ERã¯å®Ÿç”¨çš„ãªå¤šæ§˜æ€§ã‚’ç¶­æŒ
- âœ… ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•: -10.7% ER vs åœ§å€’çš„é«˜é€ŸåŒ–

---

## Core Implementation - Parallel Processing

### 1. Diversity Loss: Global Mean-Based Tracking

**âœ… ä¸¦åˆ—ç‰ˆå®Ÿè£… ([src/trainers/phase1.py](src/trainers/phase1.py))**:

```python
def compute_diversity_loss(contexts):
    """
    å¤šæ§˜æ€§æå¤±: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®å¹³å‡ã‹ã‚‰ã®åå·®ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰

    Args:
        contexts: ç¾åœ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ [num_tokens, context_dim]

    Returns:
        diversity_loss: å¤šæ§˜æ€§æå¤±ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰
    """
    context_mean = contexts.mean(dim=0)  # [context_dim]
    deviation = contexts - context_mean  # [num_tokens, context_dim]
    diversity_loss = -torch.norm(deviation, p=2) / len(contexts)
    return diversity_loss
```

**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:
- å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¹³å‡ã‹ã‚‰ã®åå·®ã‚’è¨ˆç®—
- è² ã®æå¤±ã«ã‚ˆã‚Šã€å¹³å‡ã‹ã‚‰ã®åå·®ã‚’æœ€å¤§åŒ–ï¼ˆå¤šæ§˜æ€§ä¿ƒé€²ï¼‰
- ãƒãƒƒãƒå‡¦ç†ã«æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…

### 2. ãƒ‡ãƒ¼ã‚¿ä»•æ§˜ - çµ¶å¯¾å›ºå®š

**è¨“ç·´ãƒ‡ãƒ¼ã‚¿**:
- ã‚½ãƒ¼ã‚¹: UltraChat (HuggingFaceH4/ultrachat_200k)
- ã‚µãƒ³ãƒ—ãƒ«æ•°: è¨­å®šã«ã‚ˆã‚‹ï¼ˆconfig.num_samplesï¼‰
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–: `truncation=False`ï¼ˆå…¨é•·ä½¿ç”¨ã€åˆ‡ã‚Šè©°ã‚ãªã—ï¼‰
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥: `./cache/ultrachat_{num_samples}samples_full.pt`

âš ï¸ **æ³¨æ„**: å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`*_128len.pt`ï¼‰ã¯ `max_length=128` ã§åˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¦ãŠã‚Šã€ç¾åœ¨ã®è¨­å®šã¨äº’æ›æ€§ãŒã‚ã‚Šã¾ã›ã‚“ã€‚

**æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿** (çµ¶å¯¾ä»•æ§˜):
- ã‚½ãƒ¼ã‚¹: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œ20%ã‹ã‚‰ç”Ÿæˆ
- ãƒ•ã‚¡ã‚¤ãƒ«: `./data/example_val.txt`
- **å¿…é ˆæ¡ä»¶**: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã™ã‚‹ã“ã¨
- ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ: `scripts/create_val_from_train.py`

### 3. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ«ãƒ¼ãƒ« - å³æ ¼

**ç¦æ­¢äº‹é …**:
- âŒ `val_data_source = "auto_split"` ã¯å³ç¦ï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰
- âŒ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã‚€æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
- âŒ æ‰‹å‹•ã§ä½œæˆã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãªæ¤œè¨¼ãƒ†ã‚­ã‚¹ãƒˆ

**å¿…é ˆæ‰‹é †**:
```bash
# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
python3 scripts/create_val_from_train.py

# config.pyã®è¨­å®šï¼ˆçµ¶å¯¾å›ºå®šï¼‰
val_data_source = "text_file"
val_text_file = "./data/example_val.txt"
```

### 4. é”æˆçµæœ - ä¸¦åˆ—ç‰ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (2025-11-25)

**å®Ÿæ¸¬å€¤ (ä¸¦åˆ—ç‰ˆ, dist_reg_weight=0.9)**:
- **è¨“ç·´ãƒ‡ãƒ¼ã‚¿**: ~60% Effective Rank - 6400ãƒˆãƒ¼ã‚¯ãƒ³
- **æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿**: **55.9% Effective Rank (429/768)** - 1280ãƒˆãƒ¼ã‚¯ãƒ³
- **å‡¦ç†æ™‚é–“**: ~11ç§’ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆ265ç§’ã®23xé«˜é€ŸåŒ–ï¼‰
- **åæŸç‡**: è¨“ç·´ 27.2%ï¼ˆå¤šæ§˜æ€§å„ªå…ˆã®ãŸã‚ä½ã‚ã€ã“ã‚Œã¯æ­£å¸¸ï¼‰

**ä¸¦åˆ—ç‰ˆæ€§èƒ½**:
- `dist_reg_weight = 0.9` ã«ã‚ˆã‚Šã€ä¸¦åˆ—ç‰ˆã®æƒ…å ±é…å»¶ã‚’å¤šæ§˜æ€§å¼·åŒ–ã§è£œå„Ÿ
- 23xé«˜é€ŸåŒ–ã«ã‚ˆã‚Šå®Ÿç”¨æ€§ãŒå¤§å¹…å‘ä¸Š
- ã“ã®æ•°å€¤ã‚’ä¸¦åˆ—ç‰ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã™ã‚‹

### 5. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åæŸæ€§ãƒã‚§ãƒƒã‚¯ - Validation Convergence Check

**æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åæŸç‡ã¯è¨“ç·´æ™‚ã«è¨ˆç®—ã•ã‚Œãªã„**ï¼ˆ1å›ã®é †ä¼æ’­ã®ã¿ï¼‰ã€‚
ä»£ã‚ã‚Šã«ã€å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®åæŸæ€§ã‚’ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç¢ºèªï¼š

```bash
python3 check_val_convergence.py --num_trials 10
```

**å‹•ä½œ**:
1. å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
2. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°å›é †ä¼æ’­ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10å›ï¼‰
3. å„è©¦è¡Œã§CVFPæå¤±ï¼ˆå‰å›ã¨ã®å·®åˆ†MSEï¼‰ã‚’è¨ˆç®—
4. æå¤±ã®æ¨ç§»ã‚’è¡¨ç¤ºã—ã€æ¸›å°‘å‚¾å‘ã‚’è‡ªå‹•åˆ¤å®š

**å‡ºåŠ›ä¾‹**:
```
Trial  1/10: CVFP Loss = N/A (baseline, no previous context)
Trial  2/10: CVFP Loss = 0.245123
Trial  3/10: CVFP Loss = 0.183456
Trial  4/10: CVFP Loss = 0.142789
...
Trial 10/10: CVFP Loss = 0.098234

Statistics:
  - Initial Loss (Trial 2): 0.245123
  - Final Loss (Trial 10): 0.098234
  - Reduction: -59.93%
  - Slope (linear fit): -0.018234

Verdict:
  âœ… CONVERGING: Loss is decreasing - model is converging on validation data
```

**åˆ¤å®šåŸºæº–**:
- âœ… CONVERGING: æå¤±ãŒæ˜ç¢ºã«æ¸›å°‘ï¼ˆslope < -0.001ï¼‰
- âœ… CONVERGED: æå¤±ãŒå®‰å®šï¼ˆ|slope| < 0.001 ã‹ã¤ std < 0.01ï¼‰
- âŒ DIVERGING: æå¤±ãŒå¢—åŠ ï¼ˆslope > 0.001ï¼‰
- âš ï¸ UNSTABLE: æå¤±ãŒä¸å®‰å®šï¼ˆä¸Šè¨˜ä»¥å¤–ï¼‰

**ä½¿ç”¨å ´é¢**:
- è¨“ç·´å®Œäº†å¾Œã«æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®åæŸæ€§ã‚’ç¢ºèª
- ãƒ¢ãƒ‡ãƒ«ã®å›ºå®šç‚¹å­¦ç¿’ãŒæ±åŒ–ã—ã¦ã„ã‚‹ã‹ã‚’æ¤œè¨¼
- ç•°ãªã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ¯”è¼ƒ

---

## Architecture Configuration - Parallel Version

```python
# Model Architecture
num_layers = 1                  # å›ºå®šï¼ˆå¤‰æ›´ç¦æ­¢ï¼‰
context_dim = 768               # GPT-2 aligned
embed_dim = 768                 # GPT-2 pretrained

# Diversity Regularization (ä¸¦åˆ—ç‰ˆæœ€é©åŒ–)
dist_reg_weight = 0.9           # 90% diversity, 10% CVFP (parallel optimized)

# Training
phase1_learning_rate = 0.002    # Fast convergence
phase1_max_iterations = 60      # config.pyã®å€¤ã‚’ä½¿ç”¨
```

---

## Training Pipeline - Standard Workflow

### Phase 1: CVFP Learning

```bash
# Standard test (uses fixed train/val data)
python3 test.py
```

**å®Ÿè¡Œå†…å®¹**:
1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (6400ãƒˆãƒ¼ã‚¯ãƒ³ from cache)
2. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ (1280ãƒˆãƒ¼ã‚¯ãƒ³ from text file)
3. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ (Phase1Trainer)
4. æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
5. **3ã¤ã®å¿…é ˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ** (è©³ç´°ã¯ä¸‹è¨˜)

---

## 3 Critical Checks - ABSOLUTELY REQUIRED (çµ¶å¯¾å¿…è¦ãª3ã¤ã®ãƒã‚§ãƒƒã‚¯)

**ã“ã‚Œã‚‰ã®ãƒã‚§ãƒƒã‚¯ã‚’çœãã¨å•é¡ŒãŒå¤šç™ºã—ã¾ã™ã€‚test.pyã§å¿…ãšå®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚**

### Check 1: Effective Rank (å¤šæ§˜æ€§ç¢ºèª)

**ç›®çš„**: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãŒå¤šæ§˜ãªæ¬¡å…ƒã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹ç¢ºèª

**å®Ÿè£…**: `analyze_fixed_points(contexts)` in `src/evaluation/metrics.py`

**åˆæ ¼åŸºæº–**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: 88-89% Effective Rank
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: 81-82% Effective Rank

**å¤±æ•—ä¾‹**:
- âŒ Effective Rank < 30%: æ¬¡å…ƒãŒåã£ã¦ã„ã‚‹ï¼ˆå¤šæ§˜æ€§ãªã—ï¼‰
- âŒ Global attractor: å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒåŒã˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åæŸ

### Check 2: Identity Mapping Check (æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯)

**ç›®çš„**: ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã§ãã¦ã„ã‚‹ã‹ã€å˜ãªã‚‹æ’ç­‰å†™åƒã§ãªã„ã‹ç¢ºèª

**å®Ÿè£…**: `check_identity_mapping(model, token_embeds, contexts, device)` in `src/evaluation/metrics.py`

**åˆæ ¼åŸºæº–**:
- âœ… Zero context ã¨ã®å·®åˆ† > 0.1
- âœ… Token embedding ã¨ã®é¡ä¼¼åº¦ < 0.95

**å¤±æ•—ä¾‹**:
- âŒ å­¦ç¿’å¾Œã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã¨åŒã˜ â†’ å­¦ç¿’ãªã—
- âŒ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã¨åŒä¸€ â†’ æ’ç­‰å†™åƒ

### Check 3: CVFP Convergence Check (å›ºå®šç‚¹åæŸãƒã‚§ãƒƒã‚¯)

**ç›®çš„**: å›ºå®šç‚¹å­¦ç¿’ãŒã§ãã¦ã„ã‚‹ã‹ã€åå¾©å®Ÿè¡Œã§å®‰å®šã—ãŸçµæœã«ãªã‚‹ã‹ç¢ºèª

**å®Ÿè£…**: `check_cvfp_convergence(trainer, token_ids, device)` in `src/evaluation/metrics.py`

**åˆæ ¼åŸºæº–**:
- âœ… Final diff < 1e-3 (GOODä»¥ä¸Š)
- âœ… ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã®å¤‰åŒ–ãŒæ¸›å°‘å‚¾å‘

**å¤±æ•—ä¾‹**:
- âŒ Final diff > 1e-2: å›ºå®šç‚¹ã«åæŸã—ã¦ã„ãªã„
- âŒ ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§å¤‰åŒ–ãŒå¢—åŠ  â†’ ç™ºæ•£ã—ã¦ã„ã‚‹

---

## Reproducibility - å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼

**ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š (å¿…é ˆ)**:

```python
def set_seed(seed=42):
    """å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆå™¨ã®ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
```

**ãªãœå¿…è¦ã‹**:
- åŒã˜ã‚³ãƒ¼ãƒ‰ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã§**å®Œå…¨ã«åŒã˜çµæœ**ã‚’ä¿è¨¼
- å®Ÿè£…ãŒç¶­æŒã•ã‚Œã¦ã„ã‚‹ã‹ã®ç¢ºèªã«ä¸å¯æ¬ 
- ãƒ‡ãƒãƒƒã‚°ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å®¹æ˜“ã«

**æœŸå¾…ã•ã‚Œã‚‹çµæœ**:
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ Effective Rank: **å®Œå…¨ã«åŒã˜å€¤** (å°æ•°ç‚¹ä»¥ä¸‹ã¾ã§ä¸€è‡´)
- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ Effective Rank: **å®Œå…¨ã«åŒã˜å€¤** (å°æ•°ç‚¹ä»¥ä¸‹ã¾ã§ä¸€è‡´)
- 3ã¤ã®ãƒã‚§ãƒƒã‚¯çµæœ: æ¯å›åŒã˜

---

## File Structure - Final Organization

**Main Scripts**:
- `test.py` - æ¨™æº–ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ6400è¨“ç·´ + 1280æ¤œè¨¼ï¼‰
- `train.py` - ãƒ•ãƒ«è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- `config.py` - è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

**Data Generation**:
- `scripts/create_val_from_train.py` - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰

**Core Implementation**:
- `src/trainers/phase1/memory.py` - Phase 1è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä¸¦åˆ—å‡¦ç†ç‰ˆï¼‰
- `src/trainers/phase2.py` - Phase 2è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼ï¼‰
- `src/models/llm.py` - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆCVFPï¼‰
- `src/providers/data/` - ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼

---

## Validation Data Policy - CRITICAL

### å¿…é ˆä»•æ§˜

**æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®éƒ¨åˆ†é›†åˆã§ãªã‘ã‚Œã°ãªã‚‰ãªã„**:
- å…¨ã¦ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨
- ãƒ©ãƒ³ãƒ€ãƒ ãªåˆ†å‰²ã¯ç¦æ­¢ï¼ˆ`auto_split` ä½¿ç”¨ã§ã‚¨ãƒ©ãƒ¼ï¼‰
- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥ç”Ÿæˆï¼ˆ`create_val_from_train.py`ï¼‰

### ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿãƒ­ã‚¸ãƒƒã‚¯

`loader.py` ã§å®Ÿè£…æ¸ˆã¿:
```python
if config.val_data_source == "auto_split":
    raise ValueError(
        "âŒ CRITICAL ERROR: auto_split is STRICTLY FORBIDDEN!"
        "Use val_data_source='text_file' with data/example_val.txt"
    )
```

---

## Code Quality Standards

### Principles

1. **No Hardcoding**: All hyperparameters in config.py
2. **Single Responsibility**: Each module has one clear purpose
3. **Immutable Data**: Training/validation data are fixed
4. **Error Prevention**: Auto-split is forbidden with error

### Anti-Patterns to Avoid

- âŒ Changing train/val data without regeneration
- âŒ Using auto_split for validation
- âŒ Modifying diversity loss implementation
- âŒ Changing architecture without full retraining

---

## Performance Benchmarks

**CPU Performance (Apple Silicon/Intel)**:
- Training speed: 250-330 tok/s
- 6400 tokens: ~25 seconds per iteration
- Validation: ~4 seconds (1280 tokens)

**Expected Results (commit 9ee3281 baseline)**:
- Training Effective Rank: 74.0% (568.31/768)
- Validation Effective Rank: 66.6% (511.56/768)
- Training Convergence: 30.0% (1923/6400 tokens)
- Validation Convergence: 100.0% (1280/1280 tokens)
- Full 10 iterations complete

---

## No Hardcoding Policy - Reinforced

**å…¨ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯config.pyã§å®šç¾©**:
```python
# âœ… Good
learning_rate = config.phase1_learning_rate
num_samples = config.num_samples

# âŒ Bad
learning_rate = 0.002  # Hardcoded!
num_samples = 50       # Hardcoded!
```

---

## ğŸ› CRITICAL BUG FIX HISTORY - November 24, 2025

### Bug #1: F.normalize() in CVFP Loss Calculation (src/training/phase1_trainer.py)

**Problem**:
- Location: [phase1_trainer.py:265-267](src/training/phase1_trainer.py#L265-L267)
- CVFP loss used `F.normalize()` on both `new_context` and `previous_context`
- This only enforces **cosine similarity** (direction), not **value equality**
- Fixed points require `f(x) = x` (exact values), not just same direction

**Symptoms**:
- 0% convergence rate despite 10 iterations
- MSE ~32-33 (vs threshold 0.1 = 300x larger)
- CVFP loss increasing instead of decreasing

**Root Cause**:
```python
# âŒ WRONG: Normalization prevents value convergence
cvfp_loss = F.mse_loss(
    F.normalize(new_context, p=2, dim=1),      # Only matches direction
    F.normalize(previous_context, p=2, dim=1)  # Norms can still diverge
)
```

**Fix**:
```python
# âœ… CORRECT: Raw MSE for exact value matching
cvfp_loss = F.mse_loss(new_context, previous_token_context)
```

**Affected File**: [src/training/phase1_trainer.py:267](src/training/phase1_trainer.py#L267)

---

### Bug #2: Missing context.detach() Between Tokens (src/training/phase1_trainer.py)

**Problem**:
- Location: [phase1_trainer.py:226-240](src/training/phase1_trainer.py#L226-L240)
- Context passed between tokens without `detach()`
- Gradient graph reused across token sequence
- RuntimeError: "Trying to backward through the graph a second time"

**Root Cause**:
```python
# âŒ WRONG: Gradient graph carries over
context = self._train_one_token(
    token_embed.unsqueeze(0),
    context,  # No detach - gradient accumulates across tokens
    token_idx=t
)
current_contexts[t] = context.squeeze(0)  # No detach for convergence check
```

**Fix**:
```python
# âœ… CORRECT: Detach between tokens
context = self._train_one_token(
    token_embed.unsqueeze(0),
    context.detach(),  # Break gradient flow between tokens
    token_idx=t
)
current_contexts[t] = context.squeeze(0).detach()  # Detach for convergence tracking
```

**Affected Lines**:
- [phase1_trainer.py:228](src/training/phase1_trainer.py#L228) - Training token processing
- [phase1_trainer.py:240](src/training/phase1_trainer.py#L240) - Convergence tracking

---

### Verification Results (After Fixes)

**With dist_reg_weight=0.01** (99% CVFP, 1% Diversity):
- âœ… Convergence mechanism works: 96.0% training, 100.0% validation
- âœ… CVFP loss decreases: 1.02 â†’ 0.021 â†’ 0.025
- âŒ Effective Rank collapsed: 6.9% training, 1.1% validation (vs 66.6% baseline)
- **Conclusion**: Bug fixed, but diversity weight too low

**With dist_reg_weight=0.5** (50% CVFP, 50% Diversity) - Baseline (commit 9ee3281):
- âœ… Training Effective Rank: 74.0% (568.31/768)
- âœ… Validation Effective Rank: 66.6% (511.56/768)
- âœ… Training Convergence: 30.0% (1923/6400)
- âœ… Validation Convergence: 100.0% (1280/1280)

---

## ğŸ“ NEW-LLM Detailed Architecture Specification

### Core Components

**1. ContextLayer / TokenLayer**
- Location: [src/models/llm.py](src/models/llm.py)
- ContextLayer: æ–‡è„ˆå‡¦ç†å°‚ç”¨ï¼ˆtokenç¶™ãè¶³ã—æ–¹å¼ï¼‰
- TokenLayer: ãƒˆãƒ¼ã‚¯ãƒ³å‡¦ç†å°‚ç”¨

**2. ContextBlock / TokenBlock**
- è¤‡æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é †æ¬¡å®Ÿè¡Œ
- ContextBlock: Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze
- TokenBlock: Phase 2ã§å­¦ç¿’

**3. LLM (Main Model)**
- Token Embedding: GPT-2 pretrained (768-dim, frozen in Phase 2)
- Weight Tying: token_output shares weights with token_embedding
- Output Head: Linear layer `embed_dim â†’ vocab_size`

**4. MemoryPhase1Trainer (CVFP Fixed-Point Learning)**
- Location: [src/trainers/phase1/memory.py](src/trainers/phase1/memory.py)
- ä¸¦åˆ—å‡¦ç†ç‰ˆï¼ˆ23xé«˜é€ŸåŒ–ï¼‰
- Loss function:
  - CVFP Loss: `MSE(context_t, context_{t-1})` - **NO normalization**
  - Diversity Loss: Global mean-based tracking
  - Total: `(1-w) * cvfp_loss + w * diversity_loss`

**5. Phase2Trainer (Next-Token Prediction)**
- Location: [src/trainers/phase2.py](src/trainers/phase2.py)
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
- ContextBlockå‡ºåŠ›ã‚’äº‹å‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã€TokenBlockã‚’ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†

### Key Design Decisions

**Dimension Constraints**:
- `hidden_dim = context_dim + embed_dim` (MANDATORY)
- Default: `context_dim=768, embed_dim=768, hidden_dim=1536`
- Reason: FNN output must split into delta_context + delta_token

**Context Carryover** (CRITICAL):
- Between iterations: `context = previous_contexts[-1]` (NOT zero reset)
- Between tokens: `context = context.detach()` (gradient isolation)
- Reason: Fixed-point learning requires continuity

**Gradient Management**:
- Token embeddings: Frozen (GPT-2 pretrained)
- Context params: Trained (all CVFP layers)
- Between tokens: Detached (prevent cross-token gradients)
- Reason: Stable training with efficient gradient flow

**Diversity Regularization**:
- Method: Per-dimension variance tracking with EMA
- Implementation: Negative L2 norm of deviation from mean
- Memory: O(context_dim) - 6KB for 768-dim
- Reason: Encourage usage of all dimensions

---

## Context Size Monitoring Policy

**Claude Codeã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†**:
- 100,000ãƒˆãƒ¼ã‚¯ãƒ³è¶…éæ™‚: åˆå›å ±å‘Š
- ä»¥é™10,000ãƒˆãƒ¼ã‚¯ãƒ³åˆ»ã¿ã§ç¶™ç¶šå ±å‘Š
- 190,000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Š: æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ã‚’å¼·ãæ¨å¥¨

---

Last Updated: 2025-12-01 (å…±æœ‰è¨­å®šã‚¯ãƒ©ã‚¹è¿½åŠ ã€å¤šæ§˜æ€§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 4ç¨®æ¡ç”¨ã€WMSEé™¤å¤–)
