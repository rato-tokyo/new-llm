# New-LLM Project Guidelines

---

## ğŸ¯ ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

**ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æŸ”è»Ÿãªè¨­è¨ˆã€‚**

### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

```
å¾“æ¥: 4ã¤ã®å›ºå®šãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
  PythiaModel, InfiniPythiaModel, MultiMemoryPythiaModel, HierarchicalMemoryPythiaModel

æ–°è¨­è¨ˆ: 1ã¤ã®æ±ç”¨ãƒ¢ãƒ‡ãƒ« + 4ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—
  TransformerLM + [PythiaLayer, InfiniLayer, MultiMemoryLayer, HierarchicalLayer]
```

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
TransformerLM:
  Token Embedding (512-dim)
         â†“
  Layer 0, 1, ..., N-1 (ä»»æ„ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—)
         â†“
  Final LayerNorm
         â†“
  LM Head (512 â†’ vocab)
```

### ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—

| ãƒ¬ã‚¤ãƒ¤ãƒ¼ | èª¬æ˜ |
|----------|------|
| `PythiaLayer` | æ¨™æº–Pythia (RoPE + Softmax Attention) |
| `InfiniLayer` | Infini-Attention (Memory + Linear Attention, NoPE) |
| `MultiMemoryLayer` | è¤‡æ•°ç‹¬ç«‹ãƒ¡ãƒ¢ãƒª + Attention-basedé¸æŠ |
| `HierarchicalLayer` | éšå±¤çš„ãƒ¡ãƒ¢ãƒª + å­¦ç¿’å¯èƒ½ãªå±•é–‹ã‚²ãƒ¼ãƒˆ |

---

## ğŸ­ ãƒ¢ãƒ‡ãƒ«ä½œæˆ

### create_model() ãƒ•ã‚¡ã‚¯ãƒˆãƒª

```python
from src.models import create_model

# åŸºæœ¬çš„ãªä½¿ã„æ–¹
model = create_model("pythia")       # æ¨™æº–Pythiaï¼ˆ6å±¤ï¼‰
model = create_model("infini")       # 1å±¤Infini + 5å±¤Pythia
model = create_model("multi_memory") # 1å±¤Multi-Memory + 5å±¤Pythia
model = create_model("hierarchical") # 1å±¤Hierarchical + 5å±¤Pythia

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ã
model = create_model("multi_memory", num_memories=8)
model = create_model("hierarchical", num_memories=4, use_delta_rule=False)
model = create_model("infini", num_memory_banks=2, segments_per_bank=4)
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹æˆ

```python
from src.models import TransformerLM
from src.models.layers import InfiniLayer, PythiaLayer, MultiMemoryLayer

# 2å±¤Infini + 4å±¤Pythia
layers = [
    InfiniLayer(hidden_size=512, num_heads=8, intermediate_size=2048),
    InfiniLayer(hidden_size=512, num_heads=8, intermediate_size=2048),
    *[PythiaLayer(hidden_size=512, num_heads=8, intermediate_size=2048) for _ in range(4)]
]
model = TransformerLM(layers=layers)

# å…¨å±¤Infini
layers = [InfiniLayer(512, 8, 2048) for _ in range(6)]
model = TransformerLM(layers=layers)

# æ··åˆæ§‹æˆ
layers = [
    MultiMemoryLayer(512, 8, 2048, num_memories=4),
    InfiniLayer(512, 8, 2048),
    *[PythiaLayer(512, 8, 2048) for _ in range(4)]
]
model = TransformerLM(layers=layers)
```

### åˆ©ç”¨å¯èƒ½ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | å¯¾è±¡ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|------------|------|------------|------|
| `use_delta_rule` | å…¨memoryç³» | `True` | Delta Ruleä½¿ç”¨ |
| `num_memories` | multi_memory, hierarchical | `4` | ãƒ¡ãƒ¢ãƒªæ•° |
| `num_memory_banks` | infini | `1` | ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯æ•° |
| `segments_per_bank` | infini | `4` | ãƒãƒ³ã‚¯ã‚ãŸã‚Šã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•° |

---

## ğŸ’¾ ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã®ä¿å­˜ãƒ»è»¢é€

```python
import torch
from src.models import create_model

# ===== PC A =====
model = create_model("infini")
model.reset_memory()

# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã§ãƒ¡ãƒ¢ãƒªã‚’è“„ç©
for batch in data_loader:
    _ = model(batch, update_memory=True)

# ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’ä¿å­˜
state = model.get_memory_state()
torch.save(state, "memory.pt")

# ===== PC B =====
state = torch.load("memory.pt")
model = create_model("infini")
model.set_memory_state(state)

# ãƒ¡ãƒ¢ãƒªãŒå¼•ãç¶™ãŒã‚ŒãŸçŠ¶æ…‹ã§æ¨è«–
output = model(input_ids)
```

### ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚º

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º |
|--------|--------|
| Infini (1 bank) | ~135 KB |
| Multi-Memory (4) | ~540 KB |
| Hierarchical (4) | ~540 KB |

---

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ 

```
src/models/
â”œâ”€â”€ __init__.py          # create_model() ãƒ•ã‚¡ã‚¯ãƒˆãƒª + exports
â”œâ”€â”€ layers/              # ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â”‚   â”œâ”€â”€ __init__.py      # exports
â”‚   â”œâ”€â”€ base.py          # BaseLayer åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ pythia.py        # PythiaLayer (RoPE + Softmax)
â”‚   â”œâ”€â”€ infini.py        # InfiniLayer (Memory + Linear)
â”‚   â”œâ”€â”€ multi_memory.py  # MultiMemoryLayer
â”‚   â””â”€â”€ hierarchical.py  # HierarchicalLayer
â”œâ”€â”€ model.py             # TransformerLMï¼ˆæ±ç”¨ãƒ¢ãƒ‡ãƒ«ï¼‰
â”œâ”€â”€ base_components.py   # PythiaMLP, init_weights
â”œâ”€â”€ memory_utils.py      # elu_plus_one, causal_linear_attention
â””â”€â”€ position_encoding.py # RoPE
```

---

## ğŸ§ª çµ±ä¸€å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
python3 scripts/experiment.py --models pythia infini multi_memory hierarchical

# Infiniã®ã¿
python3 scripts/experiment.py --models infini

# è¨­å®šã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
python3 scripts/experiment.py --models infini --samples 10000 --epochs 50 --lr 5e-5
```

---

## ğŸ“Š Reversal Curse è©•ä¾¡

| æŒ‡æ¨™ | å®šç¾© | è§£é‡ˆ |
|------|------|------|
| Forward PPL | é †æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãŸã‚ä½ã„ |
| Backward PPL | é€†æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ãŸã‚é«˜ã„ |
| Reversal Gap | Backward - Forward | 0ã«è¿‘ã„ã»ã©è‰¯ã„ |

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

### è¨“ç·´-è©•ä¾¡ä¸€è²«æ€§ï¼ˆTraining-Evaluation Consistencyï¼‰

**è¨“ç·´æ™‚ã¨è©•ä¾¡æ™‚ã®æ¡ä»¶ã¯å¿…ãšæƒãˆã‚‹ã€‚**

```python
# âŒ æ‚ªã„ä¾‹: é›¢æ•£çš„ãªthresholdï¼ˆè¨“ç·´æ™‚ã«ãƒã‚¤ã‚¢ã‚¹ç™ºç”Ÿï¼‰
# è¨“ç·´: threshold=0.5ã§ã‚¹ã‚­ãƒƒãƒ—ã€å‡ºåŠ›ä½ç½®ã®ã¿loss
# â†’ é«˜ç¢ºä¿¡åº¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã§lossè¨ˆç®— â†’ ç•°å¸¸ã«ä½ã„PPL
def train():
    output_mask = gate_prob > 0.5  # é›¢æ•£çš„åˆ¤å®š
    loss = (losses * output_mask).sum() / output_mask.sum()

# âœ… è‰¯ã„ä¾‹: é€£ç¶šçš„é‡ã¿ï¼ˆå…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­¦ç¿’ã«å¯„ä¸ï¼‰
# gate_probã‚’é‡ã¿ã¨ã—ã¦ä½¿ç”¨ â†’ å‹¾é…ãŒå¸¸ã«æµã‚Œã‚‹
def train():
    weighted_loss = (losses * gate_prob).sum() / gate_prob.sum()

def evaluate():
    weighted_ppl = exp(sum(gate_prob * log_loss) / sum(gate_prob))
```

**åŸå‰‡**:
1. é€£ç¶šçš„ãªé‡ã¿ã‚’ä½¿ç”¨ï¼ˆé›¢æ•£çš„ãªthresholdã¯å­¦ç¿’ãƒã‚¤ã‚¢ã‚¹ã‚’ç”Ÿã‚€ï¼‰
2. å…¨ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­¦ç¿’ã«å¯„ä¸ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
3. ç”Ÿæˆæ™‚ã®ã¿thresholdã‚’ä½¿ç”¨ï¼ˆè¨“ç·´ãƒ»è©•ä¾¡ã«ã¯ä½¿ã‚ãªã„ï¼‰

---

## âš ï¸ éå»ã®ãƒã‚°ã¨æ•™è¨“

### 1. Infini-Attention ãƒ¡ãƒ¢ãƒªå‹¾é…ãƒã‚°

```python
# âŒ ãƒã‚°: ãƒ¡ãƒ¢ãƒªæ›´æ–°ã§ã‚°ãƒ©ãƒ•ãŒæ®‹ã‚Šã€äºŒé‡backwardã‚¨ãƒ©ãƒ¼
self.memory = self.memory + memory_update

# âœ… ä¿®æ­£: detach()ã§ã‚°ãƒ©ãƒ•ã‚’åˆ‡æ–­
self.memory = (self.memory + memory_update).detach()
```

### 2. PPLç•°å¸¸å€¤ã®è¨ºæ–­åŸºæº–

| PPL | çŠ¶æ…‹ | å¯¾å‡¦ |
|-----|------|------|
| < 5 | **ç•°å¸¸** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯/å› æœãƒã‚¹ã‚¯ãƒã‚° | ã‚³ãƒ¼ãƒ‰ç‚¹æ¤œå¿…é ˆ |
| 5-30 | **ç–‘ã‚ã—ã„** - éå­¦ç¿’ã®å¯èƒ½æ€§ | ãƒ‡ãƒ¼ã‚¿é‡ãƒ»åˆ†å‰²ã‚’ç¢ºèª |
| 30-100 | æ­£å¸¸ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ | - |
| 100-500 | æ­£å¸¸ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼‰ | - |
| > 1000 | å­¦ç¿’ä¸è¶³ | epochå¢—åŠ /lrèª¿æ•´ |

### 3. Linear Attentionã®head_dimæ¬¡å…ƒæ•°å•é¡Œï¼ˆé‡è¦ï¼‰

**head_dimãŒå°ã•ã„ã¨Linear AttentionãŒæ©Ÿèƒ½ã—ãªã„ã€‚**

```python
# âŒ å•é¡Œ: head_dim=64ï¼ˆhidden_size=512 / num_heads=8ï¼‰
# â†’ 64æ¬¡å…ƒç©ºé–“ã§ã¯ç•°ãªã‚‹ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«ãŒç›´äº¤ã—ã‚„ã™ã„
# â†’ Ïƒ(Q) @ Ïƒ(K)^T â‰ˆ 0 â†’ ãƒ¡ãƒ¢ãƒªã‹ã‚‰ä½•ã‚‚å–ã‚Šå‡ºã›ãªã„

# âœ… è§£æ±º: ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆmemory_head_dim=hidden_size=512ï¼‰
# InfiniLayerã¯è‡ªå‹•çš„ã«ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’ä½¿ç”¨
```

**æ•™è¨“**:
1. Linear Attentionã«ã¯head_dim >= 256ãŒå¿…è¦
2. å¯èƒ½ãªã‚‰ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ï¼ˆhead_dim=hidden_sizeï¼‰ã‚’ä½¿ç”¨
3. alphaãŒå°ã•ã„ã¾ã¾ â†’ head_dimã‚’ç–‘ã†

### 4. PPLè©•ä¾¡æ–¹æ³•ã«ã‚ˆã‚‹PPLç•°å¸¸å€¤

**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†å‰²è©•ä¾¡ã§ç•°å¸¸ã«é«˜ã„PPLï¼ˆ10,000+ï¼‰ãŒå‡ºã‚‹åŸå› ã€‚**

```python
# âŒ å•é¡Œ: å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒç‹¬ç«‹ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãªã—ï¼‰
for start in range(0, seq_len, segment_length):
    segment = tokens[start:end]
    # â†’ ã€Œæ–‡æ›¸ã®é€”ä¸­ã€ã‚’ã€Œæ–‡æ›¸ã®å…ˆé ­ã€ã¨ã—ã¦æ‰±ã†ãŸã‚é«˜PPL

# âœ… æ­£ã—ã„è©•ä¾¡: Sliding Windowæ–¹å¼
stride = 512
for start in range(0, seq_len - 1, stride):
    input_ids = tokens[start:start+2048]
    labels = input_ids.clone()
    labels[0, :stride] = -100  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ†ã¯lossè¨ˆç®—ã‹ã‚‰é™¤å¤–
```

| è©•ä¾¡æ–¹æ³• | PPL |
|----------|-----|
| Sliding window (stride=512) | **40.96** âœ“ |
| Segment-based | 14,204 âŒ |

---

## ğŸ”§ Pretrained LLMã¸ã®Infini-Attentionå°å…¥ï¼ˆå¤±æ•—ï¼‰

**âš ï¸ Layerç½®ãæ›ãˆæ–¹å¼ã¯å…¨ã¦å¤±æ•—ã€‚**

| æ–¹å¼ | çµæœ |
|------|------|
| Layer 0ç½®ãæ›ãˆ | âŒ RoPEæå¤±ã€PPLå¤§å¹…åŠ£åŒ– |
| è’¸ç•™+Fine-tune | âŒ PPL 44â†’1237ï¼ˆ28å€åŠ£åŒ–ï¼‰ |
| Parallel Adapter | âŒ alphaãŒå­¦ç¿’ã•ã‚Œãªã„ |

**æ¨å¥¨**: ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼ˆæœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰

è©³ç´°ã¯ `docs/experiments/2025-12-06_distill_finetune_failure.md` ã‚’å‚ç…§ã€‚

---

## ğŸ”§ Selective Output LM

**ä»®èª¬: LLMã¯å³åº§ã«å‡ºåŠ›ã›ãšã€éš ã‚ŒçŠ¶æ…‹ã‚’è¿½åŠ å‡¦ç†ã—ã¦ã‹ã‚‰å‡ºåŠ›ã™ã¹ã**

### ã‚³ãƒ³ã‚»ãƒ—ãƒˆ

```
å¾“æ¥ã®Continuous:
  å…¥åŠ›A â†’ Transformerå‡¦ç† â†’ å³åº§ã«æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³"B"ã‚’äºˆæ¸¬

Selective (skip_interval=1):
  å…¥åŠ›A â†’ Transformerå‡¦ç† â†’ éš ã‚ŒçŠ¶æ…‹h1ï¼ˆã¾ã å‡ºåŠ›ã—ãªã„ï¼‰
       â†’ h1ã‚’è¿½åŠ å‡¦ç† â†’ éš ã‚ŒçŠ¶æ…‹h2 â†’ æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³"B"ã‚’äºˆæ¸¬
```

### skip_interval ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| å€¤ | å‹•ä½œ | èª¬æ˜ |
|----|------|------|
| 0 | è¿½åŠ å‡¦ç†ãªã— | å¾“æ¥ã®Continuousã¨åŒç­‰ |
| 1 | 1å›è¿½åŠ å‡¦ç† | ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›å¾Œã€1å›è¿½åŠ ã§Transformeré€šéã—ã¦ã‹ã‚‰å‡ºåŠ› |
| 2 | 2å›è¿½åŠ å‡¦ç† | ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›å¾Œã€2å›è¿½åŠ ã§Transformeré€šéã—ã¦ã‹ã‚‰å‡ºåŠ› |

### ä½¿ç”¨æ–¹æ³•

```python
from src.models import create_model

# Selective (1å›è¿½åŠ å‡¦ç†)
model = create_model("selective", skip_interval=1)

# Baseline (è¿½åŠ å‡¦ç†ãªã— = Continuous)
model = create_model("selective", skip_interval=0)

# è¨“ç·´
loss, stats = model.compute_loss(input_ids, labels, use_selective=True)

# ç”Ÿæˆ
output, stats = model.generate(input_ids, max_new_tokens=50, use_selective=True)
```

### å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# Selective (skip_interval=1)
python3 scripts/experiment_selective.py

# skip_interval=2
python3 scripts/experiment_selective.py --skip-interval 2

# Baselineã¨ã®æ¯”è¼ƒ
python3 scripts/experiment_selective.py --models baseline selective
```

---

## ğŸ”§ å­¦ç¿’å¯èƒ½ã‚²ãƒ¼ãƒˆã«ã‚ˆã‚‹Selective Outputï¼ˆå¤±æ•—ï¼‰

**âš ï¸ å‹•çš„ã‚²ãƒ¼ãƒˆæ–¹å¼ã¯è¤‡é›‘ã™ãã¦å¤±æ•—ã€‚ç¾åœ¨ã¯å›ºå®šskip_intervalã«ç°¡ç´ åŒ–ã€‚**

### è©¦ã—ãŸæ–¹å¼

| æ–¹å¼ | çµæœ |
|------|------|
| OutputGate + threshold | âŒ carry-over 96.3%ã€ã»ã¼å­¦ç¿’ã•ã‚Œãªã„ |
| max_skipå¼·åˆ¶ + threshold | âŒ åæŸãŒä¸å®‰å®šã€PPLæ”¹å–„ã›ãš |
| ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹gate_loss | âŒ ã‚²ãƒ¼ãƒˆãŒé©åˆ‡ã«å­¦ç¿’ã•ã‚Œãªã„ |

### å•é¡Œç‚¹

1. **ã‚²ãƒ¼ãƒˆå­¦ç¿’ã®ä¸å®‰å®šæ€§**: gate_probãŒthresholdã«åæŸã—ãªã„
2. **carry-overç‡ã®åˆ¶å¾¡å›°é›£**: å‹•çš„ã«æŒã¡è¶Šã—åˆ¤æ–­ã™ã‚‹ã¨äºˆæ¸¬å›°é›£
3. **å‹¾é…ã®ä¸é€£ç¶šæ€§**: thresholdåˆ¤å®šã§å‹¾é…ãŒé€”åˆ‡ã‚Œã‚‹
4. **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®è¤‡é›‘ã•**: æŒã¡è¶Šã—æ™‚ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨ˆç®—ãŒéè‡ªæ˜

### æ•™è¨“

- å‹•çš„ãªåˆ¤æ–­ã‚ˆã‚Š**å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³**ï¼ˆskip_intervalï¼‰ãŒã‚·ãƒ³ãƒ—ãƒ«
- å­¦ç¿’å¯èƒ½ã‚²ãƒ¼ãƒˆã¯è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ65Kï¼‰ã«å¯¾ã—ã¦åŠ¹æœãŒè¦‹åˆã‚ãªã„
- OutputGateã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ™ãƒ¼ã‚¹æå¤±ã¯å‰Šé™¤æ¸ˆã¿

è©³ç´°ã¯ `docs/experiments/2025-12-07_selective_output_gate_failure.md` ã‚’å‚ç…§ã€‚

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-07 | **Selective Output LMå†è¨­è¨ˆ**: éš ã‚ŒçŠ¶æ…‹ã®è¿½åŠ å‡¦ç†æ–¹å¼ã«å¤‰æ›´ï¼ˆskip_interval=è¿½åŠ å‡¦ç†å›æ•°ï¼‰ |
| 2025-12-07 | **å­¦ç¿’å¯èƒ½ã‚²ãƒ¼ãƒˆå¤±æ•—ã‚’è¨˜éŒ²**: OutputGateæ–¹å¼ã¯è¤‡é›‘ã™ãã¦å¤±æ•—ã€å›ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³ã«ç°¡ç´ åŒ– |
| 2025-12-07 | **è¨“ç·´-è©•ä¾¡ä¸€è²«æ€§ãƒãƒªã‚·ãƒ¼è¿½åŠ **: è¨“ç·´æ™‚ã¨è©•ä¾¡æ™‚ã®æ¡ä»¶ã‚’æƒãˆã‚‹ã“ã¨ã‚’å¿…é ˆåŒ– |
| 2025-12-06 | **SelectiveOutputLMè¿½åŠ **: å­¦ç¿’å¯èƒ½ã‚²ãƒ¼ãƒˆã«ã‚ˆã‚‹é¸æŠçš„å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ï¼ˆå¾Œã«å¤±æ•—ã¨åˆ¤æ˜ï¼‰ |
| 2025-12-06 | **ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ç§»è¡Œ**: TransformerLM + 4ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¿ã‚¤ãƒ—ã€ã‚³ãƒ¼ãƒ‰31%å‰Šæ¸› |
| 2025-12-06 | **Layerç½®ãæ›ãˆæ–¹å¼ã‚’å‰Šé™¤**: è’¸ç•™+Fine-tuneç­‰ã™ã¹ã¦å¤±æ•—ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ã«é›†ä¸­ |
| 2025-12-06 | **ã‚·ãƒ³ã‚°ãƒ«ãƒ˜ãƒƒãƒ‰ãƒ¡ãƒ¢ãƒªå°å…¥**: memory_head_dim=512ã§Linear Attentionã®è¡¨ç¾åŠ›ã‚’æœ€å¤§åŒ– |
| 2025-12-06 | **PPLè©•ä¾¡æ–¹æ³•ã®æ•™è¨“è¿½åŠ **: Sliding windowæ–¹å¼ãŒæ­£ã—ã„ |
| 2025-12-06 | **ãƒ¡ãƒ¢ãƒªè»¢é€APIè¿½åŠ **: get_memory_state/set_memory_stateã§åœ§ç¸®ãƒ¡ãƒ¢ãƒªã‚’åˆ¥PCã«è»¢é€å¯èƒ½ |
| 2025-12-06 | **ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¯ãƒˆãƒªè¿½åŠ **: create_model()ã§ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒ¢ãƒ‡ãƒ«ä½œæˆ |
| 2025-12-06 | **Hierarchical Memoryè¿½åŠ **: å­¦ç¿’å¯èƒ½ãªå±•é–‹åˆ¤æ–­ã€Coarse-to-Fineæ¤œç´¢ |
| 2025-12-06 | **Multi-Memory Attentionè¿½åŠ **: Attention-basedé¸æŠã§è¤‡æ•°ãƒ¡ãƒ¢ãƒªã‚’å‹•çš„æ··åˆ |
| 2025-12-05 | **Infini-Pythiaå®Ÿè£…**: 1å±¤ç›®Infini + RoPE |

---

Last Updated: 2025-12-07
