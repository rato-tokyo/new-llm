# New-LLM Project Guidelines

## ğŸ¯ Project Goal: Pythia-70M + Context-KV Attention (2025-12-03)

**Pythia-70Mã®Layer 0ã‚’Context-KV Attentionã«ç½®ãæ›ãˆã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã‚’50%å‰Šæ¸›ã™ã‚‹ã€‚**

### Target Architecture

```
Pythia-70M (6 layers, hidden_size=512, heads=8)
  â†“
Layer 0: Context-KV Attentionï¼ˆç½®ãæ›ãˆï¼‰
Layer 1-5: Original Pythia Attentionï¼ˆç¶­æŒï¼‰
```

### Key Decisions

| é …ç›® | æ±ºå®š |
|------|------|
| **ç½®ãæ›ãˆå±¤** | Layer 0 ã®ã¿ã‹ã‚‰é–‹å§‹ |
| **Contextæ¬¡å…ƒ** | 256ï¼ˆç©æ¥µçš„ãªåœ§ç¸®ï¼‰ |
| **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿** | Pileï¼ˆPythiaã¨åŒã˜ï¼‰ã€é–‹ç™ºæ™‚ã¯é™å®šã‚µãƒ³ãƒ—ãƒ« |
| **å­¦ç¿’æ–¹æ³•** | Phase 1ï¼ˆOACDï¼‰â†’ Phase 2ï¼ˆå…¨ä½“ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ |
| **è©•ä¾¡æŒ‡æ¨™** | PPL + LAMBADA |
| **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç›®æ¨™** | 50% |

---

## ğŸ¯ Context-KV Attention Architecture

**KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹Context-KV Attentionæ–¹å¼ã‚’æ¡ç”¨ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
Context-KV Attention:
  - ç­‰é–“éš”ï¼ˆintervalï¼‰ã§Contextã‚’å–å¾—
  - å¸¸ã«ã€Œç¾åœ¨ä½ç½®ã€ã‚’å«ã‚ãŸcontextã§Attention
  - ~50% KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›ï¼ˆLayer 0ã®ã¿ç½®ãæ›ãˆæ™‚ï¼‰
```

### ğŸš¨ Context Intervalæ–¹å¼ï¼ˆé‡è¦ï¼‰

**Position i ã®äºˆæ¸¬ã«ã¯ã€ç¾åœ¨ä½ç½®ã‹ã‚‰ç­‰é–“éš”ã§éå»ã®contextã‚’å–å¾—ï¼š**

```
interval = 32 ã®å ´åˆ:

Position 350:
  KV Cache = [context[350], context[318], context[286], ...]
              â†‘ç¾åœ¨          â†‘32å‰         â†‘64å‰
           = 11 context vectors + zero padding

Position 1000:
  KV Cache = [context[1000], context[968], ..., context[8]]
           = 32 context vectors (max_contexts)
```

### ğŸš¨ max_contextsï¼ˆContext Windowï¼‰è¨­è¨ˆæ–¹é‡

**é€šå¸¸LLMã®ã€Œcontext windowã€ã¨åŒæ§˜ã«ã€ä½¿ç”¨ã™ã‚‹contextæ•°ã«ä¸Šé™ã‚’è¨­ã‘ã‚‹ã€‚**

```
é€šå¸¸LLM:
  - max_length ã§å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’åˆ¶é™
  - å¤ã„ãƒˆãƒ¼ã‚¯ãƒ³ã¯åˆ‡ã‚Šæ¨ã¦

Context-KVæ–¹å¼:
  - max_contexts ã§ä½¿ç”¨ã™ã‚‹contextæ•°ã‚’åˆ¶é™
  - å¤ã„contextã¯åˆ‡ã‚Šæ¨ã¦
```

---

## ğŸ¯ OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  (Phase 1)

**Phase 1ã§ã¯OACD (Origin-Anchored Centroid Dispersion) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¡ç”¨ã€‚**

```python
def oacd_loss(contexts, centroid_weight=0.1):
    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–
    dispersion_loss = -||X - mean(X)|| / n

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = ||mean(X)||Â²

    return dispersion_loss + centroid_weight * centroid_loss
```

---

## ğŸš¨ğŸš¨ Phase 1å­¦ç¿’ã§ã¯é †æ¬¡å‡¦ç†ç¦æ­¢ (CRITICAL) ğŸš¨ğŸš¨

**Phase 1å­¦ç¿’ã§ã¯ã€é †æ¬¡å‡¦ç†ã¯å³ç¦ã€‚å¿…ãšshifted_prev_contextæ–¹å¼ã§ä¸¦åˆ—å‡¦ç†ã™ã‚‹ã“ã¨ã€‚**

```python
# âŒ ç¦æ­¢: Phase 1å­¦ç¿’ã§é †æ¬¡å‡¦ç†ï¼ˆéå¸¸ã«é…ã„ï¼‰
for i in range(num_tokens):
    new_context = model.forward_context(prev_context, token_embed)
    prev_context = new_context

# âœ… æ¨å¥¨: shifted_prev_contextæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
for iteration in range(max_iterations):
    shifted_prev_context = torch.cat([zero_init, previous_contexts[:-1]], dim=0)
    new_contexts = model.forward_context(shifted_prev_context, input_embeds)
    previous_contexts = new_contexts
```

---

## ğŸš¨ CPU/GPUãƒ†ãƒ³ã‚½ãƒ«ç®¡ç†

**å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§OOMã‚’é˜²ããŸã‚ã€ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚’å¾¹åº•ã€‚**

```python
# âŒ ä¿®æ­£å‰
batch_contexts = previous_contexts[start_idx:end_idx].detach()

# âœ… ä¿®æ­£å¾Œ
batch_contexts = previous_contexts[start_idx:end_idx].detach().to(self.device)
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒã®Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports

# å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
python3 -m ruff check scripts/experiment_context_kv.py
python3 -m mypy scripts/experiment_context_kv.py --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Pythia-70M Base Architecture

| Parameter | Value |
|-----------|-------|
| Layers | 6 |
| Hidden Size | 512 |
| Attention Heads | 8 |
| Total Parameters | 70M |

### Context-KV Replacement (Layer 0)

**1. ContextBlock**
- 1å±¤å›ºå®šã€Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze
- OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¤šæ§˜æ€§å­¦ç¿’
- context_dim = 256

**2. Context-KV Attention**
- Contextã‚’K,Vã«å¤‰æ›
- ç­‰é–“éš”ï¼ˆintervalï¼‰ã§contextã‚’å–å¾—ã—ã¦Attention
- å¸¸ã«ç¾åœ¨ä½ç½®ã®contextã‚’å«ã‚ã‚‹

### Training Pipeline

**Phase 1: Contextå¤šæ§˜æ€§å­¦ç¿’ï¼ˆOACDï¼‰**
- **å­¦ç¿’å¯¾è±¡**: ContextBlockã®ã¿
- **æå¤±**: OACDï¼ˆå¤šæ§˜æ€§æå¤±ï¼‰
- **ãƒ‡ãƒ¼ã‚¿**: Pileï¼ˆé–‹ç™ºæ™‚ã¯é™å®šã‚µãƒ³ãƒ—ãƒ«ï¼‰

**Phase 2: å…¨ä½“ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**
- **ContextBlock**: frozenï¼ˆé‡ã¿å›ºå®šï¼‰
- **Context-KV Attention**: å­¦ç¿’
- **Pythia Layer 1-5**: ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- **æå¤±**: CrossEntropyï¼ˆæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ï¼‰

---

## Code Quality Standards

### Principles

1. **No Hardcoding**: All hyperparameters in config.py
2. **Single Responsibility**: Each module has one clear purpose
3. **Type Hints Required**: é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯å‹æ³¨é‡ˆã‚’å¿…é ˆ

### ğŸš¨ğŸš¨ ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦ - å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ (CRITICAL) ğŸš¨ğŸš¨

**å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã—ãªã„ã€‚å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

**ç¦æ­¢äº‹é …:**
1. é–¢æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°ã«æ•°å€¤ã‚’ç›´æ¥æ›¸ã
2. argparseã®defaultã«æ•°å€¤ã‚’ç›´æ¥æ›¸ã
3. ã‚³ãƒ¼ãƒ‰å†…ã«ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‚’æ›¸ã

```python
# âŒ ç¦æ­¢: é–¢æ•°ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¼•æ•°ã«ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
def train_phase2(..., num_epochs: int = 40, patience: int = 3):
    ...

# âŒ ç¦æ­¢: argparseã®defaultã«ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
parser.add_argument('--samples', type=int, default=200)

# âœ… æ¨å¥¨: configã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆé–¢æ•°ï¼‰
def train_phase2(..., num_epochs: int, patience: int):  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ãªã—
    ...

# âœ… æ¨å¥¨: configã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆargparseï¼‰
default_config = Config()
parser.add_argument('--samples', type=int, default=default_config.num_samples)

# âœ… æ¨å¥¨: å‘¼ã³å‡ºã—æ™‚ã«configã‹ã‚‰å€¤ã‚’æ¸¡ã™
train_phase2(
    ...,
    num_epochs=base_config.phase2_epochs,
    patience=base_config.phase2_patience,
)
```

**Config ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ:**
- `config/base.py` - ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒ‡ãƒ¼ã‚¿è¨­å®šã€max_contextsã€context_interval
- `config/phase1.py` - Phase 1å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆmax_iterations, early_stoppingç­‰ï¼‰
- `config/phase2.py` - Phase 2å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆepochs, patience, lrç­‰ï¼‰
- `config/__init__.py` - çµ±åˆConfigã‚¯ãƒ©ã‚¹

**ã“ã®æ–¹é‡ã®ç†ç”±:**
- è¨­å®šå¤‰æ›´ã¯configãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã§å®Œçµ
- å®Ÿé¨“ã®å†ç¾æ€§ã‚’ä¿è¨¼
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€å…ƒç®¡ç†

---

## File Structure

**Main Scripts**:
- `scripts/experiment_context_kv.py` - Context-KV Attentionå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç¾è¡Œï¼‰
- `scripts/experiment_pythia_context_kv.py` - Pythiaçµ±åˆå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆäºˆå®šï¼‰

**Core Implementation**:
- `src/models/context_kv.py` - ContextKVAttentionLLM
- `src/models/blocks.py` - ContextBlockï¼ˆ1å±¤å›ºå®šï¼‰
- `src/models/layers.py` - ContextLayer
- `src/trainers/phase1/memory.py` - Phase 1è¨“ç·´ãƒ­ã‚¸ãƒƒã‚¯
- `src/losses/diversity.py` - OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

---

## Evaluation Metrics

### Primary

| Metric | Purpose |
|--------|---------|
| **PPL (Perplexity)** | è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°å“è³ª |
| **LAMBADA Accuracy** | é•·è·é›¢ä¾å­˜æ€§ï¼ˆæœ€çµ‚å˜èªäºˆæ¸¬ï¼‰ |
| **KV Cache Memory** | å®Ÿéš›ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |

### Comparison Plan

```
Baseline: Pythia-70M (original)
Ours:     Pythia-70M + Context-KV (Layer 0 replaced)

Evaluate on:
- WikiText-2 PPL
- Pile test set PPL
- LAMBADA accuracy
- torch.cuda.max_memory_allocated()
```

---

## Related Work

- **DeepSeek MLA**: Low-rank KV compression (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨)
- **æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: Context-based KV compression (intervalé–“éš”)

---

Last Updated: 2025-12-03 (Pythia-70Mçµ±åˆæ–¹é‡ã«ç§»è¡Œ)
