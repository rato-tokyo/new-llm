# New-LLM Project Guidelines

## ğŸ¯ Project Goal: Context-Pythia (2025-12-03)

**Pythia-70Mã®å…¨Layerã‚’Context-based Attentionã«ç½®ãæ›ãˆã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã‚’50%å‰Šæ¸›ã™ã‚‹ã€‚**

### Target Architecture

```
Context-Pythia:
  Token Embedding (512-dim)
       â†“
  ContextBlock: 512 â†’ 256 (åœ§ç¸®)
       â†“
  Layer 0-5: å…¨ã¦ context (256-dim) ã‚’å…¥åŠ›
       â†“
  Output Head (vocab_size)
```

### Key Decisions

| é …ç›® | æ±ºå®š |
|------|------|
| **ç½®ãæ›ãˆå±¤** | å…¨6Layer |
| **Contextæ¬¡å…ƒ** | 256ï¼ˆ50%åœ§ç¸®ï¼‰ |
| **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿** | Pileï¼ˆPythiaã¨åŒã˜ï¼‰ã€é–‹ç™ºæ™‚ã¯é™å®šã‚µãƒ³ãƒ—ãƒ« |
| **å­¦ç¿’æ–¹æ³•** | Phase 1ï¼ˆOACDï¼‰â†’ Phase 2ï¼ˆå…¨ä½“å­¦ç¿’ï¼‰ |
| **è©•ä¾¡æŒ‡æ¨™** | PPL + LAMBADA |
| **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç›®æ¨™** | 50% |

---

## ğŸ† Baseline: Pythia Scaling Suite

**æˆ‘ã€…ã®ãƒ©ã‚¤ãƒãƒ«ã€‚Pythiaãƒ¢ãƒ‡ãƒ«ã‚¹ã‚¤ãƒ¼ãƒˆã®æ€§èƒ½ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç›®æ¨™ã€‚**

### Pythia Model Suite

| Model | Params | Layers | Hidden | Heads | Training Data |
|-------|--------|--------|--------|-------|---------------|
| Pythia-70M | 70M | 6 | 512 | 8 | Pile (~300B tokens) |
| Pythia-160M | 160M | 12 | 768 | 12 | Pile (~300B tokens) |
| Pythia-410M | 410M | 24 | 1024 | 16 | Pile (~300B tokens) |
| Pythia-1B | 1B | 16 | 2048 | 8 | Pile (~300B tokens) |
| Pythia-1.4B | 1.4B | 24 | 2048 | 16 | Pile (~300B tokens) |

### Pythia-70M Specifications

- **Architecture**: GPT-NeoX (Transformer decoder)
- **Layers**: 6
- **Hidden Size**: 512
- **Attention Heads**: 8
- **Intermediate Size**: 2048
- **Position Encoding**: Rotary (RoPE, 25%)
- **Vocab Size**: 50,304
- **Training**: ~300B tokens on the Pile
- **Parallel Attention**: Yes (attention + MLP in parallel)

### Evaluation Benchmarks (from Pythia paper)

- **LAMBADA**: é•·è·é›¢ä¾å­˜æ€§ï¼ˆæœ€çµ‚å˜èªäºˆæ¸¬ï¼‰
- **WikiText**: Perplexity
- **HellaSwag**: å¸¸è­˜æ¨è«–
- **PIQA**: ç‰©ç†çš„ç›´æ„Ÿ
- **ARC**: æ¨è«–

å‚è€ƒ: [Pythia Paper](https://arxiv.org/abs/2304.01373), [GitHub](https://github.com/EleutherAI/pythia)

---

## ğŸ“ Context-Pythia Architecture

### æ–°æ–¹å¼: Contextæ¬¡å…ƒåœ§ç¸®

```
é€šå¸¸Pythia:
  KV Cache = hidden_size (512) Ã— seq_len Ã— num_layers (6)

Context-Pythia:
  KV Cache = context_dim (256) Ã— seq_len Ã— num_layers (6)

å‰Šæ¸›ç‡ = 1 - (256/512) = 50%
```

### Components

**1. ContextBlock**
- å…¥åŠ›: prev_context (256) + token_embed (512)
- å‡ºåŠ›: context (256)
- Phase 1ã§OACDå­¦ç¿’ã€Phase 2ã§freeze

**2. ContextPythiaLayer**
- å…¥åŠ›: context (256-dim)
- query_key_value: Linear(256 â†’ 1536)
- å‡ºåŠ›: hidden_states (512-dim)
- 6 Layersã€å…¨ã¦åŒã˜æ§‹é€ 

**3. Output Head**
- Linear(512 â†’ vocab_size)

---

## ğŸš¨ CRITICAL: Phase 1å­¦ç¿’ã¯å¿…é ˆ

**Phase 1ï¼ˆOACDï¼‰å­¦ç¿’ã¯Context-Pythiaã®æ ¸å¿ƒã§ã‚ã‚Šã€çµ¶å¯¾ã«ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã¯ãªã‚‰ãªã„ã€‚**

### å­¦ç¿’ãƒ•ãƒ­ãƒ¼ï¼ˆå¿…é ˆï¼‰

```
Phase 1: OACD (ContextBlockå¤šæ§˜æ€§å­¦ç¿’)
  â”œâ”€ ContextBlockã®ã¿ã‚’å­¦ç¿’
  â”œâ”€ OACDæå¤±ã§å¤šæ§˜ãªcontext vectorã‚’ç”Ÿæˆ
  â””â”€ åæŸã¾ã§å®Ÿè¡Œï¼ˆ~60 iterationsï¼‰
       â†“
Phase 2: Full Training (ContextBlock frozen)
  â”œâ”€ ContextBlockã‚’freeze
  â”œâ”€ Layers + Output Headã‚’å­¦ç¿’
  â””â”€ Cross-entropyæå¤±
```

### ãªãœPhase 1ãŒå¿…é ˆã‹

1. **å¤šæ§˜æ€§ç¢ºä¿**: Phase 1ãªã—ã§ã¯context vectorãŒç¸®é€€ã—ã€æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹
2. **å­¦ç¿’å®‰å®šæ€§**: å¤šæ§˜ãªcontextãŒãªã„ã¨Phase 2ã®å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹
3. **æ€§èƒ½**: Phase 1ã‚’çµŒã‚‹ã“ã¨ã§ã€åœ§ç¸®å¾Œã‚‚è¡¨ç¾åŠ›ã‚’ç¶­æŒã§ãã‚‹

### Phase 1ã®å®Ÿè£…ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/losses/diversity.py - å‰Šé™¤ç¦æ­¢
def oacd_loss(contexts, centroid_weight=0.1):
    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–
    dispersion_loss = -||X - mean(X)|| / n

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = ||mean(X)||Â²

    return dispersion_loss + centroid_weight * centroid_loss
```

### å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å¿…é ˆæ§‹é€ 

```python
# scripts/experiment_pythia_comparison.py
# Phase 1ã¯å¿…ãšå®Ÿè¡Œã™ã‚‹ã“ã¨

# Phase 1: OACD
phase1_loss = train_phase1_oacd(model, train_loader, device, config)

# Phase 2: Full Training (ContextBlock frozen)
model.freeze_context_block()
# ... CE loss training
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒ

### Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports
```

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
python3 scripts/experiment_pythia_comparison.py --samples 10000 --seq-length 256 --epochs 10
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ç¦æ­¢ï¼ˆé‡è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

**ã‚µãƒ³ãƒ—ãƒ«æ•°ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã€ã‚¨ãƒãƒƒã‚¯æ•°ã¯å¿…é ˆå¼•æ•°ã¨ã™ã‚‹ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯äºˆæœŸã›ã¬å•é¡Œã‚’å¼•ãèµ·ã“ã™ãŸã‚ç¦æ­¢ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢ï¼ˆå³ç¦ï¼‰

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**

ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ã§ã¯ï¼š
- è¨€èªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãªã„ãŸã‚å­¦ç¿’ä¸å¯èƒ½
- PPLãŒç†è«–å€¤ï¼ˆlog(vocab_size) â‰ˆ 10.8ï¼‰ã§åæŸã—ã€æ”¹å–„ã—ãªã„
- å®Ÿé¨“ã¨ã—ã¦ç„¡æ„å‘³

```python
# âŒ ç¦æ­¢: ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿
input_ids = torch.randint(0, vocab_size, (num_samples, seq_length))

# âœ… å¿…é ˆ: å®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨
inputs, targets = load_pile_data(num_samples, seq_length, config, device)
```

```python
# âŒ ç¦æ­¢: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚ã‚Š
parser.add_argument('--samples', type=int, default=200)

# âœ… å¿…é ˆ: required=True
parser.add_argument('--samples', type=int, required=True, help='(REQUIRED)')
parser.add_argument('--seq-length', type=int, required=True, help='(REQUIRED)')
parser.add_argument('--epochs', type=int, required=True, help='(REQUIRED)')
```

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pythia.py              # PythiaConfig, ContextPythiaConfig
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ experiment_pythia_comparison.py  # æ¯”è¼ƒå®Ÿé¨“
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pythia.py          # PythiaModel (baseline)
â”‚   â”‚   â””â”€â”€ context_pythia.py  # ContextPythiaModel (ours)
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ diversity.py       # OACD algorithm
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## Evaluation Metrics

### Primary

| Metric | Purpose |
|--------|---------|
| **PPL (Perplexity)** | è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°å“è³ª |
| **LAMBADA Accuracy** | é•·è·é›¢ä¾å­˜æ€§ï¼ˆæœ€çµ‚å˜èªäºˆæ¸¬ï¼‰ |
| **KV Cache Memory** | å®Ÿéš›ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ |

### Comparison Plan

```
Baseline: PythiaModel (our reproduction)
Ours:     ContextPythiaModel (50% KV reduction)

Evaluate on:
- WikiText-2 PPL
- LAMBADA accuracy
- torch.cuda.max_memory_allocated()
```

---

## Related Work

- **DeepSeek MLA**: Low-rank KV compression (ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨)
- **æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: Context-based dimension reduction (å…¨Layer)

---

Last Updated: 2025-12-03 (å…¨Layerç½®ãæ›ãˆæ–¹å¼ã«ç§»è¡Œ)
