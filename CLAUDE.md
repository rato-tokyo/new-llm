# New-LLM Project Guidelines

---

## âš ï¸âš ï¸âš ï¸ CLAUDE AIã¸ã®é‡è¦ãªè­¦å‘Š âš ï¸âš ï¸âš ï¸

**ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚**

### éå»ã®å•é¡Œ

2025-12-04ã«Pythiaçµ±åˆã‚’è©¦ã¿ãŸéš›ã€CLAUDE.mdã®ç·¨é›†æ™‚ã«Phase 1ã®é‡è¦ãªä»•æ§˜ãŒèª¤ã£ã¦å‰Šé™¤ã•ã‚Œã¾ã—ãŸã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€Phase 1ã®å­¦ç¿’ãŒæ­£å¸¸ã«åæŸã—ãªããªã‚Šã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä»¥å‰ã®çŠ¶æ…‹ã«ãƒªãƒãƒ¼ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚

### ãƒ«ãƒ¼ãƒ«

1. **Phase 1ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ¶å¯¾ã«å‰Šé™¤ç¦æ­¢**
2. **ContextBlock/ContextLayerã®å®Ÿè£…è©³ç´°ã¯å‰Šé™¤ç¦æ­¢**
3. **åˆæœŸåŒ–æ–¹æ³•ï¼ˆnormal_ std=0.1ï¼‰ã®è¨˜è¿°ã¯å‰Šé™¤ç¦æ­¢**
4. CLAUDE.mdã‚’ç·¨é›†ã™ã‚‹éš›ã¯ã€æ—¢å­˜ã®é‡è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ®‹ã£ã¦ã„ã‚‹ã“ã¨ã‚’å¿…ãšç¢ºèªã™ã‚‹ã“ã¨

---

## ğŸ¯ Context-Pythia Architecture (2025-12-04)

**Pythia-70Mã‚’ãƒ™ãƒ¼ã‚¹ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’50%å‰Šæ¸›ã™ã‚‹Context-Pythiaæ–¹å¼ã‚’æ¡ç”¨ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
Context-Pythia:
  Token Embedding (512-dim) â† Pythia-70M
       â†“
  ContextBlock: 512 â†’ 256 (åœ§ç¸®)
       â†“
  Layer 0-5: å…¨ã¦ context (256-dim) ã‚’å…¥åŠ›
       â†“
  Output Head (vocab_size=50304)

KVã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›: 50%
  å…ƒ: hidden_size (512) Ã— seq_len Ã— num_layers (6)
  åœ§ç¸®å¾Œ: context_dim (256) Ã— seq_len Ã— num_layers (6)
```

### Pythia-70Mä»•æ§˜

| é …ç›® | å€¤ |
|------|-----|
| Hidden Size | 512 |
| Layers | 6 |
| Attention Heads | 8 |
| Intermediate Size | 2048 |
| Position Encoding | Rotary (RoPE, 25%) |
| Vocab Size | 50,304 |
| Training Data | Pile (~300B tokens) |

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# Phase 1: ContextBlock OACDå­¦ç¿’
python3 scripts/train_phase1_pythia.py --tokens 100000

# Phase 2: æ¯”è¼ƒå®Ÿé¨“ï¼ˆæº–å‚™ä¸­ï¼‰
python3 scripts/experiment_pythia_comparison.py --samples 10000 --epochs 10
```

---

## ğŸš¨ğŸš¨ğŸš¨ Phase 1 å®Œå…¨ä»•æ§˜ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰ğŸš¨ğŸš¨ğŸš¨

**ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯è©¦è¡ŒéŒ¯èª¤ã®æœ«ã«ç¢ºç«‹ã•ã‚ŒãŸå¿…é ˆä»•æ§˜ã§ã™ã€‚çµ¶å¯¾ã«å‰Šé™¤ã—ãªã„ã§ãã ã•ã„ã€‚**

### Phase 1ã®ç›®çš„

ContextBlockã‚’ä½¿ã£ã¦ã€å¤šæ§˜ãªcontext vectorã‚’ç”Ÿæˆã™ã‚‹ã€‚
OACDï¼ˆOrigin-Anchored Centroid Dispersionï¼‰æå¤±ã§å­¦ç¿’ã—ã€åæŸç‡90%ä»¥ä¸Šã‚’ç›®æŒ‡ã™ã€‚

### ContextBlock/ContextLayerã®å®Ÿè£…ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/models/layers.py - ContextLayer
class ContextLayer(nn.Module):
    def __init__(self, context_input_dim, context_output_dim, token_input_dim):
        # FFN: Linear(input_dim â†’ output_dim) + GELU
        input_dim = context_input_dim + token_input_dim
        self.fnn = FFN(input_dim, context_output_dim)

        # LayerNormï¼ˆå¿…é ˆï¼šæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
        self.context_norm = nn.LayerNorm(context_output_dim)

        # æ®‹å·®æ¥ç¶šç”¨ã®å°„å½±ï¼ˆæ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®ã¿ï¼‰
        if context_input_dim != context_output_dim:
            self.residual_proj = nn.Linear(context_input_dim, context_output_dim)

        # âš ï¸ é‡è¦: åˆæœŸåŒ–ã¯ normal_(std=0.1)
        init_linear_weights(self)  # weight: std=0.1, bias: std=0.01

    def forward(self, context, token_embeds):
        fnn_input = torch.cat([context, token_embeds], dim=-1)
        delta_context = self.fnn(fnn_input)

        # æ®‹å·®æ¥ç¶š + LayerNorm
        new_context = self.context_norm(context + delta_context)
        return new_context
```

### åˆæœŸåŒ–æ–¹æ³•ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/utils/initialization.py
def init_linear_weights(module, weight_std=0.1, bias_std=0.01):
    for submodule in module.modules():
        if isinstance(submodule, nn.Linear):
            nn.init.normal_(submodule.weight, mean=0.0, std=0.1)  # âš ï¸ Xavierç¦æ­¢
            if submodule.bias is not None:
                nn.init.normal_(submodule.bias, mean=0.0, std=0.01)
```

**âš ï¸ è­¦å‘Š**: XavieråˆæœŸåŒ–ã‚„KaimingåˆæœŸåŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€Phase 1ãŒåæŸã—ã¾ã›ã‚“ã€‚
å¿…ãš `normal_(std=0.1)` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

### OACDæå¤±é–¢æ•°ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# src/losses/diversity.py
def oacd_loss(contexts, centroid_weight=0.1):
    context_mean = contexts.mean(dim=0)
    deviation = contexts - context_mean

    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰
    dispersion_loss = -torch.norm(deviation, p=2) / len(contexts)

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = torch.norm(context_mean, p=2) ** 2

    return dispersion_loss + centroid_weight * centroid_loss
```

### Phase 1 è¨­å®šå€¤ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| `max_iterations` | 100 | æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° |
| `convergence_threshold` | 0.03 | åæŸåˆ¤å®šã®MSEé–¾å€¤ |
| `learning_rate` | 0.003 | å­¦ç¿’ç‡ |
| `batch_size` | 5000 | ãƒãƒƒãƒã‚µã‚¤ã‚º |
| `gradient_clip` | 2.0 | å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤ |
| `context_noise` | 0.05 | ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºï¼ˆåæŸå„ªå…ˆï¼‰ |
| `early_stopping_threshold` | 0.9 | åæŸç‡90%ã§æ—©æœŸåœæ­¢ |

### embed_normï¼ˆåŸ‹ã‚è¾¼ã¿æ­£è¦åŒ–ï¼‰ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# âš ï¸ é‡è¦: åŸ‹ã‚è¾¼ã¿å¾Œã®æ­£è¦åŒ–ãŒå¿…é ˆï¼ˆPhase 1åæŸã«å¿…è¦ï¼‰
self.embed_norm = nn.LayerNorm(hidden_size)

# ä½¿ç”¨æ™‚
token_embeds = model.embed_in(token_ids)
token_embeds = model.embed_norm(token_embeds)  # âš ï¸ å¿…é ˆ
```

**âš ï¸ è­¦å‘Š**: embed_normãŒãªã„ã¨Phase 1ãŒåæŸã—ã¾ã›ã‚“ã€‚

### shifted_prev_contextæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# âŒ ç¦æ­¢: é †æ¬¡å‡¦ç†ï¼ˆéå¸¸ã«é…ã„ï¼‰
for i in range(num_tokens):
    new_context = model.forward_context(prev_context, token_embed)
    prev_context = new_context

# âœ… å¿…é ˆ: shifted_prev_contextæ–¹å¼ï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
for iteration in range(max_iterations):
    # ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‹ã‚‰é–‹å§‹
    init_ctx = torch.zeros(1, context_dim)
    shifted_prev_context = torch.cat([init_ctx, previous_contexts[:-1]], dim=0)

    # ãƒãƒƒãƒä¸¦åˆ—å‡¦ç†
    new_contexts = model.context_block(shifted_prev_context, token_embeds)
    previous_contexts = new_contexts
```

### å‹¾é…ç´¯ç©ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# ãƒãƒƒãƒã”ã¨ã«å‹¾é…ã‚’è¨ˆç®—ãƒ»ç´¯ç©
optimizer.zero_grad()
for batch in batches:
    loss = oacd_loss(batch_output)
    scaled_loss = loss / num_batches  # ãƒãƒƒãƒæ•°ã§å‰²ã‚‹
    scaled_loss.backward()  # å‹¾é…ç´¯ç©

# æœ€å¾Œã«ã¾ã¨ã‚ã¦æ›´æ–°
torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)
optimizer.step()
```

### CPU/GPUãƒ¡ãƒ¢ãƒªåˆ†é›¢ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
# token_embedsã¨previous_contextsã¯CPUã«ä¿æŒ
token_embeds = token_embeds_gpu.cpu()
previous_contexts = contexts.cpu()

# ãƒãƒƒãƒã”ã¨ã«GPUã«è»¢é€ã—ã¦å‡¦ç†
for start_idx in range(0, num_tokens, batch_size):
    batch_contexts = previous_contexts[start_idx:end_idx].to(device)
    batch_embeds = token_embeds[start_idx:end_idx].to(device)

    # å‡¦ç†å¾Œã¯å³åº§ã«CPUã«æˆ»ã™
    all_contexts_cpu.append(batch_output.detach().cpu())
```

### åæŸç‡è¨ˆç®—ï¼ˆå‰Šé™¤ç¦æ­¢ï¼‰

```python
def compute_convergence_rate(current, previous, threshold=0.03):
    token_losses = ((current - previous) ** 2).mean(dim=1)
    converged_count = (token_losses < threshold).sum()
    return converged_count / len(current)
```

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒ

### Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢ï¼ˆå³ç¦ï¼‰

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Core Components

**1. ContextPythiaModel**
- Token Embedding: Pythia-70M (512-dim)
- ContextBlock: 512 â†’ 256 (åœ§ç¸®)
- 6 Context-based Transformer Layers
- Output Head: 512 â†’ vocab_size

**2. ContextBlock**
- 1å±¤å›ºå®šã€Phase 1ã§å­¦ç¿’ã€Phase 2ã§freeze
- OACDã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§å¤šæ§˜æ€§å­¦ç¿’
- åˆæœŸåŒ–: normal_(std=0.1)

**3. PythiaModel (Baseline)**
- æ¨™æº–ã®Pythia-70Må†å®Ÿè£…
- æ¯”è¼ƒç”¨

### å­¦ç¿’ãƒ•ãƒ­ãƒ¼

```
Phase 1: OACD (ContextBlockå¤šæ§˜æ€§å­¦ç¿’)
  â”œâ”€ ContextBlockã®ã¿ã‚’å­¦ç¿’
  â”œâ”€ OACDæå¤±ã§å¤šæ§˜ãªcontext vectorã‚’ç”Ÿæˆ
  â””â”€ åæŸã¾ã§å®Ÿè¡Œï¼ˆ~60 iterations, 90%+åæŸï¼‰
       â†“
Phase 2: Full Training (ContextBlock frozen)
  â”œâ”€ ContextBlockã‚’freeze
  â”œâ”€ Transformer Layers + Output Headã‚’å­¦ç¿’
  â””â”€ Cross-entropyæå¤±
```

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ context_block_pythia_phase1.pt  # Phase 1 checkpoint
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ phase1.py              # Phase 1è¨­å®š
â”‚   â””â”€â”€ pythia.py              # PythiaConfig, ContextPythiaConfig
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_phase1_pythia.py         # Phase 1: ContextBlock OACDå­¦ç¿’
â”‚   â””â”€â”€ experiment_pythia_comparison.py # Phase 2: Pythia vs Context-Pythiaæ¯”è¼ƒ
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pythia.py          # PythiaModel (baseline)
â”‚   â”‚   â”œâ”€â”€ context_pythia.py  # ContextPythiaModel (ours)
â”‚   â”‚   â”œâ”€â”€ blocks.py          # ContextBlock
â”‚   â”‚   â””â”€â”€ layers.py          # ContextLayer
â”‚   â”œâ”€â”€ losses/
â”‚   â”‚   â””â”€â”€ diversity.py       # OACD algorithm
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ data_pythia.py     # Pileãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
â”‚       â””â”€â”€ initialization.py  # é‡ã¿åˆæœŸåŒ–
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-04 | Phase 2æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¿½åŠ ã€Phase 1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ |
| 2025-12-04 | embed_normè¿½åŠ ï¼ˆPhase 1åæŸã«å¿…é ˆï¼‰ |
| 2025-12-04 | Pythia-70Mçµ±åˆï¼ˆContext-Pythiaæ–¹å¼ã«å®Œå…¨ç§»è¡Œï¼‰ |
| 2025-12-04 | Phase 1ä»•æ§˜ã®è©³ç´°ã‚’è¿½è¨˜ï¼ˆPythiaçµ±åˆå¤±æ•—ã‹ã‚‰ã®å¾©æ—§å¾Œï¼‰ |
| 2025-12-03 | Context-KV Attentionæ–¹å¼ï¼ˆæ—§æ–¹å¼ã€å‰Šé™¤æ¸ˆã¿ï¼‰ |

---

Last Updated: 2025-12-04
