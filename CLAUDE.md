# New-LLM Project Guidelines

---

## ğŸ¯ MLA-Pythia Architecture (2025-12-05)

**Pythia-70Mã‚’ãƒ™ãƒ¼ã‚¹ã«MLAï¼ˆMulti-head Latent Attentionï¼‰ã§KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤§å¹…å‰Šæ¸›ã€‚**
**ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ALiBiï¼ˆçµ±ä¸€ã‚¹ãƒ­ãƒ¼ãƒ—ï¼‰ã‚’æ¡ç”¨ã€‚**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
Pythia (Baseline, RoPE):              MLA-Pythia (Ours, ALiBi):
Token Embedding (512-dim)             Token Embedding (512-dim)
       â†“                                     â†“
PythiaLayer Ã— 6                       MLALayer Ã— 6
  â”œâ”€ Attention (RoPE)                   â”œâ”€ MLA Attention (ALiBi)
  â”‚    K: 512-dim                       â”‚    c_kv: 128-dim (KVå…±é€šåœ§ç¸®)
  â”‚    V: 512-dim                       â”‚    å¸åãƒ¢ãƒ¼ãƒ‰
  â””â”€ MLP                                â””â”€ MLP
       â†“                                     â†“
Output Head (512 â†’ vocab)             Output Head (512 â†’ vocab)

KV Cache: K(512) + V(512) = 1024      KV Cache: c_kv(128) = 128
å‰Šæ¸›ç‡: 0%                            å‰Šæ¸›ç‡: 87.5%
```

### è¨­å®šå€¤

| é …ç›® | Baseline (Pythia) | MLA-Pythia |
|------|-------------------|------------|
| hidden_size | 512 | 512 |
| kv_dim | - | 128 |
| Layers | 6 | 6 |
| Attention Heads | 8 | 8 |
| intermediate_size | 2048 | 2048 |
| Position Encoding | RoPE (25%) | ALiBi (çµ±ä¸€ã‚¹ãƒ­ãƒ¼ãƒ—) |
| KV Cacheå‰Šæ¸› | 0% | 87.5% |

### å®Ÿé¨“ã®å®Ÿè¡Œ

```bash
# MLAå®Ÿé¨“: Pythia (RoPE) vs MLA-Pythia (ALiBi)
python3 scripts/experiment_mla.py --samples 10000 --epochs 30

# MLAã®ã¿ï¼ˆbaselineã‚¹ã‚­ãƒƒãƒ—ï¼‰
python3 scripts/experiment_mla.py --samples 10000 --skip-baseline

# kv_dimå¤‰æ›´
python3 scripts/experiment_mla.py --kv-dim 256  # 75%å‰Šæ¸›
python3 scripts/experiment_mla.py --kv-dim 64   # 93.75%å‰Šæ¸›

# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¯”è¼ƒå®Ÿé¨“ï¼ˆçµ±ä¸€ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
python3 scripts/experiment_position.py --samples 10000 --epochs 30

# ç‰¹å®šã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ã¿
python3 scripts/experiment_position.py --pos-types rope alibi
python3 scripts/experiment_position.py --pos-types none  # NoPE
```

---

## ğŸ“š DeepSeek MLA (Multi-head Latent Attention) å‚è€ƒè³‡æ–™

### MLAæ¦‚è¦

DeepSeek-V2ã§å°å…¥ã•ã‚ŒãŸKVã‚­ãƒ£ãƒƒã‚·ãƒ¥åœ§ç¸®æŠ€è¡“ã€‚K+Vã‚’å…±é€šã®ä½æ¬¡å…ƒæ½œåœ¨ãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã—ã€ã€Œå¸åã€æŠ€æ³•ã«ã‚ˆã‚Šå¾©å…ƒã›ãšã«Attentionè¨ˆç®—ã‚’å®Ÿç¾ã€‚

### å¸åãƒ¢ãƒ¼ãƒ‰ï¼ˆAbsorbed Projectionï¼‰ã®æ•°å¼

```
æ¨™æº–MHA:
  scores = Q @ K^T
  output = softmax(scores) @ V

MLAï¼ˆåœ§ç¸®ãƒ»å¾©å…ƒã‚ã‚Šï¼‰:
  c_kv = X @ W_DKV     # KVå…±é€šåœ§ç¸®: (seq, 512) â†’ (seq, 128)
  K = c_kv @ W_UK      # Kå¾©å…ƒ
  V = c_kv @ W_UV      # Vå¾©å…ƒ

MLAï¼ˆå¸åãƒ¢ãƒ¼ãƒ‰ - å¾©å…ƒä¸è¦ï¼‰:
  scores = Q @ K^T
        = Q @ (c_kv @ W_UK)^T
        = Q @ W_UK^T @ c_kv^T

  # KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ c_kv ã®ã¿ä¿å­˜
```

### Vå‡¦ç†ï¼ˆå¸åãƒ¢ãƒ¼ãƒ‰ï¼‰

```
output = softmax(scores) @ V
       = attn_weights @ (c_kv @ W_UV)
       = (attn_weights @ c_kv) @ W_UV  â† çµåˆæ³•å‰‡
         â†‘ åœ§ç¸®ç©ºé–“ã§ã®è¨ˆç®—    â†‘ æœ€å¾Œã«å¾©å…ƒ
```

### KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šæ¸›åŠ¹æœ

| æ–¹å¼ | ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†…å®¹ | ä¾‹ï¼ˆ512-dimï¼‰ | å‰Šæ¸›ç‡ |
|------|---------------|---------------|--------|
| æ¨™æº–MHA | K(512) + V(512) | 1024 | 0% |
| MLA (kv_dim=128) | c_kv(128) | 128 | 87.5% |
| MLA (kv_dim=64) | c_kv(64) | 64 | 93.75% |

### å‚è€ƒãƒªãƒ³ã‚¯

- [DeepSeek-V2 Paper](https://arxiv.org/abs/2405.04434)
- [MLA Explanation (HuggingFace)](https://huggingface.co/blog/NormalUhr/mla-explanation)
- [Understanding MLA](https://planetbanatt.net/articles/mla.html)

---

## ğŸ¯ ALiBi (Attention with Linear Biases) æ¡ç”¨æ–¹é‡

### æ¡ç”¨æ±ºå®šäº‹é …

**æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯RoPEã®ä»£ã‚ã‚Šã«ALiBiã‚’æ¡ç”¨ã™ã‚‹ã€‚**

ç†ç”±:
- ALiBiã¯MLAï¼ˆå¸åãƒ¢ãƒ¼ãƒ‰ï¼‰ã¨å®Œå…¨ã«äº’æ›æ€§ãŒã‚ã‚‹
- RoPEã¯å›è»¢è¡Œåˆ—ãŒä½ç½®ä¾å­˜ã®ãŸã‚ã€å¸åãƒ¢ãƒ¼ãƒ‰ã§äº‹å‰è¨ˆç®—ã§ããªã„
- ALiBiã¯åŠ ç®—ãƒã‚¤ã‚¢ã‚¹ã®ãŸã‚ã€å¸åå¾Œã®scoreã«å˜ç´”ã«åŠ ç®—å¯èƒ½

### ALiBiä»•æ§˜

```
score = Q @ K^T - m * distance_matrix

distance_matrix[i][j] = |i - j|  # ä½ç½®é–“ã®è·é›¢
m = slope (å…¨ãƒ˜ãƒƒãƒ‰çµ±ä¸€)
```

### ã‚¹ãƒ­ãƒ¼ãƒ—è¨­å®šï¼ˆé‡è¦ï¼‰

**å…¨ãƒ˜ãƒƒãƒ‰ã§çµ±ä¸€ã‚¹ãƒ­ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã™ã‚‹ï¼ˆãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹ã‚¹ãƒ­ãƒ¼ãƒ—ã¯ä½¿ç”¨ã—ãªã„ï¼‰**

ç†ç”±:
- ãƒ˜ãƒƒãƒ‰åˆ†å‰²ã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’ä»»æ„ã«åˆ†å‰²ã—ãŸã‚‚ã®
- ç•°ãªã‚‹æ¬¡å…ƒã«ç•°ãªã‚‹ã‚¹ãƒ­ãƒ¼ãƒ—ã‚’å‰²ã‚Šå½“ã¦ã‚‹ç†è«–çš„æ ¹æ‹ ãŒè–„ã„
- ã‚·ãƒ³ãƒ—ãƒ«ãªçµ±ä¸€ã‚¹ãƒ­ãƒ¼ãƒ—ã§ååˆ†

```python
# âœ… æ¡ç”¨: çµ±ä¸€ã‚¹ãƒ­ãƒ¼ãƒ—
slope = 0.0625  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1/16
alibi_bias = -slope * distance_matrix

# âŒ ä¸æ¡ç”¨: ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹ã‚¹ãƒ­ãƒ¼ãƒ—
# slopes = 2 ** (-8 * torch.arange(1, num_heads + 1) / num_heads)
```

### ALiBi + MLA ã®çµ„ã¿åˆã‚ã›

```
# MLAå¸åãƒ¢ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§
score = Q @ W_UK^T @ c_kv^T - m * distance_matrix
        â†‘ äº‹å‰è¨ˆç®—å¯èƒ½      â†‘ åŠ ç®—ãƒã‚¤ã‚¢ã‚¹ï¼ˆå¹²æ¸‰ãªã—ï¼‰

# RoPEã®å ´åˆï¼ˆä¸å¯èƒ½ï¼‰
score = (R_q @ Q) @ (R_k @ c_kv @ W_UK)^T
        â†‘ å›è»¢è¡Œåˆ—ãŒä½ç½®ä¾å­˜ã®ãŸã‚äº‹å‰è¨ˆç®—ä¸å¯
```

---

## ğŸ“Š Reversal Curse è©•ä¾¡

### æ¦‚è¦

Reversal Curseã¯ã€ŒA is Bã€ã‚’å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒã€ŒB is Aã€ã‚‚æ¨è«–ã§ãã‚‹ã‹ã‚’æ¸¬å®šã™ã‚‹æŒ‡æ¨™ã€‚

### æ­£ã—ã„å®Ÿé¨“è¨­è¨ˆ

```
è¨“ç·´ãƒ‡ãƒ¼ã‚¿:
  - Pileï¼ˆä¸€èˆ¬ãƒ†ã‚­ã‚¹ãƒˆï¼‰
  - é †æ–¹å‘æ–‡ã®ã¿: "The capital of France is Paris"

è©•ä¾¡ãƒ‡ãƒ¼ã‚¿:
  - é †æ–¹å‘: "The capital of France is Paris" â†’ ä½PPLæœŸå¾…
  - é€†æ–¹å‘: "Paris is the capital of France" â†’ é«˜PPLï¼ˆReversal Curseï¼‰
```

### æŒ‡æ¨™

| æŒ‡æ¨™ | å®šç¾© | è§£é‡ˆ |
|------|------|------|
| Forward PPL | é †æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ãŸã‚ä½ã„ |
| Backward PPL | é€†æ–¹å‘æ–‡ã®PPL | è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œãªã„ãŸã‚é«˜ã„ |
| Reversal Ratio | Forward / Backward | 1.0ã«è¿‘ã„ã»ã©è‰¯ã„ |
| Reversal Gap | Backward - Forward | 0ã«è¿‘ã„ã»ã©è‰¯ã„ |

### å®Ÿè£…

- è¨“ç·´ãƒ‡ãƒ¼ã‚¿: `prepare_data_loaders(include_reversal_pairs=True)`
- é †æ–¹å‘æ–‡ã¯10å›ç¹°ã‚Šè¿”ã—ã¦è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
- è©•ä¾¡: `evaluate_reversal_curse(model, tokenizer, pairs, device)`

---

## ğŸ”§ é–‹ç™ºç’°å¢ƒ

### Lint/Type Check

```bash
# Lint (ruff)
python3 -m ruff check src/

# Type check (mypy)
python3 -m mypy src/ --ignore-missing-imports
```

---

## ğŸš¨ CRITICAL: ã‚³ãƒ¼ãƒ‰å“è³ª

### å¾Œæ–¹äº’æ›æ€§ã‚³ãƒ¼ãƒ‰ç¦æ­¢

**å¤ã„æ©Ÿèƒ½ã‚’æ®‹ã™ã“ã¨ã¯å³ç¦ã€‚å¾Œæ–¹äº’æ›æ€§ã‚’æ„è­˜ã—ãŸã‚³ãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«æ›¸ã‹ãªã„ã€‚**

### ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å³ç¦

**å…¨ã¦ã®å€¤ã¯configã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚**

### ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ç¦æ­¢ï¼ˆå³ç¦ï¼‰

**å®Ÿé¨“ã§ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ï¼ˆtorch.randintç­‰ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚**
å¿…ãšå®Ÿãƒ‡ãƒ¼ã‚¿ï¼ˆPileï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

---

## âš ï¸ éå»ã®ãƒã‚°ã¨æ•™è¨“

### 1. ALiBiå› æœãƒã‚¹ã‚¯ã®è¡Œåˆ—æ–¹å‘ãƒã‚°ï¼ˆ2025-12-05ï¼‰

**ç—‡çŠ¶**: MLA-Pythiaã®PPLãŒç•°å¸¸ã«ä½ã„ï¼ˆ1.5ï¼‰ã€Pythiaã¯æ­£å¸¸ï¼ˆ424ï¼‰

**åŸå› **: `build_alibi_bias_causal`ã§è¡Œåˆ—ã®è¡Œã¨åˆ—ãŒé€†è»¢ã—ã¦ã„ãŸ

```python
# âŒ ãƒã‚°: relative_pos[i][j] = j - i ï¼ˆæœªæ¥ãŒè¦‹ãˆã¦ã„ãŸï¼‰
relative_pos = positions.unsqueeze(0) - positions.unsqueeze(1)

# âœ… ä¿®æ­£: relative_pos[i][j] = i - j ï¼ˆæ­£ã—ã„å› æœãƒã‚¹ã‚¯ï¼‰
relative_pos = positions.unsqueeze(1) - positions.unsqueeze(0)
```

**æ•™è¨“**:
- PPL < 10 ã¯ç•°å¸¸ã€‚ãƒ‡ãƒ¼ã‚¿æš—è¨˜ã¾ãŸã¯å› æœãƒã‚¹ã‚¯ã®ãƒã‚°ã‚’ç–‘ã†
- è¡Œåˆ—æ¼”ç®—ã§ã¯`unsqueeze`ã®é †åºï¼ˆè¡Œ/åˆ—ï¼‰ã‚’å¿…ãšç¢ºèª
- Attentionãƒã‚¹ã‚¯ã¯ã€ŒqueryãŒè¡Œã€keyãŒåˆ—ã€ãŒæ¨™æº–

### 2. PPLç•°å¸¸å€¤ã®è¨ºæ–­åŸºæº–

| PPL | çŠ¶æ…‹ | å¯¾å‡¦ |
|-----|------|------|
| < 5 | **ç•°å¸¸** - ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯/å› æœãƒã‚¹ã‚¯ãƒã‚° | ã‚³ãƒ¼ãƒ‰ç‚¹æ¤œå¿…é ˆ |
| 5-30 | **ç–‘ã‚ã—ã„** - éå­¦ç¿’ã®å¯èƒ½æ€§ | ãƒ‡ãƒ¼ã‚¿é‡ãƒ»åˆ†å‰²ã‚’ç¢ºèª |
| 30-100 | æ­£å¸¸ï¼ˆå°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼‰ | - |
| 100-500 | æ­£å¸¸ï¼ˆã‚¹ã‚¯ãƒ©ãƒƒãƒè¨“ç·´ï¼‰ | - |
| > 1000 | å­¦ç¿’ä¸è¶³ | epochå¢—åŠ /lrèª¿æ•´ |

### 3. å› æœãƒã‚¹ã‚¯ã®æ¤œè¨¼æ–¹æ³•

æ–°ã—ã„Attentionå®Ÿè£…ã§ã¯å¿…ãšä»¥ä¸‹ã‚’ç¢ºèªï¼š

```python
# ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
seq_len = 5
bias = build_alibi_bias_causal(seq_len, slope=0.0625)
print(bias)
# æœŸå¾…å‡ºåŠ›: ä¸Šä¸‰è§’ãŒ-infã€ä¸‹ä¸‰è§’ãŒè² ã®å€¤
# tensor([[  0., -inf, -inf, -inf, -inf],
#         [-0.0625,   0., -inf, -inf, -inf],
#         [-0.1250, -0.0625,   0., -inf, -inf],
#         ...])
```

---

## ğŸ“ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä»•æ§˜

### Core Components

**1. UnifiedPythiaModelï¼ˆçµ±ä¸€ãƒ¢ãƒ‡ãƒ«ï¼‰**
- ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨­å®šã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½
- RoPE, ALiBi, NoPEï¼ˆãªã—ï¼‰ã«å¯¾å¿œ
- ç–çµåˆè¨­è¨ˆã«ã‚ˆã‚Šæ‹¡å¼µãŒå®¹æ˜“

**2. PositionEncodingï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰**
```python
# ä½¿ç”¨ä¾‹
from src.models import UnifiedPythiaModel, PositionEncodingConfig

# RoPE
model = UnifiedPythiaModel(pos_encoding=PositionEncodingConfig(type="rope"))

# ALiBi
model = UnifiedPythiaModel(pos_encoding=PositionEncodingConfig(type="alibi"))

# NoPEï¼ˆä½ç½®æƒ…å ±ãªã—ï¼‰
model = UnifiedPythiaModel(pos_encoding=PositionEncodingConfig(type="none"))
```

**3. MLAPythiaModelï¼ˆKVã‚­ãƒ£ãƒƒã‚·ãƒ¥åœ§ç¸®ï¼‰**
- Token Embedding: vocab â†’ hidden_size (512)
- MLALayer Ã— 6: KVå…±é€šåœ§ç¸® (kv_dim=128)ã€ALiBi
- Output Head: hidden_size (512) â†’ vocab_size

**4. PythiaModel (Baseline)**
- Token Embedding: vocab â†’ hidden_size (512)
- PythiaLayer Ã— 6: RoPE (25%)
- Output Head: hidden_size (512) â†’ vocab_size

---

## ğŸ“ File Structure

```
new-llm/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pythia.py                   # PythiaConfig
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ experiment_mla.py           # MLAå®Ÿé¨“: Pythia vs MLA-Pythia
â”‚   â””â”€â”€ experiment_position.py      # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¯”è¼ƒå®Ÿé¨“
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ pythia.py               # PythiaModel (baseline, RoPE)
â”‚   â”‚   â”œâ”€â”€ mla_pythia.py           # MLAPythiaModel (ours, ALiBi)
â”‚   â”‚   â”œâ”€â”€ mla.py                  # MLAAttention, MLALayer
â”‚   â”‚   â”œâ”€â”€ alibi.py                # ALiBiå®Ÿè£…
â”‚   â”‚   â”œâ”€â”€ position_encoding.py    # ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±ä¸€ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
â”‚   â”‚   â””â”€â”€ unified_pythia.py       # UnifiedPythiaModelï¼ˆä½ç½®ã‚¨ãƒ³ã‚³åˆ‡æ›¿å¯èƒ½ï¼‰
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ training.py             # å…±é€šå­¦ç¿’ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚       â”œâ”€â”€ evaluation.py           # è©•ä¾¡é–¢æ•°
â”‚       â””â”€â”€ device.py               # ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ experiments/                # å®Ÿé¨“çµæœ
â”œâ”€â”€ CLAUDE.md
â””â”€â”€ README.md
```

---

## ğŸ“œ å¤‰æ›´å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2025-12-05 | **ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°çµ±ä¸€åŒ–**: RoPE/ALiBi/NoPEã‚’ç–çµåˆã§åˆ‡ã‚Šæ›¿ãˆå¯èƒ½ã« |
| 2025-12-05 | **ALiBiå› æœãƒã‚¹ã‚¯ãƒã‚°ä¿®æ­£**: unsqueezeé †åºã®ä¿®æ­£ã€PPLç•°å¸¸ã®è§£æ¶ˆ |
| 2025-12-05 | **Reversal Curseè©•ä¾¡è¿½åŠ **: é †æ–¹å‘/é€†æ–¹å‘PPLæ¯”è¼ƒæ©Ÿèƒ½ |
| 2025-12-05 | **MLA-Pythiaå®Ÿè£…**: V-DProjã‹ã‚‰MLAæ–¹å¼ã«ç§»è¡Œã€ALiBiæ¡ç”¨ |
| 2025-12-05 | **ALiBiæ¡ç”¨**: RoPEã‹ã‚‰ALiBiã«å¤‰æ›´ã€çµ±ä¸€ã‚¹ãƒ­ãƒ¼ãƒ—æ–¹å¼ |
| 2025-12-04 | V-DProjå®Ÿé¨“ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ï¼‰ |
| 2025-12-04 | DProj-Pythiaå®Ÿé¨“ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æ¸ˆã¿ï¼‰ |

---

## ğŸ“¦ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: DProjé–¢é€£ï¼ˆå‚è€ƒç”¨ï¼‰

ä»¥ä¸‹ã¯éå»ã®å®Ÿé¨“ã§ä½¿ç”¨ã—ãŸä»•æ§˜ã§ã™ã€‚ç¾åœ¨ã¯MLAæ–¹å¼ã«ç§»è¡Œã—ã¦ã„ã¾ã™ã€‚

<details>
<summary>DProj Training ä»•æ§˜ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§å±•é–‹ï¼‰</summary>

### DProj Trainingã®ç›®çš„

DiverseProjectionã‚’ä½¿ã£ã¦ã€å¤šæ§˜ãªprojection vectorã‚’ç”Ÿæˆã™ã‚‹ã€‚
OACDï¼ˆOrigin-Anchored Centroid Dispersionï¼‰æå¤±ã§å­¦ç¿’ã—ã€åæŸç‡90%ä»¥ä¸Šã‚’ç›®æŒ‡ã™ã€‚

### DiverseProjection/DiverseProjectionLayerã®å®Ÿè£…

```python
# src/models/dproj.py - DiverseProjectionLayer
class DiverseProjectionLayer(nn.Module):
    def __init__(self, proj_input_dim, proj_output_dim, token_input_dim):
        # FFN: Linear(input_dim â†’ output_dim) + GELU
        input_dim = proj_input_dim + token_input_dim
        self.ffn = FFN(input_dim, proj_output_dim)

        # LayerNormï¼ˆå¿…é ˆï¼šæ•°å€¤å®‰å®šæ€§ã®ãŸã‚ï¼‰
        self.proj_norm = nn.LayerNorm(proj_output_dim)

        # æ®‹å·®æ¥ç¶šç”¨ã®å°„å½±ï¼ˆæ¬¡å…ƒãŒç•°ãªã‚‹å ´åˆã®ã¿ï¼‰
        if proj_input_dim != proj_output_dim:
            self.residual_proj = nn.Linear(proj_input_dim, proj_output_dim)

        # âš ï¸ é‡è¦: åˆæœŸåŒ–ã¯ normal_(std=0.1)
        init_linear_weights(self)  # weight: std=0.1, bias: std=0.01

    def forward(self, prev_proj, token_embeds):
        ffn_input = torch.cat([prev_proj, token_embeds], dim=-1)
        delta = self.ffn(ffn_input)

        # æ®‹å·®æ¥ç¶š + LayerNorm
        new_proj = self.proj_norm(prev_proj + delta)
        return new_proj
```

### åˆæœŸåŒ–æ–¹æ³•

```python
# src/utils/initialization.py
def init_linear_weights(module, weight_std=0.1, bias_std=0.01):
    for submodule in module.modules():
        if isinstance(submodule, nn.Linear):
            nn.init.normal_(submodule.weight, mean=0.0, std=0.1)  # âš ï¸ Xavierç¦æ­¢
            if submodule.bias is not None:
                nn.init.normal_(submodule.bias, mean=0.0, std=0.01)
```

### OACDæå¤±é–¢æ•°

```python
# src/losses/diversity.py
def oacd_loss(projections, centroid_weight=0.1):
    proj_mean = projections.mean(dim=0)
    deviation = projections - proj_mean

    # Term 1: é‡å¿ƒã‹ã‚‰ã®åˆ†æ•£ã‚’æœ€å¤§åŒ–ï¼ˆè² ã®æå¤±ã§æœ€å¤§åŒ–ï¼‰
    dispersion_loss = -torch.norm(deviation, p=2) / len(projections)

    # Term 2: é‡å¿ƒã‚’åŸç‚¹ã«å¼•ãå¯„ã›ã‚‹
    centroid_loss = torch.norm(proj_mean, p=2) ** 2

    return dispersion_loss + centroid_weight * centroid_loss
```

### DProj Training è¨­å®šå€¤

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| `max_iterations` | 100 | æœ€å¤§ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•° |
| `convergence_threshold` | 0.03 | åæŸåˆ¤å®šã®MSEé–¾å€¤ |
| `learning_rate` | 0.003 | å­¦ç¿’ç‡ |
| `batch_size` | 5000 | ãƒãƒƒãƒã‚µã‚¤ã‚º |
| `gradient_clip` | 2.0 | å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å€¤ |
| `proj_noise` | 0.05 | ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚º |
| `early_stopping_threshold` | 0.95 | åæŸç‡95%ã§æ—©æœŸåœæ­¢ |

</details>

---

Last Updated: 2025-12-05
