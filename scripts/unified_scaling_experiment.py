#!/usr/bin/env python3
"""
Áµ±‰∏ÄË®≠ÂÆö„Åß„ÅÆ„Çπ„Ç±„Éº„É™„É≥„Ç∞ÂÆüÈ®ì„Çπ„ÇØ„É™„Éó„Éà

ÁõÆÁöÑ:
- 11/27„Å®11/28„ÅÆÂÆüÈ®ìÊù°‰ª∂„ÅÆÈÅï„ÅÑÔºà„Éà„Éº„ÇØ„É≥ÂåñË®≠ÂÆöÔºâ„ÇíËß£Ê∂à
- Áµ±‰∏Ä„Åó„ÅüË®≠ÂÆöÔºàtruncation=FalseÔºâ„ÅßË§áÊï∞„Çµ„É≥„Éó„É´Êï∞„Åß„ÅÆÂÆüÈ®ì„ÇíÂÆüÊñΩ
- Ê≠£Á¢∫„Å™Œ±ÂÄ§„ÇíÂ∞éÂá∫

‰ΩøÁî®ÊñπÊ≥ï:
  Colab: !python scripts/unified_scaling_experiment.py
  Local: python3 scripts/unified_scaling_experiment.py

Ë®≠ÂÆö:
- „Éà„Éº„ÇØ„É≥Âåñ: truncation=FalseÔºàÂÖ®Èï∑‰ΩøÁî®Ôºâ
- „Çµ„É≥„Éó„É´Êï∞: [50, 100, 200, 500, 1000]
- num_input_tokens: 1
- „É¢„Éá„É´: 6Â±§/768dim

Êñ∞Ë®≠Ë®àÂØæÂøú (2025-11-29):
- MemoryDataProvider: „Çµ„É≥„Éó„É´Â¢ÉÁïåÊÉÖÂ†±„ÇíÊ¥ªÁî®
- MemoryPhase1Trainer: CVFPÂõ∫ÂÆöÁÇπÂ≠¶Áøí
- Phase2Trainer: „Ç≠„É£„ÉÉ„Ç∑„É•ÊñπÂºè„Å´„Çà„ÇãÈ´òÈÄüË®ìÁ∑¥
"""

import sys
import os
import json
import time
import random
from datetime import datetime

# „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„Çí„Éë„Çπ„Å´ËøΩÂä†
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from scipy import stats

# Ë®≠ÂÆöÔºàÂÆüÈ®ìÁî®Ôºâ
SAMPLE_SIZES = [50, 100, 200, 500]  # ÂÖ®ÂÆüÈ®ì
NUM_LAYERS = 6
CONTEXT_DIM = 768
EMBED_DIM = 768
NUM_INPUT_TOKENS = 1
EMBEDDING_FREEZE = True  # ÊúÄÈ´òÊÄßËÉΩË®≠ÂÆö: EmbeddingÂáçÁµê
# tokenÁ∂ô„ÅéË∂≥„ÅóÊñπÂºè: ÂÖ®„É¨„Ç§„É§„Éº„ÅßtokenÂÖ•Âäõ„Å´‰∏ÄÊú¨ÂåñÔºà2025-11-29Ôºâ
DIST_REG_WEIGHT = 0.8  # Â§öÊßòÊÄßÊ≠£ÂâáÂåñ„ÅÆÈáç„Åø
RANDOM_SEED = 42


def print_flush(msg):
    print(msg, flush=True)


def set_seed(seed=42):
    """ÂÖ®„Å¶„ÅÆ‰π±Êï∞ÁîüÊàêÂô®„ÅÆ„Ç∑„Éº„Éâ„ÇíÂõ∫ÂÆö"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def create_val_data_from_train(train_token_ids: torch.Tensor, tokenizer, val_file_path: str, val_ratio: float = 0.1):
    """
    Ë®ìÁ∑¥„Éá„Éº„Çø„Åã„ÇâÊ§úË®º„Éá„Éº„Çø„ÇíÁîüÊàê

    Args:
        train_token_ids: Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆ„Éà„Éº„ÇØ„É≥ID
        tokenizer: „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº
        val_file_path: Ê§úË®º„Éá„Éº„Çø„ÅÆ‰øùÂ≠òÂÖà„Éë„Çπ
        val_ratio: Ê§úË®º„Éá„Éº„Çø„ÅÆÂâ≤ÂêàÔºà„Éá„Éï„Ç©„É´„Éà10%Ôºâ
    """
    # Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆÊúÄÂæå„ÅÆ‰∏ÄÈÉ®„ÇíÊ§úË®º„Éá„Éº„Çø„Å®„Åó„Å¶‰ΩøÁî®
    val_size = int(len(train_token_ids) * val_ratio)
    val_token_ids = train_token_ids[-val_size:]

    # „Éà„Éº„ÇØ„É≥„Çí„ÉÜ„Ç≠„Çπ„Éà„Å´Â§âÊèõ
    val_text = tokenizer.decode(val_token_ids.tolist())

    # „Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
    os.makedirs(os.path.dirname(val_file_path), exist_ok=True)

    # „Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò
    with open(val_file_path, 'w', encoding='utf-8') as f:
        f.write(val_text)

    print_flush(f"  Created val data: {val_file_path} ({val_size} tokens)")
    return val_size


def run_experiment(
    num_samples: int,
    device: torch.device,
):
    """
    Âçò‰∏Ä„Çµ„É≥„Éó„É´Êï∞„Åß„ÅÆÂÆüÈ®ì„ÇíÂÆüË°å

    Args:
        num_samples: ‰ΩøÁî®„Åô„Çã„Çµ„É≥„Éó„É´Êï∞
        device: „Éá„Éê„Ç§„Çπ

    Returns:
        ÂÆüÈ®ìÁµêÊûú„ÅÆËæûÊõ∏
    """
    from config import ResidualConfig
    from src.models.llm import LLM
    from src.providers.data import MemoryDataProvider
    from src.trainers.phase1 import MemoryPhase1Trainer
    from src.trainers.phase2 import Phase2Trainer
    from src.evaluation.metrics import analyze_fixed_points
    from transformers import AutoTokenizer

    print_flush(f"\n--- {num_samples} samples ---")

    # „Ç∑„Éº„Éâ„ÇíÂõ∫ÂÆöÔºàÂêÑÂÆüÈ®ì„ÅßÂêå„ÅòÂàùÊúüÂåñÔºâ
    set_seed(RANDOM_SEED)

    # Ë®≠ÂÆö„Çí‰ΩúÊàê
    config = ResidualConfig()
    config.num_layers = NUM_LAYERS
    config.context_dim = CONTEXT_DIM
    config.embed_dim = EMBED_DIM
    config.num_input_tokens = NUM_INPUT_TOKENS
    config.phase2_freeze_embedding = EMBEDDING_FREEZE
    # tokenÁ∂ô„ÅéË∂≥„ÅóÊñπÂºè„Å´‰∏ÄÊú¨ÂåñÔºà2025-11-29Ôºâ
    config.dist_reg_weight = DIST_REG_WEIGHT
    config.num_samples = num_samples

    # Ê§úË®º„Éá„Éº„Çø„Éï„Ç°„Ç§„É´„Éë„Çπ„Çí„Çµ„É≥„Éó„É´Êï∞„Å´Âøú„Åò„Å¶Ë®≠ÂÆö
    val_file_path = f"./data/ultrachat_{num_samples}samples_val.txt"
    config.val_text_file = val_file_path

    # Ê§úË®º„Éá„Éº„Çø„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØÁîüÊàê
    if not os.path.exists(val_file_path):
        print_flush("\n  Generating validation data...")
        # „Åæ„ÅöË®ìÁ∑¥„Éá„Éº„Çø„Çí„É≠„Éº„Éâ„Åô„Çã„Åü„ÇÅ„Å´‰∏ÄÊôÇÁöÑ„Å´MemoryDataProvider„Çí‰ΩøÁî®
        # ‰ΩÜ„Åó„ÄÅval_data„É≠„Éº„ÉâÂâç„Å´Ê≠¢„ÇÅ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅÁõ¥Êé•UltraChat„Åã„Çâ„É≠„Éº„Éâ
        tokenizer = AutoTokenizer.from_pretrained(
            config.tokenizer_name,
            cache_dir=os.path.join(config.cache_dir, "tokenizer")
        )
        tokenizer.pad_token = tokenizer.eos_token

        # Ë®ìÁ∑¥„Éá„Éº„Çø„Çí„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„Çâ„É≠„Éº„ÉâÔºà„Åæ„Åü„ÅØÁîüÊàêÔºâ
        cache_file = os.path.join(
            config.cache_dir,
            f"ultrachat_{num_samples}samples_full.pt"
        )

        if os.path.exists(cache_file):
            cached = torch.load(cache_file)
            train_tokens_for_val = cached['token_ids']
        else:
            # „Ç≠„É£„ÉÉ„Ç∑„É•„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„Éá„Éº„Çø„Çí„É≠„Éº„Éâ„Åó„Å¶ÁîüÊàê
            from datasets import load_dataset
            dataset = load_dataset(
                config.dataset_name,
                split=config.dataset_split,
                cache_dir=os.path.join(config.cache_dir, "datasets")
            )

            all_token_ids = []
            sample_boundaries = []
            current_pos = 0

            for idx in range(num_samples):
                messages = dataset[idx]["messages"]
                text = "\n".join([msg["content"] for msg in messages])
                tokens = tokenizer(text, truncation=False, return_tensors="pt")
                sample_tokens = tokens["input_ids"].squeeze(0)
                all_token_ids.append(sample_tokens)
                sample_len = len(sample_tokens)
                sample_boundaries.append((current_pos, current_pos + sample_len))
                current_pos += sample_len

            train_tokens_for_val = torch.cat(all_token_ids)

            # „Ç≠„É£„ÉÉ„Ç∑„É•‰øùÂ≠ò
            os.makedirs(config.cache_dir, exist_ok=True)
            torch.save({
                'token_ids': train_tokens_for_val,
                'sample_order': list(range(num_samples)),
                'sample_boundaries': sample_boundaries
            }, cache_file)
            print_flush(f"  Cached to: {cache_file}")

        # Ê§úË®º„Éá„Éº„Çø„ÇíÁîüÊàê
        create_val_data_from_train(train_tokens_for_val, tokenizer, val_file_path)

    # „Éá„Éº„Çø„É≠„Éº„ÉâÔºà„Çµ„É≥„Éó„É´Êï∞„Å´Âøú„Åò„Åü„Éá„Éº„ÇøÔºâ
    data_provider = MemoryDataProvider(config, shuffle_samples=False)
    full_train_token_ids, val_token_ids = data_provider.load_data()

    # üö® ÈáçË¶Å: Ë®ìÁ∑¥„Éá„Éº„Çø„Åã„ÇâÊ§úË®ºÈÉ®ÂàÜ„ÇíÈô§Â§ñÔºà„Éá„Éº„Çø„É™„Éº„ÇØÈò≤Ê≠¢Ôºâ
    val_size = len(val_token_ids)
    train_token_ids = full_train_token_ids[:-val_size]  # Ê§úË®ºÈÉ®ÂàÜ„ÇíÈô§Â§ñ

    train_token_ids = train_token_ids.to(device)
    val_token_ids = val_token_ids.to(device)

    print_flush(f"  Data: {len(train_token_ids):,} train / {len(val_token_ids):,} val tokens")

    # „É¢„Éá„É´‰ΩúÊàêÔºàtokenÁ∂ô„ÅéË∂≥„ÅóÊñπÂºè„Å´‰∏ÄÊú¨ÂåñÔºâ
    model = LLM(
        vocab_size=config.vocab_size,
        embed_dim=config.embed_dim,
        context_dim=config.context_dim,
        num_layers=config.num_layers,
        num_input_tokens=config.num_input_tokens,
        num_context_splits=getattr(config, 'num_context_splits', 1),
        use_pretrained_embeddings=config.use_pretrained_embeddings,
        use_weight_tying=config.use_weight_tying
    )
    model.to(device)

    # Phase 1: CVFPÂõ∫ÂÆöÁÇπÂ≠¶ÁøíÔºàÂÖ®„É¨„Ç§„É§„ÉºÂá∫Âäõ„ÇÇÂèñÂæó„Åó„Å¶Phase 2„Å´Ê∏°„ÅôÔºâ
    phase1_start = time.time()

    phase1_trainer = MemoryPhase1Trainer(model, config, device)

    # return_all_layers=True „ÅßPhase 2Áî®„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•„ÇÇÂèñÂæó
    train_result = phase1_trainer.train(
        train_token_ids,
        label=f"Train ({num_samples} samples)",
        data_provider=data_provider,
        return_all_layers=True
    )
    train_contexts, train_context_cache, train_token_embeds = train_result

    # Ë®ìÁ∑¥„Éá„Éº„Çø„ÅÆEffective Rank
    train_metrics = analyze_fixed_points(train_contexts, label="Train")

    # Ê§úË®º„Éá„Éº„Çø„ÅÆË©ï‰æ°Ôºàreturn_all_layers=True„ÅßÂÖ®„É¨„Ç§„É§„ÉºÂá∫Âäõ„ÇÇÂèñÂæóÔºâ
    val_result = phase1_trainer.evaluate(
        val_token_ids,
        label=f"Val ({num_samples} samples)",
        return_all_layers=True
    )
    # return_all_layers=True„Å™„ÅÆ„Åß„ÄÅÁµêÊûú„ÅØ(contexts, cache, embeds)„ÅÆ„Çø„Éó„É´
    assert isinstance(val_result, tuple), "Expected tuple from evaluate with return_all_layers=True"
    val_contexts, val_context_cache, val_token_embeds = val_result
    val_metrics = analyze_fixed_points(val_contexts, label="Val")

    phase1_time = time.time() - phase1_start
    # Effective Rank„ÅÆÊØîÁéá„ÇíË®àÁÆóÔºàeffective_rank / context_dimÔºâ
    train_er_ratio = train_metrics['effective_rank'] / CONTEXT_DIM
    val_er_ratio = val_metrics['effective_rank'] / CONTEXT_DIM

    # Phase 2: Next-Token PredictionÔºàPhase 1„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÂÜçÂà©Áî®Ôºâ
    phase2_start = time.time()

    phase2_trainer = Phase2Trainer(model=model, config=config)

    # train_full„ÅßË®ìÁ∑¥ÔºàPhase 1„ÅÆ„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊ∏°„Åó„Å¶„Ç≠„É£„ÉÉ„Ç∑„É•ÊßãÁØâ„Çí„Çπ„Ç≠„ÉÉ„ÉóÔºâ
    phase2_history = phase2_trainer.train_full(
        train_token_ids=train_token_ids,
        val_token_ids=val_token_ids,
        device=device,
        train_context_cache=train_context_cache,
        train_token_embeds=train_token_embeds,
        val_context_cache=val_context_cache,
        val_token_embeds=val_token_embeds
    )

    phase2_time = time.time() - phase2_start

    # „Éô„Çπ„Éà„ÅÆÁµêÊûú„ÇíÂèñÂæó
    best_epoch = phase2_history['best_epoch']
    best_val_ppl = phase2_history['val_ppl'][best_epoch - 1]
    best_val_acc = phase2_history['val_acc'][best_epoch - 1]

    # „Çµ„Éû„É™„Éº
    total_time = phase1_time + phase2_time
    print_flush(
        f"\n  ‚úì {num_samples} samples: PPL={best_val_ppl:.1f}, Acc={best_val_acc*100:.1f}%, "
        f"ER={val_er_ratio*100:.1f}%, Time={total_time/60:.1f}min"
    )

    # „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
    data_provider.close()
    del model, phase1_trainer, phase2_trainer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return {
        'num_samples': num_samples,
        'train_tokens': len(train_token_ids),
        'val_tokens': len(val_token_ids),
        'train_effective_rank': train_er_ratio,
        'val_effective_rank': val_er_ratio,
        'val_ppl': best_val_ppl,
        'val_acc': best_val_acc,
        'best_epoch': best_epoch,
        'early_stopped': phase2_history['early_stopped'],
        'phase1_time': phase1_time,
        'phase2_time': phase2_time,
    }


def calculate_scaling_law(results: list):
    """„Çπ„Ç±„Éº„É™„É≥„Ç∞Ââá„ÇíË®àÁÆó: PPL = A √ó tokens^Œ±"""
    tokens = np.array([r['train_tokens'] for r in results])
    ppl = np.array([r['val_ppl'] for r in results])

    # ÂØæÊï∞Â§âÊèõ
    log_tokens = np.log(tokens)
    log_ppl = np.log(ppl)

    # Á∑öÂΩ¢ÂõûÂ∏∞
    slope, intercept, r_value, p_value, std_err = stats.linregress(log_tokens, log_ppl)

    alpha = slope  # Ë≤†„ÅÆÂÄ§„ÅåÊúüÂæÖ„Åï„Çå„Çã
    A = np.exp(intercept)
    r_squared = r_value ** 2

    return {
        'alpha': alpha,
        'A': A,
        'r_squared': r_squared,
        'p_value': p_value,
        'std_err': std_err,
    }


def main():
    print_flush("=" * 60)
    print_flush("SCALING EXPERIMENT")
    print_flush("=" * 60)
    print_flush(f"Samples: {SAMPLE_SIZES} | Model: {NUM_LAYERS}L/{CONTEXT_DIM}D | Seed: {RANDOM_SEED}")

    # „Ç∑„Éº„ÉâÂõ∫ÂÆö
    set_seed(RANDOM_SEED)

    # „Éá„Éê„Ç§„ÇπË®≠ÂÆö
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print_flush(f"GPU: {gpu_name} ({gpu_mem:.1f}GB)")

    results = []
    total_start = time.time()

    # ÁµêÊûú‰øùÂ≠ò„Éá„Ç£„É¨„ÇØ„Éà„É™
    output_dir = './results/unified_scaling'
    os.makedirs(output_dir, exist_ok=True)

    for i, num_samples in enumerate(SAMPLE_SIZES):
        result = run_experiment(num_samples, device)
        results.append(result)

        # ÈÄî‰∏≠ÁµêÊûú„ÇíJSON‰øùÂ≠òÔºà„ÇØ„É©„ÉÉ„Ç∑„É•ÂØæÁ≠ñÔºâ
        partial_file = os.path.join(output_dir, 'partial_results.json')
        with open(partial_file, 'w') as f:
            json.dump(results, f, indent=2)

    total_time = time.time() - total_start

    scaling = calculate_scaling_law(results)

    # ÁµêÊûú„Çµ„Éû„É™„Éº
    print_flush("\n" + "=" * 60)
    print_flush("RESULTS")
    print_flush("=" * 60)
    print_flush(f"{'Samples':>8} {'Tokens':>10} {'Val PPL':>10} {'Val Acc':>8} {'Val ER':>8}")
    print_flush("-" * 50)
    for r in results:
        print_flush(
            f"{r['num_samples']:>8} {r['train_tokens']:>10,} "
            f"{r['val_ppl']:>10.1f} {r['val_acc']*100:>7.1f}% "
            f"{r['val_effective_rank']*100:>7.1f}%"
        )

    print_flush(f"\nScaling: Œ±={scaling['alpha']:.3f} (R¬≤={scaling['r_squared']:.3f})")
    print_flush(f"Total: {total_time/60:.1f} min")

    # ÁµêÊûú‰øùÂ≠ò
    output = {
        'settings': {
            'sample_sizes': SAMPLE_SIZES,
            'num_layers': NUM_LAYERS,
            'context_dim': CONTEXT_DIM,
            'embed_dim': EMBED_DIM,
            'num_input_tokens': NUM_INPUT_TOKENS,
            'token_input_all_layers': True,  # ‰∏ÄÊú¨ÂåñÔºà2025-11-29Ôºâ
            'dist_reg_weight': DIST_REG_WEIGHT,
            'embedding_freeze': EMBEDDING_FREEZE,
            'tokenization': 'truncation=False (full length)',
            'random_seed': RANDOM_SEED,
        },
        'results': results,
        'scaling_law': scaling,
        'total_time_minutes': total_time / 60,
        'timestamp': datetime.now().isoformat(),
    }

    output_file = os.path.join(output_dir, 'results.json')
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2)

    print_flush(f"Saved: {output_file}")


if __name__ == '__main__':
    main()
