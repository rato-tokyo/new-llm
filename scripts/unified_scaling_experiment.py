#!/usr/bin/env python3
"""
çµ±ä¸€è¨­å®šã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç›®çš„:
- 11/27ã¨11/28ã®å®Ÿé¨“æ¡ä»¶ã®é•ã„ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³åŒ–è¨­å®šï¼‰ã‚’è§£æ¶ˆ
- çµ±ä¸€ã—ãŸè¨­å®šï¼ˆtruncation=Falseï¼‰ã§è¤‡æ•°ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã®å®Ÿé¨“ã‚’å®Ÿæ–½
- æ­£ç¢ºãªÎ±å€¤ã‚’å°å‡º

ä½¿ç”¨æ–¹æ³•:
  Colab: !python scripts/unified_scaling_experiment.py
  Local: python3 scripts/unified_scaling_experiment.py

è¨­å®š:
- ãƒˆãƒ¼ã‚¯ãƒ³åŒ–: truncation=Falseï¼ˆå…¨é•·ä½¿ç”¨ï¼‰
- ã‚µãƒ³ãƒ—ãƒ«æ•°: [50, 100, 200, 500, 1000]
- num_input_tokens: 1
- ãƒ¢ãƒ‡ãƒ«: 6å±¤/768dim

æ–°è¨­è¨ˆå¯¾å¿œ (2025-11-29):
- MemoryDataProvider: ã‚µãƒ³ãƒ—ãƒ«å¢ƒç•Œæƒ…å ±ã‚’æ´»ç”¨
- MemoryPhase1Trainer: CVFPå›ºå®šç‚¹å­¦ç¿’
- Phase2Trainer: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼ã«ã‚ˆã‚‹é«˜é€Ÿè¨“ç·´
"""

import sys
import os
import json
import time
import random
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
import numpy as np
from scipy import stats

# è¨­å®šï¼ˆ11/27å®Ÿé¨“å†ç¾ç”¨ï¼‰
SAMPLE_SIZES = [50, 100, 200, 500]  # 11/27å®Ÿé¨“ã¨åŒã˜ã‚µãƒ³ãƒ—ãƒ«æ•°
NUM_LAYERS = 6
CONTEXT_DIM = 768
EMBED_DIM = 768
NUM_INPUT_TOKENS = 1
EMBEDDING_FREEZE = False  # 11/27å®Ÿé¨“å†ç¾: Embeddingå‡çµãªã—
RANDOM_SEED = 42


def print_flush(msg):
    print(msg, flush=True)


def set_seed(seed=42):
    """å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆå™¨ã®ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def create_val_data_from_train(train_token_ids: torch.Tensor, tokenizer, val_file_path: str, val_ratio: float = 0.1):
    """
    è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ

    Args:
        train_token_ids: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ID
        tokenizer: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
        val_file_path: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆãƒ‘ã‚¹
        val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10%ï¼‰
    """
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®ä¸€éƒ¨ã‚’æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨
    val_size = int(len(train_token_ids) * val_ratio)
    val_token_ids = train_token_ids[-val_size:]

    # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
    val_text = tokenizer.decode(val_token_ids.tolist())

    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    os.makedirs(os.path.dirname(val_file_path), exist_ok=True)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(val_file_path, 'w', encoding='utf-8') as f:
        f.write(val_text)

    print_flush(f"  Created val data: {val_file_path} ({val_size} tokens)")
    return val_size


def run_experiment(
    num_samples: int,
    device: torch.device,
):
    """
    å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã®å®Ÿé¨“ã‚’å®Ÿè¡Œ

    Args:
        num_samples: ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
        device: ãƒ‡ãƒã‚¤ã‚¹

    Returns:
        å®Ÿé¨“çµæœã®è¾æ›¸
    """
    from config import ResidualConfig
    from src.models.llm import LLM
    from src.providers.data import MemoryDataProvider
    from src.trainers.phase1 import MemoryPhase1Trainer
    from src.trainers.phase2 import Phase2Trainer
    from src.evaluation.metrics import analyze_fixed_points
    from transformers import AutoTokenizer

    print_flush(f"\n{'='*70}")
    print_flush(f"Experiment: {num_samples} samples")
    print_flush(f"{'='*70}")

    # ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šï¼ˆå„å®Ÿé¨“ã§åŒã˜åˆæœŸåŒ–ï¼‰
    set_seed(RANDOM_SEED)

    # è¨­å®šã‚’ä½œæˆ
    config = ResidualConfig()
    config.num_layers = NUM_LAYERS
    config.context_dim = CONTEXT_DIM
    config.embed_dim = EMBED_DIM
    config.num_input_tokens = NUM_INPUT_TOKENS
    config.phase2_freeze_embedding = EMBEDDING_FREEZE
    config.num_samples = num_samples

    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ã‚µãƒ³ãƒ—ãƒ«æ•°ã«å¿œã˜ã¦è¨­å®š
    val_file_path = f"./data/ultrachat_{num_samples}samples_val.txt"
    config.val_text_file = val_file_path

    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç”Ÿæˆ
    if not os.path.exists(val_file_path):
        print_flush("\n  Generating validation data...")
        # ã¾ãšè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ä¸€æ™‚çš„ã«MemoryDataProviderã‚’ä½¿ç”¨
        # ä½†ã—ã€val_dataãƒ­ãƒ¼ãƒ‰å‰ã«æ­¢ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ç›´æ¥UltraChatã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
        tokenizer = AutoTokenizer.from_pretrained(
            config.tokenizer_name,
            cache_dir=os.path.join(config.cache_dir, "tokenizer")
        )
        tokenizer.pad_token = tokenizer.eos_token

        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ï¼ˆã¾ãŸã¯ç”Ÿæˆï¼‰
        cache_file = os.path.join(
            config.cache_dir,
            f"ultrachat_{num_samples}samples_full.pt"
        )

        if os.path.exists(cache_file):
            cached = torch.load(cache_file)
            train_tokens_for_val = cached['token_ids']
        else:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”Ÿæˆ
            from datasets import load_dataset
            dataset = load_dataset(
                config.dataset_name,
                split=config.dataset_split,
                cache_dir=os.path.join(config.cache_dir, "datasets")
            )

            all_token_ids = []
            sample_boundaries = []
            current_pos = 0

            for idx in range(num_samples):
                messages = dataset[idx]["messages"]
                text = "\n".join([msg["content"] for msg in messages])
                tokens = tokenizer(text, truncation=False, return_tensors="pt")
                sample_tokens = tokens["input_ids"].squeeze(0)
                all_token_ids.append(sample_tokens)
                sample_len = len(sample_tokens)
                sample_boundaries.append((current_pos, current_pos + sample_len))
                current_pos += sample_len

            train_tokens_for_val = torch.cat(all_token_ids)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            os.makedirs(config.cache_dir, exist_ok=True)
            torch.save({
                'token_ids': train_tokens_for_val,
                'sample_order': list(range(num_samples)),
                'sample_boundaries': sample_boundaries
            }, cache_file)
            print_flush(f"  Cached to: {cache_file}")

        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        create_val_data_from_train(train_tokens_for_val, tokenizer, val_file_path)

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°ã«å¿œã˜ãŸãƒ‡ãƒ¼ã‚¿ï¼‰
    print_flush(f"\n  Loading {num_samples} samples...")
    data_provider = MemoryDataProvider(config, shuffle_samples=False)
    full_train_token_ids, val_token_ids = data_provider.load_data()

    # ğŸš¨ é‡è¦: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¤œè¨¼éƒ¨åˆ†ã‚’é™¤å¤–ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
    val_size = len(val_token_ids)
    train_token_ids = full_train_token_ids[:-val_size]  # æ¤œè¨¼éƒ¨åˆ†ã‚’é™¤å¤–

    train_token_ids = train_token_ids.to(device)
    val_token_ids = val_token_ids.to(device)

    print_flush(f"  Full data tokens: {len(full_train_token_ids):,}")
    print_flush(f"  Train tokens (excluding val): {len(train_token_ids):,}")
    print_flush(f"  Val tokens: {len(val_token_ids):,}")

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = LLM(
        vocab_size=config.vocab_size,
        embed_dim=config.embed_dim,
        context_dim=config.context_dim,
        num_layers=config.num_layers,
        num_input_tokens=config.num_input_tokens,
        num_context_splits=getattr(config, 'num_context_splits', 1),
        use_pretrained_embeddings=config.use_pretrained_embeddings,
        use_weight_tying=config.use_weight_tying,
        token_input_all_layers=getattr(config, 'token_input_all_layers', False)
    )
    model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print_flush(f"  Model parameters: {total_params:,}")

    # Phase 1: CVFPå›ºå®šç‚¹å­¦ç¿’
    print_flush("\n  Phase 1 starting...")
    phase1_start = time.time()

    phase1_trainer = MemoryPhase1Trainer(model, config, device)
    train_contexts = phase1_trainer.train(
        train_token_ids,
        label=f"Train ({num_samples} samples)",
        data_provider=data_provider
    )

    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®Effective Rank
    train_metrics = analyze_fixed_points(train_contexts, label="Train")

    # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®è©•ä¾¡ï¼ˆreturn_contexts_only=Trueã§ãƒ†ãƒ³ã‚½ãƒ«ã‚’å–å¾—ï¼‰
    val_contexts = phase1_trainer.evaluate(
        val_token_ids,
        label=f"Val ({num_samples} samples)",
        return_contexts_only=True
    )
    val_metrics = analyze_fixed_points(val_contexts, label="Val")

    phase1_time = time.time() - phase1_start
    # Effective Rankã®æ¯”ç‡ã‚’è¨ˆç®—ï¼ˆeffective_rank / context_dimï¼‰
    train_er_ratio = train_metrics['effective_rank'] / CONTEXT_DIM
    val_er_ratio = val_metrics['effective_rank'] / CONTEXT_DIM
    print_flush(f"\n  Phase 1 completed: {phase1_time/60:.1f}min")
    print_flush(f"  Train ER: {train_er_ratio*100:.1f}%")
    print_flush(f"  Val ER: {val_er_ratio*100:.1f}%")

    # Phase 2: Next-Token Predictionï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ–¹å¼ï¼‰
    print_flush("\n  Phase 2 starting...")
    phase2_start = time.time()

    phase2_trainer = Phase2Trainer(model=model, config=config)

    # train_fullã§è¨“ç·´ï¼ˆæ—©æœŸåœæ­¢ã‚ã‚Šï¼‰
    phase2_history = phase2_trainer.train_full(
        train_token_ids=train_token_ids,
        val_token_ids=val_token_ids,
        device=device
    )

    phase2_time = time.time() - phase2_start

    # ãƒ™ã‚¹ãƒˆã®çµæœã‚’å–å¾—
    best_epoch = phase2_history['best_epoch']
    best_val_ppl = phase2_history['val_ppl'][best_epoch - 1]
    best_val_acc = phase2_history['val_acc'][best_epoch - 1]

    print_flush(f"\n  Phase 2 completed: {phase2_time/60:.1f}min")
    print_flush(f"  Best Val PPL: {best_val_ppl:.2f}")
    print_flush(f"  Best Val Acc: {best_val_acc*100:.2f}%")

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    data_provider.close()
    del model, phase1_trainer, phase2_trainer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return {
        'num_samples': num_samples,
        'train_tokens': len(train_token_ids),
        'val_tokens': len(val_token_ids),
        'train_effective_rank': train_er_ratio,
        'val_effective_rank': val_er_ratio,
        'val_ppl': best_val_ppl,
        'val_acc': best_val_acc,
        'best_epoch': best_epoch,
        'early_stopped': phase2_history['early_stopped'],
        'phase1_time': phase1_time,
        'phase2_time': phase2_time,
    }


def calculate_scaling_law(results: list):
    """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã‚’è¨ˆç®—: PPL = A Ã— tokens^Î±"""
    tokens = np.array([r['train_tokens'] for r in results])
    ppl = np.array([r['val_ppl'] for r in results])

    # å¯¾æ•°å¤‰æ›
    log_tokens = np.log(tokens)
    log_ppl = np.log(ppl)

    # ç·šå½¢å›å¸°
    slope, intercept, r_value, p_value, std_err = stats.linregress(log_tokens, log_ppl)

    alpha = slope  # è² ã®å€¤ãŒæœŸå¾…ã•ã‚Œã‚‹
    A = np.exp(intercept)
    r_squared = r_value ** 2

    return {
        'alpha': alpha,
        'A': A,
        'r_squared': r_squared,
        'p_value': p_value,
        'std_err': std_err,
    }


def main():
    print_flush("=" * 70)
    print_flush("UNIFIED SCALING EXPERIMENT")
    print_flush("=" * 70)
    print_flush(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print_flush("\nSettings:")
    print_flush(f"  Sample sizes: {SAMPLE_SIZES}")
    print_flush(f"  Model: {NUM_LAYERS} layers, {CONTEXT_DIM} dim")
    print_flush(f"  num_input_tokens: {NUM_INPUT_TOKENS}")
    print_flush(f"  Embedding freeze: {EMBEDDING_FREEZE}")
    print_flush("  Tokenization: truncation=False (full length)")
    print_flush(f"  Random seed: {RANDOM_SEED}")

    # ã‚·ãƒ¼ãƒ‰å›ºå®š
    set_seed(RANDOM_SEED)

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print_flush(f"\nGPU: {gpu_name} ({gpu_mem:.1f}GB)")
    else:
        print_flush("\nUsing CPU")

    # å®Ÿé¨“å®Ÿè¡Œ
    print_flush("\n" + "=" * 70)
    print_flush("Running experiments...")
    print_flush("=" * 70)

    results = []
    total_start = time.time()

    # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = './results/unified_scaling'
    os.makedirs(output_dir, exist_ok=True)

    for i, num_samples in enumerate(SAMPLE_SIZES):
        print_flush(f"\n[{i+1}/{len(SAMPLE_SIZES)}] {num_samples} samples")
        result = run_experiment(num_samples, device)
        results.append(result)

        # é€”ä¸­çµæœã‚’JSONä¿å­˜ï¼ˆã‚¯ãƒ©ãƒƒã‚·ãƒ¥å¯¾ç­–ï¼‰
        partial_file = os.path.join(output_dir, 'partial_results.json')
        with open(partial_file, 'w') as f:
            json.dump(results, f, indent=2)

    total_time = time.time() - total_start

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡è¨ˆç®—
    print_flush("\n" + "=" * 70)
    print_flush("SCALING LAW ANALYSIS")
    print_flush("=" * 70)

    scaling = calculate_scaling_law(results)

    print_flush(f"\nPPL = {scaling['A']:.2f} Ã— tokens^({scaling['alpha']:.4f})")
    print_flush(f"Î± = {scaling['alpha']:.4f}")
    print_flush(f"RÂ² = {scaling['r_squared']:.4f}")
    print_flush(f"p-value = {scaling['p_value']:.6f}")

    # çµæœè¡¨ç¤º
    print_flush("\n" + "=" * 70)
    print_flush("RESULTS SUMMARY")
    print_flush("=" * 70)

    header = f"{'Samples':>8} | {'Tokens':>10} | {'Val PPL':>10} | {'Val Acc':>10}"
    header += f" | {'Train ER':>10} | {'Val ER':>10}"
    print_flush(f"\n{header}")
    print_flush("-" * 80)
    for r in results:
        print_flush(
            f"{r['num_samples']:>8} | "
            f"{r['train_tokens']:>10,} | "
            f"{r['val_ppl']:>10.2f} | "
            f"{r['val_acc']*100:>9.2f}% | "
            f"{r['train_effective_rank']*100:>9.1f}% | "
            f"{r['val_effective_rank']*100:>9.1f}%"
        )

    print_flush(f"\nTotal time: {total_time/60:.1f} minutes")

    # çµæœä¿å­˜
    output = {
        'settings': {
            'sample_sizes': SAMPLE_SIZES,
            'num_layers': NUM_LAYERS,
            'context_dim': CONTEXT_DIM,
            'embed_dim': EMBED_DIM,
            'num_input_tokens': NUM_INPUT_TOKENS,
            'embedding_freeze': EMBEDDING_FREEZE,
            'tokenization': 'truncation=False (full length)',
            'random_seed': RANDOM_SEED,
        },
        'results': results,
        'scaling_law': scaling,
        'total_time_minutes': total_time / 60,
        'timestamp': datetime.now().isoformat(),
    }

    output_file = os.path.join(output_dir, 'results.json')
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2)

    print_flush(f"\nResults saved to: {output_file}")

    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã®è§£é‡ˆ
    print_flush("\n" + "=" * 70)
    print_flush("SCALING LAW INTERPRETATION")
    print_flush("=" * 70)

    # 2å€ã®ãƒ‡ãƒ¼ã‚¿ã§ã©ã‚Œã ã‘PPLãŒä¸‹ãŒã‚‹ã‹
    ppl_reduction = 1 - 2 ** scaling['alpha']
    print_flush(f"\nÎ± = {scaling['alpha']:.4f}")
    print_flush(f"â†’ 2å€ã®ãƒ‡ãƒ¼ã‚¿ã§ {ppl_reduction*100:.1f}% PPLå‰Šæ¸›")

    # éå»ã®å®Ÿé¨“ã¨ã®æ¯”è¼ƒ
    print_flush("\n" + "=" * 70)
    print_flush("COMPARISON WITH PREVIOUS EXPERIMENTS")
    print_flush("=" * 70)

    print_flush("\n| Experiment | Î± | Interpretation |")
    print_flush("|------------|------|----------------|")
    print_flush("| 11/27 (max_length=128) | -0.7463 | 2å€ãƒ‡ãƒ¼ã‚¿ã§40%PPLå‰Šæ¸› |")
    print_flush("| 11/28 v2 (truncation=False) | -0.2926 | 2å€ãƒ‡ãƒ¼ã‚¿ã§22%PPLå‰Šæ¸› |")
    print_flush(f"| This experiment | {scaling['alpha']:.4f} | 2å€ãƒ‡ãƒ¼ã‚¿ã§{ppl_reduction*100:.1f}%PPLå‰Šæ¸› |")

    if scaling['alpha'] < -0.5:
        print_flush("\nâœ… è‰¯å¥½ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ï¼ˆÎ± < -0.5ï¼‰")
    elif scaling['alpha'] < -0.2:
        print_flush("\nâš ï¸ æ¨™æº–çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ï¼ˆ-0.5 < Î± < -0.2ï¼‰")
    else:
        print_flush("\nâš ï¸ ä½ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ï¼ˆÎ± > -0.2ï¼‰")


if __name__ == '__main__':
    main()
