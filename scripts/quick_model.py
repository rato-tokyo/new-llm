#!/usr/bin/env python3
"""
Quick Model Training & Evaluation Script

ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨PPLæ¸¬å®šã‚’ç°¡å˜ã«è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
æ—¥æœ¬èªWikipediaï¼ˆwikipedia 20231101.jaï¼‰ã‚’ä½¿ç”¨ã€‚

Continuous Learning Policy (CLP) å¯¾å¿œ:
- æ—¢å­˜ã®é‡ã¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã‚“ã§ç¶™ç¶šå­¦ç¿’
- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è‡ªå‹•è¨ˆç®—
- è¨“ç·´å¾Œã¯é‡ã¿ã‚’ä¿å­˜

CDR (Context-Dependent Reasoning) è¨“ç·´å¯¾å¿œ:
- --cdr-data ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®š
- knowledge + questionéƒ¨åˆ†ã¯lossè¨ˆç®—ã‹ã‚‰é™¤å¤–ã—ã€answeréƒ¨åˆ†ã®ã¿å­¦ç¿’

Usage:
    # è©•ä¾¡ã®ã¿ï¼ˆè¨“ç·´ãªã—ï¼‰
    python3 scripts/quick_model.py --model pythia

    # ç°¡å˜ãªè¨“ç·´ + è©•ä¾¡ï¼ˆCLPã«å¾“ã„ç¶™ç¶šå­¦ç¿’ï¼‰
    python3 scripts/quick_model.py --model pythia --train --epochs 3

    # è¨“ç·´ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®š
    python3 scripts/quick_model.py --model senri --train --train-tokens 100000 --epochs 5

    # CDRè¨“ç·´ï¼ˆknowledge+questionéƒ¨åˆ†ã®lossã‚’ãƒã‚¹ã‚¯ã€answerã®ã¿å­¦ç¿’ï¼‰
    python3 scripts/quick_model.py --model senri --train --cdr-data senri-fine-tuner/data/family_cdr.json
"""

import argparse
import json
import sys
import time

sys.path.insert(0, ".")

import torch

from src.config import (
    OPEN_CALM_VOCAB_SIZE,
    OPEN_CALM_TOKENIZER,
    MODEL_PRESETS,
    create_model,
)
from src.models import TransformerLM
from src.utils.clp import (
    SENRI_CHECKPOINT,
    PYTHIA_CHECKPOINT,
    load_or_create_model,
    save_model,
    get_data_offset,
    print_clp_status,
)
from src.utils.data_utils import load_wiki_ja_tokens_cached
from src.utils.io import print_flush
from src.utils.seed import set_seed
from src.utils.tokenizer_utils import get_open_calm_tokenizer, test_tokenizer_coverage
from src.utils.training import get_device


# ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å¯¾å¿œ
MODEL_CHECKPOINTS = {
    "senri": SENRI_CHECKPOINT,
    "pythia": PYTHIA_CHECKPOINT,
}


def get_model(model_type: str, use_clp: bool = True) -> tuple[TransformerLM, str, str | None]:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆCLPå¯¾å¿œ: æ—¢å­˜ã®é‡ã¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ï¼‰

    Args:
        model_type: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆ"senri", "pythia"ï¼‰
        use_clp: CLPã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰

    Returns:
        (model, description, checkpoint_path)
    """
    checkpoint_path = MODEL_CHECKPOINTS.get(model_type)

    if use_clp and checkpoint_path:
        model = load_or_create_model(
            lambda: create_model(model_type),
            checkpoint_path,
        )
    else:
        model = create_model(model_type)
        if hasattr(model, 'reset_memory'):
            model.reset_memory()

    return model, model.describe(), checkpoint_path


def train_model(
    model: TransformerLM,
    tokens: torch.Tensor,
    device: torch.device,
    seq_length: int = 128,
    num_tokens: int = 100000,
    epochs: int = 3,
    lr: float = 1e-4,
    batch_size: int = 4,
    data_offset: int = 0,
) -> dict:
    """
    ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ï¼ˆCLPå¯¾å¿œ: ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ•ã‚»ãƒƒãƒˆæŒ‡å®šå¯èƒ½ï¼‰

    Args:
        data_offset: ãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹ä½ç½®ï¼ˆCLPã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼‰
    """
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    # ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’é©ç”¨
    available_tokens = len(tokens) - data_offset - seq_length - 1
    if available_tokens < num_tokens:
        # ã‚ªãƒ•ã‚»ãƒƒãƒˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ãŒè¶³ã‚Šãªã„å ´åˆã¯å…ˆé ­ã«æˆ»ã‚‹
        print_flush(f"    [CLP] Data offset {data_offset:,} exceeds available data, wrapping to start")
        data_offset = 0

    # ãƒãƒƒãƒä½œæˆï¼ˆã‚ªãƒ•ã‚»ãƒƒãƒˆã‹ã‚‰é–‹å§‹ï¼‰
    num_sequences = min(num_tokens, len(tokens) - data_offset - seq_length - 1) // seq_length
    batches = []
    for i in range(0, num_sequences * seq_length, seq_length):
        start_idx = data_offset + i
        input_ids = tokens[start_idx:start_idx + seq_length]
        labels = tokens[start_idx + 1:start_idx + seq_length + 1]
        batches.append((input_ids, labels))

    # ãƒãƒƒãƒã‚’ã¾ã¨ã‚ã‚‹
    batch_groups = []
    for i in range(0, len(batches), batch_size):
        group = batches[i:i + batch_size]
        input_batch = torch.stack([b[0] for b in group]).to(device)
        label_batch = torch.stack([b[1] for b in group]).to(device)
        batch_groups.append((input_batch, label_batch))

    history = {"train_ppl": [], "epoch_time": []}

    for epoch in range(epochs):
        epoch_start = time.time()
        total_loss = 0.0
        total_tokens = 0

        # ãƒ¡ãƒ¢ãƒªãƒªã‚»ãƒƒãƒˆ
        if hasattr(model, 'reset_memory'):
            model.reset_memory()

        for input_ids, labels in batch_groups:
            optimizer.zero_grad()

            logits = model(input_ids, update_memory=True)

            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                reduction='sum'
            )

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            total_tokens += labels.numel()

        epoch_time = time.time() - epoch_start
        train_ppl = torch.exp(torch.tensor(total_loss / total_tokens)).item()

        history["train_ppl"].append(train_ppl)
        history["epoch_time"].append(epoch_time)

        print_flush(f"    Epoch {epoch + 1}/{epochs}: PPL={train_ppl:.1f}, Time={epoch_time:.1f}s")

    return history


def train_model_cdr(
    model: TransformerLM,
    cdr_data_path: str,
    device: torch.device,
    epochs: int = 3,
    lr: float = 1e-4,
    batch_size: int = 4,
) -> dict:
    """
    CDR (Context-Dependent Reasoning) è¨“ç·´

    knowledge + questionéƒ¨åˆ†ã¯lossè¨ˆç®—ã‹ã‚‰é™¤å¤–ã—ã€answeréƒ¨åˆ†ã®ã¿ã‚’å­¦ç¿’ã€‚
    Reversal Curseå¯¾ç­–ã¨ã—ã¦ã€æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚

    Args:
        cdr_data_path: CDRãƒ‡ãƒ¼ã‚¿ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            å½¢å¼: {"samples": [{"knowledge": "...", "question": "...", "answer": "..."}, ...]}

    lossãƒã‚¹ã‚¯:
        knowledge: "Tom is Alice's parent."  â†’ lossé™¤å¤–
        question:  "Who is Alice's parent?"  â†’ lossé™¤å¤–
        answer:    "Tom"                     â†’ lossã‚’è¨ˆç®—
    """
    tokenizer = get_open_calm_tokenizer()

    # JSONãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(cdr_data_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    samples = data.get("samples", data)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãã®å ´åˆã¨ç›´æ¥ãƒªã‚¹ãƒˆã®å ´åˆã«å¯¾å¿œ
    print_flush(f"    Loaded {len(samples)} CDR samples from {cdr_data_path}")

    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)

    # ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€ãƒãƒƒãƒã‚’æº–å‚™
    batches = []
    for sample in samples:
        knowledge = sample["knowledge"]
        question = sample["question"]
        answer = sample["answer"]

        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã§é€£çµï¼‰
        knowledge_ids = tokenizer.encode(knowledge + " ", add_special_tokens=False)
        question_ids = tokenizer.encode(question + " ", add_special_tokens=False)
        answer_ids = tokenizer.encode(answer, add_special_tokens=False)

        # å…¥åŠ›: knowledge + question + answerï¼ˆæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é™¤ãï¼‰
        context_ids = knowledge_ids + question_ids  # lossé™¤å¤–éƒ¨åˆ†
        input_ids = context_ids + answer_ids[:-1] if len(answer_ids) > 1 else context_ids

        # ãƒ©ãƒ™ãƒ«: knowledge + questionéƒ¨åˆ†ã¯-100ï¼ˆç„¡è¦–ï¼‰ã€answeréƒ¨åˆ†ã®ã¿æœ‰åŠ¹
        if len(answer_ids) > 1:
            labels = [-100] * len(context_ids) + answer_ids[1:]
        else:
            # answerãŒ1ãƒˆãƒ¼ã‚¯ãƒ³ã®å ´åˆã€questionã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰answerã‚’äºˆæ¸¬
            labels = [-100] * (len(context_ids) - 1) + answer_ids

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã—ï¼ˆå¯å¤‰é•·ï¼‰
        batches.append({
            "input_ids": torch.tensor(input_ids),
            "labels": torch.tensor(labels),
        })

    history = {"train_loss": [], "epoch_time": []}

    for epoch in range(epochs):
        epoch_start = time.time()
        total_loss = 0.0
        total_tokens = 0

        # ãƒ¡ãƒ¢ãƒªãƒªã‚»ãƒƒãƒˆ
        if hasattr(model, 'reset_memory'):
            model.reset_memory()

        # ãƒŸãƒ‹ãƒãƒƒãƒå‡¦ç†ï¼ˆç°¡æ˜“ç‰ˆï¼š1ã‚µãƒ³ãƒ—ãƒ«ãšã¤ï¼‰
        for i, batch in enumerate(batches):
            optimizer.zero_grad()

            input_ids = batch["input_ids"].unsqueeze(0).to(device)
            labels = batch["labels"].unsqueeze(0).to(device)

            logits = model(input_ids, update_memory=True)

            # contextéƒ¨åˆ†ï¼ˆ-100ï¼‰ã‚’é™¤å¤–ã—ã¦lossè¨ˆç®—
            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=-100,
                reduction='sum'
            )

            # æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            valid_tokens = (labels != -100).sum().item()
            if valid_tokens > 0:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()

                total_loss += loss.item()
                total_tokens += valid_tokens

        epoch_time = time.time() - epoch_start
        avg_loss = total_loss / max(total_tokens, 1)

        history["train_loss"].append(avg_loss)
        history["epoch_time"].append(epoch_time)

        print_flush(f"    Epoch {epoch + 1}/{epochs}: Loss={avg_loss:.4f}, Time={epoch_time:.1f}s")

    return history


def evaluate_ppl(
    model: TransformerLM,
    tokens: torch.Tensor,
    device: torch.device,
    seq_length: int = 128,
    num_tokens: int = 50000,
    update_memory: bool = True,
) -> float:
    """PPLã‚’è¨ˆç®—"""
    model.eval()
    total_loss = 0.0
    total_tokens = 0

    # ãƒ¡ãƒ¢ãƒªãƒªã‚»ãƒƒãƒˆï¼ˆSenriãƒ¢ãƒ‡ãƒ«ã®å ´åˆï¼‰
    if hasattr(model, 'reset_memory'):
        model.reset_memory()

    with torch.no_grad():
        for i in range(0, min(num_tokens, len(tokens) - seq_length), seq_length):
            input_ids = tokens[i:i + seq_length].unsqueeze(0).to(device)
            labels = tokens[i + 1:i + seq_length + 1].unsqueeze(0).to(device)

            logits = model(input_ids, update_memory=update_memory)

            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                reduction='sum'
            )

            total_loss += loss.item()
            total_tokens += labels.numel()

    ppl = torch.exp(torch.tensor(total_loss / total_tokens)).item()
    return ppl


def test_tokenizer() -> bool:
    """ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‹•ä½œç¢ºèª"""
    tokenizer = get_open_calm_tokenizer()

    test_cases = [
        ("æ—¥æœ¬èª", "ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚"),
        ("è‹±èªæ··åœ¨", "AIã®ç™ºå±•ã¯ç›®è¦šã¾ã—ã„ã€‚GPUã§å­¦ç¿’ã‚’é«˜é€ŸåŒ–ã€‚"),
        ("æŠ€è¡“ç”¨èª", "APIã‚’å‘¼ã³å‡ºã—ã¦HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã€‚"),
        ("çµµæ–‡å­—", "å®Œäº†ã—ã¾ã—ãŸï¼ğŸ‰"),
    ]

    all_passed = True
    for name, text in test_cases:
        result = test_tokenizer_coverage(tokenizer, text)
        status = "OK" if not result["has_unk"] else "NG"
        if result["has_unk"]:
            all_passed = False
        print_flush(f"    [{status}] {name}: {len(result['tokens'])} tokens")

    # vocab_sizeç¢ºèª
    if tokenizer.vocab_size != OPEN_CALM_VOCAB_SIZE:
        print_flush(f"    [NG] Vocab size mismatch: {tokenizer.vocab_size} != {OPEN_CALM_VOCAB_SIZE}")
        all_passed = False
    else:
        print_flush(f"    [OK] Vocab size: {tokenizer.vocab_size:,}")

    return all_passed


def test_generation(
    model: TransformerLM,
    device: torch.device,
    prompt: str = "ä»Šæ—¥ã¯",
    max_tokens: int = 20,
) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    tokenizer = get_open_calm_tokenizer()
    model.eval()

    # ãƒ¡ãƒ¢ãƒªãƒªã‚»ãƒƒãƒˆ
    if hasattr(model, 'reset_memory'):
        model.reset_memory()

    input_ids = tokenizer.encode(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        for _ in range(max_tokens):
            logits = model(input_ids)
            next_token_id = logits[0, -1, :].argmax().item()
            input_ids = torch.cat(
                [input_ids, torch.tensor([[next_token_id]], device=device)],
                dim=1
            )
            if next_token_id == tokenizer.eos_token_id:
                break

    return tokenizer.decode(input_ids[0], skip_special_tokens=True)


def main():
    parser = argparse.ArgumentParser(description="Quick Model Training & Evaluation")
    parser.add_argument(
        "--model", type=str, default="pythia",
        choices=list(MODEL_PRESETS.keys()),
        help="Model type (default: pythia). See src/config/models.py for details."
    )
    parser.add_argument(
        "--num-tokens", type=int, default=50000,
        help="Number of tokens for PPL evaluation (default: 50000)"
    )
    parser.add_argument(
        "--seq-length", type=int, default=128,
        help="Sequence length (default: 128)"
    )
    # è¨“ç·´ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--train", action="store_true",
        help="Enable training mode"
    )
    parser.add_argument(
        "--train-tokens", type=int, default=100000,
        help="Number of tokens for training (default: 100000)"
    )
    parser.add_argument(
        "--epochs", type=int, default=3,
        help="Number of training epochs (default: 3)"
    )
    parser.add_argument(
        "--lr", type=float, default=1e-4,
        help="Learning rate (default: 1e-4)"
    )
    parser.add_argument(
        "--batch-size", type=int, default=4,
        help="Batch size for training (default: 4)"
    )
    # CDRè¨“ç·´ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--cdr-data", type=str, default=None,
        help="Path to CDR training data JSON file (context-dependent reasoning)"
    )
    # ã‚¹ã‚­ãƒƒãƒ—ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--skip-generation", action="store_true",
        help="Skip text generation test"
    )
    parser.add_argument(
        "--skip-ppl", action="store_true",
        help="Skip PPL evaluation"
    )
    parser.add_argument(
        "--skip-tokenizer", action="store_true",
        help="Skip tokenizer test"
    )
    # CLPã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument(
        "--no-clp", action="store_true",
        help="Disable Continuous Learning Policy (start from scratch, no checkpoint)"
    )
    parser.add_argument(
        "--no-save", action="store_true",
        help="Don't save checkpoint after training"
    )

    args = parser.parse_args()

    set_seed(42)
    device = get_device()
    use_clp = not args.no_clp

    mode = "TRAINING & EVALUATION" if args.train else "EVALUATION ONLY"
    print_flush("=" * 70)
    print_flush(f"QUICK MODEL {mode}")
    if use_clp:
        print_flush("Continuous Learning Policy: ENABLED")
    else:
        print_flush("Continuous Learning Policy: DISABLED (--no-clp)")
    print_flush("=" * 70)

    # CLPçŠ¶æ…‹è¡¨ç¤º
    if use_clp:
        print_flush("\n[0] CLP Status")
        print_clp_status()

    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ†ã‚¹ãƒˆ
    if not args.skip_tokenizer:
        step_num = 1 if use_clp else 0
        print_flush(f"\n[{step_num}] Tokenizer Test")
        tokenizer_ok = test_tokenizer()
        if not tokenizer_ok:
            print_flush("    WARNING: Tokenizer test failed!")

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆCLP: æ—¢å­˜é‡ã¿ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿ï¼‰
    step_num = 2 if use_clp else 1
    print_flush(f"\n[{step_num}] Creating model: {args.model}")
    start_time = time.time()
    model, model_name, checkpoint_path = get_model(args.model, use_clp=use_clp)
    model = model.to(device)
    elapsed = time.time() - start_time

    total_params = sum(p.numel() for p in model.parameters())
    print_flush(f"    Model: {model_name}")
    print_flush(f"    Parameters: {total_params:,}")
    print_flush(f"    Created in {elapsed:.2f}s")

    # CLPãƒ‡ãƒ¼ã‚¿ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—
    data_offset = get_data_offset() if use_clp else 0

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¥æœ¬èªWikipediaï¼‰- ã‚ªãƒ•ã‚»ãƒƒãƒˆåˆ†ã‚‚å«ã‚ã¦èª­ã¿è¾¼ã¿
    step_num = 3 if use_clp else 2
    max_tokens = max(args.num_tokens, args.train_tokens if args.train else 0) + data_offset
    print_flush(f"\n[{step_num}] Loading Japanese Wikipedia ({max_tokens:,} tokens)...")
    tokens = load_wiki_ja_tokens_cached(max_tokens + args.seq_length + 1, OPEN_CALM_TOKENIZER)
    print_flush(f"    Loaded {len(tokens):,} tokens")
    if use_clp and data_offset > 0:
        print_flush(f"    [CLP] Data offset: {data_offset:,} (timestamp-based)")

    # è¨“ç·´
    train_history = None
    cdr_history = None
    if args.train:
        step_num = 4 if use_clp else 3

        # CDRãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯CDRè¨“ç·´ã‚‚å®Ÿè¡Œ
        if args.cdr_data:
            print_flush(f"\n[{step_num}] CDR Training ({args.epochs} epochs)")
            cdr_history = train_model_cdr(
                model, args.cdr_data, device,
                epochs=args.epochs,
                lr=args.lr,
                batch_size=args.batch_size,
            )
            step_num += 1

        # é€šå¸¸ã®è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°è¨“ç·´
        print_flush(f"\n[{step_num}] Training ({args.train_tokens:,} tokens, {args.epochs} epochs)")
        train_history = train_model(
            model, tokens, device,
            seq_length=args.seq_length,
            num_tokens=args.train_tokens,
            epochs=args.epochs,
            lr=args.lr,
            batch_size=args.batch_size,
            data_offset=data_offset,
        )

        # CLP: è¨“ç·´å¾Œã«é‡ã¿ã‚’ä¿å­˜
        if use_clp and checkpoint_path and not args.no_save:
            save_model(model, checkpoint_path)

    # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    step = 4 if args.train else 3
    if not args.skip_generation:
        print_flush(f"\n[{step}] Text Generation Test")
        prompts = ["ä»Šæ—¥ã¯", "äººå·¥çŸ¥èƒ½ã¯", "æ—¥æœ¬ã®é¦–éƒ½ã¯"]
        for prompt in prompts:
            generated = test_generation(model, device, prompt, max_tokens=15)
            print_flush(f"    \"{prompt}\" â†’ \"{generated}\"")
        step += 1

    # PPLè©•ä¾¡
    val_ppl = None
    if not args.skip_ppl:
        print_flush(f"\n[{step}] PPL Evaluation ({args.num_tokens:,} tokens)")
        print_flush("    Calculating PPL...")
        start_time = time.time()
        val_ppl = evaluate_ppl(
            model, tokens, device,
            seq_length=args.seq_length,
            num_tokens=args.num_tokens,
        )
        elapsed = time.time() - start_time
        print_flush(f"    Val PPL: {val_ppl:.1f}")
        print_flush(f"    Evaluated in {elapsed:.1f}s")

    # ã‚µãƒãƒªãƒ¼
    print_flush("\n" + "=" * 70)
    print_flush("SUMMARY")
    print_flush("=" * 70)
    print_flush(f"Model: {model_name}")
    print_flush(f"Parameters: {total_params:,}")
    if use_clp:
        print_flush(f"CLP: ENABLED (data offset: {data_offset:,})")
        if checkpoint_path:
            print_flush(f"Checkpoint: {checkpoint_path}")
    if args.train and cdr_history:
        print_flush(f"CDR Training: {args.epochs} epochs, final loss={cdr_history['train_loss'][-1]:.4f}")
    if args.train and train_history:
        print_flush(f"LM Training: {args.epochs} epochs, final train PPL={train_history['train_ppl'][-1]:.1f}")
    if val_ppl is not None:
        print_flush(f"Val PPL: {val_ppl:.1f}")
    print_flush("=" * 70)
    print_flush("DONE")


if __name__ == "__main__":
    main()
