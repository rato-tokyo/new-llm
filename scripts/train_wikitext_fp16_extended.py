#!/usr/bin/env python3
"""
WikiText-2 FP16 Extended Training - Resume from 50 epochs, train to 150 epochs total

This script continues training from the best checkpoint of the 50-epoch run,
extending training for an additional 100 epochs (total 150 epochs).

Usage:
    python scripts/train_wikitext_fp16_extended.py
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.amp
from src.utils.config import NewLLML4Config
from src.models.context_vector_llm import ContextVectorLLM
from src.training.wikitext_dataset import load_wikitext_data
from src.training.trainer import Trainer
from torch.utils.data import DataLoader
import time


class FP16Config(NewLLML4Config):
    """FP16æ··åˆç²¾åº¦è¨“ç·´ç”¨ã®è¨­å®šï¼ˆL4 GPUæœ€é©åŒ–ï¼‰

    This class is needed for checkpoint compatibility.
    The checkpoint was saved with FP16Config, so we need to define it here
    even though we use FP16ExtendedConfig for training.
    """
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£ï¼ˆWikiText-2ç”¨ï¼‰
    max_seq_length = 64
    vocab_size = 1000

    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆBaselineï¼‰
    embed_dim = 256
    hidden_dim = 512
    num_layers = 6
    context_vector_dim = 256
    dropout = 0.1

    # è¨“ç·´ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆNewLLML4Configã‹ã‚‰ç¶™æ‰¿ï¼‰
    num_epochs = 50
    weight_decay = 0.0
    gradient_clip = 1.0

    # Early Stopping
    patience = 15

    # FP16è¨­å®š
    use_amp = True


class FP16ExtendedConfig(NewLLML4Config):
    """FP16æ··åˆç²¾åº¦è¨“ç·´ç”¨ã®è¨­å®šï¼ˆ100ã‚¨ãƒãƒƒã‚¯å»¶é•·ç‰ˆï¼‰

    L4 GPUæœ€é©åŒ–è¨­å®šï¼ˆbatch_size=2048, learning_rate=0.0004ï¼‰ã‚’ç¶™æ‰¿
    50ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰å†é–‹ã—ã€åˆè¨ˆ150ã‚¨ãƒãƒƒã‚¯è¨“ç·´
    """
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£ï¼ˆWikiText-2ç”¨ï¼‰
    max_seq_length = 64
    vocab_size = 1000

    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆBaselineï¼‰
    embed_dim = 256
    hidden_dim = 512
    num_layers = 6
    context_vector_dim = 256
    dropout = 0.1

    # è¨“ç·´ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆNewLLML4Configã‹ã‚‰ç¶™æ‰¿ï¼‰
    # batch_size = 2048     â† NewLLML4Configã‹ã‚‰è‡ªå‹•ç¶™æ‰¿ï¼ˆL4ç”¨ï¼‰
    # learning_rate = 0.0008 â† NewLLML4Configã‹ã‚‰è‡ªå‹•ç¶™æ‰¿ï¼ˆSquare Root Scalingé©ç”¨æ¸ˆã¿ï¼‰
    # device = "cuda"       â† NewLLML4Configã‹ã‚‰è‡ªå‹•ç¶™æ‰¿
    num_epochs = 150  # åˆè¨ˆ150ã‚¨ãƒãƒƒã‚¯ï¼ˆ50ã‹ã‚‰å†é–‹ã—ã¦+100ï¼‰
    weight_decay = 0.0
    gradient_clip = 1.0

    # Early Stopping
    patience = 30  # å»¶é•·è¨“ç·´ã®ãŸã‚é•·ã‚ã«è¨­å®š

    # FP16è¨­å®š
    use_amp = True  # Automatic Mixed Precision (GPUå¿…é ˆ)

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨­å®š
    checkpoint_to_resume = "best_new_llm_wikitext_fp16.pt"


class FP16Trainer(Trainer):
    """FP16æ··åˆç²¾åº¦å¯¾å¿œã®Trainer

    torch.cuda.amp (Automatic Mixed Precision) ã‚’ä½¿ç”¨
    """
    def __init__(self, model, train_dataloader, val_dataloader, config, model_name="new_llm", use_amp=True):
        super().__init__(model, train_dataloader, val_dataloader, config, model_name)
        self.use_amp = use_amp
        self.scaler = torch.amp.GradScaler('cuda') if use_amp else None

    def train_epoch(self):
        """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´ï¼ˆFP16å¯¾å¿œï¼‰"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        for batch_idx, (input_ids, target_ids) in enumerate(self.train_dataloader):
            input_ids = input_ids.to(self.device)
            target_ids = target_ids.to(self.device)

            self.optimizer.zero_grad()

            # FP16æ··åˆç²¾åº¦ã§è¨“ç·´
            if self.use_amp:
                with torch.amp.autocast('cuda'):
                    logits = self.model(input_ids)
                    loss = self.criterion(logits, target_ids)

                # Scaled backward pass
                self.scaler.scale(loss).backward()

                # Gradient clipping (unscale before clipping)
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)

                # Optimizer step with scaling
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # FP32è¨“ç·´ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                logits = self.model(input_ids)
                loss = self.criterion(logits, target_ids)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            if (batch_idx + 1) % max(1, len(self.train_dataloader) // 5) == 0:
                progress = int((batch_idx + 1) / len(self.train_dataloader) * 100)
                print(f"{progress}% ", end="", flush=True)

        avg_loss = total_loss / num_batches
        return avg_loss


def main():
    """FP16æ··åˆç²¾åº¦è¨“ç·´ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆ100ã‚¨ãƒãƒƒã‚¯å»¶é•·ç‰ˆï¼‰"""
    print("\n" + "="*80)
    print("WikiText-2 Extended Training with FP16 Mixed Precision")
    print("Resume from 50 epochs â†’ Train to 150 epochs total (+100 epochs)")
    print("="*80)

    # Git version information
    try:
        import subprocess
        git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=os.path.dirname(__file__) + '/..').decode().strip()
        git_commit_short = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=os.path.dirname(__file__) + '/..').decode().strip()
        git_date = subprocess.check_output(['git', 'log', '-1', '--format=%cd', '--date=short'], cwd=os.path.dirname(__file__) + '/..').decode().strip()
        print(f"\nðŸ“Œ Git Version: {git_commit_short} ({git_date})")
        print(f"   Full commit: {git_commit}")
    except Exception:
        print(f"\nðŸ“Œ Git Version: Unknown (not a git repository)")

    print("="*80)

    config = FP16ExtendedConfig()

    # GPUå¿…é ˆãƒã‚§ãƒƒã‚¯
    if not torch.cuda.is_available():
        raise RuntimeError("âŒ GPU not available! FP16 training requires CUDA GPU.")

    # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±è¡¨ç¤º
    print(f"\nðŸ–¥ï¸  Device Information:")
    print(f"  Device: CUDA (GPU)")
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"  GPU: {torch.cuda.get_device_name(0)}")
    print(f"  GPU Memory: {gpu_memory:.1f} GB")
    print(f"  FP16 Mixed Precision: ENABLED âœ“")

    print(f"\nå®Ÿé¨“è¨­å®š:")
    print(f"  Model: New-LLM Baseline")
    print(f"  Context Vector Dim: {config.context_vector_dim}")
    print(f"  Num Layers: {config.num_layers}")
    print(f"  Total Epochs: {config.num_epochs} (50 done + 100 extended)")
    print(f"  Batch Size: {config.batch_size}")
    print(f"  Learning Rate: {config.learning_rate}")
    print(f"  Precision: FP16 (Mixed)")
    print(f"  Resume from: {config.checkpoint_to_resume}")
    print(f"\n{'='*80}\n")

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    print("Loading WikiText-2 dataset...")
    train_dataset, val_dataset, tokenizer = load_wikitext_data(config)

    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("\nCreating New-LLM model...")
    model = ContextVectorLLM(config)

    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)")

    # FP16 Trainerä½œæˆ
    trainer = FP16Trainer(
        model=model,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        config=config,
        model_name="new_llm_wikitext_fp16_extended",
        use_amp=config.use_amp
    )

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹
    # Note: resume_from_checkpoint() will automatically add "checkpoints/" prefix
    checkpoint_name = config.checkpoint_to_resume
    checkpoint_path = os.path.join("checkpoints", checkpoint_name)
    if os.path.exists(checkpoint_path):
        print(f"\nðŸ“‚ Resuming from checkpoint: {checkpoint_path}")
        start_epoch = trainer.resume_from_checkpoint(checkpoint_name)
        print(f"âœ“ Resumed from epoch {start_epoch}")
        print(f"ðŸ“Š Previous best validation PPL: {min(trainer.val_ppls) if trainer.val_ppls else 'N/A'}")
    else:
        raise FileNotFoundError(f"âŒ Checkpoint not found: {checkpoint_path}\n"
                                f"Please run train_wikitext_fp16.py first to generate the checkpoint.")

    # è¨“ç·´å®Ÿè¡Œ
    print("\nStarting extended training (100 more epochs)...")
    start_time = time.time()
    trainer.train()
    total_time = time.time() - start_time

    print("\n" + "="*80)
    print("Extended FP16 Mixed Precision Training Completed!")
    print("="*80)
    print(f"Total training time: {total_time/3600:.2f} hours")
    print(f"Checkpoint saved: checkpoints/best_new_llm_wikitext_fp16_extended.pt")

    # æ€§èƒ½ã‚µãƒžãƒªãƒ¼
    if trainer.val_ppls:
        best_ppl = min(trainer.val_ppls)
        best_epoch = trainer.val_ppls.index(best_ppl) + 1
        final_ppl = trainer.val_ppls[-1]

        print(f"\nðŸ“Š Performance Summary:")
        print(f"  Best Validation PPL: {best_ppl:.2f} (Epoch {best_epoch})")
        print(f"  Final Validation PPL: {final_ppl:.2f}")
        print(f"  Improvement from epoch 50: {trainer.val_ppls[49] if len(trainer.val_ppls) > 49 else 'N/A'} â†’ {final_ppl:.2f}")

    print("\n" + "="*80)


if __name__ == "__main__":
    main()
