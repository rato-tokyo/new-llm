#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WikiText-2 Advanced Training - æ‹¡å¼µå®Ÿé¨“ç”¨

å®Ÿé¨“å†…å®¹:
1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒã®æ‹¡å¼µï¼ˆ256 â†’ 512, 1024ãªã©ï¼‰
2. ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ã®æŸ”è»Ÿãªå¤‰æ›´ï¼ˆ6 â†’ 12, 24ãªã©ï¼‰
3. int8é‡å­åŒ–ã‚µãƒãƒ¼ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

ä½¿ã„æ–¹:
1. AdvancedConfigã‚¯ãƒ©ã‚¹ã§è¨­å®šã‚’å¤‰æ›´
2. python scripts/train_wikitext_advanced.py
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.quantization
from src.utils.config import NewLLMAdvancedGPUConfig
from src.models.context_vector_llm import ContextVectorLLM
from src.training.wikitext_dataset import load_wikitext_data
from src.training.trainer import Trainer


class AdvancedConfig(NewLLMAdvancedGPUConfig):
    """æ‹¡å¼µå®Ÿé¨“ç”¨ã®æŸ”è»Ÿãªè¨­å®šã‚¯ãƒ©ã‚¹

    GPUæœ€é©åŒ–è¨­å®šï¼ˆbatch_size=512, device="cuda", context_vector_dim=512, num_layers=12ï¼‰ã‚’ç¶™æ‰¿

    ç°¡å˜ã«å¤‰æ›´ã§ãã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
    - context_vector_dim: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ï¼ˆ512, 1024, 2048ãªã©ï¼‰
    - num_layers: ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°ï¼ˆ12, 24, 48ãªã©ï¼‰
    - quantization_mode: é‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰ ('none', 'int8')
    """

    # ========================================
    # å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã“ã“ã‚’å¤‰æ›´ã™ã‚‹ã ã‘ï¼ï¼‰
    # ========================================

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒ - NewLLMAdvancedGPUConfigã‹ã‚‰ç¶™æ‰¿
    # context_vector_dim = 512  â† è‡ªå‹•ç¶™æ‰¿ï¼ˆå¤‰æ›´ã—ãŸã„å ´åˆã®ã¿ä¸Šæ›¸ãï¼‰

    # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•° - NewLLMAdvancedGPUConfigã‹ã‚‰ç¶™æ‰¿
    # num_layers = 12  â† è‡ªå‹•ç¶™æ‰¿ï¼ˆå¤‰æ›´ã—ãŸã„å ´åˆã®ã¿ä¸Šæ›¸ãï¼‰

    # é‡å­åŒ–ãƒ¢ãƒ¼ãƒ‰: 'none', 'int8'
    quantization_mode = 'none'  # 'int8'ã§æœ‰åŠ¹åŒ–

    # ========================================
    # åŸºæœ¬è¨­å®šï¼ˆé€šå¸¸ã¯å¤‰æ›´ä¸è¦ï¼‰
    # ========================================

    # ãƒ‡ãƒ¼ã‚¿é–¢é€£ï¼ˆWikiText-2ç”¨ï¼‰
    max_seq_length = 64
    vocab_size = 1000

    # ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    embed_dim = 256
    hidden_dim = 512
    dropout = 0.1

    # è¨“ç·´ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆNewLLMAdvancedGPUConfigã‹ã‚‰ç¶™æ‰¿ï¼‰
    # batch_size = 2048  â† NewLLMAdvancedL4Configã‹ã‚‰è‡ªå‹•ç¶™æ‰¿
    # device = "cuda"    â† NewLLMAdvancedL4Configã‹ã‚‰è‡ªå‹•ç¶™æ‰¿
    num_epochs = 50
    learning_rate = 0.0004  # Linear Scaling Rule: batch_size 4x â†’ LR 4x
    weight_decay = 0.0
    gradient_clip = 1.0

    # Early Stopping
    patience = 15

    def get_experiment_name(self):
        """å®Ÿé¨“åã‚’è‡ªå‹•ç”Ÿæˆ"""
        name = f"new_llm_wikitext"
        name += f"_ctx{self.context_vector_dim}"
        name += f"_layers{self.num_layers}"
        if self.quantization_mode != 'none':
            name += f"_{self.quantization_mode}"
        return name


def apply_quantization(model, mode='int8'):
    """é‡å­åŒ–ã‚’é©ç”¨

    Args:
        model: é‡å­åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        mode: 'int8' or 'none'

    Returns:
        é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
    """
    if mode == 'none':
        return model

    elif mode == 'int8':
        print(f"\n{'='*60}")
        print(f"Applying int8 quantization...")
        print(f"{'='*60}")

        # int8é‡å­åŒ–ï¼ˆå‹•çš„é‡å­åŒ– - æ¨è«–æ™‚ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {torch.nn.Linear},  # ç·šå½¢å±¤ã®ã¿é‡å­åŒ–
            dtype=torch.qint8   # int8
        )

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°æ¯”è¼ƒ
        original_size = sum(p.numel() for p in model.parameters()) * 4 / (1024**2)  # MB
        quantized_size = sum(p.numel() for p in quantized_model.parameters()) * 1 / (1024**2)  # MB (int8 = 1 byte)

        print(f"Original model size: {original_size:.2f} MB (fp32)")
        print(f"Quantized model size: {quantized_size:.2f} MB (int8)")
        print(f"Compression ratio: {original_size/quantized_size:.2f}x")

        return quantized_model

    else:
        raise ValueError(f"Unknown quantization mode: {mode}")


def train_new_llm_advanced():
    """æ‹¡å¼µå®Ÿé¨“ã§New-LLMã‚’è¨“ç·´"""

    config = AdvancedConfig()
    experiment_name = config.get_experiment_name()

    print("="*80)
    print("Advanced WikiText-2 Training Experiment")
    print("="*80)

    # Git version information
    try:
        import subprocess
        git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd=os.path.dirname(__file__) + '/..').decode().strip()
        git_commit_short = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], cwd=os.path.dirname(__file__) + '/..').decode().strip()
        git_date = subprocess.check_output(['git', 'log', '-1', '--format=%cd', '--date=short'], cwd=os.path.dirname(__file__) + '/..').decode().strip()
        print(f"\nğŸ“Œ Git Version: {git_commit_short} ({git_date})")
        print(f"   Full commit: {git_commit}")
    except Exception:
        print(f"\nğŸ“Œ Git Version: Unknown (not a git repository)")

    print("="*80)

    # GPU/CPUæƒ…å ±ã‚’æ˜ç¤ºçš„ã«è¡¨ç¤º
    print(f"\nğŸ–¥ï¸  Device Information:")
    print(f"  Device: {config.device.upper()}")
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPU Memory: {gpu_memory:.1f} GB")
        print(f"  Batch Size: {config.batch_size} (optimized for GPU RAM)")

        # äºˆæƒ³GPU RAMä½¿ç”¨é‡
        model_params = 4.84  # 4.84M params
        estimated_usage = model_params * 0.004 * config.batch_size / 32  # rough estimate
        print(f"  Estimated GPU RAM usage: {estimated_usage:.1f} GB ({estimated_usage/gpu_memory*100:.0f}%)")
        print(f"  âš¡ GPU acceleration ENABLED - Maximum performance mode")

        if estimated_usage < gpu_memory * 0.5:
            print(f"  ğŸ’¡ TIP: GPU RAM underutilized. Can increase batch_size to {config.batch_size * 2}")
    else:
        print(f"  âš ï¸  WARNING: Running on CPU (will be VERY SLOW)")
        print(f"  ğŸ’¡ Solution: Runtime â†’ Change runtime type â†’ GPU (T4)")

    print(f"\nå®Ÿé¨“è¨­å®š:")
    print(f"  Context Vector Dim: {config.context_vector_dim}")
    print(f"  Number of Layers: {config.num_layers}")
    print(f"  Quantization: {config.quantization_mode}")
    print(f"  Experiment Name: {experiment_name}")
    print(f"\n{'='*80}\n")

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
    print("Loading WikiText-2 dataset...")
    train_dataset, val_dataset, tokenizer = load_wikitext_data(config)

    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("\nCreating New-LLM model...")
    model = ContextVectorLLM(config)

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¡¨ç¤º
    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {num_params:,} ({num_params/1e6:.2f}M)")

    # é‡å­åŒ–é©ç”¨ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if config.quantization_mode != 'none':
        model = apply_quantization(model, config.quantization_mode)

    # DataLoaderä½œæˆ
    from torch.utils.data import DataLoader
    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

    # Trainerä½œæˆ
    trainer = Trainer(
        model=model,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        config=config,
        model_name=experiment_name
    )

    # è¨“ç·´å®Ÿè¡Œ
    print("\nStarting training...")
    trainer.train()

    print("\n" + "="*80)
    print("Advanced Training Completed!")
    print("="*80)
    print(f"Checkpoint saved: checkpoints/best_{experiment_name}.pt")

    return trainer


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("\n" + "="*80)
    print("New-LLM Advanced Training Experiment")
    print("="*80)

    # è¨­å®šç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    config = AdvancedConfig()
    print(f"\nç¾åœ¨ã®è¨­å®š:")
    print(f"  Context Vector Dim: {config.context_vector_dim}")
    print(f"  Number of Layers: {config.num_layers}")
    print(f"  Quantization: {config.quantization_mode}")

    # å®Ÿè¡Œ
    trainer = train_new_llm_advanced()

    # çµæœã‚µãƒãƒªãƒ¼
    if trainer.val_losses:
        best_val_loss = min(trainer.val_losses)
        best_val_ppl = min(trainer.val_ppls)
        print(f"\næœ€çµ‚çµæœ:")
        print(f"  Best Val Loss: {best_val_loss:.4f}")
        print(f"  Best Val Perplexity: {best_val_ppl:.2f}")


if __name__ == "__main__":
    main()
