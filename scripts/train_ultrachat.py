#!/usr/bin/env python3
"""
Train New-LLM on UltraChat for large-scale diverse dialog

Usage (L4 GPU):
    python scripts/train_ultrachat.py --num_layers 1
    python scripts/train_ultrachat.py --num_layers 4

    # Limit samples for faster experiments
    python scripts/train_ultrachat.py --num_layers 4 --max_samples 100000

UltraChat dataset:
    - 1.5M+ diverse conversations
    - English only
    - Multi-turn dialogues
    - Very diverse topics (science, tech, history, culture, philosophy, etc.)
    - Generated by GPT-3.5 for high diversity
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
from torch.utils.data import DataLoader
from src.models.context_vector_llm import ContextVectorLLM
from src.training.fp16_trainer import FP16Trainer
from src.training.ultrachat_dataset import load_ultrachat_data
from src.utils.config import NewLLML4Config
from src.utils.train_utils import (
    print_git_info, print_gpu_info, print_model_info,
    print_config_info, print_dataset_info, print_dataloader_info
)
import argparse


class UltraChatTrainConfig(NewLLML4Config):
    """Config for UltraChat training on L4 GPU (from scratch)

    Inherits L4 optimization from NewLLML4Config:
    - batch_size = 2048
    - learning_rate = 0.0008 (Square Root Scaling)
    - device = "cuda"

    UltraChat specific adjustments:
    - Longer sequences (128 tokens) for multi-turn dialog
    - Fewer epochs due to large dataset size (1.5M+)
    """
    # Data settings (UltraChat)
    max_seq_length = 128  # Longer for multi-turn dialog
    vocab_size = 1000  # Same as other experiments

    # Model architecture
    embed_dim = 256
    hidden_dim = 512
    num_layers = 1  # Default, will be overridden in __init__
    context_vector_dim = 256
    dropout = 0.1

    # Training hyperparameters (inherited from NewLLML4Config)
    # batch_size = 2048      ‚Üê L4 GPU optimized
    # learning_rate = 0.0008 ‚Üê Square Root Scaling Rule
    # device = "cuda"
    num_epochs = 50  # Fewer epochs due to large dataset (1.5M+ samples)
    weight_decay = 0.0
    gradient_clip = 1.0

    # Early stopping
    patience = 20

    # FP16 mixed precision
    use_amp = True

    def __init__(self, num_layers=1):
        """Initialize config with specified num_layers"""
        super().__init__()
        self.num_layers = num_layers


def main():
    """Train New-LLM on UltraChat from scratch"""

    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Train New-LLM on UltraChat')
    parser.add_argument('--num_layers', type=int, default=1,
                       help='Number of layers (default: 1)')
    parser.add_argument('--max_seq_length', type=int, default=128,
                       help='Maximum sequence length (default: 128)')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='Maximum number of training samples (default: None = all 1.5M+)')
    parser.add_argument('--epochs', type=int, default=50,
                       help='Number of epochs (default: 50)')

    args = parser.parse_args()

    print("\n" + "="*80)
    if args.max_samples:
        print(f"UltraChat Training - {args.num_layers} Layer(s) ({args.max_samples:,} samples)")
    else:
        print(f"UltraChat Training - {args.num_layers} Layer(s) (full dataset)")
    print("="*80)

    # Git version information
    print_git_info()

    # Create configuration
    config = UltraChatTrainConfig(num_layers=args.num_layers)
    config.max_seq_length = args.max_seq_length
    config.num_epochs = args.epochs

    # Print configuration
    print_config_info(config)

    # GPU check and info
    print_gpu_info()

    # Load UltraChat dataset
    print("\n" + "="*80)
    if args.max_samples:
        print(f"Loading UltraChat Dataset ({args.max_samples:,} samples)")
    else:
        print(f"Loading UltraChat Dataset (full dataset)")
    print("="*80)

    train_dataset, val_dataset, tokenizer = load_ultrachat_data(
        config,
        max_samples=args.max_samples
    )
    print_dataset_info(train_dataset, val_dataset, tokenizer)

    # Create DataLoaders
    print(f"\nüì¶ Creating DataLoaders...")
    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_dataloader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)
    print_dataloader_info(train_dataloader, val_dataloader)

    # Create model
    print("\n" + "="*80)
    print("Creating Model")
    print("="*80)

    model = ContextVectorLLM(config).to(config.device)
    print_model_info(model)

    # Create trainer
    print("\n" + "="*80)
    print("Starting Training")
    print("="*80)

    trainer = FP16Trainer(
        model=model,
        train_dataloader=train_dataloader,
        val_dataloader=val_dataloader,
        config=config,
        model_name=f"new_llm_ultrachat_layers{config.num_layers}",
        use_amp=config.use_amp
    )

    # Train
    trainer.train()

    print("\n" + "="*80)
    print("‚úÖ Training Complete!")
    print(f"   Best model saved: checkpoints/best_new_llm_ultrachat_layers{config.num_layers}.pt")
    print("="*80)


if __name__ == "__main__":
    main()
