remote: Enumerating objects: 9, done.
remote: Counting objects: 100% (9/9), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)
Unpacking objects: 100% (5/5), 1.19 KiB | 1.19 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
   25bc4eb..3a3692b  main       -> origin/main
Updating 25bc4eb..3a3692b
Fast-forward
 src/evaluation/metrics.py | 48 ++++++++++++++++++++++++++++++++++++++---------
 1 file changed, 39 insertions(+), 9 deletions(-)

======================================================================
New-LLM Training for Google Colab
======================================================================

âœ… Random seed fixed: 42 (å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼)
ğŸ–¥ï¸  Device: cuda
   GPU: NVIDIA L4
   Memory: 23.8 GB

ğŸ“‹ Configuration:
   Architecture: residual_standard
   Layers: 3
   Context dim: 768
   Diversity weight: 0.5
   Phase 2 epochs: 10
   Early stopping patience: 3
   Context-Fixed Learning: context_out = C*[i] (complete fixing)

ğŸ“¥ Downloading GPT-2 tokenizer...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 160kB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.58MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 5.77MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 4.43MB/s]
config.json: 100% 665/665 [00:00<00:00, 5.99MB/s]
âœ“ Tokenizer saved

ğŸ“¦ Creating model...
2025-11-26 05:45:49.299414: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-26 05:45:49.315386: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764135949.333914   54344 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764135949.339561   54344 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764135949.356060   54344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764135949.356087   54344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764135949.356090   54344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764135949.356092   54344 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-26 05:45:49.360942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
âœ“ Model created: 122,935,633 parameters

ğŸ“Š Loading data from UltraChat...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 221kB/s]
config.json: 100% 665/665 [00:00<00:00, 2.41MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 4.63MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 6.01MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.00MB/s]
   Downloading UltraChat dataset...
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 41672.94 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 56615.29 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 73387.24 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 75963.92 examples/s]
   Cached to: ./cache/ultrachat_500samples_128len.pt
   Total tokens: 64,000
   Train: 62,720 tokens
   Val:   1,280 tokens (fixed size)
   âœ“ Validation auto-generated from training data

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP)
======================================================================


======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP) - Train
======================================================================
Iteration 1/20: é †ä¼æ’­ã®ã¿ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ï¼‰ [68.44s]
Iteration 2/20: åæŸ=100.0% | Total=-0.051728 | CVFP=0.000014 | Div=-0.103470 | Time=4.57s
Iteration 3/20: åæŸ=0.0% | Total=0.235273 | CVFP=0.576503 | Div=-0.105958 | Time=0.39s
Iteration 4/20: åæŸ=0.0% | Total=0.285708 | CVFP=0.678205 | Div=-0.106789 | Time=0.39s
Iteration 5/20: åæŸ=0.0% | Total=0.280118 | CVFP=0.667569 | Div=-0.107334 | Time=0.40s
Iteration 6/20: åæŸ=0.0% | Total=0.244685 | CVFP=0.596226 | Div=-0.106857 | Time=0.40s
Iteration 7/20: åæŸ=0.0% | Total=0.201473 | CVFP=0.509311 | Div=-0.106365 | Time=0.40s
Iteration 8/20: åæŸ=0.0% | Total=0.154015 | CVFP=0.413982 | Div=-0.105953 | Time=0.39s
Iteration 9/20: åæŸ=0.0% | Total=0.106318 | CVFP=0.317860 | Div=-0.105224 | Time=0.40s
Iteration 10/20: åæŸ=0.0% | Total=0.063176 | CVFP=0.230838 | Div=-0.104486 | Time=0.39s
Iteration 11/20: åæŸ=0.1% | Total=0.027702 | CVFP=0.159265 | Div=-0.103861 | Time=0.40s
Iteration 12/20: åæŸ=4.0% | Total=0.000465 | CVFP=0.104405 | Div=-0.103475 | Time=0.40s
Iteration 13/20: åæŸ=21.4% | Total=-0.018806 | CVFP=0.065609 | Div=-0.103222 | Time=0.40s
Iteration 14/20: åæŸ=49.6% | Total=-0.031575 | CVFP=0.040006 | Div=-0.103155 | Time=0.40s
Iteration 15/20: åæŸ=74.6% | Total=-0.039659 | CVFP=0.023910 | Div=-0.103229 | Time=0.40s
Iteration 16/20: åæŸ=89.4% | Total=-0.044630 | CVFP=0.014069 | Div=-0.103328 | Time=0.39s
Iteration 17/20: åæŸ=96.0% | Total=-0.047610 | CVFP=0.008238 | Div=-0.103458 | Time=0.40s
Iteration 18/20: åæŸ=98.6% | Total=-0.049397 | CVFP=0.004850 | Div=-0.103645 | Time=0.40s
Iteration 19/20: åæŸ=99.5% | Total=-0.050485 | CVFP=0.002882 | Div=-0.103853 | Time=0.40s
Iteration 20/20: åæŸ=99.8% | Total=-0.051162 | CVFP=0.001736 | Div=-0.104060 | Time=0.40s

Phase 1 å®Œäº†: 62605/62720 ãƒˆãƒ¼ã‚¯ãƒ³ãŒåæŸ


======================================================================
Evaluating on validation data...
======================================================================


â±ï¸  Phase 1 completed in 82.0s
ğŸ’¾ Checkpoint saved: ./checkpoints/model_latest.pt

======================================================================
FIXED-POINT ANALYSIS
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Train
======================================================================

(Sampling 5000/62720 tokens for pairwise analysis)

1. Global Attractor Detection:
  Avg L2 Distance: 36.524075 (Range: [0.224261, 45.731693])
  Avg Cosine Sim:  0.088511 (Range: [-0.402123, 0.999967])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.312475 (Range: [27.276262, 27.342703])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.3125, Median: 27.3126, Std: 0.0093
  Pairwise Dist - Mean: 36.5241
  Sparsity: 0.60% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 559.35 / 768 (72.8%)
  Top 5 Singular Values: [1201.1953125, 853.2880249023438, 754.5025634765625, 635.5288696289062, 526.506103515625]
  âœ… Good diversity
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Val
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 36.521900 (Range: [0.000000, 45.761738])
  Avg Cosine Sim:  0.087324 (Range: [-0.403037, 1.000000])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.310556 (Range: [27.277653, 27.336336])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.3106, Median: 27.3107, Std: 0.0101
  Pairwise Dist - Mean: 36.5219
  Sparsity: 0.61% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 487.13 / 768 (63.4%)
  Top 5 Singular Values: [438.29632568359375, 312.57037353515625, 264.04718017578125, 225.30870056152344, 191.8311004638672]
  âœ… Good diversity
======================================================================


======================================================================
æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯ (Identity Mapping Check)
======================================================================
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦çµ±è¨ˆ (100ã‚µãƒ³ãƒ—ãƒ«):
  å¹³å‡: 0.0652
  æœ€å¤§: 0.1647
  æœ€å°: -0.0164
  é–¾å€¤(0.95)è¶…é: 0/100 (0.0%)

âœ… æ­£å¸¸: æ’ç­‰å†™åƒã§ã¯ã‚ã‚Šã¾ã›ã‚“
    ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚
======================================================================


======================================================================
PHASE 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å­¦ç¿’
======================================================================

âœ“ Full model fine-tuning: 84,338,257/122,935,633 parameters trainable
âœ“ Context-Fixed Learning: context_out replaced with C*[i] (complete fixing)

======================================================================
PHASE 2: Next-Token Prediction Training
         (Context-Fixed Learning)
======================================================================

Training tokens: 62,720
Validation tokens: 1,280
Epochs: 10
Batch size: 512
Learning rate: 0.002
CVFP layers frozen: False
Early stopping patience: 3
âœ“ Stage 1: Initialize fixed contexts C* from training data
âœ“ Stage 2: Train with context_out = C*[i] (complete fixing)
âœ“ Prediction from concatenated C*[i] + token_out
âœ“ Gradients flow through token_out only

Stage 1: Initializing fixed contexts C*...
âœ“ Training target contexts initialized: torch.Size([62720, 768])
âœ“ Validation target contexts initialized: torch.Size([1280, 768])

Stage 2: Training with fixed contexts...
Epoch 1/10:
  Train Loss: 7.6115 | Train PPL: 2021.31
  Val Loss: 7.3743 | Val PPL: 1594.53 | Val Acc: 14.31%
  âœ“ New best validation loss: 7.3743

Epoch 2/10:
  Train Loss: 5.0282 | Train PPL: 152.66
  Val Loss: 7.7128 | Val PPL: 2236.69 | Val Acc: 15.09%
  âš ï¸ No improvement (1/3)

Epoch 3/10:
  Train Loss: 3.7206 | Train PPL: 41.29
  Val Loss: 7.9566 | Val PPL: 2854.34 | Val Acc: 15.40%
  âš ï¸ No improvement (2/3)

Epoch 4/10:
  Train Loss: 3.1058 | Train PPL: 22.33
  Val Loss: 8.0979 | Val PPL: 3287.41 | Val Acc: 15.40%
  âš ï¸ No improvement (3/3)

â›” Early stopping triggered at epoch 4
   Val loss hasn't improved for 3 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 1
Best validation loss: 7.3743
Best validation PPL: 1594.53
Best validation accuracy: 14.31%
Early stopped at epoch: 4


â±ï¸  Phase 2 completed in 2202.6s
ğŸ’¾ Final checkpoint saved: ./checkpoints/model_latest.pt


======================================================================
                    NEW-LLM TRAINING RESULTS                         
======================================================================

[PHASE 1: Context Learning (CVFP)]
  Effective Rank (Train): 72.8% (559.35/768)
  Effective Rank (Val):   63.4% (487.13/768)
  Time: 82.0s
  Status: âœ… PASSED

[PHASE 2: Token Prediction]
  Best Val PPL:    1594.53 (Epoch 1)
  Best Val Acc:    14.31%
  Final Val PPL:   3287.41
  Final Val Acc:   15.40%
  Epochs Run:      4/10
  Time: 2202.6s
  Status: âš ï¸  EARLY STOPPED at epoch 4

----------------------------------------------------------------------
  TOTAL TIME: 2310.7s
======================================================================

ğŸ“‰ Epoch-by-Epoch Progress:
--------------------------------------------------
 Epoch |  Train PPL |    Val PPL |  Val Acc
--------------------------------------------------
     1 |    2021.31 |    1594.53 |   14.31% â­
     2 |     152.66 |    2236.69 |   15.09%
     3 |      41.29 |    2854.34 |   15.40%
     4 |      22.33 |    3287.41 |   15.40%
--------------------------------------------------

âœ… Training complete!
