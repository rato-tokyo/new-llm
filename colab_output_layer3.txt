remote: Enumerating objects: 8, done.
remote: Counting objects: 100% (8/8), done.
remote: Compressing objects: 100% (4/4), done.
remote: Total 6 (delta 4), reused 4 (delta 2), pack-reused 0 (from 0)
Unpacking objects: 100% (6/6), 569 bytes | 284.00 KiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   bb04565..4a219f8  main       -> origin/main
Updating bb04565..4a219f8
Fast-forward
 config.py | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

======================================================================
COLAB SCALING EXPERIMENT (Phase1 + Phase2)
======================================================================
Start time: 2025-11-27 06:58:38
GPU: NVIDIA L4 (23.8GB)

Sample sizes: [50, 100, 200, 500]
Config:
  - num_layers: 3
  - context_dim: 768
  - phase1_max_iterations: 10
  - phase2_epochs: 10
  - phase2_batch_size: 512

======================================================================
Loading all data...
======================================================================

  Loading 510 samples from UltraChat...
  Total tokens: 40,504
  Samples loaded: 510
  Train samples: 0 ~ 499
  Val samples: 500 ~ 509

[Progress: 1/4] 残り推定時間: 不明

======================================================================
EXPERIMENT 1/4: num_samples = 50
======================================================================
2025-11-27 06:58:48.118578: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-27 06:58:48.136496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764226728.158095   13425 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764226728.164614   13425 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764226728.181159   13425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764226728.181191   13425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764226728.181194   13425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764226728.181197   13425 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-27 06:58:48.186188: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  Train tokens: 3,495
  Val tokens: 690 (固定)

--- Phase 1: CVFP Learning ---

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train (samples=50)
======================================================================
  Mode: Memory
  Tokens: 3,495
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 3,545,856
Iteration 1/10: シーケンシャル [2.03s]
Iteration 2/10: 収束=100.0% | Loss=-0.206301 | CVFP=0.004577 | Div=-0.417180 [0.40s]
Iteration 3/10: 収束=0.0% | Loss=-0.057792 | CVFP=0.337412 | Div=-0.452996 [0.01s]
Iteration 4/10: 収束=0.0% | Loss=-0.098619 | CVFP=0.252303 | Div=-0.449541 [0.01s]
Iteration 5/10: 収束=0.0% | Loss=-0.145947 | CVFP=0.156514 | Div=-0.448407 [0.01s]
Iteration 6/10: 収束=0.0% | Loss=-0.184241 | CVFP=0.081031 | Div=-0.449512 [0.01s]
Iteration 7/10: 収束=7.2% | Loss=-0.203625 | CVFP=0.044834 | Div=-0.452083 [0.01s]
Iteration 8/10: 収束=64.6% | Loss=-0.213659 | CVFP=0.028146 | Div=-0.455463 [0.01s]
Iteration 9/10: 収束=96.2% | Loss=-0.219332 | CVFP=0.020037 | Div=-0.458701 [0.01s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 3362/3495 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 690
  Trials: 10

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000954
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000954
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000064

Verdict: ✅ CONVERGED: Loss is stable - model has converged

  Phase 1 完了: 2.5s
  Train ER: 597.9 (77.9%)
  Val ER: 445.4 (58.0%)

--- Phase 2: Next-Token Prediction ---
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 42,193,489/84,338,257 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 3,495
Validation tokens: 690
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [17.4s]:
  Train Loss: 9.4196 | Train PPL: 12327.70
  Val Loss: 9.1537 | Val PPL: 9449.54 | Val Acc: 6.82%
  ✓ New best validation loss: 9.1537

Epoch 2/10 [17.3s]:
  Train Loss: 5.5621 | Train PPL: 260.38
  Val Loss: 8.8752 | Val PPL: 7152.28 | Val Acc: 10.01%
  ✓ New best validation loss: 8.8752

Epoch 3/10 [17.3s]:
  Train Loss: 4.0704 | Train PPL: 58.58
  Val Loss: 9.1947 | Val PPL: 9845.07 | Val Acc: 11.32%
  ⚠️ No improvement (1/2)

Epoch 4/10 [17.3s]:
  Train Loss: 2.5816 | Train PPL: 13.22
  Val Loss: 9.5371 | Val PPL: 13865.12 | Val Acc: 12.05%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 4
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 8.8752
Best validation PPL: 7152.28
Best validation accuracy: 10.01%
Early stopped at epoch: 4


==================================================
RESULT: 50 samples (1.4min)
==================================================
  Phase 1: Train ER=77.9%, Val ER=58.0%
  Phase 2: Val PPL=7152.28, Val Acc=10.01%

[Progress: 2/4] 残り推定時間: 4.1min

======================================================================
EXPERIMENT 2/4: num_samples = 100
======================================================================
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  Train tokens: 7,466
  Val tokens: 690 (固定)

--- Phase 1: CVFP Learning ---

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train (samples=100)
======================================================================
  Mode: Memory
  Tokens: 7,466
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 3,545,856
Iteration 1/10: シーケンシャル [3.67s]
Iteration 2/10: 収束=100.0% | Loss=-0.140741 | CVFP=0.004529 | Div=-0.286012 [0.27s]
Iteration 3/10: 収束=0.0% | Loss=0.010736 | CVFP=0.331498 | Div=-0.310026 [0.02s]
Iteration 4/10: 収束=0.0% | Loss=-0.025342 | CVFP=0.255976 | Div=-0.306660 [0.02s]
Iteration 5/10: 収束=0.0% | Loss=-0.073454 | CVFP=0.157687 | Div=-0.304596 [0.02s]
Iteration 6/10: 収束=0.1% | Loss=-0.112097 | CVFP=0.080207 | Div=-0.304401 [0.02s]
Iteration 7/10: 収束=7.5% | Loss=-0.130697 | CVFP=0.044200 | Div=-0.305595 [0.02s]
Iteration 8/10: 収束=68.2% | Loss=-0.140092 | CVFP=0.027386 | Div=-0.307570 [0.02s]
Iteration 9/10: 収束=97.3% | Loss=-0.145135 | CVFP=0.019299 | Div=-0.309568 [0.02s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 7267/7466 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 690
  Trials: 10

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000914
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000914
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000061

Verdict: ✅ CONVERGED: Loss is stable - model has converged

  Phase 1 完了: 4.2s
  Train ER: 612.1 (79.7%)
  Val ER: 440.8 (57.4%)

--- Phase 2: Next-Token Prediction ---
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 42,193,489/84,338,257 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 7,466
Validation tokens: 690
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [35.7s]:
  Train Loss: 8.9294 | Train PPL: 7550.51
  Val Loss: 8.1197 | Val PPL: 3360.15 | Val Acc: 7.40%
  ✓ New best validation loss: 8.1197

Epoch 2/10 [35.7s]:
  Train Loss: 5.9470 | Train PPL: 382.60
  Val Loss: 8.2196 | Val PPL: 3712.97 | Val Acc: 10.74%
  ⚠️ No improvement (1/2)

Epoch 3/10 [35.7s]:
  Train Loss: 4.5642 | Train PPL: 95.99
  Val Loss: 8.4455 | Val PPL: 4654.20 | Val Acc: 11.90%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 3
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 1
Best validation loss: 8.1197
Best validation PPL: 3360.15
Best validation accuracy: 7.40%
Early stopped at epoch: 3


==================================================
RESULT: 100 samples (1.9min)
==================================================
  Phase 1: Train ER=79.7%, Val ER=57.4%
  Phase 2: Val PPL=3360.15, Val Acc=7.40%

[Progress: 3/4] 残り推定時間: 3.3min

======================================================================
EXPERIMENT 3/4: num_samples = 200
======================================================================
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  Train tokens: 15,832
  Val tokens: 690 (固定)

--- Phase 1: CVFP Learning ---

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train (samples=200)
======================================================================
  Mode: Memory
  Tokens: 15,832
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 3,545,856
Iteration 1/10: シーケンシャル [8.44s]
Iteration 2/10: 収束=100.0% | Loss=-0.095841 | CVFP=0.004497 | Div=-0.196180 [0.58s]
Iteration 3/10: 収束=0.0% | Loss=0.059614 | CVFP=0.332001 | Div=-0.212774 [0.04s]
Iteration 4/10: 収束=0.0% | Loss=0.025680 | CVFP=0.261288 | Div=-0.209928 [0.04s]
Iteration 5/10: 収束=0.0% | Loss=-0.024021 | CVFP=0.159639 | Div=-0.207682 [0.04s]
Iteration 6/10: 収束=0.0% | Loss=-0.063733 | CVFP=0.079440 | Div=-0.206905 [0.04s]
Iteration 7/10: 収束=9.2% | Loss=-0.082157 | CVFP=0.043034 | Div=-0.207348 [0.04s]
Iteration 8/10: 収束=74.9% | Loss=-0.091148 | CVFP=0.026129 | Div=-0.208425 [0.04s]
Iteration 9/10: 収束=98.3% | Loss=-0.095697 | CVFP=0.018168 | Div=-0.209561 [0.04s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 15555/15832 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 690
  Trials: 10

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000916
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000916
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000061

Verdict: ✅ CONVERGED: Loss is stable - model has converged

  Phase 1 完了: 9.5s
  Train ER: 611.3 (79.6%)
  Val ER: 436.7 (56.9%)

--- Phase 2: Next-Token Prediction ---
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 42,193,489/84,338,257 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 15,832
Validation tokens: 690
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [74.3s]:
  Train Loss: 8.1820 | Train PPL: 3575.90
  Val Loss: 7.7995 | Val PPL: 2439.31 | Val Acc: 9.14%
  ✓ New best validation loss: 7.7995

Epoch 2/10 [74.4s]:
  Train Loss: 5.5449 | Train PPL: 255.93
  Val Loss: 8.2480 | Val PPL: 3820.01 | Val Acc: 11.61%
  ⚠️ No improvement (1/2)

Epoch 3/10 [74.3s]:
  Train Loss: 4.3464 | Train PPL: 77.20
  Val Loss: 8.7177 | Val PPL: 6110.24 | Val Acc: 11.32%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 3
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 1
Best validation loss: 7.7995
Best validation PPL: 2439.31
Best validation accuracy: 9.14%
Early stopped at epoch: 3


==================================================
RESULT: 200 samples (3.9min)
==================================================
  Phase 1: Train ER=79.6%, Val ER=56.9%
  Phase 2: Val PPL=2439.31, Val Acc=9.14%

[Progress: 4/4] 残り推定時間: 2.4min

======================================================================
EXPERIMENT 4/4: num_samples = 500
======================================================================
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  Train tokens: 39,814
  Val tokens: 690 (固定)

--- Phase 1: CVFP Learning ---

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train (samples=500)
======================================================================
  Mode: Memory
  Tokens: 39,814
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 3,545,856
Iteration 1/10: シーケンシャル [21.14s]
Iteration 2/10: 収束=100.0% | Loss=-0.059617 | CVFP=0.004485 | Div=-0.123719 [1.52s]
Iteration 3/10: 収束=0.0% | Loss=0.096649 | CVFP=0.327506 | Div=-0.134208 [0.11s]
Iteration 4/10: 収束=0.0% | Loss=0.066118 | CVFP=0.264331 | Div=-0.132095 [0.12s]
Iteration 5/10: 収束=0.0% | Loss=0.014534 | CVFP=0.159223 | Div=-0.130155 [0.11s]
Iteration 6/10: 収束=0.1% | Loss=-0.025837 | CVFP=0.077585 | Div=-0.129259 [0.12s]
Iteration 7/10: 収束=12.1% | Loss=-0.043942 | CVFP=0.041398 | Div=-0.129283 [0.11s]
Iteration 8/10: 収束=80.7% | Loss=-0.052470 | CVFP=0.024810 | Div=-0.129750 [0.12s]
Iteration 9/10: 収束=99.1% | Loss=-0.056621 | CVFP=0.017024 | Div=-0.130266 [0.11s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 39468/39814 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 690
  Trials: 10

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000861
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000861
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000057

Verdict: ✅ CONVERGED: Loss is stable - model has converged

  Phase 1 完了: 23.9s
  Train ER: 606.1 (78.9%)
  Val ER: 432.1 (56.3%)

--- Phase 2: Next-Token Prediction ---
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 42,193,489/84,338,257 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 39,814
Validation tokens: 690
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [185.5s]:
  Train Loss: 7.5151 | Train PPL: 1835.48
  Val Loss: 7.0657 | Val PPL: 1171.15 | Val Acc: 13.79%
  ✓ New best validation loss: 7.0657

Epoch 2/10 [185.9s]:
  Train Loss: 5.0539 | Train PPL: 156.64
  Val Loss: 7.5096 | Val PPL: 1825.52 | Val Acc: 15.97%
  ⚠️ No improvement (1/2)

Epoch 3/10 [185.7s]:
  Train Loss: 3.6585 | Train PPL: 38.80
  Val Loss: 7.9936 | Val PPL: 2961.81 | Val Acc: 17.71%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 3
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 1
Best validation loss: 7.0657
Best validation PPL: 1171.15
Best validation accuracy: 13.79%
Early stopped at epoch: 3


==================================================
RESULT: 500 samples (9.8min)
==================================================
  Phase 1: Train ER=78.9%, Val ER=56.3%
  Phase 2: Val PPL=1171.15, Val Acc=13.79%

========================================================================================================================
SUMMARY TABLE
========================================================================================================================
 Samples |     Tokens |     Time |  Train ER% |    Val ER% |    Val PPL |   Val Acc% |  P2 Epochs
---------+------------+----------+------------+------------+------------+------------+-----------
      50 |      3,495 |   1.4min |      77.9% |      58.0% |    7152.28 |     10.01% |          4
     100 |      7,466 |   1.9min |      79.7% |      57.4% |    3360.15 |      7.40% |          3
     200 |     15,832 |   3.9min |      79.6% |      56.9% |    2439.31 |      9.14% |          3
     500 |     39,814 |   9.8min |      78.9% |      56.3% |    1171.15 |     13.79% |          3

======================================================================
EXPERIMENT COMPLETE
======================================================================
Total time: 17.2min
End time: 2025-11-27 07:15:47

Results saved to: ./results/colab_scaling_experiment.json

======================================================================
SCALING TRENDS (for prediction)
======================================================================
Val PPL trend: 7152.28 (50 samples) -> 1171.15 (500 samples)
Val ER% trend: 58.0% (50 samples) -> 56.3% (500 samples)
Val Acc trend: 10.01% (50 samples) -> 13.79% (500 samples)
