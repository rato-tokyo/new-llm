======================================================================
UNIFIED SCALING EXPERIMENT
======================================================================
Start time: 2025-11-29 06:22:23

Settings:
  Sample sizes: [500]
  Model: 6 layers, 768 dim
  num_input_tokens: 1
  Embedding freeze: False
  Tokenization: truncation=False (full length)
  Random seed: 42

GPU: NVIDIA L4 (22.2GB)

======================================================================
Running experiments...
======================================================================

[1/1] 500 samples

======================================================================
Experiment: 500 samples
======================================================================

  Generating validation data...
tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 183kB/s]
config.json: 100% 665/665 [00:00<00:00, 5.57MB/s]
vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.57MB/s]
merges.txt: 100% 456k/456k [00:00<00:00, 2.01MB/s]
tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 2.91MB/s]
README.md: 3.90kB [00:00, 17.8MB/s]
data/train_sft-00000-of-00003-a3ecf92756(…): 100% 244M/244M [00:02<00:00, 119MB/s]     
data/train_sft-00001-of-00003-0a1804bcb6(…): 100% 244M/244M [00:01<00:00, 202MB/s]
data/train_sft-00002-of-00003-ee46ed25cf(…): 100% 244M/244M [00:01<00:00, 224MB/s]    
data/test_sft-00000-of-00001-f7dfac4afe5(…): 100% 81.2M/81.2M [00:00<00:00, 88.4MB/s]
data/train_gen-00000-of-00003-a6c9fb894b(…): 100% 244M/244M [00:00<00:00, 244MB/s]  
data/train_gen-00001-of-00003-d6a0402e41(…): 100% 243M/243M [00:01<00:00, 223MB/s]
data/train_gen-00002-of-00003-c0db75b92a(…): 100% 243M/243M [00:01<00:00, 223MB/s]
data/test_gen-00000-of-00001-3d4cd830914(…): 100% 80.4M/80.4M [00:00<00:00, 99.9MB/s]
Generating train_sft split: 100% 207865/207865 [00:04<00:00, 49816.70 examples/s]
Generating test_sft split: 100% 23110/23110 [00:00<00:00, 57589.18 examples/s]
Generating train_gen split: 100% 256032/256032 [00:03<00:00, 73947.63 examples/s]
Generating test_gen split: 100% 28304/28304 [00:00<00:00, 76026.58 examples/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (1464 > 1024). Running this sequence through the model will result in indexing errors
  Cached to: ./cache/ultrachat_500samples_full.pt
  Created val data: ./data/ultrachat_500samples_val.txt (58797 tokens)

  Loading 500 samples...
Loading training data...
  Loading from cache: ./cache/ultrachat_500samples_full.pt
Loading validation data...
Token indices sequence length is longer than the specified maximum sequence length for this model (58797 > 1024). Running this sequence through the model will result in indexing errors
  Train: 587970 tokens (500 samples)
  Val:   58797 tokens
  Full data tokens: 587,970
  Train tokens (excluding val): 529,173
  Val tokens: 58,797
2025-11-29 06:23:09.556061: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-29 06:23:09.572729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764397389.591214    4177 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764397389.596625    4177 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764397389.610703    4177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764397389.610730    4177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764397389.610733    4177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764397389.610736    4177 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-29 06:23:09.614918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
config.json: 100% 665/665 [00:00<00:00, 5.35MB/s]
model.safetensors: 100% 548M/548M [00:00<00:00, 571MB/s]
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 1
  token_input_all_layers: True (旧構造、高ER)
  Model parameters: 91,429,969

  Phase 1 starting...

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train (500 samples)
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 529,173
  Max iterations: 40
  Learning rate: 0.002
  Diversity weight: 0.6
  ContextBlock params: 7,091,712
Iteration 1/40: シーケンシャル [457.44s]
Iteration 2/40: 収束=100.0% | Loss=-0.165809 | CVFP=0.002154 | Div=-0.277784 [8.22s]
Iteration 3/40: 収束=0.0% | Loss=0.047207 | CVFP=0.559093 | Div=-0.294051 [7.36s]
Iteration 4/40: 収束=0.0% | Loss=-0.028143 | CVFP=0.379209 | Div=-0.299711 [7.07s]
Iteration 5/40: 収束=0.0% | Loss=-0.134463 | CVFP=0.115425 | Div=-0.301055 [7.09s]
Iteration 6/40: 収束=25.9% | Loss=-0.165322 | CVFP=0.040463 | Div=-0.302512 [7.06s]
Iteration 7/40: 収束=99.2% | Loss=-0.176565 | CVFP=0.014346 | Div=-0.303839 [7.16s]
  → Early stopping (min_iterations=5 satisfied)

Phase 1 完了: 524777/529173 トークン収束


======================================================================
FIXED-POINT ANALYSIS - Train
======================================================================

(Sampling 5000/529173 tokens for pairwise analysis)

1. Global Attractor Detection:
  Avg L2 Distance: 38.140179 (Range: [0.335210, 45.223869])
  Avg Cosine Sim:  0.052762 (Range: [-0.314383, 0.999927])
  ✅ Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.895723 (Range: [27.863201, 27.919367])
  ✅ Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.8957, Median: 27.8959, Std: 0.0067
  Pairwise Dist - Mean: 38.1402
  Sparsity: 0.62% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 583.00 / 768 (75.9%)
  Top 5 Singular Values: [1065.138671875, 696.0336303710938, 599.0383911132812, 576.7777709960938, 489.704833984375]
  ✅ Good diversity
======================================================================


Evaluating Val (500 samples) data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 58,797
  Trials: 4
  num_input_tokens: 1

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/4: CVFP Loss = N/A (baseline)
Trial  2/4: CVFP Loss = 0.000000
Trial  3/4: CVFP Loss = 0.000000
Trial  4/4: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000000
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000000

Verdict: ✅ CONVERGED: Loss is stable - model has converged


======================================================================
FIXED-POINT ANALYSIS - Val
======================================================================

(Sampling 5000/58797 tokens for pairwise analysis)

1. Global Attractor Detection:
  Avg L2 Distance: 38.199417 (Range: [0.000000, 45.542194])
  Avg Cosine Sim:  0.051556 (Range: [-0.329266, 1.000001])
  ✅ Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.936041 (Range: [27.906157, 27.963411])
  ✅ Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.9360, Median: 27.9361, Std: 0.0074
  Pairwise Dist - Mean: 38.1994
  Sparsity: 0.62% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 580.05 / 768 (75.5%)
  Top 5 Singular Values: [1060.0899658203125, 694.23681640625, 573.5112915039062, 543.9608154296875, 485.5927429199219]
  ✅ Good diversity
======================================================================


  Phase 1 completed: 11.7min
  Train ER: 75.9%
  Val ER: 75.5%

  Phase 2 starting...
✓ ContextBlock frozen
✓ token_output layer unfrozen
✓ Training TokenBlock + token_output: 45,740,881/91,429,969 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (キャッシュ方式)
======================================================================

Memory Estimation:
  Train cache: 10.60GB
  Val cache: 1.18GB
  Available GPU: 17.7GB
  ✓ OK: 13.3GB required, 17.7GB available

Training tokens: 529,173
Validation tokens: 58,797
Epochs: 10
Initial batch size: 8864
Learning rate: 0.001
Gradient clip: 1.0
Early stopping patience: 1

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN + CACHED (エポックごとに1回計算)
  - TokenBlock: TRAINING (バッチ並列処理)
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Building context cache (one-time computation)...
Cache built in 627.2s
Actual cache size: train=9301.9MB, val=1033.5MB (total=10.09GB)

Calculating optimal batch size...
  GPU Memory: total=22.2GB, allocated=13.8GB, reserved=13.9GB, free=8.3GB
  Per-token memory: 0.67MB, available: 2128.1MB
  Batch size calculation: free=8.3GB × safety=0.5 → 3171 tokens
⚠️ Auto-adjusting batch_size: 8864 → 3171
Effective batch size: 3171

Epoch 1/10 [21.0s]:
  Train Loss: 6.5645 | Train PPL: 709.49
  Val Loss: 5.8770 | Val PPL: 356.73 | Val Acc: 17.92%
  ✓ New best validation loss: 5.8770

Epoch 2/10 [21.3s]:
  Train Loss: 5.0146 | Train PPL: 150.60
  Val Loss: 5.7838 | Val PPL: 325.00 | Val Acc: 18.76%
  ✓ New best validation loss: 5.7838

Epoch 3/10 [21.1s]:
  Train Loss: 4.2820 | Train PPL: 72.39
  Val Loss: 5.9040 | Val PPL: 366.50 | Val Acc: 18.72%
  ⚠️ No improvement (1/1)

⛔ Early stopping triggered at epoch 3
   Val loss hasn't improved for 1 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 5.7838
Best validation PPL: 325.00
Best validation accuracy: 18.76%
Early stopped at epoch: 3


  Phase 2 completed: 11.5min
  Best Val PPL: 325.00
  Best Val Acc: 18.76%

======================================================================
SCALING LAW ANALYSIS
======================================================================
/content/new-llm/scripts/unified_scaling_experiment.py:310: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.
  slope, intercept, r_value, p_value, std_err = stats.linregress(log_tokens, log_ppl)

PPL = nan × tokens^(nan)
α = nan
R² = nan
p-value = nan

======================================================================
RESULTS SUMMARY
======================================================================

 Samples |     Tokens |    Val PPL |    Val Acc |   Train ER |     Val ER
--------------------------------------------------------------------------------
     500 |    529,173 |     325.00 |     18.76% |      75.9% |      75.5%

Total time: 24.1 minutes

Results saved to: ./results/unified_scaling/results.json

======================================================================
SCALING LAW INTERPRETATION
======================================================================

α = nan
→ 2倍のデータで nan% PPL削減

======================================================================
COMPARISON WITH PREVIOUS EXPERIMENTS
======================================================================

| Experiment | α | Interpretation |
|------------|------|----------------|
| 11/27 (max_length=128) | -0.7463 | 2倍データで40%PPL削減 |
| 11/28 v2 (truncation=False) | -0.2926 | 2倍データで22%PPL削減 |
| This experiment | nan | 2倍データでnan%PPL削減 |

⚠️ 低いスケーリング効率（α > -0.2）
