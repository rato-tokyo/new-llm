GPU: NVIDIA L4 (23.8GB)

======================================================================
EXPERIMENT RUNNER
======================================================================
Start time: 2025-11-28 05:39:13

Experiments:
  A: Deep & Narrow (6層, 768dim) (context_dim=768)
  B: Shallow & Wide (3層, 1536dim) (context_dim=1536)

Sample sizes: [500, 1000]
num_input_tokens: 2
Embedding freeze: True
Total experiments: 4

======================================================================
Loading data...
======================================================================

  Loading 1050 samples from UltraChat...
Token indices sequence length is longer than the specified maximum sequence length for this model (1204 > 1024). Running this sequence through the model will result in indexing errors
  Total tokens: 192,670
  Samples loaded: 1050

======================================================================
Running experiments...
======================================================================

[1/4] Running expA_s500...

  Experiment A: Deep & Narrow (6層, 768dim)
    num_layers=6, context_dim=768
    num_input_tokens=2
    num_samples=500
    ContextBlock params: 10.63M
    TokenBlock params: 10.63M
    Phase 1 学習対象: 10.63M
    Phase 2 学習対象: 10.63M
2025-11-28 05:39:21.854906: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-28 05:39:21.872260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764308361.894220    6922 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764308361.900903    6922 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764308361.917857    6922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764308361.917893    6922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764308361.917897    6922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764308361.917900    6922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-28 05:39:21.923028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 2
✓ Weight Tying enabled: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Train tokens: 86,925
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 86,925
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 10,630,656
Iteration 1/10: シーケンシャル [73.83s]
Iteration 2/10: 収束=100.0% | Loss=-0.194364 | CVFP=0.000274 | Div=-0.389002 [4.85s]
Iteration 3/10: 収束=0.0% | Loss=0.030497 | CVFP=0.477121 | Div=-0.416128 [4.64s]
Iteration 4/10: 収束=0.0% | Loss=-0.056023 | CVFP=0.307370 | Div=-0.419416 [4.51s]
Iteration 5/10: 収束=3.8% | Loss=-0.173513 | CVFP=0.073481 | Div=-0.420506 [4.49s]
Iteration 6/10: 収束=81.9% | Loss=-0.199885 | CVFP=0.023187 | Div=-0.422956 [4.54s]
Iteration 7/10: 収束=100.0% | Loss=-0.209181 | CVFP=0.006874 | Div=-0.425236 [4.56s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 86925/86925 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 2

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000010
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000010
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000001

Verdict: ✅ CONVERGED: Loss is stable - model has converged

    Phase 1 done: 1.7min
      Train ER: 637.8/768 (83.0%)
      Val ER: 631.9/768 (82.3%)
    Phase 2 starting (freeze_embedding=True)...
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 7,683,072/56,911,104 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 86,925
Validation tokens: 9,050
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [384.0s]:
  Train Loss: 7.1299 | Train PPL: 1248.81
  Val Loss: 6.5188 | Val PPL: 677.75 | Val Acc: 14.28%
  ✓ New best validation loss: 6.5188

Epoch 2/10 [384.3s]:
  Train Loss: 5.9140 | Train PPL: 370.17
  Val Loss: 6.3182 | Val PPL: 554.58 | Val Acc: 15.99%
  ✓ New best validation loss: 6.3182

Epoch 3/10 [384.8s]:
  Train Loss: 4.9970 | Train PPL: 147.96
  Val Loss: 6.6587 | Val PPL: 779.54 | Val Acc: 15.92%
  ⚠️ No improvement (1/2)

Epoch 4/10 [385.1s]:
  Train Loss: 4.0195 | Train PPL: 55.67
  Val Loss: 7.1288 | Val PPL: 1247.38 | Val Acc: 15.10%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 4
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 6.3182
Best validation PPL: 554.58
Best validation accuracy: 15.99%
Early stopped at epoch: 4

    Phase 2 done: 25.6min
    RESULT: Val PPL=554.58, Val Acc=15.99%

[2/4] Running expA_s1000...

  Experiment A: Deep & Narrow (6層, 768dim)
    num_layers=6, context_dim=768
    num_input_tokens=2
    num_samples=1000
    ContextBlock params: 10.63M
    TokenBlock params: 10.63M
    Phase 1 学習対象: 10.63M
    Phase 2 学習対象: 10.63M
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(6 layers) + TokenBlock(6 layers)
  num_input_tokens: 2
✓ Weight Tying enabled: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Train tokens: 183,620
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 183,620
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 10,630,656
Iteration 1/10: シーケンシャル [153.51s]
Iteration 2/10: 収束=100.0% | Loss=-0.192494 | CVFP=0.000273 | Div=-0.385262 [9.36s]
Iteration 3/10: 収束=0.0% | Loss=0.031934 | CVFP=0.475908 | Div=-0.412041 [9.08s]
Iteration 4/10: 収束=0.0% | Loss=-0.053938 | CVFP=0.307243 | Div=-0.415119 [9.09s]
Iteration 5/10: 収束=3.9% | Loss=-0.171707 | CVFP=0.072676 | Div=-0.416090 [9.10s]
Iteration 6/10: 収束=82.5% | Loss=-0.197656 | CVFP=0.023056 | Div=-0.418369 [9.22s]
Iteration 7/10: 収束=100.0% | Loss=-0.206744 | CVFP=0.007077 | Div=-0.420566 [9.19s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 183620/183620 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 2

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000010
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000010
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000001

Verdict: ✅ CONVERGED: Loss is stable - model has converged

    Phase 1 done: 3.5min
      Train ER: 637.5/768 (83.0%)
      Val ER: 631.6/768 (82.2%)
    Phase 2 starting (freeze_embedding=True)...
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 7,683,072/56,911,104 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 183,620
Validation tokens: 9,050
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [797.1s]:
  Train Loss: 6.7211 | Train PPL: 829.69
  Val Loss: 6.1870 | Val PPL: 486.37 | Val Acc: 16.21%
  ✓ New best validation loss: 6.1870

Epoch 2/10 [800.1s]:
  Train Loss: 5.7622 | Train PPL: 318.05
  Val Loss: 6.1030 | Val PPL: 447.22 | Val Acc: 17.54%
  ✓ New best validation loss: 6.1030

Epoch 3/10 [798.9s]:
  Train Loss: 5.0887 | Train PPL: 162.18
  Val Loss: 6.2518 | Val PPL: 518.94 | Val Acc: 17.95%
  ⚠️ No improvement (1/2)

Epoch 4/10 [799.2s]:
  Train Loss: 4.3692 | Train PPL: 78.98
  Val Loss: 6.5551 | Val PPL: 702.79 | Val Acc: 17.60%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 4
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 6.1030
Best validation PPL: 447.22
Best validation accuracy: 17.54%
Early stopped at epoch: 4

    Phase 2 done: 53.3min
    RESULT: Val PPL=447.22, Val Acc=17.54%

[3/4] Running expB_s500...

  Experiment B: Shallow & Wide (3層, 1536dim)
    num_layers=3, context_dim=1536
    num_input_tokens=2
    num_samples=500
    ContextBlock params: 14.17M
    TokenBlock params: 7.08M
    Phase 1 学習対象: 14.17M
    Phase 2 学習対象: 7.08M
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
✓ Weight Tying enabled: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Train tokens: 86,925
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 86,925
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 14,169,600
Iteration 1/10: シーケンシャル [38.18s]
Iteration 2/10: 収束=100.0% | Loss=-0.277771 | CVFP=0.004216 | Div=-0.559758 [4.97s]
Iteration 3/10: 収束=0.0% | Loss=-0.026485 | CVFP=0.535248 | Div=-0.588218 [5.06s]
Iteration 4/10: 収束=0.0% | Loss=-0.086778 | CVFP=0.421103 | Div=-0.594659 [4.99s]
Iteration 5/10: 収束=0.0% | Loss=-0.189687 | CVFP=0.220252 | Div=-0.599625 [4.89s]
Iteration 6/10: 収束=0.0% | Loss=-0.245366 | CVFP=0.113127 | Div=-0.603858 [4.95s]
Iteration 7/10: 収束=0.6% | Loss=-0.273495 | CVFP=0.061228 | Div=-0.608218 [4.87s]
Iteration 8/10: 収束=38.8% | Loss=-0.289518 | CVFP=0.032475 | Div=-0.611511 [4.90s]
Iteration 9/10: 収束=97.9% | Loss=-0.297610 | CVFP=0.019614 | Div=-0.614834 [4.88s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 85109/86925 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 2

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000081
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000081
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000005

Verdict: ✅ CONVERGED: Loss is stable - model has converged

    Phase 1 done: 1.3min
      Train ER: 1240.9/1536 (80.8%)
      Val ER: 1222.4/1536 (79.6%)
    Phase 2 starting (freeze_embedding=True)...
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 5,906,688/58,673,664 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 86,925
Validation tokens: 9,050
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [212.7s]:
  Train Loss: 7.1793 | Train PPL: 1311.97
  Val Loss: 6.5138 | Val PPL: 674.38 | Val Acc: 14.59%
  ✓ New best validation loss: 6.5138

Epoch 2/10 [212.3s]:
  Train Loss: 5.7462 | Train PPL: 312.99
  Val Loss: 6.3739 | Val PPL: 586.36 | Val Acc: 16.39%
  ✓ New best validation loss: 6.3739

Epoch 3/10 [212.5s]:
  Train Loss: 4.5562 | Train PPL: 95.22
  Val Loss: 6.7890 | Val PPL: 888.00 | Val Acc: 15.75%
  ⚠️ No improvement (1/2)

Epoch 4/10 [211.7s]:
  Train Loss: 3.3376 | Train PPL: 28.15
  Val Loss: 7.4069 | Val PPL: 1647.25 | Val Acc: 15.18%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 4
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 6.3739
Best validation PPL: 586.36
Best validation accuracy: 16.39%
Early stopped at epoch: 4

    Phase 2 done: 14.2min
    RESULT: Val PPL=586.36, Val Acc=16.39%

[4/4] Running expB_s1000...

  Experiment B: Shallow & Wide (3層, 1536dim)
    num_layers=3, context_dim=1536
    num_input_tokens=2
    num_samples=1000
    ContextBlock params: 14.17M
    TokenBlock params: 7.08M
    Phase 1 学習対象: 14.17M
    Phase 2 学習対象: 7.08M
Loading GPT-2 pretrained embeddings...
✓ Loaded GPT-2 embeddings: torch.Size([50257, 768])
Using E案 architecture: ContextBlock(3 layers) + TokenBlock(3 layers)
  num_input_tokens: 2
✓ Weight Tying enabled: token_output shares weights with token_embedding
  → Saved ~38.60M parameters
    Train tokens: 183,620
    Val tokens: 9,050
    Phase 1 starting...

======================================================================
PHASE 1: 固定点コンテキスト学習 - Train
======================================================================
  Mode: Memory (GPU-optimized)
  Tokens: 183,620
  Max iterations: 10
  Learning rate: 0.002
  Diversity weight: 0.5
  ContextBlock params: 14,169,600
Iteration 1/10: シーケンシャル [81.64s]
Iteration 2/10: 収束=100.0% | Loss=-0.275030 | CVFP=0.004210 | Div=-0.554270 [10.62s]
Iteration 3/10: 収束=0.0% | Loss=-0.024027 | CVFP=0.534443 | Div=-0.582497 [10.32s]
Iteration 4/10: 収束=0.0% | Loss=-0.083402 | CVFP=0.421796 | Div=-0.588599 [10.37s]
Iteration 5/10: 収束=0.0% | Loss=-0.186341 | CVFP=0.220572 | Div=-0.593254 [10.34s]
Iteration 6/10: 収束=0.0% | Loss=-0.241973 | CVFP=0.113403 | Div=-0.597349 [10.32s]
Iteration 7/10: 収束=0.6% | Loss=-0.270084 | CVFP=0.061421 | Div=-0.601589 [10.36s]
Iteration 8/10: 収束=38.4% | Loss=-0.286108 | CVFP=0.032551 | Div=-0.604767 [10.49s]
Iteration 9/10: 収束=97.9% | Loss=-0.294200 | CVFP=0.019630 | Div=-0.608031 [10.39s]
  → Early stopping (min_iterations=3 satisfied)

Phase 1 完了: 179777/183620 トークン収束


Evaluating Val data...

======================================================================
CVFP Convergence Check
======================================================================
  Tokens: 9,050
  Trials: 10
  num_input_tokens: 2

======================================================================
CVFP Loss Progression
======================================================================

Trial  1/10: CVFP Loss = N/A (baseline)
Trial  2/10: CVFP Loss = 0.000081
Trial  3/10: CVFP Loss = 0.000000
Trial  4/10: CVFP Loss = 0.000000
Trial  5/10: CVFP Loss = 0.000000
Trial  6/10: CVFP Loss = 0.000000
Trial  7/10: CVFP Loss = 0.000000
Trial  8/10: CVFP Loss = 0.000000
Trial  9/10: CVFP Loss = 0.000000
Trial 10/10: CVFP Loss = 0.000000

======================================================================
Convergence Analysis
======================================================================

Statistics:
  - Initial Loss: 0.000081
  - Final Loss: 0.000000
  - Reduction: +100.00%
  - Slope: -0.000005

Verdict: ✅ CONVERGED: Loss is stable - model has converged

    Phase 1 done: 2.8min
      Train ER: 1241.1/1536 (80.8%)
      Val ER: 1221.5/1536 (79.5%)
    Phase 2 starting (freeze_embedding=True)...
✓ ContextBlock frozen
✓ Embedding frozen (Weight Tying: Output Head also frozen)
  → Only TokenBlock will be trained
✓ Training TokenBlock only: 5,906,688/58,673,664 parameters

======================================================================
PHASE 2: Next-Token Prediction Training (E案 - レイヤー対応版)
======================================================================

Training tokens: 183,620
Validation tokens: 9,050
Epochs: 10
Batch size: 512
Learning rate: 0.002
Gradient clip: 1.0
Early stopping patience: 2

Architecture: E案 (ContextBlock + TokenBlock Layer-wise)
  - ContextBlock: FROZEN (Phase 1で学習済み)
  - TokenBlock: TRAINING
  - token_output: TRAINING
  - E案: TokenBlock Layer i は ContextBlock Layer i の出力を参照

Epoch 1/10 [435.6s]:
  Train Loss: 6.7379 | Train PPL: 843.82
  Val Loss: 6.1796 | Val PPL: 482.80 | Val Acc: 16.49%
  ✓ New best validation loss: 6.1796

Epoch 2/10 [435.3s]:
  Train Loss: 5.6074 | Train PPL: 272.44
  Val Loss: 6.0863 | Val PPL: 439.79 | Val Acc: 18.07%
  ✓ New best validation loss: 6.0863

Epoch 3/10 [436.2s]:
  Train Loss: 4.7423 | Train PPL: 114.70
  Val Loss: 6.3749 | Val PPL: 586.95 | Val Acc: 17.73%
  ⚠️ No improvement (1/2)

Epoch 4/10 [435.4s]:
  Train Loss: 3.8093 | Train PPL: 45.12
  Val Loss: 6.9155 | Val PPL: 1007.75 | Val Acc: 16.74%
  ⚠️ No improvement (2/2)

⛔ Early stopping triggered at epoch 4
   Val loss hasn't improved for 2 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 6.0863
Best validation PPL: 439.79
Best validation accuracy: 18.07%
Early stopped at epoch: 4

    Phase 2 done: 29.0min
    RESULT: Val PPL=439.79, Val Acc=18.07%

======================================================================
ALL EXPERIMENTS COMPLETE
======================================================================
Total time: 2.3h

Results saved to: ./results/layer_context_comparison/all_results.json
Summary saved to: ./results/layer_context_comparison/summary.md

====================================================================================================
COMPARISON RESULTS
====================================================================================================

 Exp | Layers |  CtxDim | Samples |    Val PPL |   Val Acc% |  Train ER% |    Val ER%
-------------------------------------------------------------------------------------
   A |      6 |     768 |     500 |     554.58 |     15.99% |      83.0% |      82.3%
   B |      3 |    1536 |     500 |     586.36 |     16.39% |      80.8% |      79.6%
   A |      6 |     768 |    1000 |     447.22 |     17.54% |      83.0% |      82.2%
   B |      3 |    1536 |    1000 |     439.79 |     18.07% |      80.8% |      79.5%
