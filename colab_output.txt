remote: Enumerating objects: 20, done.
remote: Counting objects: 100% (20/20), done.
remote: Compressing objects: 100% (4/4), done.
remote: Total 13 (delta 9), reused 13 (delta 9), pack-reused 0 (from 0)
Unpacking objects: 100% (13/13), 10.34 KiB | 2.07 MiB/s, done.
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   df8ec0b..ee6a00c  main       -> origin/main
Updating df8ec0b..ee6a00c
Fast-forward
 colab.py               | 188 +++++++++++----
 colab_output.txt       | 607 +++++++++++++++++++++++++++++++++++++++++++++++++
 src/data/loader.py     |  31 ++-
 src/trainers/phase2.py |  44 +++-
 4 files changed, 809 insertions(+), 61 deletions(-)
 create mode 100644 colab_output.txt

======================================================================
New-LLM Training for Google Colab
======================================================================

âœ… Random seed fixed: 42 (å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼)
ğŸ–¥ï¸  Device: cuda
   GPU: NVIDIA L4
   Memory: 23.8 GB

ğŸ“‹ Configuration:
   Architecture: residual_standard
   Layers: 6
   Context dim: 768
   Diversity weight: 0.9
   Phase 2 epochs: 10
   Early stopping patience: 3
   Context stability weight: 1.0

ğŸ“¦ Creating model...
2025-11-26 02:33:31.256426: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-26 02:33:31.275060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764124411.296517    5810 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764124411.303063    5810 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764124411.319981    5810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764124411.320008    5810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764124411.320011    5810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764124411.320015    5810 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-26 02:33:31.324879: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
âœ“ Model created: 130,027,345 parameters

ğŸ“Š Loading data from UltraChat...
   Loading from cache: ./cache/ultrachat_50samples_128len.pt
   Total tokens: 6,400
   Train: 5,120 tokens (80%)
   Val:   1,280 tokens (20% from end)
   âœ“ Validation auto-generated from training data

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP)
======================================================================


======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP) - Train
======================================================================
Iteration 1/10: é †ä¼æ’­ã®ã¿ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ï¼‰ [11.12s]
Iteration 2/10: åæŸ=0.0% | Total=-0.135137 | CVFP=1.990857 | Div=-0.371359 | Time=0.78s
Iteration 3/10: åæŸ=0.0% | Total=-0.153183 | CVFP=1.735423 | Div=-0.363029 | Time=0.06s
Iteration 4/10: åæŸ=0.0% | Total=-0.171715 | CVFP=1.509275 | Div=-0.358492 | Time=0.06s
Iteration 5/10: åæŸ=0.0% | Total=-0.183942 | CVFP=1.460635 | Div=-0.366673 | Time=0.06s
Iteration 6/10: åæŸ=0.0% | Total=-0.198840 | CVFP=1.354226 | Div=-0.371403 | Time=0.06s
Iteration 7/10: åæŸ=2.3% | Total=-0.207079 | CVFP=1.260067 | Div=-0.370095 | Time=0.06s
Iteration 8/10: åæŸ=23.0% | Total=-0.213289 | CVFP=1.226774 | Div=-0.373296 | Time=0.06s
Iteration 9/10: åæŸ=35.1% | Total=-0.219891 | CVFP=1.199713 | Div=-0.377625 | Time=0.06s
Iteration 10/10: åæŸ=44.6% | Total=-0.224912 | CVFP=1.157162 | Div=-0.378476 | Time=0.06s

Phase 1 å®Œäº†: 2284/5120 ãƒˆãƒ¼ã‚¯ãƒ³ãŒåæŸ


======================================================================
Evaluating on validation data...
======================================================================


â±ï¸  Phase 1 completed in 14.8s
ğŸ’¾ Checkpoint saved: ./checkpoints/model_latest.pt

======================================================================
FIXED-POINT ANALYSIS
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Train
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 37.067623 (Range: [0.000000, 50.015568])
  Avg Cosine Sim:  0.076941 (Range: [-0.575630, 1.000000])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 28.192616 (Range: [28.061901, 28.291323])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 28.1926, Median: 28.2093, Std: 0.0641
  Pairwise Dist - Mean: 37.0676, Median: 38.9268
  Sparsity: 0.53% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 485.47 / 768 (63.2%)
  Top 5 Singular Values: [1249.3555908203125, 666.7974853515625, 561.0498046875, 476.7111511230469, 369.3067932128906]
  âœ… Good diversity
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Val
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 33.851532 (Range: [0.000000, 41.996605])
  Avg Cosine Sim:  0.273059 (Range: [-0.099607, 1.000000])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 28.332327 (Range: [28.248571, 28.392044])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 28.3323, Median: 28.3242, Std: 0.0367
  Pairwise Dist - Mean: 33.8515, Median: 35.0204
  Sparsity: 0.60% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 455.38 / 768 (59.3%)
  Top 5 Singular Values: [553.2039184570312, 298.203125, 196.0016326904297, 147.5199432373047, 131.6497344970703]
  âœ… Good diversity
======================================================================


======================================================================
æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯ (Identity Mapping Check)
======================================================================
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦çµ±è¨ˆ (100ã‚µãƒ³ãƒ—ãƒ«):
  å¹³å‡: 0.0042
  æœ€å¤§: 0.1050
  æœ€å°: -0.0790
  é–¾å€¤(0.95)è¶…é: 0/100 (0.0%)

âœ… æ­£å¸¸: æ’ç­‰å†™åƒã§ã¯ã‚ã‚Šã¾ã›ã‚“
    ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚
======================================================================


======================================================================
PHASE 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å­¦ç¿’
======================================================================

âœ“ Full model fine-tuning: 91,429,969/130,027,345 parameters trainable
âœ“ Context stability weight: 1.0

======================================================================
PHASE 2: Next-Token Prediction Training
         (Context Propagation + Token Embed Prediction)
======================================================================

Training tokens: 5,120
Validation tokens: 1,280
Epochs: 10
Learning rate: 0.002
Context frozen: False
Context stability weight: 1.0
Early stopping patience: 3
âœ“ Context propagates forward (like Phase 1)
âœ“ Prediction from concatenated context + token_embed
âœ“ Context vectors fixed to Phase 2 start values
âœ“ Context gradient detached (no backprop through history)

Initializing target contexts...
âœ“ Training target contexts initialized: torch.Size([5120, 768])
âœ“ Validation target contexts initialized: torch.Size([1280, 768])

  [DEBUG] train_epoch called with 5120 tokens
  [DEBUG] Processing 5119 input tokens
  [DEBUG] Context initialized: torch.Size([1, 768])
  [DEBUG] Processing token 0/5119
  [DEBUG] Processing token 1000/5119
  [DEBUG] Processing token 2000/5119
  [DEBUG] Processing token 3000/5119
  [DEBUG] Processing token 4000/5119
  [DEBUG] Processing token 5000/5119
  [DEBUG] Stacking 5119 logits
  [DEBUG] Logits shape: torch.Size([5119, 50257])
  [DEBUG] Contexts shape: torch.Size([5119, 768])
  [DEBUG] Computing losses...
  [DEBUG] Prediction Loss: 10.8249
  [DEBUG] Context Stability Loss: 1.651548
  [DEBUG] Total Loss: 12.4765
  [DEBUG] Running backward pass...
  [DEBUG] Backward complete
Epoch 1/10:
  Train Loss: 10.8249 | Train PPL: 50257.02 | Context Loss: 1.651548
  Val Loss: 9.8193 | Val PPL: 18384.64 | Val Acc: 0.00%
  âœ“ New best validation loss: 9.8193

  [DEBUG] train_epoch called with 5120 tokens
  [DEBUG] Processing 5119 input tokens
  [DEBUG] Context initialized: torch.Size([1, 768])
  [DEBUG] Processing token 0/5119
  [DEBUG] Processing token 1000/5119
  [DEBUG] Processing token 2000/5119
  [DEBUG] Processing token 3000/5119
  [DEBUG] Processing token 4000/5119
  [DEBUG] Processing token 5000/5119
  [DEBUG] Stacking 5119 logits
  [DEBUG] Logits shape: torch.Size([5119, 50257])
  [DEBUG] Contexts shape: torch.Size([5119, 768])
  [DEBUG] Computing losses...
  [DEBUG] Prediction Loss: 9.2050
  [DEBUG] Context Stability Loss: 1.162569
  [DEBUG] Total Loss: 10.3676
  [DEBUG] Running backward pass...
  [DEBUG] Backward complete
Epoch 2/10:
  Train Loss: 9.2050 | Train PPL: 9946.61 | Context Loss: 1.162569
  Val Loss: 9.0814 | Val PPL: 8790.43 | Val Acc: 2.97%
  âœ“ New best validation loss: 9.0814

  [DEBUG] train_epoch called with 5120 tokens
  [DEBUG] Processing 5119 input tokens
  [DEBUG] Context initialized: torch.Size([1, 768])
  [DEBUG] Processing token 0/5119
  [DEBUG] Processing token 1000/5119
  [DEBUG] Processing token 2000/5119
  [DEBUG] Processing token 3000/5119
  [DEBUG] Processing token 4000/5119
  [DEBUG] Processing token 5000/5119
  [DEBUG] Stacking 5119 logits
  [DEBUG] Logits shape: torch.Size([5119, 50257])
  [DEBUG] Contexts shape: torch.Size([5119, 768])
  [DEBUG] Computing losses...
  [DEBUG] Prediction Loss: 7.0982
  [DEBUG] Context Stability Loss: 1.103420
  [DEBUG] Total Loss: 8.2016
  [DEBUG] Running backward pass...
  [DEBUG] Backward complete
Epoch 3/10:
  Train Loss: 7.0982 | Train PPL: 1209.80 | Context Loss: 1.103420
  Val Loss: 9.1398 | Val PPL: 9318.52 | Val Acc: 4.85%
  âš ï¸ No improvement (1/3)

  [DEBUG] train_epoch called with 5120 tokens
  [DEBUG] Processing 5119 input tokens
  [DEBUG] Context initialized: torch.Size([1, 768])
  [DEBUG] Processing token 0/5119
  [DEBUG] Processing token 1000/5119
  [DEBUG] Processing token 2000/5119
  [DEBUG] Processing token 3000/5119
  [DEBUG] Processing token 4000/5119
  [DEBUG] Processing token 5000/5119
  [DEBUG] Stacking 5119 logits
  [DEBUG] Logits shape: torch.Size([5119, 50257])
  [DEBUG] Contexts shape: torch.Size([5119, 768])
  [DEBUG] Computing losses...
  [DEBUG] Prediction Loss: 6.4082
  [DEBUG] Context Stability Loss: 1.106945
  [DEBUG] Total Loss: 7.5151
  [DEBUG] Running backward pass...
  [DEBUG] Backward complete
Epoch 4/10:
  Train Loss: 6.4082 | Train PPL: 606.79 | Context Loss: 1.106945
  Val Loss: 9.7173 | Val PPL: 16602.54 | Val Acc: 4.07%
  âš ï¸ No improvement (2/3)

  [DEBUG] train_epoch called with 5120 tokens
  [DEBUG] Processing 5119 input tokens
  [DEBUG] Context initialized: torch.Size([1, 768])
  [DEBUG] Processing token 0/5119
  [DEBUG] Processing token 1000/5119
  [DEBUG] Processing token 2000/5119
  [DEBUG] Processing token 3000/5119
  [DEBUG] Processing token 4000/5119
  [DEBUG] Processing token 5000/5119
  [DEBUG] Stacking 5119 logits
  [DEBUG] Logits shape: torch.Size([5119, 50257])
  [DEBUG] Contexts shape: torch.Size([5119, 768])
  [DEBUG] Computing losses...
  [DEBUG] Prediction Loss: 6.4170
  [DEBUG] Context Stability Loss: 1.073286
  [DEBUG] Total Loss: 7.4903
  [DEBUG] Running backward pass...
  [DEBUG] Backward complete
Epoch 5/10:
  Train Loss: 6.4170 | Train PPL: 612.18 | Context Loss: 1.073286
  Val Loss: 10.0244 | Val PPL: 22570.37 | Val Acc: 3.36%
  âš ï¸ No improvement (3/3)

â›” Early stopping triggered at epoch 5
   Val loss hasn't improved for 3 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 9.0814
Best validation PPL: 8790.43
Best validation accuracy: 2.97%
Early stopped at epoch: 5


â±ï¸  Phase 2 completed in 274.1s
ğŸ’¾ Final checkpoint saved: ./checkpoints/model_latest.pt


======================================================================
                    NEW-LLM TRAINING RESULTS                         
======================================================================

[PHASE 1: Context Learning (CVFP)]
  Effective Rank (Train): 63.2% (485.47/768)
  Effective Rank (Val):   59.3% (455.38/768)
  Time: 14.8s
  Status: âœ… PASSED

[PHASE 2: Token Prediction]
  Best Val PPL:    8790.43 (Epoch 2)
  Best Val Acc:    2.97%
  Final Val PPL:   22570.37
  Final Val Acc:   3.36%
  Epochs Run:      5/10
  Time: 274.1s
  Status: âš ï¸  EARLY STOPPED at epoch 5

----------------------------------------------------------------------
  TOTAL TIME: 299.4s
======================================================================

ğŸ“‰ Epoch-by-Epoch Progress:
--------------------------------------------------
 Epoch |  Train PPL |    Val PPL |  Val Acc
--------------------------------------------------
     1 |   50257.02 |   18384.64 |    0.00%
     2 |    9946.61 |    8790.43 |    2.97% â­
     3 |    1209.80 |    9318.52 |    4.85%
     4 |     606.79 |   16602.54 |    4.07%
     5 |     612.18 |   22570.37 |    3.36%
--------------------------------------------------

âœ… Training complete!
