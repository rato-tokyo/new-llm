remote: Enumerating objects: 32, done.
remote: Counting objects: 100% (32/32), done.
remote: Compressing objects: 100% (6/6), done.
Unpacking objects: 100% (18/18), 10.48 KiB | 1.50 MiB/s, done.
remote: Total 18 (delta 14), reused 16 (delta 12), pack-reused 0 (from 0)
From https://github.com/rato-tokyo/new-llm
 * branch            main       -> FETCH_HEAD
   af9747f..74c929d  main       -> origin/main
Updating af9747f..74c929d
Fast-forward
 CLAUDE.md                                 | 129 +++++++-----
 colab.py                                  |   5 +-
 colab_output.txt                          | 183 ++++++++---------
 config.py                                 |   8 +-
 importants/NEW-LLM_ALGORITHM_SUMMARY.md   |  60 ++++--
 importants/phase2-context-fixed-design.md | 318 +++++++++++-------------------
 src/models/llm.py                         |  44 +----
 src/trainers/phase2.py                    | 179 ++++++++---------
 train.py                                  |   3 +-
 9 files changed, 416 insertions(+), 513 deletions(-)

======================================================================
New-LLM Training for Google Colab
======================================================================

âœ… Random seed fixed: 42 (å®Œå…¨ãªå†ç¾æ€§ä¿è¨¼)
ğŸ–¥ï¸  Device: cuda
   GPU: NVIDIA L4
   Memory: 23.8 GB

ğŸ“‹ Configuration:
   Architecture: residual_standard
   Layers: 6
   Context dim: 768
   Diversity weight: 0.9
   Phase 2 epochs: 10
   Early stopping patience: 3
   Context-Fixed Learning: context_out = C*[i] (complete fixing)

ğŸ“¦ Creating model...
2025-11-26 03:39:06.771151: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-11-26 03:39:06.789173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1764128346.810873   22056 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1764128346.817509   22056 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1764128346.834101   22056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764128346.834128   22056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764128346.834131   22056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1764128346.834133   22056 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-11-26 03:39:06.839041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Loading GPT-2 pretrained embeddings...
âœ“ Loaded GPT-2 embeddings: torch.Size([50257, 768])
âœ“ Model created: 130,027,345 parameters

ğŸ“Š Loading data from UltraChat...
   Loading from cache: ./cache/ultrachat_50samples_128len.pt
   Total tokens: 6,400
   Train: 5,120 tokens (80%)
   Val:   1,280 tokens (20% from end)
   âœ“ Validation auto-generated from training data

======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP)
======================================================================


======================================================================
PHASE 1: å›ºå®šç‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ (CVFP) - Train
======================================================================
Iteration 1/10: é †ä¼æ’­ã®ã¿ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ï¼‰ [10.78s]
Iteration 2/10: åæŸ=100.0% | Total=-0.339547 | CVFP=0.000247 | Div=-0.377301 | Time=0.97s
  â†’ Early stopping: åæŸç‡ = 100.0%

Phase 1 å®Œäº†: 5119/5120 ãƒˆãƒ¼ã‚¯ãƒ³ãŒåæŸ


======================================================================
Evaluating on validation data...
======================================================================


â±ï¸  Phase 1 completed in 13.8s
ğŸ’¾ Checkpoint saved: ./checkpoints/model_latest.pt

======================================================================
FIXED-POINT ANALYSIS
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Train
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 38.166851 (Range: [28.213446, 42.890305])
  Avg Cosine Sim:  0.050771 (Range: [-0.197644, 0.481771])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.712793 (Range: [27.712784, 27.712797])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.7128, Median: 27.7128, Std: 0.0000
  Pairwise Dist - Mean: 38.1669, Median: 38.2364
  Sparsity: 0.66% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 678.15 / 768 (88.3%)
  Top 5 Singular Values: [473.3590087890625, 251.67263793945312, 203.8396759033203, 178.4014129638672, 166.4369354248047]
  âœ… Good diversity
======================================================================


======================================================================
FIXED-POINT ANALYSIS - Val
======================================================================

1. Global Attractor Detection:
  Avg L2 Distance: 37.377724 (Range: [25.458109, 42.359058])
  Avg Cosine Sim:  0.092325 (Range: [-0.163564, 0.579779])
  âœ… Token-specific fixed points

2. Zero Solution Detection:
  Avg Norm: 27.769251 (Range: [27.763552, 27.775146])
  âœ… Non-zero contexts

3. Distribution Statistics:
  Norm - Mean: 27.7693, Median: 27.7693, Std: 0.0017
  Pairwise Dist - Mean: 37.3777, Median: 37.6461
  Sparsity: 0.66% of values < 0.01

4. Information Content:
  Actual Rank: 768 / 768 (100.0%)
  Effective Rank: 612.52 / 768 (79.8%)
  Top 5 Singular Values: [328.3175354003906, 165.98971557617188, 121.27111053466797, 104.47484588623047, 96.45172119140625]
  âœ… Good diversity
======================================================================


======================================================================
æ’ç­‰å†™åƒãƒã‚§ãƒƒã‚¯ (Identity Mapping Check)
======================================================================
ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦çµ±è¨ˆ (100ã‚µãƒ³ãƒ—ãƒ«):
  å¹³å‡: 0.0012
  æœ€å¤§: 0.1141
  æœ€å°: -0.0900
  é–¾å€¤(0.95)è¶…é: 0/100 (0.0%)

âœ… æ­£å¸¸: æ’ç­‰å†™åƒã§ã¯ã‚ã‚Šã¾ã›ã‚“
    ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚
======================================================================


======================================================================
PHASE 2: ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬å­¦ç¿’
======================================================================

âœ“ Full model fine-tuning: 91,429,969/130,027,345 parameters trainable
âœ“ Context-Fixed Learning: context_out replaced with C*[i] (complete fixing)

======================================================================
PHASE 2: Next-Token Prediction Training
         (Context-Fixed Learning)
======================================================================

Training tokens: 5,120
Validation tokens: 1,280
Epochs: 10
Learning rate: 0.002
CVFP layers frozen: False
Early stopping patience: 3
âœ“ Stage 1: Initialize fixed contexts C* from training data
âœ“ Stage 2: Train with context_out = C*[i] (complete fixing)
âœ“ Prediction from concatenated C*[i] + token_out
âœ“ Gradients flow through token_out only

Stage 1: Initializing fixed contexts C*...
âœ“ Training target contexts initialized: torch.Size([5120, 768])
âœ“ Validation target contexts initialized: torch.Size([1280, 768])

Stage 2: Training with fixed contexts...
Epoch 1/10:
  Train Loss: 10.8249 | Train PPL: 50257.02
  Val Loss: 10.0619 | Val PPL: 23433.53 | Val Acc: 6.10%
  âœ“ New best validation loss: 10.0619

Epoch 2/10:
  Train Loss: 8.7021 | Train PPL: 6015.60
  Val Loss: 8.7521 | Val PPL: 6324.23 | Val Acc: 6.57%
  âœ“ New best validation loss: 8.7521

Epoch 3/10:
  Train Loss: 6.0365 | Train PPL: 418.43
  Val Loss: 8.8179 | Val PPL: 6753.88 | Val Acc: 7.35%
  âš ï¸ No improvement (1/3)

Epoch 4/10:
  Train Loss: 4.8225 | Train PPL: 124.27
  Val Loss: 9.0996 | Val PPL: 8951.81 | Val Acc: 7.66%
  âš ï¸ No improvement (2/3)

Epoch 5/10:
  Train Loss: 4.0901 | Train PPL: 59.75
  Val Loss: 9.2718 | Val PPL: 10633.47 | Val Acc: 9.54%
  âš ï¸ No improvement (3/3)

â›” Early stopping triggered at epoch 5
   Val loss hasn't improved for 3 epochs
======================================================================
Phase 2 Training Complete
======================================================================

Best epoch: 2
Best validation loss: 8.7521
Best validation PPL: 6324.23
Best validation accuracy: 6.57%
Early stopped at epoch: 5


â±ï¸  Phase 2 completed in 275.6s
ğŸ’¾ Final checkpoint saved: ./checkpoints/model_latest.pt


======================================================================
                    NEW-LLM TRAINING RESULTS                         
======================================================================

[PHASE 1: Context Learning (CVFP)]
  Effective Rank (Train): 88.3% (678.15/768)
  Effective Rank (Val):   79.8% (612.52/768)
  Time: 13.8s
  Status: âœ… PASSED

[PHASE 2: Token Prediction]
  Best Val PPL:    6324.23 (Epoch 2)
  Best Val Acc:    6.57%
  Final Val PPL:   10633.47
  Final Val Acc:   9.54%
  Epochs Run:      5/10
  Time: 275.6s
  Status: âš ï¸  EARLY STOPPED at epoch 5

----------------------------------------------------------------------
  TOTAL TIME: 299.9s
======================================================================

ğŸ“‰ Epoch-by-Epoch Progress:
--------------------------------------------------
 Epoch |  Train PPL |    Val PPL |  Val Acc
--------------------------------------------------
     1 |   50257.02 |   23433.53 |    6.10%
     2 |    6015.60 |    6324.23 |    6.57% â­
     3 |     418.43 |    6753.88 |    7.35%
     4 |     124.27 |    8951.81 |    7.66%
     5 |      59.75 |   10633.47 |    9.54%
--------------------------------------------------

âœ… Training complete!
